<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小黑屋</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://kodgv.xyz/"/>
  <updated>2019-05-25T12:44:37.670Z</updated>
  <id>http://kodgv.xyz/</id>
  
  <author>
    <name>KODGV</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>linux装软件</title>
    <link href="http://kodgv.xyz/2019/05/24/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/linux%E8%A3%85%E8%BD%AF%E4%BB%B6/"/>
    <id>http://kodgv.xyz/2019/05/24/项目管理/linux装软件/</id>
    <published>2019-05-24T13:29:03.000Z</published>
    <updated>2019-05-25T12:44:37.670Z</updated>
    
    <content type="html"><![CDATA[<p>Linux下的装软件历程</p><a id="more"></a><p>搜狗拼音：<a href="https://blog.csdn.net/lupengCSDN/article/details/80279177" target="_blank" rel="noopener">https://blog.csdn.net/lupengCSDN/article/details/80279177</a></p><p>java安装：<a href="http://www.manongjc.com/article/34273.html" target="_blank" rel="noopener">http://www.manongjc.com/article/34273.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux下的装软件历程&lt;/p&gt;
    
    </summary>
    
      <category term="项目管理" scheme="http://kodgv.xyz/categories/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="linux" scheme="http://kodgv.xyz/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux习题</title>
    <link href="http://kodgv.xyz/2019/05/21/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/Linux%E4%B9%A0%E9%A2%98/"/>
    <id>http://kodgv.xyz/2019/05/21/项目管理/Linux习题/</id>
    <published>2019-05-21T12:43:44.000Z</published>
    <updated>2019-05-25T12:44:57.559Z</updated>
    
    <content type="html"><![CDATA[<p>Linux收集习题</p><a id="more"></a><h2 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h2><p>1.请问如果我以文字模式登陆Linux主机时，我有几个终端机接口可以使用？如何切换各个不同的终端机接口？</p><p>​    共有六个， tty1 ~ tty6 ，切换的方式为 Crtl + Alt + [F1]~[F6]（六个终端无区别，只是为了避免某个终端程序卡死）</p><p>2.在Linux系统中，/VBird与/vbird是否为相同的文件？</p><p>​    两者为不同的文件，因为 Linux 系统中，大小写字母代表意义不一样！</p><p>3.Linux 提供相当多的线上查询，称为 man page，请问，我如何知道系统上有多少关于 passwd 的说明？</p><p>​    man -f passwd（查看man file的数量）</p><p>4.我使用dmtsai这个帐号登陆系统了，请问我能不能使用reboot来重新开机？ </p><p>​    理论上reboot仅能让root执行。不过，如果dmtsai是在主机前面以图形接口登陆时，则dmtsai还是可以通过图形接口功能来关机。</p><h2 id="第五章"><a href="#第五章" class="headerlink" title="　第五章"></a>　第五章</h2><p>1.请问testgroup这个群组的成员与其他人（others）是否可以进入本目录？</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drwxr-xr--  <span class="number"> 1 </span>test1    testgroup   <span class="number"> 5238 </span>Jun<span class="number"> 19 </span>10:25 groups/</span><br></pre></td></tr></table></figure><ul><li><p>文件拥有者test1[rwx]可以在本目录中进行任何工作；</p></li><li><p>而testgroup这个群组[r-x]的帐号，例如test2, test3亦可以进入本目录进行工作，但是不能在本目录下进行写入的动作；</p></li><li><p>至于other的权限中[r—]虽然有r ，但是由于没有x的权限，因此others的使用者，并不能进入此目录！</p></li></ul><p>打开该目录就是执行ｘ</p><p>2.当一个一般文件权限为 -rwxrwxrwx 则表示这个文件的意义为？</p><p>任何人皆可读取、修改或编辑、可以执行，但不一定能删除。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux收集习题&lt;/p&gt;
    
    </summary>
    
      <category term="项目管理" scheme="http://kodgv.xyz/categories/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="linux" scheme="http://kodgv.xyz/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>知识图谱综述</title>
    <link href="http://kodgv.xyz/2019/05/17/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%BB%BC%E8%BF%B0/"/>
    <id>http://kodgv.xyz/2019/05/17/知识图谱/知识图谱综述/</id>
    <published>2019-05-17T07:28:31.000Z</published>
    <updated>2019-05-25T12:46:03.990Z</updated>
    
    <content type="html"><![CDATA[<p>知识图谱学习整理的一些概念和小知识点</p><a id="more"></a><p>[TOC]</p><h2 id="知识图谱首要"><a href="#知识图谱首要" class="headerlink" title="知识图谱首要"></a>知识图谱首要</h2><ol><li>What are the <em>nodes</em>? In a knowledge graph, they will be related to semantic concepts such as persons, entities, events <em>etc.</em></li><li>What are the <em>edges</em>? They will be defined by <em>relationships</em> between nodes based on semantics.</li><li>Once you answered these two key questions, you can go to the next phase which is the <em>data acquisition strategy</em>.</li></ol><p>In general, there are two main ways of going about knowledge processing:</p><ol><li><strong>Simple heuristic approaches</strong> such as text processing based on regular expressions, simple parsing and NLP techniques. For images, this approach includes basic processing of metadata. The lack of depth in this approach can be compensated and mitigated by sheer numbers <em>i.e.</em> throwing lots and lots of data at it to cherry pick low hanging fruit.</li><li><strong>Deep learning</strong> techniques where you employ novel methods to experiment with your own kinds of knowledge. This will be great fun and you will be on the leading edge but you should also keep in mind that the edge is (b)leading too. Just do not let that deter you and keep moving through growing pains.</li></ol><p><a href="http://kns.cnki.net/KCMS/detail/11.5602.TP.20190517.1335.002.html?uid=WEEvREcwSlJHSldRa1FhdXNXaEd2Um5XT2VCQXJTY3JoN1JsS2FvQXpYYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&amp;v=MjEwNDNxRkNubFc3dkFKVjQ9TGpYZmZiRzRIOWpNcW81Q1pPc1BZdzlNem1SbjZqNTdUM2ZscVdNMENMTDdSN3FlYnVa" target="_blank" rel="noopener">民航突发事件领域本体关系提取方法的研究</a></p><ul><li><p>非结构化数据可以整理为半结构化数据：</p><p>将突发事件的文本信息整理为标题、时间、航班号、事<br>件描述的半结构化形式，将事件发生时间和航班号作为事件<br>的唯一标识；</p></li><li><p>关系标注：attribute-of/reason-of/result-of</p></li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://arxiv.org/pdf/1503.00759v1.pdf" target="_blank" rel="noopener">知识图谱开山基础</a></p><p><a href="https://kgtutorial.github.io/" target="_blank" rel="noopener">知识图谱教程</a></p><p><a href="http://blog.itpub.net/31562039/viewspace-2286939/" target="_blank" rel="noopener">事理图谱</a>以因果关系作为关系进行的推理</p><h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><ul><li>keypoint is how to define the relation</li><li>两侧实体的类型已经定义，关系好像已经没有定义的意义，问题好像不是知识图谱的问题，是实体的问题，现在是FILED-VALUE-REAL 我们尝试想用一个关系描述两个横线，甚至这两个横线的关系还不一定是一样的。</li></ul><ul><li><p>需要做的：</p><p>如何提取实体：</p></li></ul><p>  在民航领域，基于领域本体的突发事件应急管理已经取<br>  得了初步的成果[19-23]，民航突发事件领域本体的关系提取方<br>  法主要有基于 NNV 关联规则的方法[20]、基于改进的层次聚<br>  类 H_cluster 的方法[21]、基于 LDA 的方法[22]和基于 LSTM 的<br>  方法[23]。其中，基于 NNV 的方法将关联规则与自然语言处<br>  理方法相结合，完善了领域词典的构建方法、增添同义词表、<br>  丰富领域术语的过滤过程，利用关联规则法提取事务集，计<br>  算概念和非分类关系的支持度和置信度，解决了领域本体非<br>  分类关系获取中无法自动获取关系名称的问题，相较于模式<br>  匹配方法提取结果更好。但由于中文概念的多义性对非分类<br>  关系种类的影响等原因，该方法的准确率和召回率都很低；<br>  基于改进的层次聚类 H_cluster 的方法在概念获取的基础上，<br>  根据领域概念的上下文构建概念向量空间，计算概念相似度，<br>  解决了聚类结果的粒度过细问题，使其更加符合本体层次结<br>  构的需要，实现了概念间分类关系的提取，但该方法的自动<br>  化程度有限，且准确率和召回率均提升较小；基于 LDA 的<br>  方法以航空安全事件文本信息作为数据源，采用 NLPIR 自适<br>  应分词与过滤方法获取候选术语集，设计了领域本体的 LDA<br>  主题模型，通过吉布斯采样进行 LDA 模型训练与主题推断，<br>  实现了领域本体核心概念与关系的提取，可以有效解决大规<br>  模领域本体的自动更新问题。由于领域本体所有概念及其语<br>  义关系的复杂性，基于 LDA 概率分布的规则构建与本体实<br>  例自动获取的方法应进一步深入研究；基于 LSTM 的方法将<br>  深度学习模型 LSTM 应用于领域本体关系提取，首先将文本<br>  信息向量化，提取文本局部特征，然后将文本局部特征导入<br>  LSTM 模型中，获取文本整体特征，再将局部特征和整体特<br>  征进行特征融合，通过分类器进行分类。相较于传统方法，<br>  该方法能更加充分利用句子中的语义信息，更准确地表达深<br>  层语义，因此关系提取的 F 值有了较大提升，但还可以进一<br>  步优化。此外，当前互联网上包含越来越多的民航突发事件<br>  信息，涉及不同信息源(微博、微信、航空安全自愿报告系统<br>  等)对事件的不同评论，结构和内容越来越复杂，且民航突发<br>  事件领域本体的，因此迫切需要更加有效的关系抽取方法来<br>  支撑领域本体的自动构建</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;知识图谱学习整理的一些概念和小知识点&lt;/p&gt;
    
    </summary>
    
      <category term="知识图谱" scheme="http://kodgv.xyz/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
    
      <category term="知识图谱" scheme="http://kodgv.xyz/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>Linux</title>
    <link href="http://kodgv.xyz/2019/05/16/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/Linux/"/>
    <id>http://kodgv.xyz/2019/05/16/项目管理/Linux/</id>
    <published>2019-05-16T12:54:54.000Z</published>
    <updated>2019-05-25T12:43:08.058Z</updated>
    
    <content type="html"><![CDATA[<p>Linux学习</p><a id="more"></a><p>[TOC]<br><a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/49.html" target="_blank" rel="noopener">https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/49.html</a></p><h2 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h2><h3 id="基础命令"><a href="#基础命令" class="headerlink" title="基础命令"></a>基础命令</h3><ul><li>显示日期与时间的指令： date</li><li>显示日历的指令： cal [month] [year]</li><li>简单好用的计算器： bc</li></ul><h3 id="基础快捷键"><a href="#基础快捷键" class="headerlink" title="基础快捷键"></a>基础快捷键</h3><ul><li>tab补全： 命令补全；文件补全；选项补全(—)</li><li>ctrl+c:强制结束</li><li>ctrl+d:离开命令行</li><li>[shift]+{[PageUP]|[Page Down]}按键：上下翻页，仅限在命令行内</li></ul><h3 id="查询命令帮助"><a href="#查询命令帮助" class="headerlink" title="查询命令帮助"></a>查询命令帮助</h3><blockquote><p>man 命令<br>man page</p></blockquote><div class="table-container"><table><thead><tr><th>代号</th><th>内容说明</th></tr></thead><tbody><tr><td>NAME</td><td>简短的指令、数据名称说明</td></tr><tr><td>SYNOPSIS</td><td>简短的指令下达语法（syntax）简介</td></tr><tr><td>DESCRIPTION</td><td>较为完整的说明，这部分最好仔细看看！</td></tr><tr><td>OPTIONS</td><td>针对 SYNOPSIS 部分中，有列举的所有可用的选项说明</td></tr><tr><td>COMMANDS</td><td>当这个程序（软件）在执行的时候，可以在此程序（软件）中下达的指令</td></tr><tr><td>FILES</td><td>这个程序或数据所使用或参考或链接到的某些文件</td></tr><tr><td>SEE ALSO</td><td>可以参考的，跟这个指令或数据有相关的其他说明！</td></tr><tr><td>EXAMPLE</td><td>一些可以参考的范例</td></tr></tbody></table></div><blockquote><p>info 命令</p></blockquote><p>​    可读性会强很多，只是可能有些命令查不到，光标移到节点</p><ul><li>回车可进入</li><li>U返回上一节点</li><li>N进入下一节点</li><li>P进入上 一节点</li></ul><h3 id="正确的关机"><a href="#正确的关机" class="headerlink" title="正确的关机"></a>正确的关机</h3><p>关机前需要检查：</p><ul><li><p>who  看谁在线上</p></li><li><p>netstat -a 网络连线状态</p></li><li><p>ps -aux 查看背景执行程序</p></li><li><p>sync 将内存数据写入硬盘（很重要！）</p><p>因此在Linux系统中，为了加快数据的读取速度，所以在默认的情况中， 某些已经载入内存中的数据将不会直接被写回硬盘，而是先暂存在内存当中，如此一来， 如果一个数据被你重复的改写，那么由于他尚未被写入硬盘中，因此可以直接由内存当中读取出来， 在速度上一定是快上相当多的！</p></li></ul><p>需要以root执行，一般可以推迟一会关机，以免出错</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="symbol">@study</span> ~]<span class="meta"># shutdown -h now</span></span><br><span class="line">立刻关机，其中 now 相当于时间为 <span class="number">0</span> 的状态</span><br><span class="line">[root<span class="symbol">@study</span> ~]<span class="meta"># shutdown -h 20:25</span></span><br><span class="line">系统在今天的 <span class="number">20</span>:<span class="number">25</span> 分会关机，若在<span class="number">21</span>:<span class="number">25</span>才下达此指令，则隔天才关机</span><br><span class="line">[root<span class="symbol">@study</span> ~]<span class="meta"># shutdown -h +10</span></span><br><span class="line">系统再过十分钟后自动关机</span><br><span class="line">[root<span class="symbol">@study</span> ~]<span class="meta"># shutdown -r now</span></span><br><span class="line">系统立刻重新开机</span><br><span class="line">[root<span class="symbol">@study</span> ~]<span class="meta"># shutdown -r +30 <span class="string">'The system will reboot'</span> </span></span><br><span class="line">再过三十分钟系统会重新开机，并显示后面的讯息给所有在线上的使用者</span><br><span class="line">[root<span class="symbol">@study</span> ~]<span class="meta"># shutdown -c 取消关机</span></span><br></pre></td></tr></table></figure><h2 id="第五章"><a href="#第五章" class="headerlink" title="第五章"></a>第五章</h2><p>文件可存取身份三个类别：owner/group/others</p><p>可存取的权限：read/write/execute</p><h3 id="使用者与群组"><a href="#使用者与群组" class="headerlink" title="使用者与群组"></a>使用者与群组</h3><p>文件拥有者owner</p><p>Linux是个多用户多任务的系统，因此可能常常会有多人同时使用这部主机来进行工作的情况发生， 为了考虑每个人的隐私权以及每个人喜好的工作环境，因此，这个“文件拥有者”的角色就显的相当的重要。其他人无法查看你的文件</p><p>群组概念group</p><p>团队开发的时候，不同的团队在同一台主机上开发。</p><ul><li>使用者的意义：由于王家三人各自拥有自己的房间，所以， 王二毛虽然可以进入王三毛的房间，但是二毛不能翻三毛的抽屉喔！那样会被三毛K的！ 因为抽屉里面可能有三毛自己私人的东西，例如情书啦，日记啦等等的，这是“私人的空间”，所以当然不能让二毛拿啰！</li><li>群组的概念：由于共同拥有客厅，所以王家三兄弟可以在客厅打开电视机啦、 翻阅报纸啦、坐在沙发上面发呆啦等等的！ 反正，只要是在客厅的玩意儿，三兄弟都可以使用喔！ 因为大家都是一家人嘛！但是隔壁老王就不能进来。</li></ul><p>其他人的概念other</p><p>非同一群组的人，都叫其他人。因为它们相对别人来说都只是‘其他人’</p><p>天神root</p><p>可以有绝对权力，无视一切规则</p><ul><li>系统上的帐号与一般身份使用者储存在/etc/passwd</li><li>个人的密码/etc/shadow</li><li>群组名称/etc/group</li></ul><p>这三个配置文件的具体内容第十三章继续讲，很复杂的</p><h3 id="Linux-文件权限"><a href="#Linux-文件权限" class="headerlink" title="Linux 文件权限"></a>Linux 文件权限</h3><p>ls -al</p><p>这行信息要好好记住</p><p><img src="/2019/05/16/项目管理/Linux/1.gif" alt></p><p><img src="/2019/05/16/项目管理/Linux/E:/bolg\source\_posts\项目管理\Linux\2.gif" alt></p><ul><li><p>第一个字符代表这个文件是“目录、文件或链接文件等等”：</p><ul><li>当为[ d ]则是目录，例如<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html#table2.1.1" target="_blank" rel="noopener">上表</a>文件名为“.config”的那一行；</li><li>当为[ - ]则是文件，例如<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html#table2.1.1" target="_blank" rel="noopener">上表</a>文件名为“initial-setup-ks.cfg”那一行；</li><li>若是[ l ]则表示为链接文件（link file）；</li><li>若是[ b ]则表示为设备文件里面的可供储存的周边设备（可随机存取设备）；</li><li>若是[ c ]则表示为设备文件里面的序列埠设备，例如键盘、鼠标（一次性读取设备）。</li></ul></li><li><p>接下来的字符中，以三个为一组，且均为“rwx” 的三个参数的组合。</p><ul><li>r （read）：可读取此一文件的实际内容，如读取文本文件的文字内容等；</li><li>w （write）：可以编辑、新增或者是修改该文件的内容（<strong>但不含删除该文件</strong>）；</li><li>x （execute）：该文件具有可以被系统执行的权限。<br>要注意的是，这三个权限的位置不会改变，如果没有权限，就会出现减号[ - ]而已</li><li>第一组为“文件拥有者可具备的权限”，以“initial-setup-ks.cfg”那个文件为例， 该文件的拥有者可以读写，但不可执行；</li><li>第二组为“加入此群组之帐号的权限”；</li><li>第三组为“非本人且没有加入本群组之其他帐号的权限”。</li></ul></li></ul><h3 id="改变文件属性与权限"><a href="#改变文件属性与权限" class="headerlink" title="　改变文件属性与权限"></a>　改变文件属性与权限</h3><p>查看文件权限：ls -l 文件名</p><p>查看文件夹权限：ls -ld 文件夹名</p><ul><li><p>chgrp ：改变文件所属群组        chgrp [-R] 群族名 dirname/filename</p></li><li><p>chown ：改变文件拥有者            chown [-R] 帐号名称 文件或目录</p></li><li><p>chmod ：改变文件的权限, SUID, SGID, SBIT等等的特性   </p><p>chmod 644 .bashrc</p></li></ul><p>chgrp/chown应用场景，由于复制行为（cp）会复制执行者的属性与权限，所以当你想把文件给别人的时候，就会出现问题。</p><blockquote><p>r:4</p><p>w:2</p><p>x:1</p></blockquote><p>如果想rw-就是6，rwx就是7</p><h3 id="权限对目录的重要性"><a href="#权限对目录的重要性" class="headerlink" title="权限对目录的重要性"></a>权限对目录的重要性</h3><ul><li><p>r （read contents in directory）：</p><p>表示具有读取目录结构清单的权限，所以当你具有读取（r）一个目录的权限时，表示你可以查询该目录下的文件名数据。 所以你就可以利用 ls 这个指令将该目录的内容列表显示出来！</p></li><li><p>w （modify contents of directory）：</p><p>这个可写入的权限对目录来说，是很了不起的！ 因为他表示你具有异动该目录结构清单的权限，也就是下面这些权限：</p><ul><li>创建新的文件与目录；</li><li>删除已经存在的文件与目录（不论该文件的权限为何！）</li><li>将已存在的文件或目录进行更名；</li><li>搬移该目录内的文件、目录位置。 总之，目录的w权限就与该目录下面的文件名异动有关就对了啦！</li></ul></li><li><p>x （access directory）：</p><p>咦！目录的执行权限有啥用途啊？目录只是记录文件名而已，总不能拿来执行吧？没错！目录不可以被执行，目录的x代表的是使用者能否进入该目录，它相当于抽屉的钥匙，所以要想拿到抽屉里面的文件必须要先有钥匙。</p><p>| 元件 | 内容         | 叠代物件   | r            | w            | x                       |<br>| —— | —————— | ————— | —————— | —————— | ———————————- |<br>| 文件 | 详细数据data | 文件数据夹 | 读到文件内容 | 修改文件内容 | 执行文件内容            |<br>| 目录 | 文件名       | 可分类抽屉 | 读到文件名   | 修改文件名   | 进入该目录的权限（key） |</p></li></ul><h3 id="Linux目录配置的依据"><a href="#Linux目录配置的依据" class="headerlink" title="Linux目录配置的依据"></a>Linux目录配置的依据</h3><p>（需要筛选一遍重要的目录出来）</p><ul><li>/ （root, 根目录）：与开机系统有关；</li><li>/usr （unix software resource）：与软件安装/执行有关；</li><li>/var （variable）：与系统运行过程有关。</li></ul><p>根目录 （/） 的意义与内容</p><p>根目录是整个系统最重要的一个目录，它是系统的核心目录</p><div class="table-container"><table><thead><tr><th>目录</th><th>应放置文件内容</th></tr></thead><tbody><tr><td>第一部份：FHS 要求必须要存在的目录</td><td></td></tr><tr><td>/bin</td><td>系统有很多放置可执行文件的目录，但/bin比较特殊。因为/bin放置的是在单人维护模式下还能够被操作的指令。 在/bin下面的指令可以被root与一般帐号所使用，主要有：cat, chmod, chown, date, mv, mkdir, cp, bash等等常用的指令。</td></tr><tr><td>/boot</td><td>这个目录主要在放置开机会使用到的文件，包括Linux核心文件以及开机菜单与开机所需配置文件等等。 Linux kernel常用的文件名为：vmlinuz，如果使用的是grub2这个开机管理程序， 则还会存在/boot/grub2/这个目录喔！</td></tr><tr><td>/dev</td><td>在Linux系统上，任何设备与周边设备都是以文件的型态存在于这个目录当中的。 你只要通过存取这个目录下面的某个文件，就等于存取某个设备啰～ 比要重要的文件有/dev/null, /dev/zero, /dev/tty, /dev/loop<em>, /dev/sd</em>等等</td></tr><tr><td>/etc</td><td>系统主要的配置文件几乎都放置在这个目录内，例如人员的帐号密码档、 各种服务的启始档等等。一般来说，这个目录下的各文件属性是可以让一般使用者查阅的， 但是只有root有权力修改。FHS建议不要放置可可执行文件（binary）在这个目录中喔。比较重要的文件有： /etc/modprobe.d/, /etc/passwd, /etc/fstab, /etc/issue 等等。另外 FHS 还规范几个重要的目录最好要存在 /etc/ 目录下喔：/etc/opt（必要）：这个目录在放置第三方协力软件 /opt 的相关配置文件 /etc/X11/（建议）：与 X Window 有关的各种配置文件都在这里，尤其是 xorg.conf 这个 X Server 的配置文件。 /etc/sgml/（建议）：与 SGML 格式有关的各项配置文件 /etc/xml/（建议）：与 XML 格式有关的各项配置文件</td></tr><tr><td>/lib</td><td>系统的函数库非常的多，而/lib放置的则是在开机时会用到的函数库， 以及在/bin或/sbin下面的指令会调用的函数库而已。 什么是函数库呢？你可以将他想成是“外挂”，某些指令必须要有这些“外挂”才能够顺利完成程序的执行之意。 另外 FSH 还要求下面的目录必须要存在：/lib/modules/：这个目录主要放置可抽换式的核心相关模块（驱动程序）喔！</td></tr><tr><td>/media</td><td>media是“媒体”的英文，顾名思义，这个/media下面放置的就是可移除的设备啦！ 包括软盘、光盘、DVD等等设备都暂时挂载于此。常见的文件名有：/media/floppy, /media/cdrom等等。</td></tr><tr><td>/mnt</td><td>如果你想要暂时挂载某些额外的设备，一般建议你可以放置到这个目录中。 在古早时候，这个目录的用途与/media相同啦！只是有了/media之后，这个目录就用来暂时挂载用了。</td></tr><tr><td>/opt</td><td>这个是给第三方协力软件放置的目录。什么是第三方协力软件啊？ 举例来说，KDE这个桌面管理系统是一个独立的计划，不过他可以安装到Linux系统中，因此KDE的软件就建议放置到此目录下了。 另外，如果你想要自行安装额外的软件（非原本的distribution提供的），那么也能够将你的软件安装到这里来。 不过，以前的Linux系统中，我们还是习惯放置在/usr/local目录下呢！</td></tr><tr><td>/run</td><td>早期的 FHS 规定系统开机后所产生的各项信息应该要放置到 /var/run 目录下，新版的 FHS 则规范到 /run 下面。 由于 /run 可以使用内存来仿真，因此性能上会好很多！</td></tr><tr><td>/sbin</td><td>Linux有非常多指令是用来设置系统环境的，这些指令只有root才能够利用来“设置”系统，其他使用者最多只能用来“查询”而已。 放在/sbin下面的为开机过程中所需要的，里面包括了开机、修复、还原系统所需要的指令。 至于某些服务器软件程序，一般则放置到/usr/sbin/当中。至于本机自行安装的软件所产生的系统可执行文件（system binary）， 则放置到/usr/local/sbin/当中了。常见的指令包括：fdisk, fsck, ifconfig, mkfs等等。</td></tr><tr><td>/srv</td><td>srv可以视为“service”的缩写，是一些网络服务启动之后，这些服务所需要取用的数据目录。 常见的服务例如WWW, FTP等等。举例来说，WWW服务器需要的网页数据就可以放置在/srv/www/里面。 不过，系统的服务数据如果尚未要提供给网际网络任何人浏览的话，默认还是建议放置到 /var/lib 下面即可。</td></tr><tr><td>/tmp</td><td>这是让一般使用者或者是正在执行的程序暂时放置文件的地方。 这个目录是任何人都能够存取的，所以你需要定期的清理一下。当然，重要数据不可放置在此目录啊！ 因为FHS甚至建议在开机时，应该要将/tmp下的数据都删除唷！</td></tr><tr><td>/usr</td><td>第二层 FHS 设置，后续介绍</td></tr><tr><td>/var</td><td>第二曾 FHS 设置，主要为放置变动性的数据，后续介绍</td></tr><tr><td>第二部份：FHS 建议可以存在的目录</td><td></td></tr><tr><td>/home</td><td>这是系统默认的使用者主文件夹（home directory）。在你新增一个一般使用者帐号时， 默认的使用者主文件夹都会规范到这里来。比较重要的是，主文件夹有两种代号喔：~：代表目前这个使用者的主文件夹 ~dmtsai ：则代表 dmtsai 的主文件夹！</td></tr><tr><td>/lib<qual></qual></td><td>用来存放与 /lib 不同的格式的二进制函数库，例如支持 64 位的 /lib64 函数库等</td></tr><tr><td>/root</td><td>系统管理员（root）的主文件夹。之所以放在这里，是因为如果进入单人维护模式而仅挂载根目录时， 该目录就能够拥有root的主文件夹，所以我们会希望root的主文件夹与根目录放置在同一个分区中。</td></tr></tbody></table></div><p>/usr 的意义与内容：</p><p>usr叫做Unix Software Resource Unix操作系统软件资源”所放置的目录。所有系统默认的软件（distribution发布者提供的软件）都会放置到/usr下面，因此这个目录有点类似Windows 系统的“C:\Windows\ （当中的一部份） + C:\Program files\”这两个目录的综合体</p><div class="table-container"><table><thead><tr><th>目录</th><th>应放置文件内容</th></tr></thead><tbody><tr><td>第一部份：FHS 要求必须要存在的目录</td><td></td></tr><tr><td>/usr/bin/</td><td>所有一般用户能够使用的指令都放在这里！目前新的 CentOS 7 已经将全部的使用者指令放置于此，而使用链接文件的方式将 /bin 链接至此！ 也就是说， /usr/bin 与 /bin 是一模一样了！另外，FHS 要求在此目录下不应该有子目录！</td></tr><tr><td>/usr/lib/</td><td>基本上，与 /lib 功能相同，所以 /lib 就是链接到此目录中的！</td></tr><tr><td>/usr/local/</td><td>系统管理员在本机自行安装自己下载的软件（非distribution默认提供者），建议安装到此目录， 这样会比较便于管理。举例来说，你的distribution提供的软件较旧，你想安装较新的软件但又不想移除旧版， 此时你可以将新版软件安装于/usr/local/目录下，可与原先的旧版软件有分别啦！ 你可以自行到/usr/local去看看，该目录下也是具有bin, etc, include, lib…的次目录喔！</td></tr><tr><td>/usr/sbin/</td><td>非系统正常运行所需要的系统指令。最常见的就是某些网络服务器软件的服务指令（daemon）啰！不过基本功能与 /sbin 也差不多， 因此目前 /sbin 就是链接到此目录中的。</td></tr><tr><td>/usr/share/</td><td>主要放置只读架构的数据文件，当然也包括共享文件。在这个目录下放置的数据几乎是不分硬件架构均可读取的数据， 因为几乎都是文字文件嘛！在此目录下常见的还有这些次目录：/usr/share/man：线上说明文档 /usr/share/doc：软件杂项的文件说明 /usr/share/zoneinfo：与时区有关的时区文件</td></tr><tr><td>第二部份：FHS 建议可以存在的目录</td><td></td></tr><tr><td>/usr/games/</td><td>与游戏比较相关的数据放置处</td></tr><tr><td>/usr/include/</td><td>c/c++等程序语言的文件开始（header）与包含档（include）放置处，当我们以tarball方式 （*.tar.gz 的方式安装软件）安装某些数据时，会使用到里头的许多包含档喔！</td></tr><tr><td>/usr/libexec/</td><td>某些不被一般使用者惯用的可执行文件或脚本（script）等等，都会放置在此目录中。例如大部分的 X 窗口下面的操作指令， 很多都是放在此目录下的。</td></tr><tr><td>/usr/lib<qual>/</qual></td><td>与 /lib<qual>/功能相同，因此目前 /lib<qual> 就是链接到此目录中</qual></qual></td></tr><tr><td>/usr/src/</td><td>一般源代码建议放置到这里，src有source的意思。至于核心源代码则建议放置到/usr/src/linux/目录下。</td></tr></tbody></table></div><p>/var 的意义与内容：</p><p>如果/usr是安装时会占用较大硬盘容量的目录，那么/var就是在系统运行后才会渐渐占用硬盘容量的目录。 因为/var目录主要针对常态性变动的文件，包括高速缓存（cache）、登录文件（log file）以及某些软件运行所产生的文件， 包括程序文件（lock file, run file），或者例如MySQL数据库的文件等等。</p><div class="table-container"><table><thead><tr><th>目录</th><th>应放置文件内容</th></tr></thead><tbody><tr><td>第一部份：FHS 要求必须要存在的目录</td><td></td></tr><tr><td>/var/cache/</td><td>应用程序本身运行过程中会产生的一些暂存盘；</td></tr><tr><td>/var/lib/</td><td>程序本身执行的过程中，需要使用到的数据文件放置的目录。在此目录下各自的软件应该要有各自的目录。 举例来说，MySQL的数据库放置到/var/lib/mysql/而rpm的数据库则放到/var/lib/rpm去！</td></tr><tr><td>/var/lock/</td><td>某些设备或者是文件资源一次只能被一个应用程序所使用，如果同时有两个程序使用该设备时， 就可能产生一些错误的状况，因此就得要将该设备上锁（lock），以确保该设备只会给单一软件所使用。 举例来说，烧录机正在烧录一块光盘，你想一下，会不会有两个人同时在使用一个烧录机烧片？ 如果两个人同时烧录，那片子写入的是谁的数据？所以当第一个人在烧录时该烧录机就会被上锁， 第二个人就得要该设备被解除锁定（就是前一个人用完了）才能够继续使用啰。目前此目录也已经挪到 /run/lock 中！</td></tr><tr><td>/var/log/</td><td>重要到不行！这是登录文件放置的目录！里面比较重要的文件如/var/log/messages, /var/log/wtmp（记录登陆者的信息）等。</td></tr><tr><td>/var/mail/</td><td>放置个人电子邮件信箱的目录，不过这个目录也被放置到/var/spool/mail/目录中！ 通常这两个目录是互为链接文件啦！</td></tr><tr><td>/var/run/</td><td>某些程序或者是服务启动后，会将他们的PID放置在这个目录下喔！至于PID的意义我们会在后续章节提到的。 与 /run 相同，这个目录链接到 /run 去了！</td></tr><tr><td>/var/spool/</td><td>这个目录通常放置一些伫列数据，所谓的“伫列”就是排队等待其他程序使用的数据啦！ 这些数据被使用后通常都会被删除。举例来说，系统收到新信会放置到/var/spool/mail/中， 但使用者收下该信件后该封信原则上就会被删除。信件如果暂时寄不出去会被放到/var/spool/mqueue/中， 等到被送出后就被删除。如果是工作调度数据（crontab），就会被放置到/var/spool/cron/目录中！</td></tr></tbody></table></div><h2 id="第六章"><a href="#第六章" class="headerlink" title="第六章"></a>第六章</h2><h3 id="执行文件路径的变量：-PATH"><a href="#执行文件路径的变量：-PATH" class="headerlink" title="执行文件路径的变量： $PATH"></a>执行文件路径的变量： $PATH</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$PATH</span></span><br></pre></td></tr></table></figure><p>搜索过程</p><p>系统会依照PATH的设置去每个PATH定义的目录下搜寻文件名为ls的可可执行文件， 如果在PATH定义的目录中含有多个文件名为ls的可可执行文件，那么先搜寻到的同名指令先被执行！</p><p>新增PATH（仅仅在本次登录shell有效,也就是说重启后就会还原）</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">PATH</span>=<span class="string">"$&#123;PATH&#125;:/root"</span></span><br></pre></td></tr></table></figure><h2 id="Linux-快捷键"><a href="#Linux-快捷键" class="headerlink" title="Linux 快捷键"></a>Linux 快捷键</h2><p>编辑器快捷键</p><div class="table-container"><table><thead><tr><th>按键</th><th>进行工作</th></tr></thead><tbody><tr><td>空白键</td><td>向下翻一页</td></tr><tr><td>[Page Down]</td><td>向下翻一页</td></tr><tr><td>[Page Up]</td><td>向上翻一页</td></tr><tr><td>[Home]</td><td>去到第一页</td></tr><tr><td>[End]</td><td>去到最后一页</td></tr><tr><td>/string</td><td>向“下”搜寻 string 这个字串，如果要搜寻 vbird 的话，就输入 /vbird</td></tr><tr><td>?string</td><td>向“上”搜寻 string 这个字串</td></tr><tr><td>n, N</td><td>利用 / 或 ? 来搜寻字串时，可以用 n 来继续下一个搜寻 （不论是 / 或 ?） ，可以利用 N 来进行“反向”搜寻。举例来说，我以 /vbird 搜寻 vbird 字串， 那么可以 n 继续往下查询，用 N 往上查询。若以 ?vbird 向上查询 vbird 字串， 那我可以用 n 继续“向上”查询，用 N 反向查询。</td></tr><tr><td>q</td><td>结束这次的 man page</td></tr></tbody></table></div><h2 id="需要背的快捷键"><a href="#需要背的快捷键" class="headerlink" title="需要背的快捷键"></a>需要背的快捷键</h2><ul><li><p>tab补全： 命令补全；文件补全；选项补全(—)</p></li><li><p>ctrl+c:强制结束</p></li><li><p>ctrl+d:离开命令行</p></li><li><p>[shift]+{[PageUP]|[Page Down]}按键：上下翻页，仅限在命令行内</p></li></ul><p>编辑器快捷键</p><div class="table-container"><table><thead><tr><th>按键</th><th>进行工作</th></tr></thead><tbody><tr><td>空白键</td><td>向下翻一页</td></tr><tr><td>[Page Down]</td><td>向下翻一页</td></tr><tr><td>[Page Up]</td><td>向上翻一页</td></tr><tr><td>[Home]</td><td>去到第一页</td></tr><tr><td>[End]</td><td>去到最后一页</td></tr><tr><td>/string</td><td>向“下”搜寻 string 这个字串，如果要搜寻 vbird 的话，就输入 /vbird</td></tr><tr><td>?string</td><td>向“上”搜寻 string 这个字串</td></tr><tr><td>n, N</td><td>利用 / 或 ? 来搜寻字串时，可以用 n 来继续下一个搜寻 （不论是 / 或 ?） ，可以利用 N 来进行“反向”搜寻。举例来说，我以 /vbird 搜寻 vbird 字串， 那么可以 n 继续往下查询，用 N 往上查询。若以 ?vbird 向上查询 vbird 字串， 那我可以用 n 继续“向上”查询，用 N 反向查询。</td></tr><tr><td>q</td><td>结束这次的 man page</td></tr></tbody></table></div><p>cd 选项<br><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">.  </span>代表此层目录</span><br><span class="line"><span class="bullet">..        </span>代表上一层目录</span><br><span class="line"><span class="bullet">-         </span>代表前一个工作目录</span><br><span class="line">~         代表“目前使用者身份”所在的主文件夹</span><br><span class="line">~account  代表 account 这个使用者的主文件夹（account是个帐号名称）</span><br></pre></td></tr></table></figure></p><p>安装包</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dpkg -<span class="selector-tag">i</span> deb包</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux学习&lt;/p&gt;
    
    </summary>
    
      <category term="项目管理" scheme="http://kodgv.xyz/categories/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="linux" scheme="http://kodgv.xyz/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>git教程</title>
    <link href="http://kodgv.xyz/2019/05/16/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/git%E6%95%99%E7%A8%8B/"/>
    <id>http://kodgv.xyz/2019/05/16/项目管理/git教程/</id>
    <published>2019-05-16T07:19:55.000Z</published>
    <updated>2019-05-25T12:43:34.135Z</updated>
    
    <content type="html"><![CDATA[<p>git学习通关    <a href="https://learngitbranching.js.org/" target="_blank" rel="noopener">https://learngitbranching.js.org/</a> </p><a id="more"></a><p>[TOC]</p><p>记住多个分支，但是它们都是在一棵树上的</p><p>这些命令的功能和选项还有很多，只是为了一个初步的了解，细化的需求还需要继续查看API</p><h2 id="初级命令"><a href="#初级命令" class="headerlink" title="初级命令"></a>初级命令</h2><h3 id="Git-Commit"><a href="#Git-Commit" class="headerlink" title="Git Commit"></a>Git Commit</h3><p>Git 仓库中的提交记录保存的是你的目录下所有文件的快照，就像是把整个目录复制，然后再粘贴一样，但比复制粘贴优雅许多！</p><p>Git 希望提交记录尽可能地轻量，因此在你每次进行提交时，它并不会盲目地复制整个目录。条件允许的情况下，它会将当前版本与仓库中的上一个版本进行对比，并把所有的差异打包到一起作为一个提交记录。</p><h3 id="Git-Branch"><a href="#Git-Branch" class="headerlink" title="Git Branch"></a>Git Branch</h3><p>Git 的分支也非常轻量。它们只是简单地指向某个提交纪录 —— 仅此而已。所以许多 Git 爱好者传颂：</p><blockquote><p> 早建分支！多用分支！</p></blockquote><p>这是因为即使创建再多分的支也不会造成储存或内存上的开销，并且按逻辑分解工作到不同的分支要比维护那些特别臃肿的分支简单多了。</p><blockquote><p>git checkout 切换分支(提交记录是基于当前主分支)</p><p>git branch 新建分支</p></blockquote><h3 id="git-merge"><a href="#git-merge" class="headerlink" title="git merge"></a>git merge</h3><p>git merge 分支1 分支2</p><p> 将当前分支和分支进行合并从合并点网上可以遍历所有分支，即merge后是所有代码的修改历史</p><h3 id="git-Rebase"><a href="#git-Rebase" class="headerlink" title="git Rebase"></a>git Rebase</h3><blockquote><p>git rebase   复制到目标分支 被复制分支</p></blockquote><p>​    Rebase 实际上就是取出一系列的提交记录，“复制”它们，然后在另外一个地方逐个的放下去。Rebase 的优势就是可以创造更线性的提交历史，这听上去有些难以理解。如果只允许使用 Rebase 的话，代码库的提交历史将会变得异常清晰。</p><p>​    它会复制一个分支的状态到另一个分支下而不是单纯只是合并，此时另一个分支的状态还在，如果是Merge的话，两个状态会合并到一个新状态</p><p><img src="/2019/05/16/项目管理/git教程/1" alt="1557992194391"></p><h3 id="合并分支和HEAD"><a href="#合并分支和HEAD" class="headerlink" title="合并分支和HEAD"></a>合并分支和HEAD</h3><p>git branch -f master HEAD^</p><h2 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h2><h3 id="分离HEAD"><a href="#分离HEAD" class="headerlink" title="　分离HEAD"></a>　分离HEAD</h3><p>​    HEAD 是一个对当前检出记录的符号引用 —— 也就是指向你正在其基础上进行工作的提交记录。</p><p>​    HEAD 总是指向当前分支上最近一次提交记录。大多数修改提交树的 Git 命令都是从改变 HEAD 的指向开始的。</p><p>​    HEAD 通常情况下是指向分支名的（如 bugFix）。在你提交时，改变了 bugFix 的状态，这一变化通过 HEAD 变得可见。</p><p>就是让其指向了某个具体的提交记录而不是分支名,</p><p>git checkout 哈希值id</p><h3 id="相对引用"><a href="#相对引用" class="headerlink" title="相对引用"></a>相对引用</h3><p>​    通过指定提交记录哈希值的方式在 Git 中移动不太方便。在实际应用时，并没有像本程序中这么漂亮的可视化提交树供你参考，所以你就不得不用 <code>git log</code> 来查查看提交记录的哈希值。</p><p>​    并且哈希值在真实的 Git 世界中也会更长（译者注：基于 SHA-1，共 40 位）。例如前一关的介绍中的提交记录的哈希值可能是 <code>fed2da64c0efc5293610bdd892f82a58e8cbc5d8</code>。舌头都快打结了吧…</p><p>​    比较令人欣慰的是，Git 对哈希的处理很智能。你只需要提供能够唯一标识提交记录的前几个字符即可。因此我可以仅输入<code>fed2</code> 而不是上面的一长串字符。</p><p>另一种方法是相对引用（要有一个作为基准，如master或其他分支）</p><ul><li>使用 <code>^</code> 向上移动 1 个提交记录</li><li>使用 <code>~&lt;num&gt;</code> 向上移动多个提交记录，如 <code>~3</code></li></ul><p>~支持一条链上的移动；^支持分叉点，即^1和^2指的是C4和C5</p><p><img src="/2019/05/16/项目管理/git教程/7" alt="1558520332866"></p><h3 id="撤销变更"><a href="#撤销变更" class="headerlink" title="撤销变更"></a>撤销变更</h3><p>git reset HEAD~1</p><p>把 master 分支从C2移回到 <code>C1</code>；现在我们的本地代码库根本就不知道有 <code>C2</code> 这个提交了。(但是C2状态还在，只是不加入暂存区)，无法影响远程，提交之后会和远程的继续Merge</p><p>Git Revert HEAD</p><p>会新建一个HEAD~1的状态，所以可以把你的更改推送到远程仓库与别人分享啦。</p><h3 id="整理提交记录"><a href="#整理提交记录" class="headerlink" title="整理提交记录"></a>整理提交记录</h3><p><a href="https://www.jianshu.com/p/4b769b934472?utm_campaign" target="_blank" rel="noopener">https://www.jianshu.com/p/4b769b934472?utm_campaign</a></p><p>cherry-pick的作用是，将某次或者某几次的提交,在另一个分支上进行重演<br> 例如,我们有个稳定版本的分支，叫v2.0，另外还有个开发版本的分支v3.0，我们不能直接把两个分支合并，这样会导致稳定版本混乱，但是又想增加一个v3.0中的功能到v2.0中，这里就可以使用cherry-pick了.</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout v2<span class="number">.0</span>分支</span><br><span class="line"></span><br><span class="line">$ git cherry-pick <span class="number">38361</span>a55 # 这个 <span class="number">38361</span>a55 号码，位于v3<span class="number">.0</span>分支中：</span><br></pre></td></tr></table></figure><p>也可以 按照提交记录，提交多个记录</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout v2<span class="number">.0</span>分支</span><br><span class="line"></span><br><span class="line">$ git cherry-pick <span class="number">38361</span>a55 <span class="number">38361</span>a5aaa  <span class="number">38361</span>a57adfd    <span class="number">38361</span>abbb</span><br></pre></td></tr></table></figure><p>如果遇到了冲突，那么可以在手动解决完冲突后，自己使用<code>git cherry-pick --continue</code>继续，也可以放弃<code>git cherry-pick --aboart</code>，这时候的放弃操作不会对之前的分支造成任何影响</p><ul><li><p>git cherry-pick 是将特定的某次或者某些提交进行合并</p></li><li><p>git merge和git rebase都是基于最新状态进行合并</p></li><li><p>注意git cherry-pick是不能够复制自身之前的状态</p></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git rebase -<span class="selector-tag">i</span> HEAD~<span class="number">4</span></span><br></pre></td></tr></table></figure><ul><li>通过交互式界面-i，来复制一连串的提交记录，无需要记住每个状态的哈希值</li><li>复制的起点是HEAD~4，它只能针对一条链</li><li>复制的终点是HEAD~5为起点重新建一条</li></ul><p><strong>举例</strong></p><p>来看一个在开发中经常会遇到的情况：我正在解决某个特别棘手的 Bug，为了便于调试而在代码中添加了一些调试命令并向控制台打印了一些信息。</p><p>这些调试和打印语句都在它们各自的提交记录里。最后我终于找到了造成这个 Bug 的根本原因，解决掉以后觉得沾沾自喜！</p><p>最后就差把 <code>bugFix</code> 分支里的工作合并回 <code>master</code> 分支了。你可以选择通过 fast-forward 快速合并到 <code>master</code> 分支上，但这样的话 <code>master</code> 分支就会包含我这些调试语句了。</p><p>实际我们只要让 Git 复制解决问题的那一个提交记录就可以了。跟之前我们在“整理提交记录”中学到的一样，我们可以使用git merge两个分支就可以了</p><h3 id="git-tags"><a href="#git-tags" class="headerlink" title="git tags"></a>git tags</h3><p>标签,由于分支会在各个状态中变换，所以需要有一个标签来记录每个状态。</p><p>首先说一下作用：Git 中的tag指向一次commit的id，通常用来给开发分支做一个标记，如标记一个版本号。</p><p>1.添加标签： git tag -a version -m “note”<br>注解：git tag 是打标签的命令，-a 是添加标签，其后要跟新标签号，-m 及后面的字符串是对该标签的注释。</p><p>2.提交标签到远程仓库 ：git push origin -tags<br>注解：就像git push origin master 把本地修改提交到远程仓库一样，-tags可以把本地的打的标签全部提交到远程仓库。</p><p>3.删除标签：git tag -d version<br>注解：-d 表示删除，后面跟要删除的tag名字</p><p>4.删除远程标签：git push origin :refs/tags/version<br>注解：就像git push origin :branch_1 可以删除远程仓库的分支branch_1一样， 冒号前为空表示删除远程仓库的tag。</p><p>5.查看标签：git tag或者git tag -l</p><p>git describe <ref></ref></p><p>它输出的结果是这样的：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">tag</span>&gt;</span>_<span class="tag">&lt;<span class="name">numCommits</span>&gt;</span>_g<span class="tag">&lt;<span class="name">hash</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>tag</code> 表示的是离 <code>ref</code> 最近的标签， <code>numCommits</code> 是表示这个 <code>ref</code> 与 <code>tag</code> 相差有多少个提交记录， <code>hash</code> 表示的是你所给定的 <code>ref</code> 所表示的提交记录哈希值的前几位，是指ref。</p><h2 id="远程"><a href="#远程" class="headerlink" title="远程"></a>远程</h2><p>远程分支有一个特别的属性，在你检出时自动进入分离 HEAD 状态。Git 这么做是出于不能直接在这些分支上进行操作的原因, 你必须在别的地方完成你的工作, （更新了远程分支之后）再用远程分享你的工作成果。</p><p>远程分支：origin/分支名</p><p>正如你所见，Git 变成了分离 HEAD 状态，当添加新的提交时 <code>o/master</code> 也不会更新。这是因为 <code>o/master</code> 只有在远程仓库中相应的分支更新了以后才会更新</p><p><strong>都可以指定特定的分支</strong></p><p><strong>git fetch</strong></p><blockquote><p>git fetch origin <source></p></blockquote><p><code>git fetch</code> 完成了仅有的但是很重要的两步:</p><ul><li>从远程仓库下载本地仓库中缺失的提交记录</li><li>更新远程分支指针(如 <code>o/master</code>)</li></ul><p><code>git fetch</code> 实际上将本地仓库中的远程分支更新成了远程仓库相应分支最新的状态,它不会改变你的本地仓库，也不会改变你的分支.如果不带参数，它将下载所有远程提交记录</p><p>既然我们已经知道了如何用 <code>git fetch</code> 获取远程的数据, 现在我们学习如何将这些变化更新到我们的工作当中。</p><p>其实有很多方法的 —— 当远程分支中有新的提交时，你可以像合并本地分支那样来合并远程分支。也就是说就是你可以执行以下命令:</p><ul><li><code>git cherry-pick o/master</code></li><li><code>git rebase o/master</code></li><li><code>git merge o/master</code></li><li>等等</li></ul><p>也就是说你可以选你喜欢的进行合并</p><p><strong>git pull</strong></p><p>先抓取更新再合并到本地分支</p><blockquote><p>git pull origin <source>:<destination></destination></p></blockquote><p>git fetch+git merge</p><p><strong>git push</strong></p><blockquote><p>git push origin <source>:<destination></destination></p></blockquote><p>git push 提交成功之后本地对应的远程分支也会对应更新</p><p>可以用^~也可以直接用哈希值</p><p><strong>偏离的工作</strong></p><p>我们用 <code>git fetch</code> 更新了本地仓库中的远程分支，然后用 rebase 将工们的工作移动到最新的提交记录下，最后再用 <code>git push</code> 推送到远程仓</p><p>我们用 <code>git fetch</code> 更新了本地仓库中的远程分支，然后merge了新变更到我们的本地分支（为了包含远程仓库的变更），最后我们用 <code>git push</code> 把工作推送到远程仓库</p><p>在开发社区里，有许多关于 merge 与 rebase 的讨论。以下是关于 rebase 的优缺点：</p><p>优点:</p><ul><li>Rebase 使你的提交树变得很干净, 所有的提交都在一条线上</li></ul><p>缺点:</p><ul><li>Rebase 修改了提交树的历史</li></ul><p><strong>git remote show origin</strong></p><p>这个命令列出了当你在特定的分支上执行 <code>git push</code> 会自动地推送到哪一个远程分支。 它也同样地列出了哪些远程分支不在你的本地，哪些远程分支已经从服务器上移除了，还有当你执行 <code>git pull</code> 时哪些分支会自动合并。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;git学习通关    &lt;a href=&quot;https://learngitbranching.js.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://learngitbranching.js.org/&lt;/a&gt; &lt;/p&gt;
    
    </summary>
    
      <category term="项目管理" scheme="http://kodgv.xyz/categories/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="git" scheme="http://kodgv.xyz/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>java核心技术基础知识</title>
    <link href="http://kodgv.xyz/2019/04/30/JAVA%E5%AD%A6%E4%B9%A0/java%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/"/>
    <id>http://kodgv.xyz/2019/04/30/JAVA学习/java核心技术/</id>
    <published>2019-04-30T11:30:59.000Z</published>
    <updated>2019-05-23T14:58:24.825Z</updated>
    
    <content type="html"><![CDATA[<p>java基础知识<br><a id="more"></a></p><p>[TOC]</p><h2 id="java程序设计环境"><a href="#java程序设计环境" class="headerlink" title="java程序设计环境"></a>java程序设计环境</h2><p><img src="/2019/04/30/JAVA学习/java核心技术/1" alt="1557976698685"></p><ul><li><p>JDK 是 Java Development Kit 的缩写开发人员必须要安装，Java 运行时环境（JRE), 它包含虚拟机 但不包含编译器。</p></li><li><p>Java SE 会大量出现， 相对于 Java EE ( Enterprise Edition) 和 Java ME ( Micro Edition), 它是 Java 的标准版。 </p></li></ul><h2 id="java基本程序设计结构"><a href="#java基本程序设计结构" class="headerlink" title="java基本程序设计结构"></a>java基本程序设计结构</h2><ul><li>类是构建所有 Java 应用程序和 applet 的构建块。Java 应用程序中的全部内容都必须放置在类中。 </li></ul><p><strong>数据类型</strong></p><p>整型</p><p><img src="/2019/04/30/JAVA学习/java核心技术/E:/bolg\source\_posts\JAVA学习\java核心技术\2" alt="1557977861052"></p><p>浮点型</p><p><img src="/2019/04/30/JAVA学习/java核心技术/E:/bolg\source\_posts\JAVA学习\java核心技术\3" alt="1557977876477"></p><p>有三种类型，正无穷大，负无穷大，NAN：</p><ul><li>Double_POSITIVE_INFINITY</li><li>Double.NEGATIVEJNFINITY </li><li>Double.NaN </li></ul><blockquote><p>NAN与任何数字包括NAN都不相等，不能用==Double.NaN,而是要用isNaN()</p></blockquote><p>字符型</p><p><strong>byte</strong></p><p>byte 字节，数据存储容量1byte，byte作为基本数据类型表示的也是一个存储范围上的概念，有别于int、long等专门存数字的类型，这种类型的大小就是1byte,而int是4byte。<br>存数字的话就是1byte=8位，2^8=256 即-128-127。字符的话包括字母和汉字，一个字母是1byte，一个汉字2byte。也就是可以用byte变量去存储一个英文字符，但是却存不下一个中文汉字，因为一个汉字占2byte。</p><p>总结，byte是java中的一个基本数据类型，这个数据类型的长度是1byte，此byte就是彼byte,即是基本数据类型也是存储空间的基本计量单位。</p><p><strong>char</strong></p><p>char是Java中的保留字，与别的语言不同的是，char在Java中是16位的，因为Java用的是Unicode。不过8位的ASCII码包含在Unicode中，是从0~127的。</p><p>Java中使用Unicode的原因是，Java的Applet允许全世界范围内运行，那它就需要一种可以表述人类所有语言的字符编码。Unicode。</p><p>char本质上是一个固定占用两个字节的无符号正整数，这个正整数对应于Unicode编号，用于表示那个Unicode编号对应的字符。</p><p>由于固定占用两个字节，char只能表示Unicode编号在65536以内的字符，而不能表示超出范围的字符。</p><p><strong>Unicode</strong></p><p>需要注意的是，Unicode只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储。</p><p>比如，汉字”严”的unicode是十六进制数4E25，转换成二进制数足足有15位（100111000100101），也就是说这个符号的表示至少需要2个字节。表示其他更大的符号，可能需要3个字节或者4个字节，甚至更多。</p><p>这里就有两个严重的问题，第一个问题是，如何才能区别Unicode和ASCII？计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？第二个问题是，我们已经知道，英文字母只用一个字节表示就够了，如果Unicode统一规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。</p><p>它们造成的结果是：1）出现了Unicode的多种存储方式，也就是说有许多种不同的二进制格式，可以用来表示Unicode。2）Unicode在很长一段时间内无法推广，直到互联网的出现。</p><p><strong>UTF-8</strong></p><p>互联网的普及，强烈要求出现一种统一的编码方式。UTF-8就是在互联网上使用最广的一种Unicode的实现方式。其他实现方式还包括UTF-16（字符用两个字节或四个字节表示）和UTF-32（字符用四个字节表示），不过在互联网上基本不用。重复一遍，这里的关系是，UTF-8是Unicode的实现方式之一。</p><p>以utf8为例，utf8是一个变长编码标准，可以以1~4个字节表示一个字符，而中文占3个字节，ascII字符占1个字节。</p><p>为什么我们在java里面可以用一个char来表示一个中文呢？</p><p>因为java是以unicode作为编码方式的。unicode是一个定长的编码标准，每个字符都是2个字节，也就是1个char类型的空间。</p><p>在编译时会把utf8的中文字符转换成对应的unicode来进行传输运算。</p><p>(<strong>总结</strong>)[<a href="https://www.zhihu.com/question/23374078" target="_blank" rel="noopener">https://www.zhihu.com/question/23374078</a>]</p><ul><li><p>unicode是编码集，UTF8只是实现方式</p></li><li><p>一个char可以储存一个中文字符，因为它是两个byte</p></li><li>UTF8采用8位动长的方式，可以节省内存，区别中英文</li><li>unicode不够的时候，会采取增补代码点，即用2个2位字节来表示增补字符</li></ul><p><strong>常量</strong></p><p>final 关键字  一旦赋值不能够再更改</p><p><strong>String 字符串</strong></p><p>Java 字符串就是 Unicode 字符序列。Java 没有内置的字符串类型， 而是在标准 Java类库中提供了 一个预定义类，很自然地叫做 String。它是不可变的对象，每次赋值都会新开一个内存空间。可以把string理解成是char*的指针。</p><p>不要用==来比较字符串，而是要使用s.equal(s1)的方式</p><p>它有null和“”空串，两种不同形式，if (str != null &amp;&amp; str.length != 0) </p><p><strong>格式化输出</strong></p><p><img src="/2019/04/30/JAVA学习/java核心技术/4" alt="1558017586500"></p><p>String message = String.format(“Hello, %s. Next year, you’ll be %d”, name, age); </p><p><strong>大数值</strong></p><p>Biglnteger类实现了任意精度的整数运算； BigDecimal 实现了任意精度的浮点数运算。它们没有+和*必须用内置的函数</p><p>Biglnteger a = Biglnteger.valueOf(100);<br>Biglnteger c = a.add(b); // c = a + b<br>Biglnteger d = c.nultipiy(b.add(Biglnteger.valueOf(2))); // d = c * (b + 2)</p><p><strong>数组</strong><br>int[ ] a = new int[100];<br>使用以下直接访问数组内的元素，它将会遍历数组， 而不需要使用下标值<br>for (int element : a)<br>​    System.out.println(element): </p><p>数组拷贝</p><p>浅拷贝： 两个变量将引用同 一个数组</p><p>深拷贝：int[] copiedLuckyNumbers = Arrays.copyOf(luckyNumbers, luckyNumbers.length);（可以使用该方法增长数组长度）</p><p>数组排序：</p><p>Arrays.sort(a) </p><h2 id="对象与类"><a href="#对象与类" class="headerlink" title="对象与类"></a>对象与类</h2><p>要想使用 OOP, —定要清楚对象的三个主要特性： </p><ul><li><p>对象的行为（behavior)— —可以对对象施加哪些操作，或可以对对象施加哪些方法？ </p></li><li><p>对象的状态（state)— —当施加那些方法时，对象如何响应？ </p></li><li><p>对象标识（identity)— —如何辨别具有相同行为与状态的不同对象</p></li></ul><ul><li>对象状态的改变必须通过调用方法实现 </li><li>对象的状态并不能完全描述一个对象。每个对象都有一个唯一的身份（identity)。</li><li>对象的这些关键特性在彼此之间相互影响着。例如， 对象的状态影响它的行为（如果一 个订单 “ 已送货” 或“ 已付款”， 就应该拒绝调用具有增删订单中条目的方法。</li></ul><p>最常见类的关系：•依赖（ “ uses-a”） •聚合（ “ has-a”） •继承（is-a)</p><p>要用UML图画类的关系，设计的时候。</p><p><strong>对象变量</strong></p><p>一个对象变量并没有实际包含一个对象，而仅仅引用一个对象</p><blockquote><p>Date deadline<br>deadline = new Date()<br>deadline是对象变量，必须初始化它</p></blockquote><ul><li>只 访 问 对 象 而 不 修 改 对 象 的 方 法 有 时 称 为 访 问 器 方 法</li><li>修改对象的方法称更改器方法 ( mutator method ) </li><li>如果需要返回一个可变对象的引用， 应该首先对它进行克隆（clone)，否则破坏封装性。</li></ul><p><strong>构造器</strong></p><p>•构造器与类同名 </p><p>•每个类可以有一个以上的构造器 </p><p>•构造器可以有 0 个、1 个或多个参数 </p><p>•构造器没有返回值</p><p> •构造器总是伴随着 new 操作一起调用</p><p>关键字 this 表示隐式参数，就是说通过this.变量或方法来调用自身域内。</p><p><strong>final实例域</strong></p><p>实例域定义为 final。 构建对象时必须初始化这样的域。也就是说， 必须确保在每 一个构造器执行之后，这个域的值被设置， 并且在后面的操作中， 不能够再对它进行修改</p><p>private final String name;</p><p>final如果使用的是可变对象，那么只是说它不会指向其他位置，但是当前位置的内容仍然可以改变</p><p><strong>static 静态</strong></p><p>静态域：每一个对象对于所有的实例域 有自己的一份拷贝，对于静态域只有一个，它属于类，而不属于对象，所以可以直接类.静态域也可以调用。</p><p>静态方法：它只能访问静态域和显式参数（传参）无法访问实例域，一般直接用类.方法名使用</p><p><strong>方法参数</strong></p><p>call by value：表示方法接收的是调用者提供的值</p><p>call by reference：表示方法接收的是调用者提供的变量地址</p><p>java都是按值调用！！！只是当传入的时对象时，它拷贝的是对象引用~</p><blockquote><p>public static void tripieValue(double x) // doesn’t work<br>{ x = 3 * x; }<br>double percent = 10;<br>tripieValue(percent); </p></blockquote><p>1 ) x 被初始化为 percent 值的一个拷贝（也就 是 10 ) </p><p>2 ) x 被乘以 3后等于 30。 但是 percent 仍然 是 10</p><blockquote><p>public static void tripieSalary(Employee x)<br>{ x.raiseSa1ary(200); }<br>harry = new Employee(. . .);<br>tripieSalary(harry);</p></blockquote><p>1 ) X 被初始化为 harry 值的拷贝，这里是一个对象的引用。 </p><p>2 ) raiseSalary 方法应用于这个对象引用。x 和 harry 同时引用的那个 Employee 对象的薪 金提高了 200%。 </p><p>3 ) 方法结束后，参数变量 x 不再使用。当然，对象变量 harry 继续引用那个薪金增至 3 倍的雇员对象</p><p>比如说如果你用swap想交换两个对象是不行的，因为你交换的是两个拷贝后的对象，原对象无影响。</p><p><strong>对象构造</strong></p><p>重载：相同的名字、不同的参数，便产生了重载，重载的是签名，不能出现两个相同名字，相同参数，返回值不同的方法</p><p>默认域初始化 ：被自动初始化为默认值（0、false 或 null)</p><p>构造器：只有当你不提供任何构造器的时候，系统会提供一个默认的构造器，将所有域变量初始化为默认值。</p><p>this:当作是当前类的一个指针，而且可以在构造函数里面使用this(参数)，引用另一个构造函数。</p><p><strong>调用构造器的具体处理步骤：</strong></p><p>1 ) 所有数据域被初始化为默认值（0、false 或 null)。 </p><p>2 ) 按照在类声明中出现的次序， 依次执行所有域初始化语句和初始化块</p><p>3 ) 如果构造器第一行调用了第二个构造器，则执行第二个构造器主体</p><p> 4 ) 执行这个构造器的主体.</p><p><strong>包</strong></p><ul><li>import xx.*是不会有内存影响，有时会出现两个包下函数同名冲突</li><li>编译器不会检查目录结构，如果它没有在它声明的package下，不会出现编译错误，但是最终程序无法允许，虚拟机找不到类。</li></ul><p>可见性：</p><ul><li>public 的部分可以被任意的类使用</li><li>private 的部分只能被定义它们的类使用</li><li>果没有指定 public 或 private, 这个部分（类、方法或变量）可以被同一个包中的所有方法访问</li></ul><p><strong>文档注释</strong></p><p>javadoc, 它可以由源文件生成一个 HTML 文档</p><p>javadoc 实用程序（utility) 从下面几个特性中抽取信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> •包 </span><br><span class="line"></span><br><span class="line">•公有类与接口 </span><br><span class="line"></span><br><span class="line">•公有的和受保护的构造器及方法</span><br><span class="line"></span><br><span class="line"> •公有的和受保护的域</span><br></pre></td></tr></table></figure><p>类注释要放在import语句之后，类定义之前</p><p>方法注释放在方法之前。</p><p>@param 变量描述 这个标记将对当前方法的“ param” （参数）部分添加一个条目。这个描述可以占据多 行， 并可以使用 HTML 标记。一个方法的所有@param 标记必须放在一起。 </p><p>@return 描述 这个标记将对当前方法添加“ return” （返回）部分。这个描述可以跨越多行， 并可以 使用 HTML 标记。 </p><p>©throws 类描述 这个标记将添加一个注释， 用于表示这个方法有可能抛出异常。有关异常的详细内容。</p><p>@author 姓名 这个标记将产生一个 “author” (作者）条目。可以使用多个@author 标记，每个@ author 标记对应一个作者 </p><p>@version 这个标记将产生一个“ version”（版本）条目。这里的文本可以是对当前版本的任何描 述。 下面的标记可以用于所有的文档注释中。 </p><p>@since 文本 这个标记将产生一个“ since” （始于）条目。这里的 text 可以是对引人特性的版本描 述。例如，©since version 1.7.10</p><p>/<em>*<br>@param  car  ….</em>/</p><p><strong>类设计技巧</strong></p><ol><li>一定要保证数据私有</li><li>一定要对数据初始化 </li><li>不要在类中使用过多的基本类型（抽象）</li><li>不是所有的域都需要独立的域访问器和域更改器 （状态的相互影响）</li><li>类名和方法名要能够体现它们的职责</li></ol><h2 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h2><p><strong>简单介绍</strong></p><p>public class Manager extends Employee</p><ul><li>使用extends关键字</li><li>所有继承都是公有继承，没有C++中的私有继承</li><li>使用关键字super调用父类的方法</li><li>关键字 this 有两个用途： 一是引用隐式参数，二是调用该类其他的构 造器 ， 同样，super 关键字也有两个用途：一是调用超类的方法，二是调用超类的构造器。 </li></ul><p><strong>多态</strong></p><p>一个对象变量（例如， 变量 e) 可以指示多种实际类型的现象被称为多态（polymorphism)。 在运行时能够自动地选择调用哪个方法的现象称为动态绑定（dynamic binding)。</p><p>即函数写的是父类，但是调用时传入的是不同的子类，该父类可以自动调用多种实际类的覆盖方法</p><p><strong>方法调用顺序</strong></p><p>1.编译器査看对象的声明类型和方法名。编译器将会一一列举所有 C 类中名为 f 的方法和其超类中 访问属性为 public 且名为 f 的方法（超类的私有方法不可访问）</p><p>2.编译器将査看调用方法时提供的参数类型。如果在所有名为 f 的方法中存在 一个与提供的参数类型完全匹配，就选择这个方法。</p><p>至此， 编译器已获得需要调用的方法名字和参数类型。</p><p>3.静态绑定： 如果是 private 方法、 static 方法、final 方法，编译器将可以准确地知道应该调用哪个方法</p><p>4.动态绑定：调用的方法依赖于隐式参数的实际类型，在运行时虚拟机一定调用与 x 所引用对象的实 际类型最合适的那个类的方法</p><p>每次调用方法都要进行搜索，时间开销相当大。因此，虚拟机预先为每个类创建了一个 方法表（method table), 其中列出了所有方法的签名和实际调用的方法</p><p><strong>警告</strong>：在覆盖一个方法的时候，子类方法不能低于超类方法的可见性。</p><p><strong>final 类和方法</strong> </p><p>不允许扩展的类被称为 final 类</p><p>类中的特定方法也可以被声明为 final。如果这样做，子类就不能覆盖这个方法（final 类中的所有方法自动地成为 final 方法)。写上会比较好，这样可以避免一些不必要的覆盖。</p><p>本包是指包名相同的情况下</p><p>1 ) 仅对本类可见 private。<br>2 ) 对所有类可见 public<br>3 ) 对本包和所有子类可见 protected。<br>4 ) 对本包可见— —默认（很遗憾)， 不需要修饰符</p><p><strong>Object</strong></p><p>这是所有类的超类，其有一些方法需要注意：</p><ul><li>equal 判断对象是否相等</li><li>hashCode 每个对象的储存地址</li><li>toString 返回表示对象值的字符 串</li></ul><p><strong>泛型数组列表</strong></p><p>ArrayList<employee> staff = new ArrayList<eniployee>（）; </eniployee></employee></p><p>它可以动态更改数组大小</p><p>数组列表管理着对象引用的一个内部数组。最终， 数组的全部空间有可能被用尽。这就显现出数组列表的操作魅力： 如果调用 add且内部数组已经满了，数组列表就将自动地创建 一个更大的数组，并将所有的对象从较小的数组中拷贝到较大的数组中。</p><p>有两种声明长度的方法：</p><ol><li><p>ArrayList<employee> staff = new ArrayList&lt;&gt;(100);</employee></p><p>数组这样声明，系统会为数组分配100个元素的储存空间。而数组列表这样声明，只是表示拥有保存100个元素的潜力，在最初数组列表不会含有任何元素。</p></li><li><p>ensureCapacity(100）</p><p>如果已经清楚或能够估计出数组可能存储的元素数量， 就可以在填充数组之前调用 ensureCapacity方法，会分配一个包含100个对象的内部数组。</p></li></ol><p>API方法：</p><p>add：添加新元素</p><p>set：覆盖旧元素</p><p>get: 获取元素</p><p>remove:删除元素</p><p>注意对这类型的数组中间元素进行插入和删除，过程会比较费时</p><p>类型化与原始数组列表的兼容性<br>　　可以将一个类型化的数组列表传递给update方法，而不需要进行任何类型转换：即只要是ArrayList类对象，不管是什么具体类型化都可以编译通过。</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ArrayList&lt;Employee&gt; staff = ...<span class="comment">;</span></span><br><span class="line">employeeDB.update(staff)<span class="comment">;</span></span><br></pre></td></tr></table></figure><p>　　但是，如果将一个原始的没有类型化的ArrayList赋值给一个类型化ArrayList就会得到警告：因为得到的可能不是Employee类的类型的数组列表</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ArrayList&lt;Employee&gt; result = employeeDB.<span class="builtin-name">find</span>(<span class="built_in">..</span><span class="built_in">..</span>);</span><br></pre></td></tr></table></figure><p>　　这个时候可以使用强制类型转换：这个时候会得到另外一个警告，因为虚拟机中没有类型参数</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ArrayList&lt;Employee&gt; result = (ArrayList&lt;Employee&gt;)employeeDB.<span class="builtin-name">find</span>(<span class="built_in">..</span><span class="built_in">..</span>);</span><br></pre></td></tr></table></figure><p>　　这个时候，如果确保不会造成严重的后果，可以使用@SuppressWarnings(“unchecked”)来标记这个变量能够接受类型转换：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">@<span class="keyword">SuppressWarnings</span>("<span class="keyword">unchecked</span>")</span><br><span class="line"><span class="keyword">ArrayList</span>&lt;<span class="keyword">Employee</span>&gt; result = (ArrayList&lt;Employee&gt;)employeeDB.find(....);</span><br></pre></td></tr></table></figure><p>201</p><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><ul><li>要永远记住浮点数的计算是会有误差，永远都无法精确，所以不要使用等于这个关系符</li><li>null的坑要填，判断的时候要永远记得空和长度为零都要同时写上。</li><li>非常小心可变对象的修改。’</li><li>类是类，对象是new，对象变量是引用</li></ul><p>疑惑</p><ul><li>声明类型未赋值后是Null，未声明类型是什么，内存的占用情况(未赋值时引用，占的时引用内存)</li><li>可变对象是什么</li><li>类初始化的执行顺序</li><li>类的搜索顺序</li><li>javadoc是否需要掌握使用</li><li>java.util和java.lang要新开一章</li><li>保护机制</li></ul><h2 id="奇怪的地方"><a href="#奇怪的地方" class="headerlink" title="奇怪的地方"></a>奇怪的地方</h2><p>class Employee {<br>public boolean equals(Employee other)<br> { return name.equals(other.name); }<br>}<br>if (harry,equals(boss))<br>方法可以访问所调用对象的私有数据。一个方法可以访问所属类的所有 对象的私有数据</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;java基础知识&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="java" scheme="http://kodgv.xyz/categories/java/"/>
    
    
      <category term="java" scheme="http://kodgv.xyz/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>elo比赛心得</title>
    <link href="http://kodgv.xyz/2019/04/30/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/elo%E6%AF%94%E8%B5%9B%E5%BF%83%E5%BE%97/"/>
    <id>http://kodgv.xyz/2019/04/30/竞赛经验/elo比赛心得/</id>
    <published>2019-04-30T11:30:59.000Z</published>
    <updated>2019-05-25T12:36:31.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="比赛简介"><a href="#比赛简介" class="headerlink" title="比赛简介"></a>比赛简介</h2><ul><li>回归问题</li><li><p>有异常点</p></li><li><p>商品信息，</p></li><li>每个用户的刷卡时间。</li><li>预测用户的忠诚度。</li></ul><a id="more"></a><p>[TOC]</p><h2 id="11th-place-solution"><a href="#11th-place-solution" class="headerlink" title="11th place solution"></a><a href="https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/82127#latest-502682" target="_blank" rel="noopener">11th place solution</a></h2><p><strong>FEATURE ENGINEERING</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> I refer the Kaggle Rank System Compute Formula（link:[https://www.kaggle.com/progression][<span class="number">4</span>])</span><br><span class="line">    df_data[<span class="string">'duration_sqrt_counts'</span>] = df_data[<span class="string">'durations'</span>]/sqrt(df_data[<span class="string">'card_id_counts'</span>])</span><br><span class="line">    df_data[<span class="string">'duration_log1p_counts'</span>] = df_data[<span class="string">'durations'</span>]/log1p(df_data[<span class="string">'card_id_counts'</span>])</span><br><span class="line">    df_data[<span class="string">'duration_counts'</span>] = df_data[<span class="string">'durations'</span>]/df_data[<span class="string">'card_id_counts'</span>]</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> Categorical features: frequence, Maxfrequence, MaxfrequenceRatio</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> card_id/merchant_id/mechant_category_id/city_id (visit sequence to sequence embedding)</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> purchase_amount:hist/new</span><br><span class="line"></span><br><span class="line"><span class="number">5.</span> features interactions between hist/new</span><br><span class="line">            df[<span class="string">'purchase_amount_ratio_v3'</span>] =                              df[<span class="string">'new_purchase_amount_max'</span>]/df[<span class="string">'hist_purchase_amount_sum'</span>]</span><br><span class="line">            df[<span class="string">'purchase_amount_diff_v1'</span>] = df[<span class="string">'new_purchase_amount_sum'</span>]-df[<span class="string">'hist_purchase_amount_sum'</span>]</span><br><span class="line">            df[<span class="string">'purchase_amount_diff_v2'</span>] = df[<span class="string">'new_purchase_amount_mean'</span>]-df[<span class="string">'hist_purchase_amount_mean'</span>]</span><br><span class="line">            df[<span class="string">'purchase_amount_diff_v3'</span>] = df[<span class="string">'new_purchase_amount_max'</span>]-df[<span class="string">'hist_purchase_amount_max'</span>]</span><br><span class="line">            df[<span class="string">'purchase_amount_diff_v4'</span>] = df[<span class="string">'new_purchase_amount_min'</span>]-df[<span class="string">'hist_purchase_amount_min'</span>]</span><br><span class="line">            df[<span class="string">'pa_mlag_ratio'</span>] = df[<span class="string">'new_purchase_amount_sum'</span>]/(df[<span class="string">'month_lag_mean'</span>] - <span class="number">1</span>)</span><br><span class="line">            df[<span class="string">'pa_new_hist_ratio'</span>] = df[<span class="string">'new_purchase_amount_sum'</span>]/(df[<span class="string">'hist_purchase_amount_sum'</span>])</span><br><span class="line">            df[<span class="string">'pa_new_hist_mean_ratio'</span>] = df[<span class="string">'new_purchase_amount_mean'</span>]/(df[<span class="string">'hist_purchase_amount_mean'</span>] )</span><br><span class="line">            df[<span class="string">'pa_new_hist_min_ratio'</span>] = df[<span class="string">'new_purchase_amount_min'</span>]/(df[<span class="string">'hist_purchase_amount_min'</span>] )</span><br><span class="line">            df[<span class="string">'pa_new_hist_max_ratio'</span>] = df[<span class="string">'new_purchase_amount_max'</span>]/(df[<span class="string">'hist_purchase_amount_max'</span>] )</span><br></pre></td></tr></table></figure><p>我们有两个单独的feature sets。一个具有+1000个feature ，另一个具有+200个feature </p><p>然后，我们取+200特征集的相关矩阵，<strong>将每个特征与其关联最小的特征配对。然后我们对每一对应用了大量的聚合</strong>，结果得到了非常强大的特征。</p><p>所以我们最终得到了两个功能集，每个功能集+1000个功能。”</p><p><strong>STACKING</strong></p><p>We stacked around 32 models using bayesian regression. Our models were well varied that it yielded a score of CV:3.630X LB :3.675</p><p><strong>STUFF THAT DID NOT WORK</strong></p><p>Of course these last two months were not all roses and rainbows. We pulled our hair trying a lot of things and we failed miserably.</p><p>Here are the bloopers of our participation :D :</p><ul><li>NN. We tried designing different architectures with the main focus on having a simple NN with heavy regularization (BatchNorm and Strong Dropout)</li><li>In the middle of the competition, we tried tackling the outliers detection as an anomaly detection problem using AutoEncoders trained only on the non outliers data</li><li>We tried PCA for more features. And it didn’t work</li><li>We tried TSNE. It didn’t work</li><li>We tried FM and FFM. It did not work</li><li>We tried isolation forest. Nope. Did not work.</li><li>We had a Ridge-based pairwise ranker that we intended to use for outliers detection but it didn’t match with the approach we had.</li><li>We tried a lot of weak models in the hope of adding diversity (simple tree-based, linear, svm, etc.). And guess what? It did not work.</li></ul><p>如何识别异常点？</p><p>两个不同的特征集怎么做？</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;比赛简介&quot;&gt;&lt;a href=&quot;#比赛简介&quot; class=&quot;headerlink&quot; title=&quot;比赛简介&quot;&gt;&lt;/a&gt;比赛简介&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;回归问题&lt;/li&gt;
&lt;li&gt;&lt;p&gt;有异常点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;商品信息，&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;每个用户的刷卡时间。&lt;/li&gt;
&lt;li&gt;预测用户的忠诚度。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>后端学习历程</title>
    <link href="http://kodgv.xyz/2019/04/30/JAVA%E5%AD%A6%E4%B9%A0/java%E5%8F%82%E8%80%83%E8%B7%AF%E7%BA%BF/"/>
    <id>http://kodgv.xyz/2019/04/30/JAVA学习/java参考路线/</id>
    <published>2019-04-30T11:30:59.000Z</published>
    <updated>2019-05-25T12:32:49.938Z</updated>
    
    <content type="html"><![CDATA[<p>java学习路线与推荐<br><a id="more"></a></p><h2 id="6个月-Java-服务端入门和进阶指南"><a href="#6个月-Java-服务端入门和进阶指南" class="headerlink" title="6个月 Java 服务端入门和进阶指南"></a>6个月 Java 服务端入门和进阶指南</h2><p><a href="https://www.zhihu.com/question/29581524" target="_blank" rel="noopener">https://www.zhihu.com/question/29581524</a><br><img src="/2019/04/30/JAVA学习/java参考路线/1" alt="1557919436614"></p><p><img src="/2019/04/30/JAVA学习/java参考路线/2" alt="1557919469076"></p><p><img src="/2019/04/30/JAVA学习/java参考路线/3" alt="1557919495443"></p><p>资料参考，先学第一阶段，学完再来看第二阶段的任务吧</p><p><a href="https://www.zhihu.com/question/22340525" target="_blank" rel="noopener">https://www.zhihu.com/question/22340525</a></p><p><a href="https://www.zhihu.com/question/307096748/answers/updated" target="_blank" rel="noopener">https://www.zhihu.com/question/307096748/answers/updated</a></p><p><a href="https://mp.weixin.qq.com/s/GxIRqj8s1zrnnPSOJ0203Q" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/GxIRqj8s1zrnnPSOJ0203Q</a></p><p><a href="https://zhuanlan.zhihu.com/p/34880504" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34880504</a></p><p><a href="https://h2pl.github.io/" target="_blank" rel="noopener">后端校招以及大牛成长转折点</a></p><p><a href="https://www.nowcoder.com/discuss/16124" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/16124</a></p><p>知识点总结网站</p><p><a href="https://github.com/TransientWang/KnowledgeBase" target="_blank" rel="noopener">https://github.com/TransientWang/KnowledgeBase</a></p><p><a href="https://github.com/Snailclimb/JavaGuide" target="_blank" rel="noopener">https://github.com/Snailclimb/JavaGuide</a></p><h2 id="自己整理"><a href="#自己整理" class="headerlink" title="自己整理"></a>自己整理</h2><h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3><p><strong>JAVA基础的学习</strong></p><p>《Java 核心技术：卷1 基础知识》(或者《Java 编程思想》)必看</p><p>《Effective Java》</p><p>java.util包和java.lang包</p><p><a href="http://how2j.cn/" target="_blank" rel="noopener">做题网站</a></p><p><strong>代码规范</strong></p><p>《Git 权威指南》 <a href="https://link.zhihu.com/?target=https%3A//learngitbranching.js.org/" target="_blank" rel="noopener">Learn Git Branching通关网站</a></p><p>《Maven 实战》</p><p><a href="https://link.zhihu.com/?target=https%3A//book.douban.com/subject/4262627/" target="_blank" rel="noopener">《重构_改善既有代码的设计》</a></p><p>学习代码规范。我们大致上遵循 oracle 的 Java 语言编码规范，你可以先阅读并熟悉它。Code Formatting 文件在 git@xxx/coding-standard.git，在编写代码之前，请把它导入到 IDE 中。另外，确认 IDE 已经安装 Findbugs 和 CheckStyle 插件。</p><p><strong>开发工具</strong></p><ul><li>熟练使用一种 IDE。Intellij IDEA或者 Eclipse 都可以，推荐使用前者。至少熟悉常用的快捷键，会 debug(包括远程 debug)项目。</li><li>熟悉一种编辑器。比如 Vim/Emacs/Sublime Text，至少学会搜索/替换/代码补全。</li></ul><p><strong>开发环境</strong></p><p>Linux 的基本使用可以通过《鸟哥的Linux私房菜：基础学习篇（第三版）》学习</p><p> bash shell 脚本可以参考《Linux Shell脚本攻略》</p><h2 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h2><p>刷题：</p><p>先从简单的图解算法看起，然后做题的时候不要做太多，做别人整理好的经典题比较重要。TopCoder，codeforces。然后注意题要抄在本子上，便于作弊，注意它们说的板子。</p><p>图解HTTP协议和servlet。再JAVA进阶，然后框架和数据库并行。</p><p>设计模式要优先于框架，这样才能更好掌握</p><p>需要掌握三大框架SSM，需要掌握各类数据库，需要掌握各类协议</p><p>设计模式</p><p>学Unix</p><p>基本的单元测试</p><p><strong>JAVA进阶</strong></p><ul><li><p><strong>《Java并发编程的艺术》《深入理解Java虚拟机》</strong></p></li><li><p>并发编程网：<strong>并发编程网 - ifeve.com</strong> 重点掌握java内存模型，各种锁的原理及应用，JVM GC垃圾回收原理。</p></li></ul><h2 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h2><p>后端校招以及大牛成长转折点</p><p>源码</p><p>微服务</p><p>微架构</p><p>各种组件</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;java学习路线与推荐&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="java" scheme="http://kodgv.xyz/categories/java/"/>
    
    
      <category term="java" scheme="http://kodgv.xyz/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>比赛常用图</title>
    <link href="http://kodgv.xyz/2019/04/29/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E6%AF%94%E8%B5%9B%E5%B8%B8%E7%94%A8%E5%9B%BE/"/>
    <id>http://kodgv.xyz/2019/04/29/数据竞赛/比赛常用图/</id>
    <published>2019-04-29T08:30:12.000Z</published>
    <updated>2019-05-25T12:41:02.929Z</updated>
    
    <content type="html"><![CDATA[<p>比赛常见的EDA总结</p><a id="more"></a><p><strong>要注意把以后整理函数的时候把图补上</strong></p><p>plt.show()会阻碍当前程序的执行，请再最后执行<br>如果想要不阻碍请执行plt.ion()，但是这样当程序结束时会自动关闭当前图像，所以需要再最后执行plt.ioff()以阻碍图像不被关闭</p><p>[TOC]</p><h2 id="曲线分布图"><a href="#曲线分布图" class="headerlink" title="曲线分布图"></a>曲线分布图</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/gpreda/santander-eda-and-prediction</span></span><br><span class="line">def plot_feature_distribution(df1, df2, label1, label2, features):</span><br><span class="line">    <span class="attr">i</span> = <span class="number">0</span></span><br><span class="line">    sns.set_style('whitegrid')</span><br><span class="line">    plt.figure()</span><br><span class="line">    fig, <span class="attr">ax</span> = plt.subplots(<span class="number">10</span>,<span class="number">10</span>,<span class="attr">figsize=(18,22))</span></span><br><span class="line"></span><br><span class="line">    for feature <span class="keyword">in</span> features:</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        plt.subplot(<span class="number">10</span>,<span class="number">10</span>,i)</span><br><span class="line">        sns.distplot(df1[feature], <span class="attr">hist=False,label=label1)</span></span><br><span class="line">        sns.distplot(df2[feature], <span class="attr">hist=False,label=label2)</span></span><br><span class="line">        plt.xlabel(feature, <span class="attr">fontsize=9)</span></span><br><span class="line">        locs, <span class="attr">labels</span> = plt.xticks()</span><br><span class="line">        plt.tick_params(<span class="attr">axis='x',</span> <span class="attr">which='major',</span> <span class="attr">labelsize=6,</span> <span class="attr">pad=-6)</span></span><br><span class="line">        plt.tick_params(<span class="attr">axis='y',</span> <span class="attr">which='major',</span> <span class="attr">labelsize=6)</span></span><br><span class="line">    plt.show();</span><br><span class="line">    </span><br><span class="line"><span class="attr">t0</span> = train_df.loc[train_df['target'] == <span class="number">0</span>]</span><br><span class="line"><span class="attr">t1</span> = train_df.loc[train_df['target'] == <span class="number">1</span>]</span><br><span class="line"><span class="attr">features</span> = train_df.columns.values[<span class="number">2</span>:<span class="number">102</span>]</span><br><span class="line">plot_feature_distribution(t0, t1, '<span class="number">0</span>', '<span class="number">1</span>', features)</span><br></pre></td></tr></table></figure><p><img src="/2019/04/29/数据竞赛/比赛常用图/2.png" alt></p><h2 id="条形分布图"><a href="#条形分布图" class="headerlink" title="条形分布图"></a>条形分布图</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.<span class="built_in">title</span>(<span class="string">"Distribution of mean values per column in the train and test set"</span>)</span><br><span class="line">sns.distplot(train_df[<span class="built_in">features</span>].<span class="built_in">mean</span>(axis=<span class="number">0</span>),<span class="built_in">color</span>=<span class="string">"magenta"</span>,kde=True,bins=<span class="number">120</span>, <span class="built_in">label</span>='train')</span><br><span class="line">sns.distplot(test_df[<span class="built_in">features</span>].<span class="built_in">mean</span>(axis=<span class="number">0</span>),<span class="built_in">color</span>=<span class="string">"darkblue"</span>, kde=True,bins=<span class="number">120</span>, <span class="built_in">label</span>='test')</span><br><span class="line">plt.<span class="built_in">legend</span>()</span><br><span class="line">plt.<span class="built_in">show</span>()</span><br></pre></td></tr></table></figure><p><img src="/2019/04/29/数据竞赛/比赛常用图/2.png" alt></p><h2 id="条形图"><a href="#条形图" class="headerlink" title="条形图"></a>条形图</h2><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#画条形图</span></span><br><span class="line">sns.barplot<span class="params">(<span class="attr">x</span>='Sex', <span class="attr">y</span>='Survived', <span class="attr">data</span>=train)</span>;<span class="comment"># seaborn 的 barplot() 利用矩阵条的高度反映数值变量的集中趋势，展示的是变量的平均值</span></span><br><span class="line">sns.barplot<span class="params">(<span class="attr">x</span>='Sex', <span class="attr">y</span>='Survived', <span class="attr">hue</span> = 'Pclass', <span class="attr">data</span>=train)</span>;<span class="comment">#加了图例的功能</span></span><br></pre></td></tr></table></figure><h2 id="相关性热度图"><a href="#相关性热度图" class="headerlink" title="　相关性热度图"></a>　相关性热度图</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">colormap</span> = plt.cm.RdBu</span><br><span class="line">plt.figure(<span class="attr">figsize=(14,12))</span></span><br><span class="line">plt.title('Pearson Correlation of Features', <span class="attr">y=1.05,</span> <span class="attr">size=15)</span></span><br><span class="line">sns.heatmap(train.astype(float).corr(),<span class="attr">linewidths=0.1,vmax=1.0,</span> </span><br><span class="line">            <span class="attr">square=True,</span> <span class="attr">cmap=colormap,</span> <span class="attr">linecolor='white',</span> <span class="attr">annot=True)</span></span><br></pre></td></tr></table></figure><h2 id="多变量相关性图"><a href="#多变量相关性图" class="headerlink" title="　多变量相关性图"></a>　多变量相关性图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多变量相关性图 要注意哦这里的变量所有的成对一一对应的</span></span><br><span class="line">g = sns.pairplot(train[[<span class="string">u'Survived'</span>, <span class="string">u'Pclass'</span>, <span class="string">u'Sex'</span>, <span class="string">u'Age'</span>, <span class="string">u'Parch'</span>, <span class="string">u'Fare'</span>, <span class="string">u'Embarked'</span>,</span><br><span class="line">       <span class="string">u'FamilySize'</span>, <span class="string">u'Title'</span>]], hue=<span class="string">'Survived'</span>, palette = <span class="string">'seismic'</span>,size=<span class="number">1.2</span>,diag_kind = <span class="string">'kde'</span>,diag_kws=dict(shade=<span class="literal">True</span>),plot_kws=dict(s=<span class="number">10</span>) )</span><br></pre></td></tr></table></figure><h2 id="箱图"><a href="#箱图" class="headerlink" title="箱图"></a>箱图</h2><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">''<span class="symbol">'Create</span> a <span class="keyword">function</span> <span class="keyword">to</span> count total outliers. <span class="keyword">And</span> plot variables <span class="keyword">with</span> <span class="keyword">and</span> without outliers.'''</span><br><span class="line">def outliers(<span class="keyword">variable</span>):</span><br><span class="line">    # Calculate <span class="number">1</span>st, <span class="number">3</span>rd quartiles <span class="keyword">and</span> iqr.</span><br><span class="line">    q1, q3 = <span class="keyword">variable</span>.quantile(<span class="number">0.25</span>), <span class="keyword">variable</span>.quantile(<span class="number">0.75</span>)</span><br><span class="line">    iqr = q3 - q1</span><br><span class="line">    </span><br><span class="line">    # Calculate lower fence <span class="keyword">and</span> upper fence <span class="keyword">for</span> outliers</span><br><span class="line">    l_fence, u_fence = q1 - <span class="number">1.5</span>*iqr , q3 + <span class="number">1.5</span>*iqr   # Any values less than l_fence <span class="keyword">and</span> greater than u_fence are outliers.</span><br><span class="line">    </span><br><span class="line">    # Observations that are outliers</span><br><span class="line">    outliers = <span class="keyword">variable</span>[(<span class="keyword">variable</span>&lt;l_fence) | (<span class="keyword">variable</span>&gt;u_fence)]</span><br><span class="line">    print(<span class="symbol">'Total</span> Outliers <span class="keyword">of</span>', <span class="keyword">variable</span>.name,':', outliers.count())</span><br><span class="line">    </span><br><span class="line">    # Drop obsevations that are outliers</span><br><span class="line">    filtered = <span class="keyword">variable</span>.drop(outliers.index, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    # Create subplots</span><br><span class="line">    fig, (ax1, ax2) = plt.subplots(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    # Gives space between two subplots</span><br><span class="line">    fig.subplots_adjust(hspace = <span class="number">1</span>) </span><br><span class="line">    </span><br><span class="line">    # Plot <span class="keyword">variable</span> <span class="keyword">with</span> outliers</span><br><span class="line">    <span class="keyword">variable</span>.plot.box(vert = <span class="literal">False</span>, color = <span class="symbol">'coral</span>', grid = <span class="literal">False</span>, ax = ax1, title = <span class="symbol">'Distribution</span> <span class="keyword">with</span> Outliers <span class="keyword">for</span> %s' %<span class="keyword">variable</span>.name)</span><br><span class="line"></span><br><span class="line">    # Plot <span class="keyword">variable</span> without outliers</span><br><span class="line">    filtered.plot.box(vert = <span class="literal">False</span>, color = <span class="symbol">'coral</span>', grid = <span class="literal">False</span>, ax = ax2, title = <span class="symbol">'Distribution</span> without Outliers <span class="keyword">for</span> %s' %<span class="keyword">variable</span>.name)</span><br></pre></td></tr></table></figure><h2 id="刻画变量的不平衡度"><a href="#刻画变量的不平衡度" class="headerlink" title="刻画变量的不平衡度"></a>刻画变量的不平衡度</h2><p>If skewness is less than −1 or greater than +1, the distribution can be considered as highly skewed.</p><p>If skewness is between −1 and −½ or between +½ and +1, the distribution can be considered as moderately skewed.</p><p>And finally if skewness is between −½ and +½, the distribution can be considered as approximately symmetric.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''#2.Density plot with skewness.'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">density_plot_and_skewness</span><span class="params">(variable)</span>:</span></span><br><span class="line">    variable.plot.hist(density = <span class="literal">True</span>)</span><br><span class="line">    variable.plot.kde(style = <span class="string">'k--'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'%s'</span>%variable.name)</span><br><span class="line">    plt.title(<span class="string">'Distribution of %s with Density Plot &amp; Histogram'</span> %variable.name)</span><br><span class="line">    print(<span class="string">'Skewness of '</span>, variable.name, <span class="string">':'</span>)</span><br><span class="line">    skewness = variable.skew()</span><br><span class="line">    <span class="keyword">return</span> display(skewness)</span><br></pre></td></tr></table></figure><h2 id="分类变量和分类变量的关系图"><a href="#分类变量和分类变量的关系图" class="headerlink" title="分类变量和分类变量的关系图"></a>分类变量和分类变量的关系图</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#############################<span class="number">2</span>X2列联表展示########################################</span><br><span class="line"><span class="string">''</span><span class="string">'#1.Create a function that calculates absolute and relative frequency of Survived variable by a categorical variable. And then plots the absolute and relative frequency of Survived by a categorical variable.'</span><span class="string">''</span></span><br><span class="line">def crosstab(cat, cat_target):</span><br><span class="line">    <span class="string">''</span><span class="string">'cat = categorical variable, cat_target = our target categorical variable.'</span><span class="string">''</span></span><br><span class="line">    global ax, ax1</span><br><span class="line">    cat_grouped_by_cat_target = pd.crosstab(index = cat, columns = cat_target)</span><br><span class="line">    cat_grouped_by_cat_target.rename(&#123;<span class="number">0</span>:<span class="string">'Victims'</span>, <span class="number">1</span>:<span class="string">'Survivors'</span>&#125;, axis = <span class="string">'columns'</span>, inplace = <span class="literal">True</span>)  # Renaming the columns</span><br><span class="line">    pct_cat_grouped_by_cat_target = round(pd.crosstab(index = cat, columns = cat_target, normalize = <span class="string">'index'</span>)*<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">    pct_cat_grouped_by_cat_target.rename(&#123;<span class="number">0</span>:<span class="string">'Victims(%)'</span>, <span class="number">1</span>:<span class="string">'Survivors(%)'</span>&#125;, axis = <span class="string">'columns'</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    print(<span class="string">'Survivals and Deaths by'</span>, cat.name,<span class="string">':'</span>, <span class="string">'\n'</span>,cat_grouped_by_cat_target )</span><br><span class="line">    print(<span class="string">'\nPercentage Survivals and Deaths by'</span>, cat.name, <span class="string">':'</span>,<span class="string">'\n'</span>, pct_cat_grouped_by_cat_target)</span><br><span class="line">    </span><br><span class="line">    # Plot absolute frequency <span class="keyword">of</span> Survived by a categorical variable</span><br><span class="line">    ax =  cat_grouped_by_cat_target.plot.bar(color = [<span class="string">'r'</span>, <span class="string">'g'</span>])</span><br><span class="line">    plt.title(<span class="string">'Survival vs Death Count by %s'</span> %cat.name)</span><br><span class="line">    abs_bar_labels()</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    # Plot relative frequrncy <span class="keyword">of</span> Survived by a categorical variable</span><br><span class="line">    ax1 = pct_cat_grouped_by_cat_target.plot.bar(color = [<span class="string">'r'</span>, <span class="string">'g'</span>])</span><br><span class="line">    plt.title(<span class="string">'Percentage Survival vs Death Count by %s'</span> %cat.name)</span><br><span class="line">    pct_bar_labels()</span><br><span class="line">    plt.show()</span><br><span class="line">###########################################卡方检验#######################################</span><br><span class="line"><span class="string">''</span><span class="string">'#2.Create a function to calculate chi_square test between a categorical and target categorical variable.'</span><span class="string">''</span></span><br><span class="line">def chi_square(cat, cat_target):</span><br><span class="line">    cat_grouped_by_cat_target = pd.crosstab(index = cat, columns = cat_target)</span><br><span class="line">    test_result = stats.chi2_contingency (cat_grouped_by_cat_target)</span><br><span class="line">    print(<span class="string">'Chi_square test result between Survived &amp; %s'</span> %cat.name)</span><br><span class="line">    return display(test_result)</span><br><span class="line"></span><br><span class="line">#############################################bonferroni adjusted检验###############################</span><br><span class="line"><span class="string">''</span><span class="string">'#3.Finally create another function to calculate Bonferroni-adjusted pvalue for a categorical and target categorical variable.'</span><span class="string">''</span></span><br><span class="line">def bonferroni_adjusted(cat, cat_target):</span><br><span class="line">    dummies = pd.get_dummies(cat)</span><br><span class="line">    for columns <span class="keyword">in</span> dummies:</span><br><span class="line">        crosstab = pd.crosstab(dummies[columns], cat_target)</span><br><span class="line">        print(stats.chi2_contingency(crosstab))</span><br><span class="line">    print(<span class="string">'\nColumns:'</span>, dummies.columns)</span><br></pre></td></tr></table></figure><h2 id="多个变量组合对因变量的影响图"><a href="#多个变量组合对因变量的影响图" class="headerlink" title="多个变量组合对因变量的影响图"></a>多个变量组合对因变量的影响图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''Create a function that plots the impact of 3 predictor variables at a time on a target variable.'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multivariate_analysis</span><span class="params">(cat1, cat2, cat3, cat_target)</span>:</span></span><br><span class="line">    grouped = round(pd.crosstab(index = [cat1, cat2, cat3], columns = cat_target, normalize = <span class="string">'index'</span>)*<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">    grouped.rename(&#123;<span class="number">0</span>:<span class="string">'Died%'</span>, <span class="number">1</span>:<span class="string">'Survived%'</span>&#125;, axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    ax = grouped.plot.bar(color = [<span class="string">'r'</span>, <span class="string">'g'</span>])</span><br><span class="line">    plt.ylabel(<span class="string">'Relative Frequency (%)'</span>)</span><br></pre></td></tr></table></figure><h2 id="热度图"><a href="#热度图" class="headerlink" title="热度图"></a>热度图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920</span></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">sns.heatmap(x, cmap=<span class="string">'RdBu_r'</span>, center=<span class="number">0.0</span>) </span><br><span class="line">plt.title(<span class="string">'VAR_'</span>+str(j)+<span class="string">' Predictions without Magic'</span>,fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xticks(np.linspace(<span class="number">0</span>,<span class="number">49</span>,<span class="number">5</span>),np.round(np.linspace(mn,mx,<span class="number">5</span>),<span class="number">1</span>))</span><br><span class="line">plt.xlabel(<span class="string">'Var_'</span>+str(j))</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.ylabel(<span class="string">''</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190412150953124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NjYxMjMw,size_16,color_FFFFFF,t_70" alt></p><p>通过热度图可以看出模型是否明显利用了两个特征。虽然可能特征重要度也会说明，但是这样的热度图明显会更具说服力，比如说var_0，模型可以得出竖线15，横线4之类的分界线。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛常见的EDA总结&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="预处理" scheme="http://kodgv.xyz/tags/%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>迁移学习</title>
    <link href="http://kodgv.xyz/2019/04/29/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    <id>http://kodgv.xyz/2019/04/29/数据竞赛/迁移学习/</id>
    <published>2019-04-29T07:56:28.000Z</published>
    <updated>2019-05-25T12:40:41.962Z</updated>
    
    <content type="html"><![CDATA[<p>待完善</p><a id="more"></a><p>！！！说不一定可以切分原数据集为新旧数据集</p><ul><li>新旧特征：新数据集为1，旧数据集为0</li><li>合并新旧数据：合并新旧来做特征处理然后有两种操作<br>第一种是用旧训练模型，用模型预测新数据集的train和test概率，然后加在后面做特征，然后在用新数据重新做特征处理然后预测（这个比较好）。 第二种是直接上新旧合并数据训练的模型，然后直接加模型概率做新特征（这个可能会leak，但是在目前的模板上有cv应该会好一点）然后在用新数据重新做特征处理然后预测</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;待完善&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>去除重复值</title>
    <link href="http://kodgv.xyz/2019/04/29/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/%E5%8E%BB%E9%99%A4%E9%87%8D%E5%A4%8D%E5%80%BC/"/>
    <id>http://kodgv.xyz/2019/04/29/竞赛经验/去除重复值/</id>
    <published>2019-04-29T07:23:45.000Z</published>
    <updated>2019-04-29T07:24:35.299Z</updated>
    
    <content type="html"><![CDATA[<p>不去重的数据训练的模型 = 80%的非重复样本准确率 + 100%重复样本准确率<br>去重的数据训练的模型 = 85%的非重复样本准确率 + 85%重复样本准确率（直接规则覆盖后100%）<br>深度提取重复样本的真正目的是删除它们以提高模型预测非重复样本的准确率，非重复样本的准确率才是对业务有价值的</p><a id="more"></a><p><strong>1.Giba的home credit 里的重复样本</strong></p><p>如果八月份参加过kaggle的home credit比赛的一定知道，在最后一周才加入比赛的GM Giba通过丰富的比赛经验找到了重复样本trick帮助onodera队伍拿到了比赛的第二名，而另一位GM raddar在最后八个小时加入比赛就直接发现了重复样本问题，并solo到了45名。今天就先来分析一下他们的思路，<a href="https://link.zhihu.com/?target=https%3A//www.kaggle.com/raddar/a-competition-without-a-leak-or-is-it" target="_blank" rel="noopener">A competition without a leak. Or is it?</a></p><p>其实说起来简单，在home credit比赛里，每个客户都有多达两百列特征，但是人的基本特征是不会变的，性别、年龄、生日、银行开户时间等等。打个比方，尽管北京有三千万人，但是如果两样本同一天生日，同一天领驾照，办身份证，同一天结婚，同一天生娃，那么从统计上来说，这两样本肯定就是一个人，而不必再核对身高血型样貌了。radder 就是仅用[DAYS_BIRTH,DAYS_EMPLOYED,DAYS_REGISTRATION,DAYS_ID_PUBLISH,CODE_GENDER,REGION_POPULATION_RELATIVE]六个维度的特征，就把数据集里的重复样本给找出来了，其实其它两百列特征是否一样已经不重要了。</p><p><strong>小结</strong>：虽然我们在比赛里这个称之为trick，但背后是统计学意义的，也可以应用在工作之中。</p><p><strong>2.活学活用</strong></p><p>在我们最近参加的一个关于通讯用户套餐的比赛里，我们的GM piupiu也提到了这个trick。因为出题方把用户的流量消费精确到了byte，金钱消费精确到了分，通讯时间精确到了秒，所以如果用户A上月花了45.32元，用了1545MB 457kb 123byte的流量，打了78分12秒的电话，用户B上月也花了45.32元，用了1545MB 457kb 123byte的流量，打了78分12秒的电话，别管这家通讯公司用户量有多大，A和B肯定是同一用户。所以在其他队伍还在用给定的全部特征判定重复样本的时候，我们用八个特征就可以判定重复样本，把test里和train重复的样本给挑出来了。</p><p><strong>吐槽：</strong>因而我们得到了比其他队伍更多test里的重复样本，然后定义了白名单（piupiu的职业习惯）。在我们代码开源后有一小撮萌新看不懂我们的代码不说，还觉得是piupiu把竞赛网站黑了拿到test的label（滑稽）</p><p><strong>3.进阶</strong></p><p>有人会说了，拿到了test里的重复的样本的label，是不是可以上一波分了～～</p><p>答案是：<strong>并不能。</strong>因为xgb/lgb是具备非常强大的拟合能力，你不刻意找这些重复样本，xgb/lgb也能给你学出来，通过比较你会发现test里的重复样本已经全部给你预测对了。在home credit里是有重复用户的时序信息在里面可以利用，但是在我们这个比赛及大部分比赛就用不上了。</p><p>回到这个比赛，重复样本达到了全部数据的15%，test里的重复样本都预测对了，说明模型把train/test之间的重复样本的特性全学会了，有时候xgb/lgb学习能力太强也不是好事。这时候我们的GM piupiu提到了，test其实可以分为重复样本（15%）和非重复样本两部分（85%）来看待，真正对业务有意义的是非重复样本，可是所有参赛者却把重复样本过拟合的很好，我们真的目的应该是为出题方提供一个有效预测非重复样本的模型，因此提议我们应该把train里的重复样本去掉。打个比方，<strong>valid数据包含重复的和不重复两部分，不去重的数据训练的模型需要1000轮才能达到最优，去重的模型之后500轮最优，那说明有500轮是在过拟合重复样本，但这对于不重复样本来说是一个严重的过拟合，伤害了模型的泛化能力</strong>。而这个通讯比赛的重复样本达到了15%,对模型伤害很大。如果再根据包含重复样本的数据模型调参和特征工程，伤害就更大了。</p><p>简单的打个比方：</p><p>不去重的数据训练的模型 = 80%的非重复样本准确率 + 100%重复样本准确率</p><p>去重的数据训练的模型 = 85%的非重复样本准确率 + 85%重复样本准确率（直接规则覆盖后100%）</p><p>如果你要模型上线，你会上线哪个模型？</p><p>重复样本的准确率并不重要，无论是实际业务还是比赛，一个sql就搞定了。</p><p>根据piupiu的建议，于是我照着做了，果然模型变得特别稳定，也没有其他队伍所提到的抖动也消除了，提升的分数刚好是我们最后领先第二名的差距。</p><p>所以在这个比赛里，<strong>深度提取重复样本的真正目的是删除它们以提高模型预测非重复样本的准确率，非重复样本的准确率才是对业务有价值的。</strong></p><p>有些选手因为看到test里有重复样本就舍不得删的train里的重复样本，难道不用lgb预测的结果就不是结果了吗（流汗）</p><p><strong>小结：</strong>虽然第二节一直在说怎么更多的提取重复样本，但我们真正提升的是模型预测非重复样本的预测能力。</p><p><strong>吐槽：</strong>在我们代码开源后，有一小撮萌新看到我们在lgb输出的结果通过规则覆盖能上好多分，就觉得我们是在用重复样本的leak提升重复样本准确率，却不想想大家既然都能100%预测重复样本的情况下，差距在哪里…</p><p><strong>最后的吐槽：</strong>第一次代码开源就体会了当年plantgo开源携程代码的蛋疼。<a href="https://www.zhihu.com/question/64350623" target="_blank" rel="noopener">如何看待携程举办的大数据比赛？</a> 分享代码有一小撮萌新看不懂学不会清洗重复数据这种常规操作也就算了，还根据自己对代码自己的理解莫名其妙揣测，写本文章主要是帮助我们队友piupiu辟谣，顺便也给大家分享了点有价值的干货。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不去重的数据训练的模型 = 80%的非重复样本准确率 + 100%重复样本准确率&lt;br&gt;去重的数据训练的模型 = 85%的非重复样本准确率 + 85%重复样本准确率（直接规则覆盖后100%）&lt;br&gt;深度提取重复样本的真正目的是删除它们以提高模型预测非重复样本的准确率，非重复样本的准确率才是对业务有价值的&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>异常值检测</title>
    <link href="http://kodgv.xyz/2019/04/29/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B/"/>
    <id>http://kodgv.xyz/2019/04/29/竞赛经验/异常值检测/</id>
    <published>2019-04-29T07:12:19.000Z</published>
    <updated>2019-04-29T07:12:33.534Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/38066650" target="_blank" rel="noopener">https://www.zhihu.com/question/38066650</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/38066650&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.zhihu.com/question/38066650&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>缺失值处理</title>
    <link href="http://kodgv.xyz/2019/04/29/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/"/>
    <id>http://kodgv.xyz/2019/04/29/竞赛经验/缺失值处理/</id>
    <published>2019-04-29T02:52:21.000Z</published>
    <updated>2019-04-29T07:09:25.653Z</updated>
    
    <content type="html"><![CDATA[<p>数据缺失值处理</p><a id="more"></a><p>[TOC]</p><h2 id="数据丢失的原因。"><a href="#数据丢失的原因。" class="headerlink" title="数据丢失的原因。"></a>数据丢失的原因。</h2><ul><li><p>随机缺失(MAR):随机缺失意味着数据点缺失的倾向性与缺失的数据无关，而是与一些观察到的数据相关</p></li><li><p>完全随机缺失(MCAR):某个值缺失的事实与它的假设值以及其他变量的值无关。</p></li><li><p>非随机缺失(MNAR):两个可能的原因是,缺失值取决于假设的值(例如,工资高的人通常不愿透露他们的收入调查)或缺失值依赖于其他变量的值(例如假设女性一般不愿透露他们的年龄!此处年龄变量缺失值受性别变量影响)</p></li></ul><p>在前两种情况下，根据缺失值的出现情况删除缺失值的数据是安全的，而在第三种情况下，删除缺失值的观察值会在模型中产生偏差。所以在移除观测结果之前，我们必须非常小心。注意，归罪法不一定能给出更好的结果。</p><p><img src="/2019/04/29/竞赛经验/缺失值处理/1.png" alt></p><h4 id="删除"><a href="#删除" class="headerlink" title="　删除"></a>　删除</h4><p><strong>成列删除（listwise deletion）</strong></p><p>列表删除(完全案例分析)删除包含一个或多个缺失值的所有数据。特别是如果缺少的数据仅限于少量的观察，您可以选择从分析中删除这些情况。然而，在大多数情况下，使用列表删除通常是不利的。这是因为MCAR的假设(完全随机缺失)通常很少得到支持。因此，列表删除方法产生有偏差的参数和估计。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">newdata &lt;- na.omit(mydata)</span><br><span class="line"><span class="comment"># In python</span></span><br><span class="line">mydata.dropna(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>成对删除（pairwise deletion）</strong></p><p>一般的备选方案，在进行多变量的联立时，只删除掉需要执行的变量的缺失数据。例如在ABC三个变量间，需要计算A和C的协方差，那么只有同时具备A/C的数据会被使用。</p><p>文献指出，当变量间的相关性普遍较低时，成对删除会产生更有效的估计值。然而当变量间的相关性较高时，建议还是使用成列删除。</p><p>理论上成对删除不建议作为成列删除的备选方案。</p><p><strong>虚拟变量调整（哑变量，dummy variables）</strong></p><p>新建两个变量，其中一个变量D为“是否缺失”，缺失值设为0，存在值设为1。</p><p>另一个变量X’，将缺失值设为c（可以是任何常数），存在值设为本身。</p><p>随后，对X’，D和其他变量（因变量和其他预设模型中的自变量）进行回归。这种调整的好处是它利用了所有可用的缺失数据的信息（是否缺失）。为了便利，一个好的c的设置方式是现有非缺失数据X的均数。</p><p>这样做的好处是，D的系数可以被解释成“在控制了其他变量的情况下，X具缺失数据的个体其Y的预测值减去具X平均数的个体于Y的预测值”</p><h4 id="Time-Series-Specific-Methods"><a href="#Time-Series-Specific-Methods" class="headerlink" title="Time-Series Specific Methods"></a>Time-Series Specific Methods</h4><p><strong>Last Observation Carried Forward (LOCF) &amp; Next Observation Carried Backward (NOCB)</strong></p><p><strong>Linear Interpolation</strong></p><p><strong>Seasonal Adjustment + Linear Interpolation</strong></p><h4 id="Mean-Median-and-Mode"><a href="#Mean-Median-and-Mode" class="headerlink" title="Mean, Median and Mode"></a>Mean, Median and Mode</h4><p>计算总体均值、中值或模态是一种非常基本的推算方法，但它没有利用时间序列特征，也没有使用变量之间的关系。它非常快，但有明显的缺点。缺点之一是平均输入减少了数据集中的方差。</p><h4 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h4><p>首先，使用相关矩阵标识值缺失的变量的几个预测器。在回归方程中，选取最优预测因子作为自变量。缺少数据的变量作为因变量。采用预测变量数据完整的案例生成回归方程;然后，该方程用于预测不完全情况下的缺失值。在迭代过程中，插入缺失变量的值，然后用所有的情况来预测因变量。重复这些步骤，直到预测值之间的差值很小，即它们收敛。</p><p>它“理论上”为缺失的值提供了很好的估计。然而，这种模式有几个缺点，往往超过了优点。首先，由于替换后的值是由其他变量预测的，它们往往“太好”地匹配在一起，因此标准误差被缩小了。我们还必须假设回归方程中使用的变量之间存在线性关系，而回归方程中可能没有线性关系。</p><h4 id="Multiple-Imputation"><a href="#Multiple-Imputation" class="headerlink" title="Multiple Imputation"></a>Multiple Imputation</h4><ol><li><strong>Imputation</strong>: Impute the missing entries of the incomplete data sets <em>m</em>times (<em>m</em>=3 in the figure). Note that imputed values are drawn from a distribution. Simulating random draws doesn’t include uncertainty in model parameters. Better approach is to use Markov Chain Monte Carlo (MCMC) simulation. This step results in m complete data sets.</li><li><strong>Analysis</strong>: Analyze each of the <em>m</em> completed data sets.</li><li><strong>Pooling</strong>: Integrate the <em>m</em> analysis results into a final result</li></ol><p><img src="/2019/04/29/竞赛经验/缺失值处理/2.png" alt></p><p>This is by far the most preferred method for imputation for the following reasons:</p><ul><li>Easy to use</li><li>No biases (if imputation model is correct)</li></ul><p>实现的代码包:fancyimpute</p><h4 id="Imputation-of-Categorical-Variables"><a href="#Imputation-of-Categorical-Variables" class="headerlink" title="Imputation of Categorical Variables"></a>Imputation of Categorical Variables</h4><p>把缺失的值作为一个类别进行填补</p><h4 id="KNN-K-Nearest-Neighbors"><a href="#KNN-K-Nearest-Neighbors" class="headerlink" title="KNN (K Nearest Neighbors)"></a>KNN (K Nearest Neighbors)</h4><p> XGBoost and Random Forest 同样也有 data imputation</p><p>该方法基于距离测度选取k个邻域，并以邻域的平均值作为估计的归一化方法。该方法需要选择最近邻的数目和距离度量。KNN既可以预测离散属性(k个近邻中最频繁的值)，也可以预测连续属性(k个近邻中均值)</p><p>距离度量根据数据的类型而变化:</p><ol><li><p>连续数据:连续数据常用的距离度量是欧式、Manhattan和cos</p></li><li><p>分类数据:本例中一般使用汉明距离。它接受所有的分类属性，如果两个点之间的值不相同，则对每个属性进行计数。然后，汉明距离等于值不同的属性的数量。</p></li></ol><p>KNN算法最吸引人的特点之一是易于理解和实现。KNN的非参数特性使其在某些数据可能非常“不寻常”的情况下具有优势。</p><p>KNN算法的一个明显缺点是，在分析大型数据集时非常耗时，因为它在整个数据集中搜索类似的实例。此外，由于最近邻和最近邻之间的距离相差不大，高维数据会严重降低KNN的精度</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据缺失值处理&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>胶囊网络</title>
    <link href="http://kodgv.xyz/2019/04/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C/"/>
    <id>http://kodgv.xyz/2019/04/27/神经网络/胶囊网络/</id>
    <published>2019-04-27T08:30:18.000Z</published>
    <updated>2019-05-25T12:38:37.579Z</updated>
    
    <content type="html"><![CDATA[<p>胶囊网络</p><a id="more"></a><p>来源:<a href="https://spaces.ac.cn/archives/4819" target="_blank" rel="noopener">https://spaces.ac.cn/archives/4819</a></p><p>直接看上面的这个文章，描述的非常详细</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;胶囊网络&lt;/p&gt;
    
    </summary>
    
      <category term="神经网络" scheme="http://kodgv.xyz/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="网络结构" scheme="http://kodgv.xyz/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>FocalLoss针对不平衡数据</title>
    <link href="http://kodgv.xyz/2019/04/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/FocalLoss%E9%92%88%E5%AF%B9%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE/"/>
    <id>http://kodgv.xyz/2019/04/22/神经网络/FocalLoss针对不平衡数据/</id>
    <published>2019-04-22T10:32:49.000Z</published>
    <updated>2019-05-25T12:37:58.417Z</updated>
    
    <content type="html"><![CDATA[<p>Focal loss 一种特别的损失函数，其特点为专注于那些无法分辨的样本</p><a id="more"></a><p>[TOC]</p><p>参考来源:</p><p><a href="https://zhuanlan.zhihu.com/p/32423092" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32423092</a></p><p><a href="https://www.zhihu.com/question/63581984" target="_blank" rel="noopener">https://www.zhihu.com/question/63581984</a></p><p><a href="https://zhuanlan.zhihu.com/p/28527749" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28527749</a></p><h2 id="讲解"><a href="#讲解" class="headerlink" title="讲解"></a>讲解</h2><p>​    本质上讲，Focal Loss 就是一个解决<strong>分类问题中类别不平衡、分类难度差异</strong>的一个 loss，总之这个工作一片好评就是了。大家还可以看知乎的讨论：<a href="https://www.zhihu.com/question/63581984" target="_blank" rel="noopener">如何评价 Kaiming 的 Focal Loss for Dense Object Detection？</a></p><p><strong>核心思想</strong></p><p>这样的做法就是：<strong>正样本的预测值大于 0.5 的，或者负样本的预测值小于 0.5 的，我都不更新了，把注意力集中在预测不准的那些样本，当然这个阈值可以调整。</strong>这样做能部分地达到目的，但是所需要的迭代次数会大大增加。</p><p>原因是这样的：以正样本为例，<strong>我只告诉模型正样本的预测值大于 0.5 就不更新了，却没有告诉它要“保持”大于 0.5</strong>，所以下一阶段，它的预测值就很有可能变回小于 0.5 了。当然，如果是这样的话，下一回合它又被更新了，这样反复迭代，理论上也能达到目的，但是迭代次数会大大增加。</p><p>所以，要想改进的话，重点就是<strong>“不只是要告诉模型正样本的预测值大于0.5就不更新了，而是要告诉模型当其大于0.5后就只需要保持就好了”</strong>。好比老师看到一个学生及格了就不管了，这显然是不行的。如果学生已经及格，那么应该要想办法要他保持目前这个状态甚至变得更好，而不是不管。</p><p>所以除了单纯的区分外，必须使该loss可导，这样才可以告诉模型。</p><p><strong>目的是通过减少易分类样本的权重，从而使得模型在训练时更专注于难分类的样本</strong>。</p><p>Kaiming 大神的 Focal Loss 形式是：</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f1.jpg" alt></p><p>如果落实到 <em>ŷ =σ(x)</em> 这个预测，那么就有：</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f3.jpg" alt="img"></p><p>特别地，<strong>如果</strong> <strong>K</strong> <strong>和</strong> <strong>γ</strong> <strong>都取 1，那么</strong> <strong>L∗∗=Lfl</strong>。</p><p>事实上 <em>K</em> 和 <em>γ</em> 的作用都是一样的，都是调节权重曲线的陡度，只是调节的方式不一样。注意<em>L∗∗</em>或 <em>Lfl</em> 实际上都已经包含了对不均衡样本的解决方法，或者说，类别不均衡本质上就是分类难度差异的体现。</p><p>​    首先y’的范围是0到1，所以不管γ是多少，这个调制系数都是大于等于0的。易分类的样本再多，你的权重很小，那么对于total loss的共享也就不会太大。那么怎么控制样本权重呢？举个例子，假设一个二分类，样本x1属于类别1的y’=0.9，样本x2属于类别1的y’=0.6，显然前者更可能是类别1，假设γ=1，那么对于y’=0.9，调制系数则为0.1；对于y’=0.6，调制系数则为0.4，这个调制系数就是这个样本对loss的贡献程度，也就是权重，所以难分的样本（pt=0.6）的权重更大。</p><p>​    <strong>比如负样本远比正样本多的话，模型肯定会倾向于数目多的负类（可以想象全部样本都判为负类），这时候，负类的</strong> <strong>*ŷ γ*</strong> <strong>或</strong> <strong>σ(Kx) 都很小，而正类的</strong> <strong>(1−ŷ )γ</strong> <strong>或</strong> <strong>*σ(−Kx)*</strong> <strong>就很大，这时候模型就会开始集中精力关注正样本。</strong></p><p>当然，Kaiming 大神还发现对 <em>Lfl</em> 做个权重调整，结果会有微小提升。</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f2.jpg" alt="img"></p><p>通过一系列调参，得到 <em>α=0.25, γ=2</em>（在他的模型上）的效果最好。注意在他的任务中，正样本是属于少数样本，也就是说，本来正样本难以“匹敌”负样本，但经过 <em>(1−ŷ )γ</em> 和 <em>ŷγ</em> 的“操控”后，也许形势还逆转了，还要对正样本降权。</p><p>不过我认为这样调整只是经验结果，理论上很难有一个指导方案来决定 <em>α</em> 的值，如果没有大算力调参，倒不如直接让 <em>α=0.5</em>（均等）。</p><h2 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a><strong>多分类</strong></h2><p>Focal Loss 在多分类中的形式也很容易得到，其实就是：</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f4.jpg" alt="img"></p><p><em>ŷt</em> 是目标的预测值，一般就是经过 softmax 后的结果。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>多分类</p><p><a href="https://github.com/marvis/pytorch-yolo2/blob/master/FocalLoss.py" target="_blank" rel="noopener">https://github.com/marvis/pytorch-yolo2/blob/master/FocalLoss.py</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">r"""</span></span><br><span class="line"><span class="string">        This criterion is a implemenation of Focal Loss, which is proposed in </span></span><br><span class="line"><span class="string">        Focal Loss for Dense Object Detection.</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            Loss(x, class) = - \alpha (1-softmax(x)[class])^gamma \log(softmax(x)[class])</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        The losses are averaged across observations for each minibatch.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            alpha(1D Tensor, Variable) : the scalar factor for this criterion</span></span><br><span class="line"><span class="string">            gamma(float, double) : gamma &gt; 0; reduces the relative loss for well-classiﬁed examples (p &gt; .5), </span></span><br><span class="line"><span class="string">                                   putting more focus on hard, misclassiﬁed examples</span></span><br><span class="line"><span class="string">            size_average(bool): size_average(bool): By default, the losses are averaged over observations for each minibatch.</span></span><br><span class="line"><span class="string">                                However, if the field size_average is set to False, the losses are</span></span><br><span class="line"><span class="string">                                instead summed for each minibatch.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, class_num, alpha=None, gamma=<span class="number">2</span>, size_average=True)</span>:</span></span><br><span class="line">        super(FocalLoss, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> alpha <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.alpha = Variable(torch.ones(class_num, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> isinstance(alpha, Variable):</span><br><span class="line">                self.alpha = alpha</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.alpha = Variable(alpha)</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.class_num = class_num</span><br><span class="line">        self.size_average = size_average</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets)</span>:</span></span><br><span class="line">        N = inputs.size(<span class="number">0</span>)</span><br><span class="line">        print(N)</span><br><span class="line">        C = inputs.size(<span class="number">1</span>)</span><br><span class="line">        P = F.softmax(inputs)</span><br><span class="line"><span class="comment"># 这是为了获取onehot</span></span><br><span class="line">        class_mask = inputs.data.new(N, C).fill_(<span class="number">0</span>)</span><br><span class="line">        class_mask = Variable(class_mask)</span><br><span class="line">        ids = targets.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        class_mask.scatter_(<span class="number">1</span>, ids.data, <span class="number">1.</span>)</span><br><span class="line">        <span class="comment">#print(class_mask)</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs.is_cuda <span class="keyword">and</span> <span class="keyword">not</span> self.alpha.is_cuda:</span><br><span class="line">            self.alpha = self.alpha.cuda()</span><br><span class="line">        alpha = self.alpha[ids.data.view(<span class="number">-1</span>)]</span><br><span class="line">        </span><br><span class="line">        probs = (P*class_mask).sum(<span class="number">1</span>).view(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        log_p = probs.log()</span><br><span class="line">        <span class="comment">#print('probs size= &#123;&#125;'.format(probs.size()))</span></span><br><span class="line">        <span class="comment">#print(probs)</span></span><br><span class="line"></span><br><span class="line">        batch_loss = -alpha*(torch.pow((<span class="number">1</span>-probs), self.gamma))*log_p </span><br><span class="line">        <span class="comment">#print('-----bacth_loss------')</span></span><br><span class="line">        <span class="comment">#print(batch_loss)</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.size_average:</span><br><span class="line">            loss = batch_loss.mean()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = batch_loss.sum()</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    alpha = torch.rand(<span class="number">21</span>, <span class="number">1</span>)</span><br><span class="line">    print(alpha)</span><br><span class="line">    FL = FocalLoss(class_num=<span class="number">5</span>, gamma=<span class="number">0</span> )</span><br><span class="line">    CE = nn.CrossEntropyLoss()</span><br><span class="line">    N = <span class="number">4</span></span><br><span class="line">    C = <span class="number">5</span></span><br><span class="line">    inputs = torch.rand(N, C)</span><br><span class="line">    targets = torch.LongTensor(N).random_(C)</span><br><span class="line">    inputs_fl = Variable(inputs.clone(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    targets_fl = Variable(targets.clone())</span><br><span class="line"></span><br><span class="line">    inputs_ce = Variable(inputs.clone(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    targets_ce = Variable(targets.clone())</span><br><span class="line">    print(<span class="string">'----inputs----'</span>)</span><br><span class="line">    print(inputs)</span><br><span class="line">    print(<span class="string">'---target-----'</span>)</span><br><span class="line">    print(targets)</span><br><span class="line"></span><br><span class="line">    fl_loss = FL(inputs_fl, targets_fl)</span><br><span class="line">    ce_loss = CE(inputs_ce, targets_ce)</span><br><span class="line">    print(<span class="string">'ce = &#123;&#125;, fl =&#123;&#125;'</span>.format(ce_loss.data[<span class="number">0</span>], fl_loss.data[<span class="number">0</span>]))</span><br><span class="line">    fl_loss.backward()</span><br><span class="line">    ce_loss.backward()</span><br><span class="line">    <span class="comment">#print(inputs_fl.grad.data)</span></span><br><span class="line">    print(inputs_ce.grad.data)</span><br></pre></td></tr></table></figure><p>单分类</p><p><a href="https://www.kaggle.com/aakashnain/diving-deep-into-focal-loss" target="_blank" rel="noopener">https://www.kaggle.com/aakashnain/diving-deep-into-focal-loss</a></p><p><a href="https://www.kaggle.com/sfzero/focal-loss-feature-0-99994/comments" target="_blank" rel="noopener">https://www.kaggle.com/sfzero/focal-loss-feature-0-99994/comments</a></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, alpha=<span class="number">1</span>, gamma=<span class="number">2</span>, logits=True, reduction=<span class="string">'elementwise_mean'</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>(FocalLoss, <span class="keyword">self</span>).__init_<span class="number">_</span>()</span><br><span class="line">        <span class="keyword">self</span>.alpha = alpha</span><br><span class="line">        <span class="keyword">self</span>.gamma = gamma</span><br><span class="line">        <span class="keyword">self</span>.logits = logits</span><br><span class="line">        <span class="keyword">self</span>.reduction = reduction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, inputs, targets)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">logits:</span></span><br><span class="line">            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=<span class="string">'none'</span>)</span><br><span class="line">        <span class="symbol">else:</span></span><br><span class="line">            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction=<span class="string">'none'</span>)</span><br><span class="line">        pt = torch.exp(-BCE_loss)</span><br><span class="line">        F_loss = <span class="keyword">self</span>.alpha * (<span class="number">1</span>-pt)**<span class="keyword">self</span>.gamma * BCE_loss</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.reduction is <span class="symbol">None:</span></span><br><span class="line">            <span class="keyword">return</span> F_loss</span><br><span class="line">        <span class="symbol">else:</span></span><br><span class="line">            <span class="keyword">return</span> torch.mean(F_loss)</span><br></pre></td></tr></table></figure><p>简单版代码</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/83363#486607</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">staticWeightLoss</span><span class="params">(<span class="literal">true</span>,pred)</span></span><span class="symbol">:</span></span><br><span class="line">    loss = K.binary_crossentropy(<span class="literal">true</span>, pred)</span><br><span class="line">    positiveLoss = positiveWeights * loss</span><br><span class="line">    negativeLoss = negativeWeights * loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> K.switch(K.greater(<span class="literal">true</span>, <span class="number">0</span>.<span class="number">5</span>), positiveLoss, negativeLoss)</span><br></pre></td></tr></table></figure><p>!要注意softmax是要有两列以上，sigmod才是一列</p><h2 id="引申"><a href="#引申" class="headerlink" title="引申"></a>引申</h2><p>这就是为什么之前别人做数据增强的时候，把预测很高的数据当作1把预测很低的数据当作0放进去加强训练。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Focal loss 一种特别的损失函数，其特点为专注于那些无法分辨的样本&lt;/p&gt;
    
    </summary>
    
      <category term="神经网络" scheme="http://kodgv.xyz/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="损失函数" scheme="http://kodgv.xyz/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>比赛心得集合</title>
    <link href="http://kodgv.xyz/2019/04/21/%E6%AF%94%E8%B5%9B%E5%BF%83%E5%BE%97%E9%9B%86%E5%90%88/"/>
    <id>http://kodgv.xyz/2019/04/21/比赛心得集合/</id>
    <published>2019-04-21T06:45:42.000Z</published>
    <updated>2019-05-25T12:47:23.531Z</updated>
    
    <content type="html"><![CDATA[<p>收集好的竞赛网站和NLP博客，方便借鉴和模仿</p><a id="more"></a><p>[TOC]</p><h1 id="比赛心得"><a href="#比赛心得" class="headerlink" title="比赛心得"></a>比赛心得</h1><p>机器翻译注意力机制及其PyTorch实现</p><p><a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/" target="_blank" rel="noopener">https://plmsmile.github.io/2017/10/12/Attention-based-NMT/</a></p><p>各个NLP模型实现</p><p><a href="http://www.zhongruitech.com/921029206.html" target="_blank" rel="noopener">http://www.zhongruitech.com/921029206.html</a></p><p>苏剑林大神博客</p><p><a href="https://spaces.ac.cn/category/Resources" target="_blank" rel="noopener">https://spaces.ac.cn/category/Resources</a></p><p>QuroaNLP分类比赛心得</p><p><a href="https://www.kaggle.com/c/quora-insincere-questions-classification" target="_blank" rel="noopener">Quroa 识别不良句子</a></p><p><a href="https://www.getit01.com/p20190314357550039/" target="_blank" rel="noopener">https://www.getit01.com/p20190314357550039/</a></p><p>腾讯广告大赛比赛心得</p><p><a href="https://zhuanlan.zhihu.com/p/38341881" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38341881</a></p><p>摩拜杯目的地预测比赛心得</p><p><a href="https://zhuanlan.zhihu.com/p/32151090" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32151090</a></p><h1 id="比赛"><a href="#比赛" class="headerlink" title="比赛"></a>比赛</h1><p>如何不过拟合:<a href="https://www.kaggle.com/c/dont-overfit-ii" target="_blank" rel="noopener">https://www.kaggle.com/c/dont-overfit-ii</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;收集好的竞赛网站和NLP博客，方便借鉴和模仿&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="比赛收集" scheme="http://kodgv.xyz/tags/%E6%AF%94%E8%B5%9B%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>NLP代码汇总</title>
    <link href="http://kodgv.xyz/2019/04/20/NLP%E4%BB%A3%E7%A0%81%E6%B1%87%E6%80%BB/"/>
    <id>http://kodgv.xyz/2019/04/20/NLP代码汇总/</id>
    <published>2019-04-20T13:57:19.000Z</published>
    <updated>2019-04-27T07:13:55.279Z</updated>
    
    <content type="html"><![CDATA[<p>NLP汇总</p><a id="more"></a><p>[TOC]</p><h2 id="动态padding，节省时间"><a href="#动态padding，节省时间" class="headerlink" title="动态padding，节省时间"></a>动态padding，节省时间</h2><p>比起一开始固定化padding，动态padding，可以针对当前batch的长度来Padding，明显会增快速度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, text, lens, y=None)</span>:</span></span><br><span class="line">        self.text = text</span><br><span class="line">        self.y = y</span><br><span class="line">        self.lens = lens</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.lens)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self.text[index], self.lens[index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.text[index], self.lens[index], self.y[index]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(batch)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    batch = [dataset[i] for i in N]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    size = len(batch[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> size == <span class="number">3</span>:</span><br><span class="line">        texts, lens, y = zip(*batch)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        texts, lens = zip(*batch)</span><br><span class="line">    lens = np.array(lens)</span><br><span class="line">    sort_idx = np.argsort(<span class="number">-1</span> * lens)</span><br><span class="line">    reverse_idx = np.argsort(sort_idx)</span><br><span class="line">    max_len = min(int(np.percentile(lens, PERCENTILE)), MAX_LEN)</span><br><span class="line">    </span><br><span class="line">    lens = np.clip(lens, <span class="number">0</span>, max_len)[sort_idx]</span><br><span class="line">    texts = torch.tensor(sequence.pad_sequences(texts, maxlen=max_len)[sort_idx], dtype=torch.long).cuda()</span><br><span class="line">    <span class="keyword">if</span> size == <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">return</span> texts, lens, reverse_idx, torch.tensor(y, dtype=torch.float32).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> texts, lens, reverse_idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_data_loader</span><span class="params">(texts, lens, y=None, batch_size=BATCH_SIZE)</span>:</span></span><br><span class="line">    dset = MyDataset(texts, lens, y)</span><br><span class="line">    dloader = data.DataLoader(dset, batch_size=batch_size, shuffle=<span class="literal">False</span>, collate_fn=collate_fn)</span><br><span class="line">    <span class="keyword">return</span> dloader</span><br><span class="line">seqs = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>]]</span><br><span class="line">lens = [len(i) <span class="keyword">for</span> i <span class="keyword">in</span> seqs]</span><br><span class="line"></span><br><span class="line">data_loader = build_data_loader(seqs, lens)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data_loader:</span><br><span class="line">    seq_batch, lens_batch, reverse_idx_batch = batch</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">print(<span class="string">f'original seqs:'</span>)</span><br><span class="line">print(seqs)</span><br><span class="line">print(<span class="string">f'batch seqs, already sort by lens, and padding dynamic in batch:'</span>)</span><br><span class="line">print(seq_batch)</span><br><span class="line">print(<span class="string">f'reverse batch seqs:'</span>)</span><br><span class="line">print(seq_batch[reverse_idx_batch])</span><br><span class="line">h_embedding_pack = pack_padded_sequence(seq_batch,lens_batch,batch_first=<span class="literal">True</span>)</span><br><span class="line">print(h_embedding_pack)</span><br></pre></td></tr></table></figure><h2 id="mask-loss-避免无用结果的求导影响"><a href="#mask-loss-避免无用结果的求导影响" class="headerlink" title="mask loss 避免无用结果的求导影响"></a>mask loss 避免无用结果的求导影响</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> func</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CustomLoss,self).__init__()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, Y_hat, Y)</span>:</span></span><br><span class="line">       <span class="comment"># TRICK 3 ********************************</span></span><br><span class="line">       <span class="comment"># before we calculate the negative log likelihood, we need to mask out the activations</span></span><br><span class="line">       <span class="comment"># this means we don't want to take into account padded items in the output vector</span></span><br><span class="line">       <span class="comment"># simplest way to think about this is to flatten ALL sequences into a REALLY long sequence</span></span><br><span class="line">       <span class="comment"># and calculate the loss on that.</span></span><br><span class="line">       <span class="comment"># flatten all the labels</span></span><br><span class="line">        Y = Y.view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># flatten all predictions</span></span><br><span class="line">        Y_hat = Y_hat.view(<span class="number">-1</span>, self.nb_tags)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># create a mask by filtering out all tokens that ARE NOT the padding token</span></span><br><span class="line">        tag_pad_token = self.tags[<span class="string">'&lt;PAD&gt;'</span>]</span><br><span class="line">        mask = (Y &gt; tag_pad_token).float()</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># count how many tokens we have</span></span><br><span class="line">        nb_tokens = int(torch.sum(mask).data[<span class="number">0</span>])</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># pick the values for the label and zero out the rest with the mask</span></span><br><span class="line">        Y_hat = Y_hat[range(Y_hat.shape[<span class="number">0</span>]), Y] * mask</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># compute cross entropy loss which ignores all &lt;PAD&gt; tokens</span></span><br><span class="line">        ce_loss = -torch.sum(Y_hat) / nb_tokens</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ce_loss</span><br></pre></td></tr></table></figure><h2 id="问题解答专区"><a href="#问题解答专区" class="headerlink" title="问题解答专区"></a>问题解答专区</h2><h3 id="为什么LSTM不同batch的句子长度可以不一致"><a href="#为什么LSTM不同batch的句子长度可以不一致" class="headerlink" title="为什么LSTM不同batch的句子长度可以不一致"></a>为什么LSTM不同batch的句子长度可以不一致</h3><p>LSTM只要保证每个batch的句子长度一致即可，所以可以使用动态padding,但是为什么不同batch的长度可以不一样？</p><p>因为LSTM需要更新的参数是共享，就是每个状态的权重是共享的，所以就无所谓不同长度了。只要保证每个batch内能够更新权重即可。</p><h3 id="深度学习中-number-of-training-epochs-中的-epoch到底指什么？"><a href="#深度学习中-number-of-training-epochs-中的-epoch到底指什么？" class="headerlink" title="深度学习中 number of training epochs 中的 epoch到底指什么？"></a>深度学习中 number of training epochs 中的 epoch到底指什么？</h3><p>对于初学者来讲，有几个概念容易混淆：</p><p>（1）iteration：表示1次迭代（也叫training step），每次迭代更新1次网络结构的参数；</p><p>（2）batch-size：1次迭代所使用的样本量；</p><p>（3）epoch：1个epoch表示过了1遍训练集中的所有样本。</p><p>一次epoch=所有训练数据forward+backward后更新参数的过程。<br>一次iteration=[batch size]个训练数据forward+backward后更新参数过程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NLP汇总&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://kodgv.xyz/categories/NLP/"/>
    
    
      <category term="汇总" scheme="http://kodgv.xyz/tags/%E6%B1%87%E6%80%BB/"/>
    
  </entry>
  
  <entry>
    <title>python多进程</title>
    <link href="http://kodgv.xyz/2019/04/18/%E4%BB%A3%E7%A0%81%E5%9F%BA%E7%A1%80/python%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    <id>http://kodgv.xyz/2019/04/18/代码基础/python多进程/</id>
    <published>2019-04-18T01:46:38.000Z</published>
    <updated>2019-05-25T12:35:07.119Z</updated>
    
    <content type="html"><![CDATA[<p>python multiprocessing模块多进程详解</p><a id="more"></a><p>[TOC]</p><h2 id="multiprocessing模块API"><a href="#multiprocessing模块API" class="headerlink" title="multiprocessing模块API"></a>multiprocessing模块API</h2><p>Pool类用于需要执行的目标很多，而手动限制进程数量又太繁琐时，如果目标少且不用控制进程数量则可以用<a href="http://thief.one/2016/11/24/Multiprocessing-Process" target="_blank" rel="noopener">Process</a>类。</p><p>构造方法</p><ul><li>Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]])</li><li>processes ：使用的工作进程的数量，如果processes是None那么使用 os.cpu_count()返回的数量。</li><li>initializer： 如果initializer是None，那么每一个工作进程在开始的时候会调用initializer(*initargs)。</li><li>maxtasksperchild：工作进程退出之前可以完成的任务数，完成后用一个新的工作进程来替代原进程，来让闲置的资源被释放。maxtasksperchild默认是None，意味着只要Pool存在工作进程就会一直存活。</li><li>context: 用在制定工作进程启动时的上下文，一般使用 multiprocessing.Pool() 或者一个context对象的Pool()方法来创建一个池，两种方法都适当的设置了context。</li></ul><p>实例方法</p><ul><li>apply_async(func[, args[, kwds[, callback]]]) 它是非阻塞。</li><li>apply(func[, args[, kwds]])是阻塞的。</li><li>close() 关闭pool，使其不在接受新的任务。</li><li>terminate() 关闭pool，结束工作进程，不在处理未完成的任务。</li><li>join() 主进程阻塞，等待子进程的退出， join方法要在close或terminate之后使用。</li></ul><h3 id="Pool使用方法"><a href="#Pool使用方法" class="headerlink" title="Pool使用方法"></a>Pool使用方法</h3><p>Pool+map函数</p><p>说明：此写法缺点在于只能通过map向函数传递一个参数。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing import Pool</span><br><span class="line">def test(i):</span><br><span class="line">    <span class="builtin-name">print</span> i</span><br><span class="line"><span class="keyword">if</span> <span class="attribute">__name__</span>=="__main__":</span><br><span class="line">lists=[1,2,3]</span><br><span class="line"><span class="attribute">pool</span>=Pool(processes=2) #定义最大的进程数</span><br><span class="line">pool.map(test,lists)        #p必须是一个可迭代变量。</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure><p>异步进程池（非阻塞）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">pool = Pool(processes=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i  <span class="keyword">in</span> xrange(<span class="number">500</span>):</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">For循环中执行步骤：</span></span><br><span class="line"><span class="string">（1）循环遍历，将500个子进程添加到进程池（相对父进程会阻塞）</span></span><br><span class="line"><span class="string">（2）每次执行10个子进程，等一个子进程执行完后，立马启动新的子进程。（相对父进程不阻塞）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">apply_async为异步进程池写法。</span></span><br><span class="line"><span class="string">异步指的是启动子进程的过程，与父进程本身的执行（print）是异步的，而For循环中往进程池添加子进程的过程，与父进程本身的执行却是同步的。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">    pool.apply_async(test, args=(i,)) <span class="comment">#维持执行的进程总数为10，当一个进程执行完后启动一个新进程.       </span></span><br><span class="line"><span class="keyword">print</span> “test”</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure><p>执行顺序：For循环内执行了2个步骤，第一步：将500个对象放入进程池（阻塞）。第二步：同时执行10个子进程（非阻塞），有结束的就立即添加，维持10个子进程运行。（apply_async方法的会在执行完for循环的添加步骤后，直接执行后面的print语句，而apply方法会等所有进程池中的子进程运行完以后再执行后面的print语句）</p><p>注意：调用join之前，先调用close或者terminate方法，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束。</p><h2 id="多进程示例代码"><a href="#多进程示例代码" class="headerlink" title="多进程示例代码"></a>多进程示例代码</h2><p>纯建立Process<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#encoding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> os, time, random</span><br><span class="line"></span><br><span class="line"><span class="comment">#线程启动后实际执行的代码块</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">r1</span><span class="params">(process_name)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">print</span> process_name, os.getpid()     <span class="comment">#打印出当前进程的id</span></span><br><span class="line">        time.sleep(random.random())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">r2</span><span class="params">(process_name)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">print</span> process_name, os.getpid()     <span class="comment">#打印出当前进程的id</span></span><br><span class="line">        time.sleep(random.random())</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"main process run..."</span></span><br><span class="line">        p1 = Process(target=r1, args=(<span class="string">'process_name1'</span>, ))       <span class="comment">#target:指定进程执行的函数，args:该函数的参数，需要使用tuple,只需要不断往后加参数即可</span></span><br><span class="line">        p2 = Process(target=r2, args=(<span class="string">'process_name2'</span>, ))</span><br><span class="line">        </span><br><span class="line">        p1.start()    <span class="comment">#通过调用start方法启动进程，跟线程差不多。</span></span><br><span class="line">        p2.start()    <span class="comment">#但run方法在哪呢？待会说。。。</span></span><br><span class="line">        p1.join()     <span class="comment">#join方法也很有意思，寻思了一下午，终于理解了。待会演示。</span></span><br><span class="line">        p2.join()</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"main process runned all lines..."</span></span><br></pre></td></tr></table></figure></p><p>POOL池管理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(data, index, size)</span>:</span>  <span class="comment"># data 传入数据，index 数据分片索引，size进程数</span></span><br><span class="line">    size = math.ceil(len(data) / size)</span><br><span class="line">    start = size * index</span><br><span class="line">    end = (index + <span class="number">1</span>) * size <span class="keyword">if</span> (index + <span class="number">1</span>) * size &lt; len(data) <span class="keyword">else</span> len(data)</span><br><span class="line">    temp_data = data[start:end]</span><br><span class="line">    <span class="comment"># do something</span></span><br><span class="line">    <span class="keyword">return</span> data  <span class="comment"># 可以返回数据，在后面收集起来</span></span><br><span class="line"></span><br><span class="line">processor = <span class="number">40</span></span><br><span class="line">res = []</span><br><span class="line">p = Pool(processor)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(processor):</span><br><span class="line">    res.append(p.apply_async(run, args=(data, i, processor,)))</span><br><span class="line">    print(str(i) + <span class="string">' processor started !'</span>)</span><br><span class="line">p.close()</span><br><span class="line">p.join()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> res:</span><br><span class="line">    print(i.get())  <span class="comment"># 使用get获得多进程处理的结果</span></span><br></pre></td></tr></table></figure></p><h2 id="进程注意事项"><a href="#进程注意事项" class="headerlink" title="进程注意事项"></a>进程注意事项</h2><h3 id="进程之间内存独立"><a href="#进程之间内存独立" class="headerlink" title="进程之间内存独立"></a>进程之间内存独立</h3><p>多进程与多线程最大的不同在于，多进程的每一个进程都有一份变量的拷贝，进程之间的操作互不影响，Process进程创建时，子进程会将主进程的Process对象完全复制一份，这样在主进程和子进程各有一个 Process对象，但是p.start()启动的是子进程，主进程中的Process对象作为一个静态对象存在，不执行。</p><p>就是这个地方，要注意的是，对于多进程来说，它是不会复制内存的，所以不用担心main中的变量被复制，但是它会复制main函数以外的变量</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import multiprocessing</span><br><span class="line">import <span class="built_in">time</span></span><br><span class="line"></span><br><span class="line"><span class="literal">zero</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">def change_zero():</span><br><span class="line">    <span class="built_in">global</span> <span class="literal">zero</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        <span class="literal">zero</span> = <span class="literal">zero</span> + <span class="number">1</span></span><br><span class="line">        print(multiprocessing.current_process().name, <span class="literal">zero</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    p1 = multiprocessing.Process(target = change_zero)</span><br><span class="line">    p2 = multiprocessing.Process(target = change_zero)</span><br><span class="line">    p1.<span class="built_in">start</span>()</span><br><span class="line">    p2.<span class="built_in">start</span>()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.join()</span><br><span class="line">    print(<span class="literal">zero</span>)</span><br></pre></td></tr></table></figure><p>运行结果如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Process-1 1</span><br><span class="line">Process-1 2</span><br><span class="line">Process-1 3</span><br><span class="line">Process-2 1</span><br><span class="line">Process-2 2</span><br><span class="line">Process-2 3</span><br><span class="line">0</span><br></pre></td></tr></table></figure><h3 id="共享变量Queue"><a href="#共享变量Queue" class="headerlink" title="共享变量Queue"></a>共享变量Queue</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">下面我们要讨论第一种情况，如果真的要在两个进程之间共享变量需要怎么办</span><br><span class="line"></span><br><span class="line">队列</span><br><span class="line">这里介绍进程之间的第一种交流方式——队列。multiprocessing模块中提供了multiprocessing.Queue，它和Queue.Queue的区别在于，它里面封装了进程之间的数据交流，不同进程可以操作同一个multiprocessing.Queue。</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Queue</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addone</span><span class="params">(q)</span>:</span></span><br><span class="line">    q.put(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addtwo</span><span class="params">(q)</span>:</span></span><br><span class="line">    q.put(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    q = Queue()</span><br><span class="line">    p1 = Process(target=addone, args = (q, ))</span><br><span class="line">    p2 = Process(target=addtwo, args = (q, ))</span><br><span class="line">    p1.start()</span><br><span class="line">    p2.start()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.join()</span><br><span class="line">    print(q.get())</span><br><span class="line">    print(q.get())</span><br><span class="line">运行结果如下</span><br><span class="line"></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">这个队列是线程、进程安全的，即对队列的每一次修改中间不会被中断从而造成结果错误。</span><br></pre></td></tr></table></figure><h3 id="进程锁"><a href="#进程锁" class="headerlink" title="进程锁"></a>进程锁</h3><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">进程锁</span><br><span class="line">既然变量在进程之间可以共享了，那么同时操作一个变量导致的不安全也随之出现。同多线程一样，进程也是通过锁来解决，而且使用方法都和多线程里相同。</span><br><span class="line"><span class="built_in">lock</span> = multiprocessing.<span class="built_in">Lock</span>()</span><br><span class="line"><span class="built_in">lock</span>.acquire()</span><br><span class="line"><span class="built_in">lock</span>.release()</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">lock</span>:</span><br><span class="line">这些用法和功能都和多线程是一样的</span><br><span class="line">另外，multiprocessing.Semaphore Condition Event RLock也和多线程相同</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python multiprocessing模块多进程详解&lt;/p&gt;
    
    </summary>
    
    
      <category term="多进程" scheme="http://kodgv.xyz/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>CNN</title>
    <link href="http://kodgv.xyz/2019/04/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN/"/>
    <id>http://kodgv.xyz/2019/04/17/神经网络/CNN/</id>
    <published>2019-04-17T02:32:46.000Z</published>
    <updated>2019-04-27T14:57:16.609Z</updated>
    
    <content type="html"><![CDATA[<p>CNN学习</p><a id="more"></a><p>[TOC]</p><h1 id="CNN结构基础"><a href="#CNN结构基础" class="headerlink" title="CNN结构基础"></a>CNN结构基础</h1><p>首先，提出这样一个问题：识别一幅图片是包含有字母”X”还是字母”O”？</p><p>为了帮助指导你理解卷积神经网络，我们讲采用一个非常简化的例子：确定一幅图像是包含有”X”还是”O”？<br><img src="/2019/04/17/神经网络/CNN/p1" alt="img"><br>这个例子足够说明CNN背后的原理，同时它足够简单，能够避免陷入不必要的细节。</p><p>在CNN中有这样一个问题，就是每次给你一张图，你需要判断它是否含有”X”或者”O”。并且假设必须两者选其一，不是”X”就是”O”。理想的情况就像下面这个样子：<br><img src="/2019/04/17/神经网络/CNN/p2" alt="img"><br>标准的”X”和”O”，字母位于图像的正中央，并且比例合适，无变形</p><p>对于计算机来说，只要图像稍稍有一点变化，不是标准的，那么要解决这个问题还是不是那么容易的：<br><img src="/2019/04/17/神经网络/CNN/p3" alt="img"></p><p>计算机要解决上面这个问题，一个比较天真的做法就是先保存一张”X”和”O”的标准图像（就像前面给出的例子），然后将其他的新给出的图像来和这两张标准图像进行对比，看看到底和哪一张图更匹配，就判断为哪个字母。</p><p>但是这么做的话，其实是非常不可靠的，因为计算机还是比较死板的。在计算机的“视觉”中，一幅图看起来就像是一个二维的像素数组（可以想象成一个棋盘），每一个位置对应一个数字。在我们这个例子当中，像素值”1”代表白色，像素值”-1”代表黑色。<br><img src="/2019/04/17/神经网络/CNN/p4" alt="img"></p><p>当比较两幅图的时候，如果有任何一个像素值不匹配，那么这两幅图就不匹配，至少对于计算机来说是这样的。</p><p>对于这个例子，计算机认为上述两幅图中的白色像素除了中间的3*3的小方格里面是相同的，其他四个角上都不同：<br><img src="/2019/04/17/神经网络/CNN/p5" alt="img"></p><p>因此，从表面上看，计算机判别右边那幅图不是”X”，两幅图不同，得出结论：<br><img src="/2019/04/17/神经网络/CNN/p6" alt="img"></p><p>但是这么做，显得太不合理了。理想的情况下，我们希望，对于那些仅仅只是做了一些像平移，缩放，旋转，微变形等简单变换的图像，计算机仍然能够识别出图中的”X”和”O”。就像下面这些情况，我们希望计算机依然能够很快并且很准的识别出来：<br><img src="/2019/04/17/神经网络/CNN/p7" alt="img"></p><p>这也就是CNN出现所要解决的问题。</p><p>Features<br><img src="/2019/04/17/神经网络/CNN/p8" alt="img"></p><p>对于CNN来说，它是一块一块地来进行比对。它拿来比对的这个“小块”我们称之为Features（特征）。在两幅图中大致相同的位置找到一些粗糙的特征进行匹配，CNN能够更好的看到两幅图的相似性，相比起传统的整幅图逐一比对的方法。</p><p>每一个feature就像是一个小图（就是一个比较小的有值的二维数组）。不同的Feature匹配图像中不同的特征。在字母”X”的例子中，那些由对角线和交叉线组成的features基本上能够识别出大多数”X”所具有的重要特征。<br><img src="/2019/04/17/神经网络/CNN/p" alt="img"></p><p>这些features很有可能就是匹配任何含有字母”X”的图中字母X的四个角和它的中心。那么具体到底是怎么匹配的呢？如下：<br><img src="/2019/04/17/神经网络/CNN/p9" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p10" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p11" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p12" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p13" alt="img"></p><p>看到这里是不是有了一点头目呢。但其实这只是第一步，你知道了这些Features是怎么在原图上面进行匹配的。但是你还不知道在这里面究竟进行的是怎样的数学计算，比如这个下面3*3的小块到底干了什么？<br><img src="/2019/04/17/神经网络/CNN/p14" alt="img"><br>接下来就跟进介绍里面的数学操作，也就是我们常说的“卷积”操作。</p><p>卷积(Convolution)<br><img src="/2019/04/17/神经网络/CNN/p15" alt="img"><br>Convolution</p><p><img src="/2019/04/17/神经网络/CNN/p16" alt="img"><br>当给你一张新的图时，CNN并不能准确地知道这些features到底要匹配原图的哪些部分，所以它会在原图中每一个可能的位置进行尝试。这样在原始整幅图上每一个位置进行匹配计算，我们相当于<strong>把这个feature变成了一个过滤器</strong>。这个我们用来匹配的过程就被称为卷积操作，这也就是卷积神经网络名字的由来。</p><p>这个卷积操作背后的数学知识其实非常的简单。要计算一个feature和其在原图上对应的某一小块的结果，只需要简单地将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可。</p><p>如果两个像素点都是白色（也就是值均为1），那么1<em>1 = 1，如果均为黑色，那么(-1)</em>(-1) = 1。不管哪种情况，每一对能够匹配上的像素，其相乘结果为1。类似地，任何不匹配的像素相乘结果为-1。如果一个feature（比如n<em>n）内部所有的像素都和原图中对应一小块（n</em>n）匹配上了，那么它们对应像素值相乘再累加就等于n2，然后除以像素点总个数n2，结果就是1。同理，如果每一个像素都不匹配，那么结果就是-1。</p><p>具体过程如下：</p><p><img src="/2019/04/17/神经网络/CNN/p17" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p18" alt="img"><img src="https://img-blog.csdn.net/2018030618132177" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p19" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p20" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p21" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p22" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p23" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p24" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p25" alt="img"></p><p>对于中间部分，也是一样的操作：</p><p><img src="/2019/04/17/神经网络/CNN/p26" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p27" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p28" alt="img"><br><img src="https://img-blog.csdn.net/20180306181612616" alt="img"><br>最后整张图算完，大概就像下面这个样子：<br><img src="/2019/04/17/神经网络/CNN/p29" alt="img"><br>然后换用其他feature进行同样的操作，最后得到的结果就是这样了：<br><img src="/2019/04/17/神经网络/CNN/p30" alt="img"><br>为了完成我们的卷积，我们不断地重复着上述过程，将feature和图中每一块进行卷积操作。最后通过每一个feature的卷积操作，我们会得到一个新的二维数组。</p><p>这也可以理解为对原始图像进行过滤的结果，我们称之为feature map，它是每一个feature从原始图像中提取出来的“特征”。其中的值，越接近为1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。<br><img src="/2019/04/17/神经网络/CNN/p31" alt="img"><br>这样我们的原始图，经过不同feature的卷积操作就变成了一系列的feature map。我们可以很方便，直观地将这整个操作视为一个单独的处理过程。在CNN中，我们称之为卷积层(convolution layer)，这样你可能很快就会想到后面肯定还有其他的layer。没错，后面会提到。</p><p>我们可以将卷积层看成下面这个样子：<br><img src="/2019/04/17/神经网络/CNN/p32" alt="img"><br>因此可想而知，CNN其实做的操作也没什么复杂的。但是尽管我们能够以这一点篇幅就描述了CNN的工作，其内部的加法，乘法和除法操作的次数其实会增加地很快。从数学的角度来说，它们会随着图像的大小，每一个filter的大小和filter的数目呈线性增长。由于有这么多因素的影响，很容易使得这个问题的计算量变得相当的庞大，这也难怪很多微处理器制造商现在都在生产制造专业的芯片来跟上CNN计算的需求。</p><p>池化(Pooling)<br><img src="/2019/04/17/神经网络/CNN/p33" alt="img"><br>Pooling</p><p>CNN中使用的另一个有效的工具被称为“池化(Pooling)”。池化可以将一幅大的图像缩小，同时又保留其中的重要信息。池化背后的数学顶多也就是小学二年级水平。它就是将输入图像进行缩小，减少像素信息，只保留重要信息。通常情况下，池化都是2<em>2大小，比如对于max-pooling来说，就是取输入图像中2</em>2大小的块中的最大值，作为结果的像素值，相当于将原始图像缩小了4倍(注：同理，对于average-pooling来说，就是取2*2大小块的平均值作为结果的像素值)。</p><p>对于本文的这个例子，池化操作具体如下：</p><p><img src="/2019/04/17/神经网络/CNN/p34" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p35" alt="img"><br><img src="/2019/04/17/神经网络/CNN/p37" alt="img"></p><p>不足的外面补”0”：</p><p><img src="/2019/04/17/神经网络/CNN/p36" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p38" alt="img"><br>经过最大池化操作（比如2*2大小）之后，一幅图就缩小为原来的四分之一了：<br><img src="/2019/04/17/神经网络/CNN/p39" alt="img"><br>然后对所有的feature map执行同样的操作，得到如下结果：<br><img src="/2019/04/17/神经网络/CNN/p40" alt="img"><br>因为最大池化（max-pooling）保留了每一个小块内的最大值，所以它相当于保留了这一块最佳的匹配结果（因为值越接近1表示匹配越好）。这也就意味着它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。这也就能够看出，CNN能够发现图像中是否具有某种特征，而不用在意到底在哪里具有这种特征。这也就能够帮助解决之前提到的计算机逐一像素匹配的死板做法。</p><p>当对所有的feature map执行池化操作之后，相当于一系列输入的大图变成了一系列小图。同样地，我们可以将这整个操作看作是一个操作，这也就是CNN中的池化层(pooling layer)，如下：<br><img src="/2019/04/17/神经网络/CNN/p41" alt="img"><br>通过加入池化层，可以很大程度上减少计算量，降低机器负载。</p><p>Normalization<br>激活函数Relu (Rectified Linear Units)<br>这是一个很小但是很重要的操作，叫做Relu(Rectified Linear Units)，或者修正线性单元。它的数学公式也很简单:<br>f(x) = max(0, x)</p><p>对于输入的负值，输出全为0，对于正值，原样输出。关于其功能，更多详见这里。</p><p>下面我们看一下本文的离例子中relu激活函数具体操作：<br><img src="/2019/04/17/神经网络/CNN/p42" alt="img"><br><img src="/2019/04/17/神经网络/CNN/p43" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p44" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p45" alt="img"></p><p>最后，对整幅图操作之后，结果如下：<br><img src="/2019/04/17/神经网络/CNN/p46" alt="img"><br>同样地，在CNN中，我们这一系列操作视为一个操作，那么就得到Relu Layer，如下：<br><img src="/2019/04/17/神经网络/CNN/p47" alt="img"><br>Deep Learning<br>最后，我们将上面所提到的卷积，池化，激活放在一起，就是下面这个样子：<br><img src="/2019/04/17/神经网络/CNN/p48" alt="img"><br>然后，我们加大网络的深度，增加更多的层，就得到深度神经网络了：<br><img src="/2019/04/17/神经网络/CNN/p49" alt="img"><br>然后在不同的层，我们进行可视化，就可以看到本文开头提到的先验知识里面的结果了：<br><img src="/2019/04/17/神经网络/CNN/p50" alt="img"><br>全连接层(Fully connected layers)<br><img src="/2019/04/17/神经网络/CNN/p51" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p52" alt="img"><img src="/2019/04/17/神经网络/CNN/p53" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p54" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p55" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p56" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p57" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p58" alt="img"></p><p>根据结果判定为”X”：<br><img src="/2019/04/17/神经网络/CNN/p59" alt="img"></p><p>在这个过程中，我们定义这一系列操作为”全连接层“(Fully connected layers)：<br><img src="/2019/04/17/神经网络/CNN/p60" alt="img"><br>全连接层也能够有很多个，如下：<br><img src="/2019/04/17/神经网络/CNN/p61" alt="img"><br>【综合上述所有结构】<br><img src="/2019/04/17/神经网络/CNN/p62" alt="img"></p><h1 id="CNN三大核心思想"><a href="#CNN三大核心思想" class="headerlink" title="CNN三大核心思想"></a><strong>CNN三大核心思想</strong></h1><p>卷积神经网络CNN的出现是为了解决MLP多层感知器全连接和梯度发散的问题。其引入三个核心思想：1.局部感知(local field)，2.权值共享(Shared Weights)，3.下采样(subsampling)。极大地提升了计算速度，减少了连接数量。</p><p><strong>2.1 局部感知</strong></p><p>形象地说，就是模仿你的眼睛，想想看，你在看东西的时候，目光是聚焦在一个相对很小的局部的吧？严格一些说，普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上，而在卷积神经网络中，每个隐层节点只连接到图像某个足够小局部的像素点上，从而大大减少需要训练的权值参数。</p><p>对于一个 1000∗1000 的输入图像而言，如果下一个隐藏层的神经元数目为 106 个，采用全连接则有 1000∗1000∗106=1012 个权值参数，如此数目巨大的参数几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中 10∗10的局部图像相连接，那么此时的权值参数数量为 10∗10∗106=108，将直接减少4个数量级。</p><p>如下图所示，左边是每个像素的全连接，右边是每行隔两个像素作为局部连接，因此在数量上，少了很多权值参数数量（每一条连接每一条线需要有一个权值参数，具体忘记了的可以回顾单个[神经元模型]。因此 局部感知就是：<br>通过卷积操作，把 全连接变成局部连接 ，因为多层网络能够抽取高阶统计特性， 即使网络为局部连接，由于格外的突触连接和额外的神经交互作用，也可以使网络在不十分严格的意义下获得一个全局关系。</p><p><img src="/2019/04/17/神经网络/CNN/p63.jpg" alt="img"></p><p>下面我们来详细看一下到底局部感知如何使 全连接变成局部连接，按照人工神经网络的方法，把输入图像的像素一字排开之后，每一个像素值就是一个神经元输入，需要对隐层或者输出层做全连接，如上图左侧所示。卷积神经网络引入卷积概念后，卷积核通过原图像，然后卷积核对原图像上符合卷积核大小的像素进行加权求和，每一次只是对符合卷积核的图像像素做卷积，这就是局部感知的概念，使 全连接变成局部连接。</p><p><img src="/2019/04/17/神经网络/CNN/p64.png" alt="img"></p><p><strong>2.2 权值共享</strong></p><p>尽管局部感知使计算量减少了几个数量级，但权重参数数量依然很多。能不能再进一步减少呢？方法就是权值共享。</p><p>权值共享：不同的图像或者同一张图像共用一个卷积核，减少重复的卷积核。同一张图像当中可能会出现相同的特征，共享卷积核能够进一步减少权值参数。</p><p>如下图所示，为了找到鸟嘴，一个激活函数A需要检测图像左侧有没有鸟嘴，另外一个激活函数B需要检测另外一张图像中间有没有类似的鸟嘴。其实，鸟嘴都可能具有同样的特征，只需要一个激活函数C就可以了，这个时候，就可以共享同样的权值参数（也就是卷积核）。</p><p><img src="/2019/04/17/神经网络/CNN/p65.png" alt="img"></p><p>如果使用了权值共享（共同使用一个卷积核），那么将可以大大减少卷积核的数量，加快运算速度。天下武功，唯快不破。</p><p><img src="/2019/04/17/神经网络/CNN/p66.png" alt="img"></p><p>举个栗子，在局部连接中隐藏层的每一个神经元连接的是一个 10∗10 的局部图像，因此有 10∗10 个权值参数，将这 10∗10 个权值参数共享给剩下的神经元，也就是说隐藏层中 106 个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10∗10 个权值参数（也就是卷积核(也称滤波器)的大小。</p><p>尽管只有这么少的参数，依旧有出色的性能。但是，这样仅提取了图像的一种特征，如果要多提取出一些特征，需要增加多个卷积核，不同的卷积核能够得到图像的不同映射下的特征，称之为 Feature Map。如果有100个卷积核，最终的权值参数也仅为 100∗100=104 个而已。另外，偏置参数b也是共享的，同一种滤波器共享一个。</p><p><strong>2.3 池化</strong></p><p>在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。</p><p>pooling的好处有什么？<br>\1. 这些统计特征能够有更低的维度，减少计算量。<br>\2. 不容易过拟合，当参数过多的时候很容易造成过度拟合。<br>\3. 缩小图像的规模，提升计算速度。</p><p>如下图所示，原图是一张500∗500 的图像，经过subsampling之后哦，变成了一张 250∗250 的图像。这样操作的好处非常明显，虽然经过权值共享和局部连接后的图像权值参数已经大大减少，但是对于计算量来说，还是非常巨大，需要消费很大的计算时间，于是为了进一步减少计算量，于是加入了subsampling这个概念，不仅仅使图像像素减少了， 同时也减少计算时间。</p><p><img src="/2019/04/17/神经网络/CNN/p67.png" alt="img"></p><p>举个栗子：以最大池化（Max Pooling）为例，1000×1000的图像经过10×10的卷积核卷积后，得到的是991×991的特征图，然后使用2×2的池化规模，即每4个点组成的小方块中，取最大的一个作为输出，最终得到的是496×496大小的特征图。</p><p>下采样，即池化，目的是减小特征图，池化规模一般为2×2。常用的池化方法有：</p><p>Pooling算法</p><p>最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。<br>均值池化（Mean Pooling）。取4个点的均值。<br>可训练池化。训练函数 f ，接受4个点为输入，出入1个点。</p><p>由于特征图的变长不一定是2的倍数，所以在边缘处理上也有两种方案：</p><p>保留边缘。将特征图的变长用0填充为2的倍数，然后再池化。<br>忽略边缘。将多出来的边缘直接省去。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CNN学习&lt;/p&gt;
    
    </summary>
    
      <category term="神经网络" scheme="http://kodgv.xyz/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="网络结构" scheme="http://kodgv.xyz/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Attention</title>
    <link href="http://kodgv.xyz/2019/04/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Attention/"/>
    <id>http://kodgv.xyz/2019/04/15/神经网络/Attention/</id>
    <published>2019-04-15T08:07:35.000Z</published>
    <updated>2019-05-25T12:37:09.876Z</updated>
    
    <content type="html"><![CDATA[<p>Attention本质就是一系列注意力分配系数，也就是一系列权重参数罢了。<br><a id="more"></a><br>[TOC]<br>来源：<a href="https://www.cnblogs.com/guoyaohua/p/9429924.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/9429924.html</a></p><h1 id="1-什么是Attention机制？"><a href="#1-什么是Attention机制？" class="headerlink" title="1. 什么是Attention机制？"></a>1. 什么是Attention机制？</h1><p>　　最近两年，注意力模型（Attention Model）被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，是深度学习技术中最值得关注与深入了解的核心技术之一。</p><p>　　当我们人在看一样东西的时候，我们当前时刻关注的一定是我们当前正在看的这样东西的某一地方，换句话说，当我们目光移到别处时，注意力随着目光的移动也在转移，这意味着，当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。————-（思考：对于图片，会有些特别显眼的场景会率先吸引住注意力，那是因为脑袋中对这类东西很敏感。对于文本，我们大都是带目的性的去读，顺序查找，顺序读，但是在理解的过程中，我们是根据我们自带的目的去理解，去关注的。 注意力模型应该与具体的目的(或者任务)相结合。）</p><p>　　从Attention的作用角度出发，我们就可以从两个角度来分类Attention种类：<strong>Spatial Attention 空间注意力</strong>和<strong>Temporal Attention 时间注意力</strong>。更具实际的应用，也可以将Attention分为<strong>Soft Attention</strong>和<strong>Hard Attention</strong>。<strong>Soft Attention是所有的数据都会注意，都会计算出相应的注意力权值，不会设置筛选条件。Hard Attention会在生成注意力权重后筛选掉一部分不符合条件的注意力，让它的注意力权值为0，即可以理解为不再注意这些不符合条件的部分。</strong></p><h1 id="2-先了解编码-解码框架：Encoder-Decoder框架"><a href="#2-先了解编码-解码框架：Encoder-Decoder框架" class="headerlink" title="2. 先了解编码-解码框架：Encoder-Decoder框架"></a>2. 先了解编码-解码框架：Encoder-Decoder框架</h1><p>　　目前绝大多数文献中出现的AM模型是附着在Encoder-Decoder框架下的，当然，其实AM模型可以看作一种通用的思想，本身并不依赖于Encoder-Decoder模型，这点需要注意。<strong>Encoder-Decoder框架可以看作是一种文本处理领域的研究模式</strong>，应用场景异常广泛，本身就值得细谈。</p><p><img src="/2019/04/15/神经网络/Attention/p1.png" alt="img"></p><p>图1 抽象的Encoder-Decoder框架</p><p>　　Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对<x,y>。 ————（思考：<x,y>对很通用，X是一个问句，Y是答案；X是一个句子，Y是抽取的关系三元组；X是汉语句子，Y是汉语句子的英文翻译。等等），我们的目标是给定输入句子X，期待通过Encoder-Decoder框架来生成目标句子Y。X和Y可以是同一种语言，也可以是两种不同的语言。而X和Y分别由各自的单词序列构成：</x,y></x,y></p><p><img src="/2019/04/15/神经网络/Attention/f1.png" alt="img"></p><p>　　Encoder顾名思义就是对输入句子X进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p><p><img src="/2019/04/15/神经网络/Attention/f2.png" alt="img"></p><p>　　对于解码器Decoder来说，其任务是根据句子X的中间语义表示C和之前已经生成的历史信息y1,y2….yi-1来生成i时刻要生成的单词yi ：</p><p><img src="/2019/04/15/神经网络/Attention/f3.png" alt="img"></p><p>　　每个yi都依次这么产生，那么看起来就是整个系统根据输入句子X生成了目标句子Y。 ———（思考：其实这里的Encoder-Decoder是一个序列到序列的模型seq2seq，这个模型是对顺序有依赖的。）</p><p>　　Encoder-Decoder是个非常通用的计算框架，至于Encoder和Decoder具体使用什么模型都是由研究者自己定的，常见的比如 CNN / RNN / BiRNN / GRU / LSTM / Deep LSTM 等，这里的变化组合非常多。</p><hr><p>（思考：人的学习过程包括输入、输出、外界评价。Encoder模型类似于人的输入学习过程，Decoder模型类似于人的输出学习过程，对输出的内容进行评价就类似于损失函数。英语老师给我上了几堂英语课，我在不断的输入Encoder；突然有一个随堂测试，我得做题输出Decoder；最后英语老师改卷子，给我一个分数，不对的地方我得反思调整我对输入数据的加工方式。）</p><hr><p>（再思考：关于英语翻译。课本上的单词和课文是原始数据输入，相当于X；我在大脑里加工这些数据，相当于Encoder模型，我的脑子里有很多加工后的数据，相当于C；现在要让我翻译一个英语句子，这个任务相当于Y，我不能翻课本，所以我只能借助我脑袋里加工的数据C去翻译这个句子，即我得动脑子，相当于Decoder。 学习的过程是什么都要学，要分类整理，要增加线索，并不知道未来的某天能用到什么，所以Encoder-Decoder是一个泛泛学习的框架）</p><h1 id="3-Attention-Model"><a href="#3-Attention-Model" class="headerlink" title="3. Attention Model"></a>3. Attention Model</h1><p>　　以上介绍的Encoder-Decoder模型是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Y中每个单词的生成过程如下：</p><p><img src="/2019/04/15/神经网络/Attention/f4.png" alt="img"></p><p>　　其中<strong>f是decoder的非线性变换函数</strong>。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，是y1,y2也好，还是y3也好，他们使用的句子X的语义编码C都是一样的，没有任何区别。而语义编码C是由句子X的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实<strong>句子X中任意单词对生成某个目标单词yi来说影响力都是相同的，没有任何区别</strong>（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，估计这也是为何Google提出Sequence to Sequence模型时发现把输入句子逆序输入做翻译效果会更好的小Trick的原因）。这就是为何说这个模型<strong>没有体现出注意力</strong>的缘由。</p><p>　　引入AM模型，以翻译一个英语句子举例：输入X：Tom chase Jerry。 理想输出：汤姆追逐杰瑞。</p><p>　　应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：</p><p>（Tom,0.3）（Chase,0.2）（Jerry,0.5）</p><p>　　每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词Yi的时候，<strong>原先都是相同的中间语义表示C会替换成根据当前生成单词而不断变化的Ci。理解AM模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci</strong>。</p><p><img src="/2019/04/15/神经网络/Attention/p2.png" alt="img"></p><p>图2 引入AM模型的Encoder-Decoder框架</p><p>　　即生成目标句子单词的过程成了下面的形式：</p><p><img src="/2019/04/15/神经网络/Attention/p3.png" alt="img"></p><p>　　而<strong>每个Ci可能对应着不同的源语句子单词的注意力分配概率分布</strong>，比如对于上面的英汉翻译来说，其对应的信息可能如下：</p><p><img src="/2019/04/15/神经网络/Attention/p4.png" alt="img"></p><p>　　其中，<strong>f2函数代表Encoder对输入英文单词的某种变换函数</strong>，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；<strong>g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数</strong>，一般的做法中，<strong>g函数就是对构成元素加权求和</strong>，也就是常常在论文里看到的下列公式：</p><p><img src="/2019/04/15/神经网络/Attention/p5.png" alt="img"></p><p>　　假设Ci中那个i就是上面的“汤姆”，那么<strong>Tx就是3，代表输入句子的长度</strong>，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)，对应的注意力模型权值分别是0.6,0.2,0.2，<strong>所以g函数就是个加权求和函数</strong>。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的<strong>中间语义表示Ci的形成过程</strong>类似下图：</p><p><img src="/2019/04/15/神经网络/Attention/p6.png" alt="img"></p><p>图3 Ci的形成过程</p><p>　　这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，<strong>你怎么知道AM模型所需要的输入句子单词注意力分配概率分布值呢？</strong>就是说“汤姆”对应的概率分布：</p><p>划重点(注意力权重获取的过程)（Tom,0.3）（Chase,0.2）（Jerry,0.5）是如何得到的呢？</p><p>　　为了便于说明，我们假设对图1的非AM模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图1的图转换为下图：</p><p><img src="/2019/04/15/神经网络/Attention/p7.png" alt="img"></p><p>图4 RNN作为具体模型的Encoder-Decoder框架</p><p>　　注意力分配概率分布值的通用计算过程：</p><p><img src="/2019/04/15/神经网络/Attention/p8.png" alt="img"></p><p>图5 AM注意力分配概率计算</p><p>　　对于采用RNN的Decoder来说，如果要生成 yi 单词，在时刻 i ，我们是可以知道在生成 Yi 之前的隐层节点i时刻的输出值 Hi 的，而我们的目的是要计算生成 Yi 时的输入句子单词“Tom”、“Chase”、“Jerry”对 Yi 来说的注意力分配概率分布，那么可以用i时刻的<strong>隐层节点状态 Hi 去一一和输入句子中每个单词对应的RNN隐层节点状态 hj 进行对比</strong>，即<strong>通过函数 F(hj,Hi) 来获得目标单词 Yi 和每个输入单词对应的对齐可能性</strong>，这个F函数在不同论文里可能会采取不同的方法，然后<strong>函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值（这就得到了注意力权重）</strong>。图5显示的是当输出单词为“汤姆”时刻对应的输入句子单词的对齐概率。绝大多数AM模型都是<strong>采取上述的计算框架来计算注意力分配概率分布信息</strong>，<strong>区别只是在F的定义上可能有所不同</strong>。</p><p>　　<strong>上述内容就是论文里面常常提到的Soft Attention Model</strong>（任何数据都会给一个权值，没有筛选条件）的基本思想，你能在文献里面看到的大多数AM模型基本就是这个模型，区别很可能只是把这个模型用来解决不同的应用问题。那么<strong>怎么理解AM模型的物理含义呢？</strong>一般文献里会把AM模型看作是<strong>单词对齐模型</strong>，这是非常有道理的。目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的<strong>对齐概率</strong>，这在机器翻译语境下是非常直观的：<strong>传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤</strong>，<strong>而注意力模型其实起的是相同的作用</strong>。在其他应用里面把AM模型理解成输入句子和目标句子单词之间的对齐概率也是很顺畅的想法。</p><p><img src="/2019/04/15/神经网络/Attention/p9.png" alt="img"></p><p>图6 Google 神经网络机器翻译系统结构图</p><p>　　图6所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。</p><p>当然，从概念上理解的话，<strong>把AM模型理解成影响力模型也是合理的</strong>，就是说生成目标单词的时候，输入句子每个单词对于生成这个单词有多大的影响程度。这种想法也是比较好理解AM模型物理意义的一种思维方式。</p><p>　　图7是论文“<a href="http://www.aclweb.org/anthology/D15-1044" target="_blank" rel="noopener">A Neural Attention Model for Sentence Summarization</a>”中，Rush用AM模型来做生成式摘要给出的一个AM的一个非常直观的例子。</p><p><img src="/2019/04/15/神经网络/Attention/怕0.png" alt="img"></p><p>图7 句子生成式摘要例子</p><p>　　这个例子中，Encoder-Decoder框架的输入句子X是：“russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism”。对应图中纵坐标的句子。系统生成的摘要句子Y是：“russia calls for joint front against terrorism”，对应图中横坐标的句子。可以看出模型已经把句子主体部分正确地抽出来了。<strong>矩阵中每一列代表生成的目标单词对应输入句子每个单词的AM分配概率</strong>，颜色越深代表分配到的概率越大。这个例子对于直观理解AM是很有帮助作用。</p><p>　　《<a href="http://www.aclweb.org/anthology/D15-1044" target="_blank" rel="noopener">A Neural Attention Model for Sentence Summarization</a>》论文提供的实验数据集链接(开放可用)：<a href="https://duc.nist.gov/data.html" target="_blank" rel="noopener">DUC 2004</a>，感兴趣的朋友可以下载看看。</p><p><img src="/2019/04/15/神经网络/Attention/p11.png" alt="img"></p><p>图8 摘要生成 开放数据集</p><h1 id="4-Attention机制的本质思想"><a href="#4-Attention机制的本质思想" class="headerlink" title="4. Attention机制的本质思想"></a><strong>4. Attention机制的本质思想</strong></h1><p>　　如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806165033943-1072442256.png" alt="img"></p><p>图9 Attention机制的本质思想</p><p>　　我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的<key,value>数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</key,value></p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806165351485-870137528.png" alt="img"></p><p>　　其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p><p>　　当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p><p>　　从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p><p>　　至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806191525966-820975705.png" alt="img"></p><p>图10 三阶段计算Attention过程</p><p>　　在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个 Keyi ，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806191654338-264846698.png" alt="img"></p><p>　　第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><p>  <img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806195333945-948374778.png" alt="img"></p><p>　　第二阶段的计算结果 ai 即为 Valuei 对应的权重系数，然后进行加权求和即可得到Attention数值：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806195421837-251685236.png" alt="img"></p><p>　　通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p><h1 id="5-Self-Attention模型"><a href="#5-Self-Attention模型" class="headerlink" title="5. Self Attention模型"></a><strong>5. Self Attention模型</strong></h1><p>　　通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。<strong>Self Attention也经常被称为intra Attention（内部Attention）</strong>，最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型(抛弃了传统的RNN)。</p><p>　　在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素和Source中的所有元素之间。<strong>而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。</strong>其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。</p><p>　　elf Attention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention不同，它分别在source端和target端进行，仅与source input或者target input自身相关的Self Attention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self Attention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。因此，self Attention Attention比传统的Attention mechanism效果要好，主要原因之一是，传统的Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806200854591-1266493040.png" alt="img"></p><p>图11 可视化Self Attention实例</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806200949175-1322518214.png" alt="img"></p><p>图12 可视化Self Attention实例</p><p>　　从两张图（图11、图12）可以看出，<strong>Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。</strong></p><p>　　很明显，<strong>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征</strong>，<strong>因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</strong></p><p>　　但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以<strong>远距离依赖特征之间的距离被极大缩短</strong>，有利于有效地利用这些特征。除此外，<strong>Self Attention对于增加计算的并行性也有直接帮助作用</strong>。这是为何Self Attention逐渐被广泛使用的主要原因。</p><p>​    <strong>但是attention无法记录词序，所以在self-attention中增加了position embedding</strong></p><h1 id="五种attention模型"><a href="#五种attention模型" class="headerlink" title="五种attention模型"></a>五种attention模型</h1><h2 id="hard-attention-amp-soft-attention"><a href="#hard-attention-amp-soft-attention" class="headerlink" title="hard attention&amp;soft attention"></a>hard attention&amp;soft attention</h2><p><img src="https://image.jiqizhixin.com/uploads/editor/72f889a2-df04-4a85-9c2c-1591c5375537/1528709501819.png" alt="img"></p><p>■ 论文 | Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/812" target="_blank" rel="noopener">https://www.paperweekly.site/papers/812</a></p><p>■ 源码 | <a href="https://github.com/kelvinxu/arctic-captions" target="_blank" rel="noopener">https://github.com/kelvinxu/arctic-captions</a></p><p>文章讨论的场景是图像描述生成（Image Caption Generation），对于这种场景，先放一张图，感受一下 attention 的框架。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/33073f25-48d8-4b17-a179-49191bdb5357/1528709501703.png" alt="img"></p><p>文章提出了两种 attention 模式，即 hard attention 和 soft attention，来感受一下这两种 attention。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/fe4ede76-44de-4055-93e6-3ce99541db21/1528709502108.png" alt="img"></p><p>可以看到，hard attention 会专注于很小的区域，而 soft attention 的注意力相对发散。模型的 encoder 利用 CNN (VGG net)，提取出图像的 L 个 D 维的向量<img src="https://image.jiqizhixin.com/uploads/editor/7a76e6b0-4f08-4f55-8b01-839552bd0de1/1528709501860.png" alt="img">，每个向量表示图像的一部分信息。</p><p>decoder 是一个 LSTM，每个 timestep t 的输入包含三个部分，即 context vector Zt 、前一个 timestep 的 hidden state<img src="https://image.jiqizhixin.com/uploads/editor/a498b30a-f97c-483d-b121-1499bcabe34f/1528709501897.png" alt="img">、前一个 timestep 的 output<img src="https://image.jiqizhixin.com/uploads/editor/89174d88-3afc-4554-ba26-b48065473355/1528709502635.png" alt="img">。 Zt 由 {ai} 和权重 {αti} 通过加权得到。这里的权重 αti 通过attention模型 <em>f</em>att 来计算得到，而本文中的 <em>f</em>att 是一个多层感知机（multilayer perceptron）。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/55f3f8bd-91c7-4d65-917d-292e51d2962f/1528709502715.png" alt="img"></p><p>从而可以计算<img src="https://image.jiqizhixin.com/uploads/editor/b57ff198-4b7c-4e6d-a534-4ca57b825d3b/1528709502754.png" alt="img">。接下来文章重点讨论 hard（也叫 stochastic attention）和 soft（也叫 deterministic）两种 attention 模式。</p><p><strong>1. Stochastic “Hard” Attention</strong> </p><p>记 St 为 decoder 第 t 个时刻的 attention 所关注的位置编号， Sti 表示第 t 时刻 attention 是否关注位置 i ， Sti 服从多元伯努利分布（multinoulli distribution）， 对于任意的 t ，Sti,i=1,2,…,L 中有且只有取 1，其余全部为 0，所以 [St1,St2,…,stL] 是 one-hot 形式。这种 attention 每次只 focus 一个位置的做法，就是“hard”称谓的来源。 Zt 也就被视为一个变量，计算如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/afec742f-d647-43c6-b4dc-9618ac878628/1528709502797.png" alt="img"></p><p>问题是 αti 怎么算呢？把 αti 视为隐变量，研究模型的目标函数，进而研究目标函数对参数的梯度。直观理解，模型要根据 a=(a1,…,aL) 来生成序列 y=(y1,…,yC) ，所以目标可以是最大化 log p(y|a) ，但这里没有显式的包含 s ，所以作者利用著名的 Jensen 不等式（Jensen’s inequality）对目标函数做了转化，得到了目标函数的一个 lower bound，如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/97d7901e-3b6d-4fbc-879a-d81293368368/1528709503022.png" alt="img"></p><p>这里的 s ={ s1,…,sC }，是时间轴上的重点 focus 的序列，理论上这种序列共有个。 然后就用 log p(y|a) 代替原始的目标函数，对模型的参数 W 算 gradient。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/0f5a2605-cea7-4530-8415-e9d3dfb1d3b5/1528709503215.png" alt="img"></p><p>然后利用蒙特卡洛方法对 s 进行抽样，我们做 N 次这样的抽样实验，记每次取到的序列是<img src="https://image.jiqizhixin.com/uploads/editor/8b676888-0fab-4a40-9706-d829823fce0c/1528709503296.png" alt="img">，易知<img src="https://image.jiqizhixin.com/uploads/editor/1adc9914-c26a-47d7-a3e7-3889f9090160/1528709503251.png" alt="img">的概率为<img src="https://image.jiqizhixin.com/uploads/editor/5d82f812-b08a-48fd-bdb9-575fe66c15ce/1528709503333.png" alt="img">，所以上面的求 gradient 的结果即为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/5c7be181-dfdc-4458-b477-765b1613b234/1528709503396.png" alt="img"></p><p>接下来的一些细节涉及reinforcement learning，感兴趣的同学可以去看这篇 paper。</p><p><strong>2. Deterministic “Soft” Attention</strong> </p><p>说完“硬”的 attention，再来说说“软”的 attention。 相对来说 soft attention 很好理解，在 hard attention 里面，每个时刻 t 模型的序列 [ St1,…,StL ] 只有一个取 1，其余全部为 0，也就是说每次只 focus 一个位置，而 soft attention 每次会照顾到全部的位置，只是不同位置的权重不同罢了。这时 Zt 即为 ai 的加权求和：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/419b690d-2d19-4506-8f2e-67edf525fa7c/1528709503873.png" alt="img"></p><p>这样 soft attention 是光滑的且可微的（即目标函数，也就是 LSTM 的目标函数对权重αti 是可微的，原因很简单，因为目标函数对 Zt 可微，而 Zt 对 αti 可微，根据 chain rule 可得目标函数对 αti 可微）。</p><p>文章还对这种 soft attention 做了微调：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/caa7b6b6-ce70-4e19-aa1b-8a77a7914c09/1528709503914.png" alt="img"></p><p>其中<img src="https://image.jiqizhixin.com/uploads/editor/7de8d1a3-3be6-4a66-801e-25cb0bee7b00/1528709504003.png" alt="img">，用来调节 context vector 在 LSTM 中的比重（相对于<img src="https://image.jiqizhixin.com/uploads/editor/ae0fcc90-f8ef-40d8-be2a-8f9721dae24d/1528709501949.png" alt="img"><img src="https://image.jiqizhixin.com/uploads/editor/90abe04d-a4ea-4f7b-9d6b-d5832ad531f1/1528709502672.png" alt="img">的比重）。</p><p>btw，模型的 loss function 加入了 αti 的正则项。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/3193dce5-51b7-4ae2-893d-0696635c9ec9/1528709504051.png" alt="img"></p><h2 id="global-attention-amp-local-attention"><a href="#global-attention-amp-local-attention" class="headerlink" title="global attention &amp; local attention"></a>global attention &amp; local attention</h2><p><img src="https://image.jiqizhixin.com/uploads/editor/e3a3408b-7e64-4752-b8f9-a71563bb06c6/1528709504652.png" alt="img"></p><p>■ 论文 | Effective Approaches to Attention-based Neural Machine Translation</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/806" target="_blank" rel="noopener">https://www.paperweekly.site/papers/806</a></p><p>■ 源码 | <a href="https://github.com/lmthang/nmt.matlab" target="_blank" rel="noopener">https://github.com/lmthang/nmt.matlab</a></p><p>文章提出了两种 attention 的改进版本，即 global attention 和 local attention。先感受一下 global attention 和 local attention 长什么样子。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/55e64daf-5e26-474f-bdea-b99669de205c/1528709504213.png" alt="img"></p><p><strong>▲</strong> Global Attention</p><p><img src="https://image.jiqizhixin.com/uploads/editor/4df6785f-b67e-4a41-9234-401d03c07b59/1528709504569.png" alt="img"></p><p><strong>▲</strong> Local Attention</p><p>文章指出，local attention 可以视为 hard attention 和 soft attention 的混合体（优势上的混合），因为它的计算复杂度要低于 global attention、soft attention，而且与 hard attention 不同的是，local attention 几乎处处可微，易与训练。 文章以机器翻译为场景， x1,…,xn 为 source sentence， y1,…,ym 为 target sentence， c1,…,cm 为 encoder 产生的 context vector，objective function 为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/129e5871-39db-4005-8062-ca744a216335/1528709504768.png" alt="img"></p><p>Ct 来源于 encoder 中多个 source position 所产生的 hidden states，global attention 和 local attention 的主要区别在于 attention 所 forcus 的 source positions 数目的不同：如果 attention forcus 全部的 position，则是 global attention，反之，若只 focus 一部分 position，则为 local attention。 </p><p>由此可见，这里的 global attention、local attention 和 soft attention 并无本质上的区别，两篇 paper 模型的差别只是在 LSTM 结构上有微小的差别。 </p><p>在 decoder 的时刻 t ，在利用 global attention 或 local attention 得到 context vector Ct之后，结合 ht ，对二者做 concatenate 操作，得到 attention hidden state。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/7acdb6ad-0741-4531-a96a-a9e128afda32/1528709505007.png" alt="img"></p><p>最后利用 softmax 产出该时刻的输出：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/078f1af3-0c67-4b13-9359-14f1b48aea4d/1528709505051.png" alt="img"></p><p>下面重点介绍 global attention、local attention。</p><p><strong>1. global attention</strong> </p><p>global attention 在计算 context vector ct 的时候会考虑 encoder 所产生的全部hidden state。记 decoder 时刻 t 的 target hidden为 ht，encoder 的全部 hidden state 为<img src="https://image.jiqizhixin.com/uploads/editor/1a9b3f6f-cd94-4ba5-94d9-96e096dd0802/1528709505151.png" alt="img">，对于其中任意<img src="https://image.jiqizhixin.com/uploads/editor/cab720ef-628c-4af9-b827-f2839ba387a5/1528709505228.png" alt="img">，其权重 αts 为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/ebea9e73-7f59-42a1-975b-9cd9f44ee7ba/1528709505312.png" alt="img"></p><p>而其中的<img src="https://image.jiqizhixin.com/uploads/editor/31b5d177-4123-47f7-b75a-766690049b6f/1528709505690.png" alt="img">，文章给出了四种种计算方法（文章称为 alignment function）：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/5b89594b-f47c-4870-be8a-f3406b38088b/1528709505811.png" alt="img"></p><p><img src="https://image.jiqizhixin.com/uploads/editor/1f8e7063-9c3f-4c96-a528-a841c6117825/1528709505852.png" alt="img"></p><p>四种方法都比较直观、简单。在得到这些权重后， ct 的计算是很自然的，即为<img src="https://image.jiqizhixin.com/uploads/editor/1b37607a-9316-442c-9226-49595d34ff02/1528709505264.png" alt="img">的 weighted summation。</p><p><strong>2. local attention</strong> </p><p>global attention 可能的缺点在于每次都要扫描全部的 source hidden state，计算开销较大，对于长句翻译不利，为了提升效率，提出 local attention，每次只 focus 一小部分的 source position。 </p><p>这里，context vector ct 的计算只 focus 窗口 [pt-D,pt+D] 内的 2D+1 个source hidden states（若发生越界，则忽略界外的 source hidden states）。</p><p>其中 pt 是一个 source position index，可以理解为 attention 的“焦点”，作为模型的参数， D 根据经验来选择（文章选用 10）。 关于 pt 的计算，文章给出了两种计算方案：</p><ul><li><strong>Monotonic alignment (local-m)</strong></li></ul><p><img src="https://image.jiqizhixin.com/uploads/editor/4fe24e96-b61e-4bb4-9cb6-88760dc0f228/1528709505970.png" alt="img"></p><ul><li><strong>Predictive alignment (local-p)</strong></li></ul><p><img src="https://image.jiqizhixin.com/uploads/editor/ec31a794-00f9-4dba-b9d8-89295f1aa1d0/1528709506099.png" alt="img"></p><p>其中 Wp 和 vp 是模型的参数， S 是 source sentence 的长度，易知 pt∈[0,S] 。 权重αt(s) 的计算如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/703a445f-62ed-43dd-a313-256883bac239/1528709506142.png" alt="img"></p><p>可以看出，距离中心 pt 越远的位置，其位置上的 source hidden state 对应的权重就会被压缩地越厉害。</p><h2 id="self-attention-amp-multiple-head-attention"><a href="#self-attention-amp-multiple-head-attention" class="headerlink" title="self-attention &amp; multiple-head attention"></a>self-attention &amp; multiple-head attention</h2><p><img src="https://image.jiqizhixin.com/uploads/editor/d993ae54-145e-4e12-93dd-1f3c94d156f9/1528709506751.png" alt="img"></p><p>■ 论文 | Attention Is All You Need</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/224" target="_blank" rel="noopener">https://www.paperweekly.site/papers/224</a></p><p>■ 源码 | <a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">https://github.com/Kyubyong/transformer</a></p><p><img src="https://image.jiqizhixin.com/uploads/editor/da5cb6b1-aa95-48b4-a728-01bde8de45b4/1528709506930.png" alt="img"></p><p>■ 论文 | Weighted Transformer Network for Machine Translation</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/2013" target="_blank" rel="noopener">https://www.paperweekly.site/papers/2013</a></p><p>■ 源码 | <a href="https://github.com/JayParks/transformer" target="_blank" rel="noopener">https://github.com/JayParks/transformer</a></p><p>作者首先指出，结合了 RNN（及其变体）和注意力机制的模型在序列建模领域取得了不错的成绩，但由于 RNN 的循环特性导致其不利于并行计算，所以模型的训练时间往往较长，在 GPU 上一个大一点的 seq2seq 模型通常要跑上几天，所以作者对 RNN 深恶痛绝，遂决定舍弃 RNN，只用注意力模型来进行序列的建模。 </p><p>作者提出一种新型的网络结构，并起了个名字 Transformer，里面所包含的注意力机制称之为 self-attention。作者骄傲地宣称他这套 Transformer 是能够计算 input 和 output 的 representation 而不借助 RNN 的唯一的 model，所以作者说有 attention 就够了。</p><p>模型同样包含 encoder 和 decoder 两个 stage，encoder 和 decoder 都是抛弃 RNN，而是用堆叠起来的 self-attention，和 fully-connected layer 来完成，模型的架构如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/43ca78f9-024c-4a20-aa8c-681a42b6a734/1528709507321.png" alt="img"></p><p>从图中可以看出，模型共包含三个 attention 成分，分别是 encoder 的 self-attention，decoder 的 self-attention，以及连接 encoder 和 decoder 的 attention。  </p><p>这三个 attention block 都是 multi-head attention 的形式，输入都是 query Q 、key K 、value V 三个元素，只是 Q 、 K 、 V 的取值不同罢了。接下来重点讨论最核心的模块 multi-head attention（多头注意力）。 </p><p>multi-head attention 由多个 scaled dot-product attention 这样的基础单元经过 stack 而成。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/2a179490-64ac-4b22-b42d-1c3de5e08b5c/1528709507410.png" alt="img"></p><p>那重点就变成 scaled dot-product attention 是什么鬼了。按字面意思理解，scaled dot-product attention 即缩放了的点乘注意力，我们来对它进行研究。 </p><p>在这之前，我们先回顾一下上文提到的传统的 attention 方法（例如 global attention，score 采用 dot 形式）。</p><p>记 decoder 时刻 t 的 target hidden state 为 ht，encoder 得到的全部 source hidden state为<img src="https://image.jiqizhixin.com/uploads/editor/9f26e09b-d1f7-4e77-b1e8-2e8e5642ae9c/1528709507675.png" alt="img">，则 decoder 的 context vector ct 的计算过程如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/57b33413-699a-4c04-ae99-0bddaf11ea32/1528709507900.png" alt="img"></p><p>作者先抛出三个名词 query Q、key K、value V，然后计算这三个元素的 attention。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/cbffe9ff-5149-4959-ac8b-f6c4714ff9c9/1528709509122.png" alt="img"></p><p>我的写法与论文有细微差别，但为了接下来说明的简便，我姑且简化成这样。这个 Attention 的计算跟上面的 (*) 式有几分相似。</p><p>那么 Q、K、V 到底是什么？论文里讲的比较晦涩，说说我的理解。encoder 里的 attention 叫 self-attention，顾名思义，就是自己和自己做 attention。</p><p>抛开这篇论文的做法，让我们激活自己的创造力，在传统的 seq2seq 中的 encoder 阶段，我们得到 n 个时刻的 hidden states 之后，可以用每一时刻的 hidden state hi，去分别和任意的 hidden state hj,j=1,2,…,n 计算 attention，这就有点 self-attention 的意思。</p><p>回到当前的模型，由于抛弃了 RNN，encoder 过程就没了 hidden states，那拿什么做 self-attention 来自嗨呢？</p><p>可以想到，假如作为 input 的 sequence 共有 n 个 word，那么我可以先对每一个 word 做 embedding 吧？就得到 n 个 embedding，然后我就可以用 embedding 代替 hidden state 来做 self-attention 了。所以 Q 这个矩阵里面装的就是全部的 word embedding，K、V 也是一样。</p><p>所以为什么管 Q 叫query？就是你每次拿一个 word embedding，去“查询”其和任意的 word embedding 的 match 程度（也就是 attention 的大小），你一共要做 n 轮这样的操作。 </p><p>我们记 word embedding 的 dimension 为 dmodel ，所以 Q 的 shape 就是 n*dmodel， K、V 也是一样，第 i 个 word 的 embedding 为 vi，所以该 word 的 attention 应为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/17ebfb62-21b6-4df4-99ea-08d3bf275d36/1528709509168.png" alt="img"></p><p>那同时做全部 word 的 attention，则是：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/529a6cbf-f2fe-4d8c-a90d-f8a0567f6bf8/1528709508901.png" alt="img"></p><p>scaled dot-product attention 基本就是这样了。基于 RNN 的传统 encoder 在每个时刻会有输入和输出，而现在 encoder 由于抛弃了 RNN 序列模型，所以可以一下子把序列的全部内容输进去，来一次 self-attention 的自嗨。</p><p>理解了 scaled dot-product attention 之后，multi-head attention 就好理解了，因为就是 scaled dot-product attention 的 stacking。</p><p>先把 Q、K、V 做 linear transformation，然后对新生成的 Q’、K’、V’ 算 attention，重复这样的操作 h 次，然后把 h 次的结果做 concat，最后再做一次 linear transformation，就是 multi-head attention 这个小 block 的输出了。 </p><p><img src="https://image.jiqizhixin.com/uploads/editor/b39171e2-f71c-4ea9-a9de-0d6c8d983550/1528709508952.png" alt="img"></p><p>以上介绍了 encoder 的 self-attention。decoder 中的 encoder-decoder attention 道理类似，可以理解为用 decoder 中的每个 vi 对 encoder 中的 vj 做一种交叉 attention。</p><p>decoder 中的 self-attention 也一样的道理，只是要注意一点，decoder 中你在用 vi 对 vj 做 attention 时，有一些 pair 是不合法的。原因在于，虽然 encoder 阶段你可以把序列的全部 word 一次全输入进去，但是 decoder 阶段却并不总是可以，想象一下你在做 inference，decoder 的产出还是按从左至右的顺序，所以你的 vi 是没机会和 vj ( j&gt;i ) 做 attention 的。</p><p>那怎么将这一点体现在 attention 的计算中呢？文中说只需要令 score(vi,vj)=-∞ 即可。为何？因为这样的话：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/13be2088-2463-474a-855b-b2193a08ce44/1528709509073.png" alt="img"></p><p>所以在计算 vi 的 self-attention 的时候，就能够把 vj 屏蔽掉。所以这个问题也就解决了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Attention本质就是一系列注意力分配系数，也就是一系列权重参数罢了。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="神经网络" scheme="http://kodgv.xyz/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="网络结构" scheme="http://kodgv.xyz/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
</feed>
