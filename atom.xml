<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小黑屋</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://kodgv.xyz/"/>
  <updated>2019-04-27T11:15:57.363Z</updated>
  <id>http://kodgv.xyz/</id>
  
  <author>
    <name>KODGV</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>胶囊网络</title>
    <link href="http://kodgv.xyz/2019/04/27/%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C/"/>
    <id>http://kodgv.xyz/2019/04/27/胶囊网络/</id>
    <published>2019-04-27T08:30:18.000Z</published>
    <updated>2019-04-27T11:15:57.363Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="网络结构" scheme="http://kodgv.xyz/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>神经网络学习率</title>
    <link href="http://kodgv.xyz/2019/04/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    <id>http://kodgv.xyz/2019/04/22/神经网络/神经网络学习率/</id>
    <published>2019-04-22T14:54:39.000Z</published>
    <updated>2019-04-27T08:08:04.372Z</updated>
    
    <content type="html"><![CDATA[<p>神经网络学习率</p><a id="more"></a><p>[TOC]</p><p>来源:<a href="http://ruder.io/optimizing-gradient-descent/index.html#momentum" target="_blank" rel="noopener">http://ruder.io/optimizing-gradient-descent/index.html#momentum</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>​    调参就是指调学习率</p><p>​    学习速率是指导我们该如何通过损失函数的梯度调整网络权重的超参数。学习率越低，损失函数的变化速度就越慢。虽然使用低学习率可以确保我们不会错过任何局部极小值，但也意味着我们将花费更长的时间来进行收敛，特别是在被困在高原区域的情况下。</p><p>​    一般而言，用户可以利用过去的经验（或其他类型的学习资料）直观地设定学习率的最佳值。</p><p>因此，想得到最佳学习速率是很难做到的。下图演示了配置学习速率时可能遇到的不同情况。</p><p><img src="/2019/04/22/神经网络/神经网络学习率/p1.png" alt></p><p>对于太慢的学习速率来说，损失函数可能减小，但是按照非常浅薄的速率减小的。当进入了最优学习率区域，你将会观察到在损失函数上一次非常大的下降。进一步增加学习速率会造成损失函数值「跳来跳去」甚至在最低点附近发散。记住，最好的学习速率搭配着损失函数上最陡的下降，所以我们主要关注分析图的坡度。</p><p><img src="/2019/04/22/神经网络/神经网络学习率/p2.jpeg" alt></p><h2 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h2><p><strong>make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.</strong></p><h3 id="Batch-gradient-descent"><a href="#Batch-gradient-descent" class="headerlink" title="Batch gradient descent"></a>Batch gradient descent</h3><script type="math/tex; mode=display">\theta = \theta - \eta \cdot \nabla_\theta J( \theta)</script><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="title">range</span>(<span class="params">nb_epochs</span>):</span></span><br><span class="line"><span class="function">  params_grad</span> = evaluate_gradient(loss_function, data, <span class="keyword">params</span>)</span><br><span class="line">  <span class="keyword">params</span> = <span class="keyword">params</span> - learning_rate * params_grad</span><br></pre></td></tr></table></figure><p>优点：保证批量梯度下降收敛于凸误差曲面的全局最小值和非凸曲面的局部最小值。</p><p>缺点：下降可能非常慢，而且要求每次计算整个epcho的数据，对内存要求比较高，而且它不允许在线更新。</p><h3 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h3><script type="math/tex; mode=display">\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})</script><p>注意每次都需要对数据进行shuffle打乱</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_epochs):</span><br><span class="line">  <span class="built_in">np</span>.<span class="built_in">random</span>.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> <span class="built_in">example</span> <span class="keyword">in</span> data:</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, <span class="built_in">example</span>, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure><p>优点：减少计算的冗余，由于每次只更新一次，速度快，对在线更新友好。</p><p>缺点：会拥有比较高的方差，会震荡比较明显，如果学习率不足够小的话可能会收敛于局部点。</p><p>SGD的学习率一般要配合退火，不断变小</p><h3 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h3><script type="math/tex; mode=display">\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})</script><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_epochs):</span><br><span class="line">  <span class="built_in">np</span>.<span class="built_in">random</span>.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> <span class="built_in">batch</span> <span class="keyword">in</span> get_batches(data, batch_size=<span class="number">50</span>):</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, <span class="built_in">batch</span>, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure><p>优点：减少方差</p><p>缺点：batch需要调节，在内存和速度上trade-off</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>简而言之，虽然有mini-batch，但是学习率仍然需要精心设计学习速率过小会导致收敛异常缓慢，而学习速率过大则会阻碍收敛，导致损失函数在最小值附近波动，甚至出现发散。</p><h2 id="Adaptive-Optimizers"><a href="#Adaptive-Optimizers" class="headerlink" title="Adaptive Optimizers"></a>Adaptive Optimizers</h2><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><script type="math/tex; mode=display">\begin{align} \begin{split} v_t &= \gamma v_{t-1} + \eta \nabla_\theta J( \theta) \\ \theta &= \theta - v_t \end{split} \end{align}</script><p>要理解vt-1是上一次的更新，如果符号一致就会加速，如果符号不一致说明震荡，就会减速，这就是命名动量的意义，就像一个ball向下滚，它向下加速，向上却会受到空气阻力</p><p>优点：动量项对于梯度指向相同方向的维度增加，对于梯度改变方向的维度减少更新。因此，我们获得更快的收敛速度和减少振荡。</p><h3 id="Nesterov-accelerated-gradient"><a href="#Nesterov-accelerated-gradient" class="headerlink" title="Nesterov accelerated gradient"></a>Nesterov accelerated gradient</h3><p>然而，一个从山上滚下来的球，盲目地跟着斜坡走，是非常令人不满意的。我们想要一个更聪明的球，一个知道它要去哪里的球，这样它就知道在山再次倾斜之前减速。</p><script type="math/tex; mode=display">\begin{align} \begin{split} v_t &= \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} ) \\ \theta &= \theta - v_t \end{split} \end{align}</script><p>跟上面Momentum公式的唯一区别在于，梯度不是根据当前参数位置<img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bi-1%7D" alt="\theta_{i-1}">，而是根据先走了本来计划要走的一步后，达到的参数位置<img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bi-1%7D+-+%5Calpha%5Cbeta+d_%7Bi-1%7D" alt="\theta_{i-1} - \alpha\beta d_{i-1}">计算出来的。</p><p>对于这个改动，很多文章给出的解释是，能够让算法提前看到前方的地形梯度，如果前面的梯度比当前位置的梯度大，那我就可以把步子迈得比原来大一些，如果前面的梯度比现在的梯度小，那我就可以把步子迈得小一些。这个大一些、小一些，都是相对于原来不看前方梯度、只看当前位置梯度的情况来说的。</p><p>有很复杂的推导过程:<a href="https://zhuanlan.zhihu.com/p/22810533" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22810533</a></p><p>在原始形式中，Nesterov Accelerated Gradient（NAG）算法相对于Momentum的改进在于，以“向前看”看到的梯度而不是当前位置梯度去更新。经过变换之后的等效形式中，NAG算法相对于Momentum多了一个本次梯度相对上次梯度的变化量，这个变化量本质上是对目标函数二阶导的近似。由于利用了二阶导的信息，NAG算法才会比Momentum具有更快的收敛速度。</p><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>针对不同的参数设置不同的学习率，根据频繁更新的学习率小，不频繁更新的学习率大。</p><p>For brevity, we use gtgt to denote the gradient at time step tt. gt,igt,i is then the partial derivative of the objective function w.r.t. to the parameter θiθi at time step tt:</p><script type="math/tex; mode=display">g_{t, i} = \nabla_\theta J( \theta_{t, i})</script><p>The SGD update for every parameter θiθi at each time step tt then becomes:</p><script type="math/tex; mode=display">\theta_{t+1, i} = \theta_{t, i} - \eta \cdot g_{t, i}</script><p>In its update rule, Adagrad modifies the general learning rate ηη at each time step tt for every parameter θiθi based on the past gradients that have been computed for θiθi:</p><script type="math/tex; mode=display">\theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}</script><p>Gt∈Rd×d是一个对角矩阵，每个对角线位置i,i的值累加到t次迭代的对应参数 θi 梯度平方和。ϵ是平滑项，防止除零操作，一般取值1e−8。为什么分母要进行平方根的原因是去掉平方根操作算法的表现会大打折扣。</p><p>缺点：G_t会了累积越来越大，分母累积的和会越来越大。这样学习到后来的阶段，网络的更新能力会越来越弱，能学到的更多知识的能力也越来越弱</p><p>优点：因为有G_t的存在，所以n不用调，一般设0.01默认就可以了。</p><h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><p>修复上面的缺点，梯度和是递归的定义成历史梯度平方的衰减平均值。动态平均值E[g^2]_t<br>仅仅取决于当前的梯度值与上一时刻的平均值.这样就不需要算累积的量</p><script type="math/tex; mode=display">E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t</script><script type="math/tex; mode=display">\Delta \theta_t = - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}\\\Delta \theta_t = - \dfrac{\eta}{RMS[g]_{t}} g_t</script><script type="math/tex; mode=display">RMS[\Delta \theta]_{t} = \sqrt{E[\Delta \theta^2]_t + \epsilon}</script><p>在此处Adadelta其实还是依赖于全局学习率的，但是作者做了一定处理，经过近似牛顿迭代法之后：</p><script type="math/tex; mode=display">\begin{align} \begin{split} \Delta \theta_t &= - \dfrac{RMS[\Delta \theta]_{t-1}}{RMS[g]_{t}} g_{t} \\ \theta_{t+1} &= \theta_t + \Delta \theta_t \end{split} \end{align}</script><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p><em>相比于AdaGrad的历史梯度：</em></p><p><img src="/2019/04/22/神经网络/神经网络学习率/f6.jpg" alt="img"></p><p><em>RMSProp增加了一个衰减系数来控制历史信息的获取多少：</em></p><p><img src="/2019/04/22/神经网络/神经网络学习率/f7.jpg" alt="img"></p><p><strong>简单来讲，设置全局学习率之后，每次通过，全局学习率逐参数的除以经过衰减系数控制的历史梯度平方和的平方根，使得每个参数的学习率不同</strong></p><p>那么它起到的作用是什么呢？</p><p><strong>起到的效果是在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。</strong></p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p><strong>其实就是Momentum+RMSProp的结合，然后再修正其偏差</strong></p><script type="math/tex; mode=display">\begin{align} \begin{split} m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \end{split} \end{align}</script><p>修正偏差：</p><script type="math/tex; mode=display">\begin{align} \begin{split} \hat{m}_t &= \dfrac{m_t}{1 - \beta^t_1} \\ \hat{v}_t &= \dfrac{v_t}{1 - \beta^t_2} \end{split} \end{align}</script><p>计算梯度：</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t</script><p>The authors propose default values of 0.9 for β1, 0.999 for β2, and 10−8for ϵ.</p><p><strong>1.Adams可能不收敛</strong></p><p>文中各大优化算法的学习率：其中，SGD没有用到二阶动量，因此学习率是恒定的（实际使用过程中会采用学习率衰减策略，因此学习率递减）。AdaGrad的二阶动量不断累积，单调递增，因此学习率是单调递减的。因此，这两类算法会使得学习率不断递减，最终收敛到0，模型也得以收敛。</p><p>但AdaDelta和Adam则不然。二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得 <img src="https://www.zhihu.com/equation?tex=V_t" alt="V_t"> 可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。</p><p><strong>2.Adams可能错失全局最优解</strong></p><p>​       吐槽Adam最狠的 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1705.08292" target="_blank" rel="noopener">The Marginal Value of Adaptive Gradient Methods in Machine Learning</a> 。文中说到，同样的一个优化问题，不同的优化算法可能会找到不同的答案，但自适应学习率的算法往往找到非常差的答案。他们通过一个特定的数据例子说明，自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>那么，现在应该使用哪个优化器呢?如果您的输入数据是稀疏的，那么您可能使用自适应学习率方法之一获得最佳结果。另一个好处是，您不需要调优学习率，但是可以使用缺省值获得最佳结果。</p><p>总之，RMSprop是Adagrad的一个扩展，它处理的是学习速度的急剧下降。它与Adadelta相同，只是Adadelta在numinator update规则中使用参数更新的RMS。最后，Adam为RMSprop添加了偏差修正和动量。到目前为止，RMSprop、Adadelta和Adam都是非常相似的算法，它们在相似的环境中都表现得很好。Kingma等人[14:1]表明，当梯度变得更稀疏时，它的偏倚校正帮助Adam在优化的最后略微优于RMSprop。到目前为止，Adam可能是最好的选择。</p><p>有趣的是，许多最近的论文使用SGD和一个简单的学习速率退火时间表。正如所示，SGD通常能够找到最小值，但是它可能比一些优化器花费的时间要长得多，更依赖于健壮的初始化和退火调度，并且可能会陷入鞍点而不是局部极小值。因此，如果你关心快速收敛和训练一个深度或复杂的神经网络，你应该选择一种自适应学习速率方法。</p><h2 id="Additional-strategies"><a href="#Additional-strategies" class="headerlink" title="Additional strategies"></a>Additional strategies</h2><h3 id="Shuffling-and-Curriculum-Learning"><a href="#Shuffling-and-Curriculum-Learning" class="headerlink" title="Shuffling and Curriculum Learning"></a>Shuffling and Curriculum Learning</h3><p>​    通常，我们希望避免以有意义的顺序为模型提供训练示例，因为这可能会影响优化算法。因此，在每个epoch之后重新整理训练数据通常是一个好主意。</p><p>​    另一方面，在某些情况下，我们的目标是逐步解决更难的问题，按有意义的顺序提供训练示例实际上可能会提高性能和更好的收敛性。建立这种有意义顺序的方法称为课程学习</p><h3 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h3><p>​    为了便于学习，我们通常用零均值和单位方差初始化参数的初始值，从而对参数的初始值进行标准化。随着训练的进展，我们在不同程度上更新参数，我们失去了这种标准化，这减慢了训练的速度，并随着网络变得更深而放大了变化。</p><p>​    批处理规范化[27]为每个小批处理重新建立这些规范化，并通过操作反向传播更改。通过将标准化作为模型体系结构的一部分，我们可以使用更高的学习率，而不太关注初始化参数。批处理规范化还可以作为一个正则化器，减少(有时甚至消除)退出的需要。</p><h3 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h3><p>​    你应该观察验证集上的误差，并且停止如果它没有足够的提高了。</p><h2 id="newest-algorithns"><a href="#newest-algorithns" class="headerlink" title="newest algorithns"></a>newest algorithns</h2><h3 id="Cyclical-Learning-Rates"><a href="#Cyclical-Learning-Rates" class="headerlink" title="Cyclical Learning Rates"></a>Cyclical Learning Rates</h3><p>两个特点：</p><ul><li>它为我们提供了一种在训练过程中有效地控制学习率的方法，方法是在上下界之间以三角形的方式改变学习率</li><li>它为我们提供了一个非常不错的估计，即哪种学习率的范围适合您的特定网络。</li></ul><p><img src="/2019/04/22/神经网络/神经网络学习率/p2.png" alt></p><p>There are a number of parameters to play around with here:</p><ul><li><strong>step size</strong>: during how many epochs will the LR go up from the lower bound, up to the upper bound.</li><li><strong>max_lr</strong>: the highest LR in the schedule.</li><li><strong>base_lr</strong>: the lowest LR in the schedule, in practice: the author of the paper suggests to take this a factor R smaller than the <strong>max_lr</strong>. Our used factor was 6.</li></ul><p>它的核心思想:这个学习率策略的本质来自于一个观察，增加学习率会有短暂的负面影响，但是长远来看有好处。这个观察启发了我们的想法，让学习率在一个范围内变化，而不是用常值或指数递减啥的。所以只需要设置上下界和周期变化就可以了。大量的实验尝试了各种形式，triangular window (linear), a Welch window (parabolic) and a Hann window (sinusoidal)，他们的结果差不多。就采用triangular窗吧</p><p><strong>代码实现</strong></p><h4 id="Step-1-find-the-upper-LR"><a href="#Step-1-find-the-upper-LR" class="headerlink" title="Step 1: find the upper LR"></a>Step 1: find the upper LR</h4><ul><li>define an initial learning rate, the lower boundary of the range you want to test (let’s say 1e-7)</li><li>define an upper boundary of the range (let’s say 0.1)</li><li>define an exponential scheme to run through this step by step:</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (lr_find_epochs * len( dataloaders[<span class="string">"train"</span>])))</span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)</span><br></pre></td></tr></table></figure><ul><li>Next, do a run (I used two epochs) through your network. At each step (each batch size): capture the LR, capture the loss and optimize the gradients:</li></ul><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the model</span></span><br><span class="line"><span class="attr">model</span> = CNN().to(device)</span><br><span class="line"></span><br><span class="line"><span class="attr">optimizer</span> = torch.optim.SGD(model.parameters(), start_lr)</span><br><span class="line"><span class="attr">criterion</span> = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make lists to capture the logs</span></span><br><span class="line"></span><br><span class="line"><span class="attr">lr_find_loss</span> = []</span><br><span class="line"><span class="attr">lr_find_lr</span> = []</span><br><span class="line"></span><br><span class="line"><span class="attr">iter</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="attr">smoothing</span> = <span class="number">0.05</span></span><br><span class="line"></span><br><span class="line">for i <span class="keyword">in</span> range(lr_find_epochs):</span><br><span class="line">  print(<span class="string">"epoch &#123;&#125;"</span>.format(i))</span><br><span class="line">  for inputs, labels <span class="keyword">in</span> dataloaders[<span class="string">"train"</span>]:</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Send to device</span></span><br><span class="line">    <span class="attr">inputs</span> = inputs.to(device)</span><br><span class="line">    <span class="attr">labels</span> = labels.to(device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Training mode and zero gradients</span></span><br><span class="line">    model.train()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get outputs to calc loss</span></span><br><span class="line">    <span class="attr">outputs</span> = model(inputs)</span><br><span class="line">    <span class="attr">loss</span> = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update LR</span></span><br><span class="line">    scheduler.step()</span><br><span class="line">    <span class="attr">lr_step</span> = optimizer.state_dict()[<span class="string">"param_groups"</span>][<span class="number">0</span>][<span class="string">"lr"</span>]</span><br><span class="line">    lr_find_lr.append(lr_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># smooth the loss</span></span><br><span class="line">    <span class="keyword">if</span> <span class="attr">iter==0:</span></span><br><span class="line">      lr_find_loss.append(loss)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="attr">loss</span> = smoothing  * loss + (<span class="number">1</span> - smoothing) * lr_find_loss[-<span class="number">1</span>]</span><br><span class="line">      lr_find_loss.append(loss)    </span><br><span class="line">    iter += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>观察图，确定一个范围。根据fast.ai的课程描述，一个好的上界不是在最低点，而是左移10倍左右，一个好的下界是上界，除以一个因子6</p><p><img src="/2019/04/22/神经网络/神经网络学习率/p3.png" alt></p><h4 id="Step-2-CLR-scheduler"><a href="#Step-2-CLR-scheduler" class="headerlink" title="Step 2: CLR scheduler"></a>Step 2: CLR scheduler</h4><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def cyclical_lr(stepsize, min_lr=<span class="number">3e-2</span>, max_lr=<span class="number">3e-3</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Scaler: we can adapt this if we do not want the triangular CLR</span></span><br><span class="line">    scaler = lambda x: <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Lambda function to calculate the LR</span></span><br><span class="line">    lr_lambda = lambda <span class="keyword">it</span>: min_lr + (max_lr - min_lr) * <span class="built_in">relative</span>(<span class="keyword">it</span>, stepsize)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Additional function to see where on the cycle we are</span></span><br><span class="line">    def <span class="built_in">relative</span>(<span class="keyword">it</span>, stepsize):</span><br><span class="line">        cycle = math.floor(<span class="number">1</span> + <span class="keyword">it</span> / (<span class="number">2</span> * stepsize))</span><br><span class="line">        x = <span class="built_in">abs</span>(<span class="keyword">it</span> / stepsize - <span class="number">2</span> * cycle + <span class="number">1</span>)</span><br><span class="line">        <span class="literal">return</span> <span class="built_in">max</span>(<span class="number">0</span>, (<span class="number">1</span> - x)) * scaler(cycle)</span><br><span class="line"></span><br><span class="line">    <span class="literal">return</span> lr_lambda</span><br></pre></td></tr></table></figure><h4 id="Step-3-wrap-it"><a href="#Step-3-wrap-it" class="headerlink" title="Step 3: wrap it"></a>Step 3: wrap it</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = CNN().<span class="keyword">to</span>(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">    </span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), <span class="attribute">lr</span>=1.)</span><br><span class="line">step_size = 4*len(train_loader)</span><br><span class="line">clr = cyclical_lr(step_size, <span class="attribute">min_lr</span>=end_lr/factor, <span class="attribute">max_lr</span>=end_lr)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr])</span><br></pre></td></tr></table></figure><h4 id="Step-4-train"><a href="#Step-4-train" class="headerlink" title="Step 4: train"></a>Step 4: train</h4><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (images, labels) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">  images, labels = images.<span class="keyword">to</span>(device), labels.<span class="keyword">to</span>(device)</span><br><span class="line">  <span class="comment">#Clear the gradients</span></span><br><span class="line">  optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">  <span class="comment">#Forward propagation </span></span><br><span class="line">  outputs = model(images)   </span><br><span class="line">            </span><br><span class="line">  <span class="comment">#Calculating loss with softmax to obtain cross entropy loss</span></span><br><span class="line">  loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#Backward propation</span></span><br><span class="line">  loss.backward()</span><br><span class="line">  scheduler.step() <span class="comment"># &gt; Where the magic happens</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">#Updating gradients</span></span><br><span class="line">  optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="带有热重启的随机梯度下降（SGDR）"><a href="#带有热重启的随机梯度下降（SGDR）" class="headerlink" title="带有热重启的随机梯度下降（SGDR）"></a>带有热重启的随机梯度下降（SGDR）</h2><p><img src="/2019/04/22/神经网络/神经网络学习率/p4.jpeg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;神经网络学习率&lt;/p&gt;
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习率" scheme="http://kodgv.xyz/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>NLP比赛心得搜集集合</title>
    <link href="http://kodgv.xyz/2019/04/22/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/%E6%AF%94%E8%B5%9B%E5%BF%83%E5%BE%97%E6%90%9C%E9%9B%86%E9%9B%86%E5%90%88/"/>
    <id>http://kodgv.xyz/2019/04/22/竞赛经验/比赛心得搜集集合/</id>
    <published>2019-04-22T14:22:41.000Z</published>
    <updated>2019-04-22T14:24:17.818Z</updated>
    
    <content type="html"><![CDATA[<p>NLP比赛心得</p><a id="more"></a><p><a href="https://www.kaggle.com/c/quora-insincere-questions-classification" target="_blank" rel="noopener">Quroa 识别不良句子</a></p><p><a href="https://www.getit01.com/p20190314357550039/" target="_blank" rel="noopener">https://www.getit01.com/p20190314357550039/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NLP比赛心得&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>FocalLoss针对不平衡数据</title>
    <link href="http://kodgv.xyz/2019/04/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/FocalLoss%E9%92%88%E5%AF%B9%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE/"/>
    <id>http://kodgv.xyz/2019/04/22/神经网络/FocalLoss针对不平衡数据/</id>
    <published>2019-04-22T10:32:49.000Z</published>
    <updated>2019-04-22T13:00:27.644Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>[TOC]</p><p>参考来源:</p><p><a href="https://zhuanlan.zhihu.com/p/32423092" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32423092</a></p><p><a href="https://www.zhihu.com/question/63581984" target="_blank" rel="noopener">https://www.zhihu.com/question/63581984</a></p><p><a href="https://zhuanlan.zhihu.com/p/28527749" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28527749</a></p><h2 id="讲解"><a href="#讲解" class="headerlink" title="讲解"></a>讲解</h2><p>​    本质上讲，Focal Loss 就是一个解决<strong>分类问题中类别不平衡、分类难度差异</strong>的一个 loss，总之这个工作一片好评就是了。大家还可以看知乎的讨论：<a href="https://www.zhihu.com/question/63581984" target="_blank" rel="noopener">如何评价 Kaiming 的 Focal Loss for Dense Object Detection？</a></p><p><strong>核心思想</strong></p><p>这样的做法就是：<strong>正样本的预测值大于 0.5 的，或者负样本的预测值小于 0.5 的，我都不更新了，把注意力集中在预测不准的那些样本，当然这个阈值可以调整。</strong>这样做能部分地达到目的，但是所需要的迭代次数会大大增加。</p><p>原因是这样的：以正样本为例，<strong>我只告诉模型正样本的预测值大于 0.5 就不更新了，却没有告诉它要“保持”大于 0.5</strong>，所以下一阶段，它的预测值就很有可能变回小于 0.5 了。当然，如果是这样的话，下一回合它又被更新了，这样反复迭代，理论上也能达到目的，但是迭代次数会大大增加。</p><p>所以，要想改进的话，重点就是<strong>“不只是要告诉模型正样本的预测值大于0.5就不更新了，而是要告诉模型当其大于0.5后就只需要保持就好了”</strong>。好比老师看到一个学生及格了就不管了，这显然是不行的。如果学生已经及格，那么应该要想办法要他保持目前这个状态甚至变得更好，而不是不管。</p><p>所以除了单纯的区分外，必须使该loss可导，这样才可以告诉模型。</p><p><strong>目的是通过减少易分类样本的权重，从而使得模型在训练时更专注于难分类的样本</strong>。</p><p>Kaiming 大神的 Focal Loss 形式是：</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f1.jpg" alt></p><p>​    首先y’的范围是0到1，所以不管γ是多少，这个调制系数都是大于等于0的。易分类的样本再多，你的权重很小，那么对于total loss的共享也就不会太大。那么怎么控制样本权重呢？举个例子，假设一个二分类，样本x1属于类别1的y’=0.9，样本x2属于类别1的y’=0.6，显然前者更可能是类别1，假设γ=1，那么对于y’=0.9，调制系数则为0.1；对于y’=0.6，调制系数则为0.4，这个调制系数就是这个样本对loss的贡献程度，也就是权重，所以难分的样本（pt=0.6）的权重更大。</p><p>​    <strong>比如负样本远比正样本多的话，模型肯定会倾向于数目多的负类（可以想象全部样本都判为负类），这时候，负类的</strong> <strong>*ŷ γ*</strong> <strong>或</strong> <strong>σ(Kx) 都很小，而正类的</strong> <strong>(1−ŷ )γ</strong> <strong>或</strong> <strong>*σ(−Kx)*</strong> <strong>就很大，这时候模型就会开始集中精力关注正样本。</strong></p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f2.jpg" alt></p><p>通过一系列调参，得到 <em>α=0.25, γ=2</em>（在他的模型上）的效果最好。注意在他的任务中，正样本是属于少数样本，也就是说，本来正样本难以“匹敌”负样本，但经过 <em>(1−ŷ )γ</em> 和 <em>ŷγ</em> 的“操控”后，也许形势还逆转了，还要对正样本降权。</p><p>不过我认为这样调整只是经验结果，理论上很难有一个指导方案来决定 <em>α</em> 的值，如果没有大算力调参，倒不如直接让 <em>α=0.5</em>（均等）。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>本文实验中采用的Focal Loss 代码如下。</p><p>关于Focal Loss 的数学推倒在文章：<strong>Focal Loss 的前向与后向公式推导</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">r"""</span></span><br><span class="line"><span class="string">        This criterion is a implemenation of Focal Loss, which is proposed in </span></span><br><span class="line"><span class="string">        Focal Loss for Dense Object Detection.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            Loss(x, class) = - \alpha (1-softmax(x)[class])^gamma \log(softmax(x)[class])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The losses are averaged across observations for each minibatch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            alpha(1D Tensor, Variable) : the scalar factor for this criterion</span></span><br><span class="line"><span class="string">            gamma(float, double) : gamma &gt; 0; reduces the relative loss for well-classiﬁed examples (p &gt; .5), </span></span><br><span class="line"><span class="string">                                   putting more focus on hard, misclassiﬁed examples</span></span><br><span class="line"><span class="string">            size_average(bool): By default, the losses are averaged over observations for each minibatch.</span></span><br><span class="line"><span class="string">                                However, if the field size_average is set to False, the losses are</span></span><br><span class="line"><span class="string">                                instead summed for each minibatch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, class_num, alpha=None, gamma=<span class="number">2</span>, size_average=True)</span>:</span></span><br><span class="line">        super(FocalLoss, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> alpha <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.alpha = Variable(torch.ones(class_num, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> isinstance(alpha, Variable):</span><br><span class="line">                self.alpha = alpha</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.alpha = Variable(alpha)</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.class_num = class_num</span><br><span class="line">        self.size_average = size_average</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets)</span>:</span></span><br><span class="line">        N = inputs.size(<span class="number">0</span>)</span><br><span class="line">        C = inputs.size(<span class="number">1</span>)</span><br><span class="line">        P = F.softmax(inputs)</span><br><span class="line"></span><br><span class="line">        class_mask = inputs.data.new(N, C).fill_(<span class="number">0</span>)</span><br><span class="line">        class_mask = Variable(class_mask)</span><br><span class="line">        ids = targets.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        class_mask.scatter_(<span class="number">1</span>, ids.data, <span class="number">1.</span>)</span><br><span class="line">        <span class="comment">#print(class_mask)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs.is_cuda <span class="keyword">and</span> <span class="keyword">not</span> self.alpha.is_cuda:</span><br><span class="line">            self.alpha = self.alpha.cuda()</span><br><span class="line">        alpha = self.alpha[ids.data.view(<span class="number">-1</span>)]</span><br><span class="line"></span><br><span class="line">        probs = (P*class_mask).sum(<span class="number">1</span>).view(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        log_p = probs.log()</span><br><span class="line">        <span class="comment">#print('probs size= &#123;&#125;'.format(probs.size()))</span></span><br><span class="line">        <span class="comment">#print(probs)</span></span><br><span class="line"></span><br><span class="line">        batch_loss = -alpha*(torch.pow((<span class="number">1</span>-probs), self.gamma))*log_p </span><br><span class="line">        <span class="comment">#print('-----bacth_loss------')</span></span><br><span class="line">        <span class="comment">#print(batch_loss)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.size_average:</span><br><span class="line">            loss = batch_loss.mean()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = batch_loss.sum()</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="引申"><a href="#引申" class="headerlink" title="引申"></a>引申</h2><p>这就是为什么之前别人做数据增强的时候，把预测很高的数据当作1把预测很低的数据当作0放进去加强训练。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;参考来源:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32423092&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanl
      
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="不平衡数据" scheme="http://kodgv.xyz/tags/%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>好的参考汇总</title>
    <link href="http://kodgv.xyz/2019/04/21/%E5%A5%BD%E7%9A%84%E5%8F%82%E8%80%83%E6%B1%87%E6%80%BB/"/>
    <id>http://kodgv.xyz/2019/04/21/好的参考汇总/</id>
    <published>2019-04-21T06:45:42.000Z</published>
    <updated>2019-04-27T07:49:16.079Z</updated>
    
    <content type="html"><![CDATA[<p>好的代码借鉴，必要的时候可以直接抄</p><a id="more"></a><p>[TOC]</p><h1 id="比赛心得"><a href="#比赛心得" class="headerlink" title="比赛心得"></a>比赛心得</h1><h2 id="机器翻译注意力机制及其PyTorch实现"><a href="#机器翻译注意力机制及其PyTorch实现" class="headerlink" title="机器翻译注意力机制及其PyTorch实现"></a>机器翻译注意力机制及其PyTorch实现</h2><p><a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/" target="_blank" rel="noopener">https://plmsmile.github.io/2017/10/12/Attention-based-NMT/</a></p><h2 id="各个NLP模型实现"><a href="#各个NLP模型实现" class="headerlink" title="各个NLP模型实现"></a>各个NLP模型实现</h2><p><a href="http://www.zhongruitech.com/921029206.html" target="_blank" rel="noopener">http://www.zhongruitech.com/921029206.html</a></p><h1 id="比赛"><a href="#比赛" class="headerlink" title="比赛"></a>比赛</h1><p>如何不过拟合:<a href="https://www.kaggle.com/c/dont-overfit-ii" target="_blank" rel="noopener">https://www.kaggle.com/c/dont-overfit-ii</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;好的代码借鉴，必要的时候可以直接抄&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>数据挖掘问答汇总</title>
    <link href="http://kodgv.xyz/2019/04/21/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E9%97%AE%E7%AD%94%E6%B1%87%E6%80%BB/"/>
    <id>http://kodgv.xyz/2019/04/21/数据挖掘问答汇总/</id>
    <published>2019-04-21T02:43:51.000Z</published>
    <updated>2019-04-21T02:45:22.116Z</updated>
    
    <content type="html"><![CDATA[<p>数据挖掘竞赛汇总</p><a id="more"></a><p>[TOC]</p><h2 id="如何知道树模型怎么可以提高？"><a href="#如何知道树模型怎么可以提高？" class="headerlink" title="如何知道树模型怎么可以提高？"></a>如何知道树模型怎么可以提高？</h2><p>通过画树的图，分析树当前无法分割的知识是什么，给它补充进数据里面。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据挖掘竞赛汇总&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>NLP代码汇总</title>
    <link href="http://kodgv.xyz/2019/04/20/NLP%E4%BB%A3%E7%A0%81%E6%B1%87%E6%80%BB/"/>
    <id>http://kodgv.xyz/2019/04/20/NLP代码汇总/</id>
    <published>2019-04-20T13:57:19.000Z</published>
    <updated>2019-04-27T07:13:55.279Z</updated>
    
    <content type="html"><![CDATA[<p>NLP汇总</p><a id="more"></a><p>[TOC]</p><h2 id="动态padding，节省时间"><a href="#动态padding，节省时间" class="headerlink" title="动态padding，节省时间"></a>动态padding，节省时间</h2><p>比起一开始固定化padding，动态padding，可以针对当前batch的长度来Padding，明显会增快速度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, text, lens, y=None)</span>:</span></span><br><span class="line">        self.text = text</span><br><span class="line">        self.y = y</span><br><span class="line">        self.lens = lens</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.lens)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self.text[index], self.lens[index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.text[index], self.lens[index], self.y[index]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(batch)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    batch = [dataset[i] for i in N]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    size = len(batch[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> size == <span class="number">3</span>:</span><br><span class="line">        texts, lens, y = zip(*batch)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        texts, lens = zip(*batch)</span><br><span class="line">    lens = np.array(lens)</span><br><span class="line">    sort_idx = np.argsort(<span class="number">-1</span> * lens)</span><br><span class="line">    reverse_idx = np.argsort(sort_idx)</span><br><span class="line">    max_len = min(int(np.percentile(lens, PERCENTILE)), MAX_LEN)</span><br><span class="line">    </span><br><span class="line">    lens = np.clip(lens, <span class="number">0</span>, max_len)[sort_idx]</span><br><span class="line">    texts = torch.tensor(sequence.pad_sequences(texts, maxlen=max_len)[sort_idx], dtype=torch.long).cuda()</span><br><span class="line">    <span class="keyword">if</span> size == <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">return</span> texts, lens, reverse_idx, torch.tensor(y, dtype=torch.float32).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> texts, lens, reverse_idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_data_loader</span><span class="params">(texts, lens, y=None, batch_size=BATCH_SIZE)</span>:</span></span><br><span class="line">    dset = MyDataset(texts, lens, y)</span><br><span class="line">    dloader = data.DataLoader(dset, batch_size=batch_size, shuffle=<span class="literal">False</span>, collate_fn=collate_fn)</span><br><span class="line">    <span class="keyword">return</span> dloader</span><br><span class="line">seqs = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>]]</span><br><span class="line">lens = [len(i) <span class="keyword">for</span> i <span class="keyword">in</span> seqs]</span><br><span class="line"></span><br><span class="line">data_loader = build_data_loader(seqs, lens)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data_loader:</span><br><span class="line">    seq_batch, lens_batch, reverse_idx_batch = batch</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">print(<span class="string">f'original seqs:'</span>)</span><br><span class="line">print(seqs)</span><br><span class="line">print(<span class="string">f'batch seqs, already sort by lens, and padding dynamic in batch:'</span>)</span><br><span class="line">print(seq_batch)</span><br><span class="line">print(<span class="string">f'reverse batch seqs:'</span>)</span><br><span class="line">print(seq_batch[reverse_idx_batch])</span><br><span class="line">h_embedding_pack = pack_padded_sequence(seq_batch,lens_batch,batch_first=<span class="literal">True</span>)</span><br><span class="line">print(h_embedding_pack)</span><br></pre></td></tr></table></figure><h2 id="mask-loss-避免无用结果的求导影响"><a href="#mask-loss-避免无用结果的求导影响" class="headerlink" title="mask loss 避免无用结果的求导影响"></a>mask loss 避免无用结果的求导影响</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> func</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CustomLoss,self).__init__()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, Y_hat, Y)</span>:</span></span><br><span class="line">       <span class="comment"># TRICK 3 ********************************</span></span><br><span class="line">       <span class="comment"># before we calculate the negative log likelihood, we need to mask out the activations</span></span><br><span class="line">       <span class="comment"># this means we don't want to take into account padded items in the output vector</span></span><br><span class="line">       <span class="comment"># simplest way to think about this is to flatten ALL sequences into a REALLY long sequence</span></span><br><span class="line">       <span class="comment"># and calculate the loss on that.</span></span><br><span class="line">       <span class="comment"># flatten all the labels</span></span><br><span class="line">        Y = Y.view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># flatten all predictions</span></span><br><span class="line">        Y_hat = Y_hat.view(<span class="number">-1</span>, self.nb_tags)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># create a mask by filtering out all tokens that ARE NOT the padding token</span></span><br><span class="line">        tag_pad_token = self.tags[<span class="string">'&lt;PAD&gt;'</span>]</span><br><span class="line">        mask = (Y &gt; tag_pad_token).float()</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># count how many tokens we have</span></span><br><span class="line">        nb_tokens = int(torch.sum(mask).data[<span class="number">0</span>])</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># pick the values for the label and zero out the rest with the mask</span></span><br><span class="line">        Y_hat = Y_hat[range(Y_hat.shape[<span class="number">0</span>]), Y] * mask</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># compute cross entropy loss which ignores all &lt;PAD&gt; tokens</span></span><br><span class="line">        ce_loss = -torch.sum(Y_hat) / nb_tokens</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ce_loss</span><br></pre></td></tr></table></figure><h2 id="问题解答专区"><a href="#问题解答专区" class="headerlink" title="问题解答专区"></a>问题解答专区</h2><h3 id="为什么LSTM不同batch的句子长度可以不一致"><a href="#为什么LSTM不同batch的句子长度可以不一致" class="headerlink" title="为什么LSTM不同batch的句子长度可以不一致"></a>为什么LSTM不同batch的句子长度可以不一致</h3><p>LSTM只要保证每个batch的句子长度一致即可，所以可以使用动态padding,但是为什么不同batch的长度可以不一样？</p><p>因为LSTM需要更新的参数是共享，就是每个状态的权重是共享的，所以就无所谓不同长度了。只要保证每个batch内能够更新权重即可。</p><h3 id="深度学习中-number-of-training-epochs-中的-epoch到底指什么？"><a href="#深度学习中-number-of-training-epochs-中的-epoch到底指什么？" class="headerlink" title="深度学习中 number of training epochs 中的 epoch到底指什么？"></a>深度学习中 number of training epochs 中的 epoch到底指什么？</h3><p>对于初学者来讲，有几个概念容易混淆：</p><p>（1）iteration：表示1次迭代（也叫training step），每次迭代更新1次网络结构的参数；</p><p>（2）batch-size：1次迭代所使用的样本量；</p><p>（3）epoch：1个epoch表示过了1遍训练集中的所有样本。</p><p>一次epoch=所有训练数据forward+backward后更新参数的过程。<br>一次iteration=[batch size]个训练数据forward+backward后更新参数过程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NLP汇总&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://kodgv.xyz/categories/NLP/"/>
    
    
      <category term="汇总" scheme="http://kodgv.xyz/tags/%E6%B1%87%E6%80%BB/"/>
    
  </entry>
  
  <entry>
    <title>python多进程</title>
    <link href="http://kodgv.xyz/2019/04/18/%E4%BB%A3%E7%A0%81%E5%9F%BA%E7%A1%80/python%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    <id>http://kodgv.xyz/2019/04/18/代码基础/python多进程/</id>
    <published>2019-04-18T01:46:38.000Z</published>
    <updated>2019-04-18T08:36:10.690Z</updated>
    
    <content type="html"><![CDATA[<p>python multiprocessing模块多进程详解</p><a id="more"></a><p>[TOC]</p><h2 id="multiprocessing模块API"><a href="#multiprocessing模块API" class="headerlink" title="multiprocessing模块API"></a>multiprocessing模块API</h2><p>Pool类用于需要执行的目标很多，而手动限制进程数量又太繁琐时，如果目标少且不用控制进程数量则可以用<a href="http://thief.one/2016/11/24/Multiprocessing-Process" target="_blank" rel="noopener">Process</a>类。</p><p>构造方法</p><ul><li>Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]])</li><li>processes ：使用的工作进程的数量，如果processes是None那么使用 os.cpu_count()返回的数量。</li><li>initializer： 如果initializer是None，那么每一个工作进程在开始的时候会调用initializer(*initargs)。</li><li>maxtasksperchild：工作进程退出之前可以完成的任务数，完成后用一个新的工作进程来替代原进程，来让闲置的资源被释放。maxtasksperchild默认是None，意味着只要Pool存在工作进程就会一直存活。</li><li>context: 用在制定工作进程启动时的上下文，一般使用 multiprocessing.Pool() 或者一个context对象的Pool()方法来创建一个池，两种方法都适当的设置了context。</li></ul><p>实例方法</p><ul><li>apply_async(func[, args[, kwds[, callback]]]) 它是非阻塞。</li><li>apply(func[, args[, kwds]])是阻塞的。</li><li>close() 关闭pool，使其不在接受新的任务。</li><li>terminate() 关闭pool，结束工作进程，不在处理未完成的任务。</li><li>join() 主进程阻塞，等待子进程的退出， join方法要在close或terminate之后使用。</li></ul><h3 id="Pool使用方法"><a href="#Pool使用方法" class="headerlink" title="Pool使用方法"></a>Pool使用方法</h3><p>Pool+map函数</p><p>说明：此写法缺点在于只能通过map向函数传递一个参数。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing import Pool</span><br><span class="line">def test(i):</span><br><span class="line">    <span class="builtin-name">print</span> i</span><br><span class="line"><span class="keyword">if</span> <span class="attribute">__name__</span>=="__main__":</span><br><span class="line">lists=[1,2,3]</span><br><span class="line"><span class="attribute">pool</span>=Pool(processes=2) #定义最大的进程数</span><br><span class="line">pool.map(test,lists)        #p必须是一个可迭代变量。</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure><p>异步进程池（非阻塞）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">pool = Pool(processes=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i  <span class="keyword">in</span> xrange(<span class="number">500</span>):</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">For循环中执行步骤：</span></span><br><span class="line"><span class="string">（1）循环遍历，将500个子进程添加到进程池（相对父进程会阻塞）</span></span><br><span class="line"><span class="string">（2）每次执行10个子进程，等一个子进程执行完后，立马启动新的子进程。（相对父进程不阻塞）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">apply_async为异步进程池写法。</span></span><br><span class="line"><span class="string">异步指的是启动子进程的过程，与父进程本身的执行（print）是异步的，而For循环中往进程池添加子进程的过程，与父进程本身的执行却是同步的。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">    pool.apply_async(test, args=(i,)) <span class="comment">#维持执行的进程总数为10，当一个进程执行完后启动一个新进程.       </span></span><br><span class="line"><span class="keyword">print</span> “test”</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure><p>执行顺序：For循环内执行了2个步骤，第一步：将500个对象放入进程池（阻塞）。第二步：同时执行10个子进程（非阻塞），有结束的就立即添加，维持10个子进程运行。（apply_async方法的会在执行完for循环的添加步骤后，直接执行后面的print语句，而apply方法会等所有进程池中的子进程运行完以后再执行后面的print语句）</p><p>注意：调用join之前，先调用close或者terminate方法，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束。</p><h2 id="多进程示例代码"><a href="#多进程示例代码" class="headerlink" title="多进程示例代码"></a>多进程示例代码</h2><p>纯建立Process<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#encoding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> os, time, random</span><br><span class="line"></span><br><span class="line"><span class="comment">#线程启动后实际执行的代码块</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">r1</span><span class="params">(process_name)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">print</span> process_name, os.getpid()     <span class="comment">#打印出当前进程的id</span></span><br><span class="line">        time.sleep(random.random())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">r2</span><span class="params">(process_name)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">print</span> process_name, os.getpid()     <span class="comment">#打印出当前进程的id</span></span><br><span class="line">        time.sleep(random.random())</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"main process run..."</span></span><br><span class="line">        p1 = Process(target=r1, args=(<span class="string">'process_name1'</span>, ))       <span class="comment">#target:指定进程执行的函数，args:该函数的参数，需要使用tuple,只需要不断往后加参数即可</span></span><br><span class="line">        p2 = Process(target=r2, args=(<span class="string">'process_name2'</span>, ))</span><br><span class="line">        </span><br><span class="line">        p1.start()    <span class="comment">#通过调用start方法启动进程，跟线程差不多。</span></span><br><span class="line">        p2.start()    <span class="comment">#但run方法在哪呢？待会说。。。</span></span><br><span class="line">        p1.join()     <span class="comment">#join方法也很有意思，寻思了一下午，终于理解了。待会演示。</span></span><br><span class="line">        p2.join()</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"main process runned all lines..."</span></span><br></pre></td></tr></table></figure></p><p>POOL池管理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(data, index, size)</span>:</span>  <span class="comment"># data 传入数据，index 数据分片索引，size进程数</span></span><br><span class="line">    size = math.ceil(len(data) / size)</span><br><span class="line">    start = size * index</span><br><span class="line">    end = (index + <span class="number">1</span>) * size <span class="keyword">if</span> (index + <span class="number">1</span>) * size &lt; len(data) <span class="keyword">else</span> len(data)</span><br><span class="line">    temp_data = data[start:end]</span><br><span class="line">    <span class="comment"># do something</span></span><br><span class="line">    <span class="keyword">return</span> data  <span class="comment"># 可以返回数据，在后面收集起来</span></span><br><span class="line"></span><br><span class="line">processor = <span class="number">40</span></span><br><span class="line">res = []</span><br><span class="line">p = Pool(processor)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(processor):</span><br><span class="line">    res.append(p.apply_async(run, args=(data, i, processor,)))</span><br><span class="line">    print(str(i) + <span class="string">' processor started !'</span>)</span><br><span class="line">p.close()</span><br><span class="line">p.join()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> res:</span><br><span class="line">    print(i.get())  <span class="comment"># 使用get获得多进程处理的结果</span></span><br></pre></td></tr></table></figure></p><h2 id="进程注意事项"><a href="#进程注意事项" class="headerlink" title="进程注意事项"></a>进程注意事项</h2><h3 id="进程之间内存独立"><a href="#进程之间内存独立" class="headerlink" title="进程之间内存独立"></a>进程之间内存独立</h3><p>多进程与多线程最大的不同在于，多进程的每一个进程都有一份变量的拷贝，进程之间的操作互不影响，Process进程创建时，子进程会将主进程的Process对象完全复制一份，这样在主进程和子进程各有一个 Process对象，但是p.start()启动的是子进程，主进程中的Process对象作为一个静态对象存在，不执行。</p><p>就是这个地方，要注意的是，对于多进程来说，它是不会复制内存的，所以不用担心main中的变量被复制，但是它会复制main函数以外的变量</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import multiprocessing</span><br><span class="line">import <span class="built_in">time</span></span><br><span class="line"></span><br><span class="line"><span class="literal">zero</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">def change_zero():</span><br><span class="line">    <span class="built_in">global</span> <span class="literal">zero</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        <span class="literal">zero</span> = <span class="literal">zero</span> + <span class="number">1</span></span><br><span class="line">        print(multiprocessing.current_process().name, <span class="literal">zero</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    p1 = multiprocessing.Process(target = change_zero)</span><br><span class="line">    p2 = multiprocessing.Process(target = change_zero)</span><br><span class="line">    p1.<span class="built_in">start</span>()</span><br><span class="line">    p2.<span class="built_in">start</span>()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.join()</span><br><span class="line">    print(<span class="literal">zero</span>)</span><br></pre></td></tr></table></figure><p>运行结果如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Process-1 1</span><br><span class="line">Process-1 2</span><br><span class="line">Process-1 3</span><br><span class="line">Process-2 1</span><br><span class="line">Process-2 2</span><br><span class="line">Process-2 3</span><br><span class="line">0</span><br></pre></td></tr></table></figure><h3 id="共享变量Queue"><a href="#共享变量Queue" class="headerlink" title="共享变量Queue"></a>共享变量Queue</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">下面我们要讨论第一种情况，如果真的要在两个进程之间共享变量需要怎么办</span><br><span class="line"></span><br><span class="line">队列</span><br><span class="line">这里介绍进程之间的第一种交流方式——队列。multiprocessing模块中提供了multiprocessing.Queue，它和Queue.Queue的区别在于，它里面封装了进程之间的数据交流，不同进程可以操作同一个multiprocessing.Queue。</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Queue</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addone</span><span class="params">(q)</span>:</span></span><br><span class="line">    q.put(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addtwo</span><span class="params">(q)</span>:</span></span><br><span class="line">    q.put(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    q = Queue()</span><br><span class="line">    p1 = Process(target=addone, args = (q, ))</span><br><span class="line">    p2 = Process(target=addtwo, args = (q, ))</span><br><span class="line">    p1.start()</span><br><span class="line">    p2.start()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.join()</span><br><span class="line">    print(q.get())</span><br><span class="line">    print(q.get())</span><br><span class="line">运行结果如下</span><br><span class="line"></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">这个队列是线程、进程安全的，即对队列的每一次修改中间不会被中断从而造成结果错误。</span><br></pre></td></tr></table></figure><h3 id="进程锁"><a href="#进程锁" class="headerlink" title="进程锁"></a>进程锁</h3><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">进程锁</span><br><span class="line">既然变量在进程之间可以共享了，那么同时操作一个变量导致的不安全也随之出现。同多线程一样，进程也是通过锁来解决，而且使用方法都和多线程里相同。</span><br><span class="line"><span class="built_in">lock</span> = multiprocessing.<span class="built_in">Lock</span>()</span><br><span class="line"><span class="built_in">lock</span>.acquire()</span><br><span class="line"><span class="built_in">lock</span>.release()</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">lock</span>:</span><br><span class="line">这些用法和功能都和多线程是一样的</span><br><span class="line">另外，multiprocessing.Semaphore Condition Event RLock也和多线程相同</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python multiprocessing模块多进程详解&lt;/p&gt;
    
    </summary>
    
    
      <category term="多进程" scheme="http://kodgv.xyz/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>CNN</title>
    <link href="http://kodgv.xyz/2019/04/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN/"/>
    <id>http://kodgv.xyz/2019/04/17/神经网络/CNN/</id>
    <published>2019-04-17T02:32:46.000Z</published>
    <updated>2019-04-17T03:50:38.490Z</updated>
    
    <content type="html"><![CDATA[<p>CNN学习</p><a id="more"></a><p>[TOC]</p><h1 id="CNN结构基础"><a href="#CNN结构基础" class="headerlink" title="CNN结构基础"></a>CNN结构基础</h1><p>首先，提出这样一个问题：识别一幅图片是包含有字母”X”还是字母”O”？</p><p>为了帮助指导你理解卷积神经网络，我们讲采用一个非常简化的例子：确定一幅图像是包含有”X”还是”O”？<br><img src="https://img-blog.csdn.net/20180306180334512" alt="img"><br>这个例子足够说明CNN背后的原理，同时它足够简单，能够避免陷入不必要的细节。</p><p>在CNN中有这样一个问题，就是每次给你一张图，你需要判断它是否含有”X”或者”O”。并且假设必须两者选其一，不是”X”就是”O”。理想的情况就像下面这个样子：<br><img src="https://img-blog.csdn.net/20180306180403855" alt="img"><br>标准的”X”和”O”，字母位于图像的正中央，并且比例合适，无变形</p><p>对于计算机来说，只要图像稍稍有一点变化，不是标准的，那么要解决这个问题还是不是那么容易的：<br><img src="https://img-blog.csdn.net/20180306180420908" alt="img"></p><p>计算机要解决上面这个问题，一个比较天真的做法就是先保存一张”X”和”O”的标准图像（就像前面给出的例子），然后将其他的新给出的图像来和这两张标准图像进行对比，看看到底和哪一张图更匹配，就判断为哪个字母。</p><p>但是这么做的话，其实是非常不可靠的，因为计算机还是比较死板的。在计算机的“视觉”中，一幅图看起来就像是一个二维的像素数组（可以想象成一个棋盘），每一个位置对应一个数字。在我们这个例子当中，像素值”1”代表白色，像素值”-1”代表黑色。<br><img src="https://img-blog.csdn.net/20180306180438461" alt="img"></p><p>当比较两幅图的时候，如果有任何一个像素值不匹配，那么这两幅图就不匹配，至少对于计算机来说是这样的。</p><p>对于这个例子，计算机认为上述两幅图中的白色像素除了中间的3*3的小方格里面是相同的，其他四个角上都不同：<br><img src="https://img-blog.csdn.net/20180306180450259" alt="img"></p><p>因此，从表面上看，计算机判别右边那幅图不是”X”，两幅图不同，得出结论：<br><img src="https://img-blog.csdn.net/20180306180500462" alt="img"></p><p>但是这么做，显得太不合理了。理想的情况下，我们希望，对于那些仅仅只是做了一些像平移，缩放，旋转，微变形等简单变换的图像，计算机仍然能够识别出图中的”X”和”O”。就像下面这些情况，我们希望计算机依然能够很快并且很准的识别出来：<br><img src="https://img-blog.csdn.net/201803061805080" alt="img"></p><p>这也就是CNN出现所要解决的问题。</p><p>Features<br><img src="https://img-blog.csdn.net/20180306180518406" alt="img"></p><p>对于CNN来说，它是一块一块地来进行比对。它拿来比对的这个“小块”我们称之为Features（特征）。在两幅图中大致相同的位置找到一些粗糙的特征进行匹配，CNN能够更好的看到两幅图的相似性，相比起传统的整幅图逐一比对的方法。</p><p>每一个feature就像是一个小图（就是一个比较小的有值的二维数组）。不同的Feature匹配图像中不同的特征。在字母”X”的例子中，那些由对角线和交叉线组成的features基本上能够识别出大多数”X”所具有的重要特征。<br><img src="https://img-blog.csdn.net/20180306180528299" alt="img"></p><p>这些features很有可能就是匹配任何含有字母”X”的图中字母X的四个角和它的中心。那么具体到底是怎么匹配的呢？如下：<br><img src="https://img-blog.csdn.net/20180306180835951" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306180847317" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306180853914" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306180901666" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306180908456" alt="img"></p><p>看到这里是不是有了一点头目呢。但其实这只是第一步，你知道了这些Features是怎么在原图上面进行匹配的。但是你还不知道在这里面究竟进行的是怎样的数学计算，比如这个下面3*3的小块到底干了什么？<br><img src="https://img-blog.csdn.net/20180306180937780" alt="img"><br>接下来就跟进介绍里面的数学操作，也就是我们常说的“卷积”操作。</p><p>卷积(Convolution)<br><img src="https://img-blog.csdn.net/20180306180949227" alt="img"><br>Convolution</p><p><img src="https://img-blog.csdn.net/20180306181142569" alt="img"><br>当给你一张新的图时，CNN并不能准确地知道这些features到底要匹配原图的哪些部分，所以它会在原图中每一个可能的位置进行尝试。这样在原始整幅图上每一个位置进行匹配计算，我们相当于<strong>把这个feature变成了一个过滤器</strong>。这个我们用来匹配的过程就被称为卷积操作，这也就是卷积神经网络名字的由来。</p><p>这个卷积操作背后的数学知识其实非常的简单。要计算一个feature和其在原图上对应的某一小块的结果，只需要简单地将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可。</p><p>如果两个像素点都是白色（也就是值均为1），那么1<em>1 = 1，如果均为黑色，那么(-1)</em>(-1) = 1。不管哪种情况，每一对能够匹配上的像素，其相乘结果为1。类似地，任何不匹配的像素相乘结果为-1。如果一个feature（比如n<em>n）内部所有的像素都和原图中对应一小块（n</em>n）匹配上了，那么它们对应像素值相乘再累加就等于n2，然后除以像素点总个数n2，结果就是1。同理，如果每一个像素都不匹配，那么结果就是-1。</p><p>具体过程如下：</p><p><img src="https://img-blog.csdn.net/20180306181259497" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181307663" alt="img"><img src="https://img-blog.csdn.net/2018030618132177" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181348268" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181401547" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181407914" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181416401" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181427106" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181438929" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181456531" alt="img"></p><p>对于中间部分，也是一样的操作：</p><p><img src="https://img-blog.csdn.net/20180306181538273" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306190609622" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181558447" alt="img"><br><img src="https://img-blog.csdn.net/20180306181612616" alt="img"><br>最后整张图算完，大概就像下面这个样子：<br><img src="https://img-blog.csdn.net/20180306181626302" alt="img"><br>然后换用其他feature进行同样的操作，最后得到的结果就是这样了：<br><img src="https://img-blog.csdn.net/20180306181640996" alt="img"><br>为了完成我们的卷积，我们不断地重复着上述过程，将feature和图中每一块进行卷积操作。最后通过每一个feature的卷积操作，我们会得到一个新的二维数组。</p><p>这也可以理解为对原始图像进行过滤的结果，我们称之为feature map，它是每一个feature从原始图像中提取出来的“特征”。其中的值，越接近为1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。<br><img src="https://img-blog.csdn.net/20180306181719344" alt="img"><br>这样我们的原始图，经过不同feature的卷积操作就变成了一系列的feature map。我们可以很方便，直观地将这整个操作视为一个单独的处理过程。在CNN中，我们称之为卷积层(convolution layer)，这样你可能很快就会想到后面肯定还有其他的layer。没错，后面会提到。</p><p>我们可以将卷积层看成下面这个样子：<br><img src="https://img-blog.csdn.net/20180306181740905" alt="img"><br>因此可想而知，CNN其实做的操作也没什么复杂的。但是尽管我们能够以这一点篇幅就描述了CNN的工作，其内部的加法，乘法和除法操作的次数其实会增加地很快。从数学的角度来说，它们会随着图像的大小，每一个filter的大小和filter的数目呈线性增长。由于有这么多因素的影响，很容易使得这个问题的计算量变得相当的庞大，这也难怪很多微处理器制造商现在都在生产制造专业的芯片来跟上CNN计算的需求。</p><p>池化(Pooling)<br><img src="https://img-blog.csdn.net/20180306181756444" alt="img"><br>Pooling</p><p>CNN中使用的另一个有效的工具被称为“池化(Pooling)”。池化可以将一幅大的图像缩小，同时又保留其中的重要信息。池化背后的数学顶多也就是小学二年级水平。它就是将输入图像进行缩小，减少像素信息，只保留重要信息。通常情况下，池化都是2<em>2大小，比如对于max-pooling来说，就是取输入图像中2</em>2大小的块中的最大值，作为结果的像素值，相当于将原始图像缩小了4倍(注：同理，对于average-pooling来说，就是取2*2大小块的平均值作为结果的像素值)。</p><p>对于本文的这个例子，池化操作具体如下：</p><p><img src="https://img-blog.csdn.net/20180306181913201" alt="img"></p><p><img src="https://img-blog.csdn.net/2018030618192589" alt="img"><br><img src="https://img-blog.csdn.net/20180306181939786" alt="img"></p><p>不足的外面补”0”：</p><p><img src="https://img-blog.csdn.net/20180306185831170" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181956608" alt="img"><br>经过最大池化操作（比如2*2大小）之后，一幅图就缩小为原来的四分之一了：<br><img src="https://img-blog.csdn.net/20180306182032388" alt="img"><br>然后对所有的feature map执行同样的操作，得到如下结果：<br><img src="https://img-blog.csdn.net/20180306182045131" alt="img"><br>因为最大池化（max-pooling）保留了每一个小块内的最大值，所以它相当于保留了这一块最佳的匹配结果（因为值越接近1表示匹配越好）。这也就意味着它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。这也就能够看出，CNN能够发现图像中是否具有某种特征，而不用在意到底在哪里具有这种特征。这也就能够帮助解决之前提到的计算机逐一像素匹配的死板做法。</p><p>当对所有的feature map执行池化操作之后，相当于一系列输入的大图变成了一系列小图。同样地，我们可以将这整个操作看作是一个操作，这也就是CNN中的池化层(pooling layer)，如下：<br><img src="https://img-blog.csdn.net/20180306182105596" alt="img"><br>通过加入池化层，可以很大程度上减少计算量，降低机器负载。</p><p>Normalization<br>激活函数Relu (Rectified Linear Units)<br>这是一个很小但是很重要的操作，叫做Relu(Rectified Linear Units)，或者修正线性单元。它的数学公式也很简单:<br>![][01]<br>[01]:<a href="http://latex.codecogs.com/png.latex?f(x" target="_blank" rel="noopener">http://latex.codecogs.com/png.latex?f(x</a>) = max(0, x)</p><p>对于输入的负值，输出全为0，对于正值，原样输出。关于其功能，更多详见这里。</p><p>下面我们看一下本文的离例子中relu激活函数具体操作：<br><img src="https://img-blog.csdn.net/20180306182143253" alt="img"><br><img src="https://img-blog.csdn.net/20180306182153402" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182210933" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182232154" alt="img"></p><p>最后，对整幅图操作之后，结果如下：<br><img src="https://img-blog.csdn.net/20180306182300573" alt="img"><br>同样地，在CNN中，我们这一系列操作视为一个操作，那么就得到Relu Layer，如下：<br><img src="https://img-blog.csdn.net/20180306182321381" alt="img"><br>Deep Learning<br>最后，我们将上面所提到的卷积，池化，激活放在一起，就是下面这个样子：<br><img src="https://img-blog.csdn.net/20180306182433190" alt="img"><br>然后，我们加大网络的深度，增加更多的层，就得到深度神经网络了：<br><img src="https://img-blog.csdn.net/20180306182451356" alt="img"><br>然后在不同的层，我们进行可视化，就可以看到本文开头提到的先验知识里面的结果了：<br><img src="https://img-blog.csdn.net/20180306182513523" alt="img"><br>全连接层(Fully connected layers)<br><img src="https://img-blog.csdn.net/20180306182530284" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182546940" alt="img"><img src="https://img-blog.csdn.net/20180306182538949" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182605783" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182622832" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182633541" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182741524" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182723860" alt="img"></p><p>根据结果判定为”X”：<br><img src="https://img-blog.csdn.net/20180306182758331" alt="img"></p><p>在这个过程中，我们定义这一系列操作为”全连接层“(Fully connected layers)：<br><img src="https://img-blog.csdn.net/20180306182819200" alt="img"><br>全连接层也能够有很多个，如下：<br><img src="https://img-blog.csdn.net/20180306182839210" alt="img"><br>【综合上述所有结构】<br><img src="https://img-blog.csdn.net/20180306182901787" alt="img"></p><h1 id="CNN三大核心思想"><a href="#CNN三大核心思想" class="headerlink" title="CNN三大核心思想"></a><strong>CNN三大核心思想</strong></h1><p>卷积神经网络CNN的出现是为了解决MLP多层感知器全连接和梯度发散的问题。其引入三个核心思想：1.局部感知(local field)，2.权值共享(Shared Weights)，3.下采样(subsampling)。极大地提升了计算速度，减少了连接数量。</p><p><strong>2.1 局部感知</strong></p><p>形象地说，就是模仿你的眼睛，想想看，你在看东西的时候，目光是聚焦在一个相对很小的局部的吧？严格一些说，普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上，而在卷积神经网络中，每个隐层节点只连接到图像某个足够小局部的像素点上，从而大大减少需要训练的权值参数。</p><p>对于一个 1000∗1000 的输入图像而言，如果下一个隐藏层的神经元数目为 106 个，采用全连接则有 1000∗1000∗106=1012 个权值参数，如此数目巨大的参数几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中 10∗10的局部图像相连接，那么此时的权值参数数量为 10∗10∗106=108，将直接减少4个数量级。</p><p>如下图所示，左边是每个像素的全连接，右边是每行隔两个像素作为局部连接，因此在数量上，少了很多权值参数数量（每一条连接每一条线需要有一个权值参数，具体忘记了的可以回顾单个[神经元模型]。因此 局部感知就是：<br>通过卷积操作，把 全连接变成局部连接 ，因为多层网络能够抽取高阶统计特性， 即使网络为局部连接，由于格外的突触连接和额外的神经交互作用，也可以使网络在不十分严格的意义下获得一个全局关系。</p><p><img src="http://xilinx.eetrend.com/files-eetrend-xilinx/article/201612/10827-27690-06.jpg" alt="img"></p><p>下面我们来详细看一下到底局部感知如何使 全连接变成局部连接，按照人工神经网络的方法，把输入图像的像素一字排开之后，每一个像素值就是一个神经元输入，需要对隐层或者输出层做全连接，如上图左侧所示。卷积神经网络引入卷积概念后，卷积核通过原图像，然后卷积核对原图像上符合卷积核大小的像素进行加权求和，每一次只是对符合卷积核的图像像素做卷积，这就是局部感知的概念，使 全连接变成局部连接。</p><p><img src="http://xilinx.eetrend.com/files-eetrend-xilinx/article/201612/10827-27691-07.png" alt="img"></p><p><strong>2.2 权值共享</strong></p><p>尽管局部感知使计算量减少了几个数量级，但权重参数数量依然很多。能不能再进一步减少呢？方法就是权值共享。</p><p>权值共享：不同的图像或者同一张图像共用一个卷积核，减少重复的卷积核。同一张图像当中可能会出现相同的特征，共享卷积核能够进一步减少权值参数。</p><p>如下图所示，为了找到鸟嘴，一个激活函数A需要检测图像左侧有没有鸟嘴，另外一个激活函数B需要检测另外一张图像中间有没有类似的鸟嘴。其实，鸟嘴都可能具有同样的特征，只需要一个激活函数C就可以了，这个时候，就可以共享同样的权值参数（也就是卷积核）。</p><p><img src="http://xilinx.eetrend.com/files-eetrend-xilinx/article/201612/10827-27692-08.png" alt="img"></p><p>如果使用了权值共享（共同使用一个卷积核），那么将可以大大减少卷积核的数量，加快运算速度。天下武功，唯快不破。</p><p><img src="http://xilinx.eetrend.com/files-eetrend-xilinx/article/201612/10827-27693-09.png" alt="img"></p><p>举个栗子，在局部连接中隐藏层的每一个神经元连接的是一个 10∗10 的局部图像，因此有 10∗10 个权值参数，将这 10∗10 个权值参数共享给剩下的神经元，也就是说隐藏层中 106 个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10∗10 个权值参数（也就是卷积核(也称滤波器)的大小。</p><p>尽管只有这么少的参数，依旧有出色的性能。但是，这样仅提取了图像的一种特征，如果要多提取出一些特征，需要增加多个卷积核，不同的卷积核能够得到图像的不同映射下的特征，称之为 Feature Map。如果有100个卷积核，最终的权值参数也仅为 100∗100=104 个而已。另外，偏置参数b也是共享的，同一种滤波器共享一个。</p><p><strong>2.3 池化</strong></p><p>在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。</p><p>pooling的好处有什么？<br>\1. 这些统计特征能够有更低的维度，减少计算量。<br>\2. 不容易过拟合，当参数过多的时候很容易造成过度拟合。<br>\3. 缩小图像的规模，提升计算速度。</p><p>如下图所示，原图是一张500∗500 的图像，经过subsampling之后哦，变成了一张 250∗250 的图像。这样操作的好处非常明显，虽然经过权值共享和局部连接后的图像权值参数已经大大减少，但是对于计算量来说，还是非常巨大，需要消费很大的计算时间，于是为了进一步减少计算量，于是加入了subsampling这个概念，不仅仅使图像像素减少了， 同时也减少计算时间。</p><p><img src="http://xilinx.eetrend.com/files-eetrend-xilinx/article/201612/10827-27694-10.png" alt="img"></p><p>举个栗子：以最大池化（Max Pooling）为例，1000×1000的图像经过10×10的卷积核卷积后，得到的是991×991的特征图，然后使用2×2的池化规模，即每4个点组成的小方块中，取最大的一个作为输出，最终得到的是496×496大小的特征图。</p><p>下采样，即池化，目的是减小特征图，池化规模一般为2×2。常用的池化方法有：</p><p>Pooling算法</p><p>最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。<br>均值池化（Mean Pooling）。取4个点的均值。<br>可训练池化。训练函数 f ，接受4个点为输入，出入1个点。</p><p>由于特征图的变长不一定是2的倍数，所以在边缘处理上也有两种方案：</p><p>保留边缘。将特征图的变长用0填充为2的倍数，然后再池化。<br>忽略边缘。将多出来的边缘直接省去。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CNN学习&lt;/p&gt;
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="CNN" scheme="http://kodgv.xyz/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>Attention</title>
    <link href="http://kodgv.xyz/2019/04/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Attention/"/>
    <id>http://kodgv.xyz/2019/04/15/神经网络/Attention/</id>
    <published>2019-04-15T08:07:35.000Z</published>
    <updated>2019-04-27T07:11:19.484Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]<br>Attention本质就是一系列注意力分配系数，也就是一系列权重参数罢了。<br><a id="more"></a></p><p>来源：<a href="https://www.cnblogs.com/guoyaohua/p/9429924.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/9429924.html</a></p><h1 id="1-什么是Attention机制？"><a href="#1-什么是Attention机制？" class="headerlink" title="1. 什么是Attention机制？"></a>1. 什么是Attention机制？</h1><p>　　最近两年，注意力模型（Attention Model）被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，是深度学习技术中最值得关注与深入了解的核心技术之一。</p><p>　　当我们人在看一样东西的时候，我们当前时刻关注的一定是我们当前正在看的这样东西的某一地方，换句话说，当我们目光移到别处时，注意力随着目光的移动也在转移，这意味着，当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。————-（思考：对于图片，会有些特别显眼的场景会率先吸引住注意力，那是因为脑袋中对这类东西很敏感。对于文本，我们大都是带目的性的去读，顺序查找，顺序读，但是在理解的过程中，我们是根据我们自带的目的去理解，去关注的。 注意力模型应该与具体的目的(或者任务)相结合。）</p><p>　　从Attention的作用角度出发，我们就可以从两个角度来分类Attention种类：<strong>Spatial Attention 空间注意力</strong>和<strong>Temporal Attention 时间注意力</strong>。更具实际的应用，也可以将Attention分为<strong>Soft Attention</strong>和<strong>Hard Attention</strong>。<strong>Soft Attention是所有的数据都会注意，都会计算出相应的注意力权值，不会设置筛选条件。Hard Attention会在生成注意力权重后筛选掉一部分不符合条件的注意力，让它的注意力权值为0，即可以理解为不再注意这些不符合条件的部分。</strong></p><h1 id="2-先了解编码-解码框架：Encoder-Decoder框架"><a href="#2-先了解编码-解码框架：Encoder-Decoder框架" class="headerlink" title="2. 先了解编码-解码框架：Encoder-Decoder框架"></a>2. 先了解编码-解码框架：Encoder-Decoder框架</h1><p>　　目前绝大多数文献中出现的AM模型是附着在Encoder-Decoder框架下的，当然，其实AM模型可以看作一种通用的思想，本身并不依赖于Encoder-Decoder模型，这点需要注意。<strong>Encoder-Decoder框架可以看作是一种文本处理领域的研究模式</strong>，应用场景异常广泛，本身就值得细谈。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806135448102-667913176.png" alt="img"></p><p>图1 抽象的Encoder-Decoder框架</p><p>　　Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对<x,y>。 ————（思考：<x,y>对很通用，X是一个问句，Y是答案；X是一个句子，Y是抽取的关系三元组；X是汉语句子，Y是汉语句子的英文翻译。等等），我们的目标是给定输入句子X，期待通过Encoder-Decoder框架来生成目标句子Y。X和Y可以是同一种语言，也可以是两种不同的语言。而X和Y分别由各自的单词序列构成：</x,y></x,y></p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806141247005-858346593.png" alt="img"></p><p>　　Encoder顾名思义就是对输入句子X进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806141320980-818442456.png" alt="img"></p><p>　　对于解码器Decoder来说，其任务是根据句子X的中间语义表示C和之前已经生成的历史信息y1,y2….yi-1来生成i时刻要生成的单词yi ：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806141350194-1066590310.png" alt="img"></p><p>　　每个yi都依次这么产生，那么看起来就是整个系统根据输入句子X生成了目标句子Y。 ———（思考：其实这里的Encoder-Decoder是一个序列到序列的模型seq2seq，这个模型是对顺序有依赖的。）</p><p>　　Encoder-Decoder是个非常通用的计算框架，至于Encoder和Decoder具体使用什么模型都是由研究者自己定的，常见的比如 CNN / RNN / BiRNN / GRU / LSTM / Deep LSTM 等，这里的变化组合非常多。 ———（思考：人的学习过程包括输入、输出、外界评价。Encoder模型类似于人的输入学习过程，Decoder模型类似于人的输出学习过程，对输出的内容进行评价就类似于损失函数。英语老师给我上了几堂英语课，我在不断的输入Encoder；突然有一个随堂测试，我得做题输出Decoder；最后英语老师改卷子，给我一个分数，不对的地方我得反思调整我对输入数据的加工方式。）———-（再思考：关于英语翻译。课本上的单词和课文是原始数据输入，相当于X；我在大脑里加工这些数据，相当于Encoder模型，我的脑子里有很多加工后的数据，相当于C；现在要让我翻译一个英语句子，这个任务相当于Y，我不能翻课本，所以我只能借助我脑袋里加工的数据C去翻译这个句子，即我得动脑子，相当于Decoder。 学习的过程是什么都要学，要分类整理，要增加线索，并不知道未来的某天能用到什么，所以Encoder-Decoder是一个泛泛学习的框架）</p><h1 id="3-Attention-Model"><a href="#3-Attention-Model" class="headerlink" title="3. Attention Model"></a>3. Attention Model</h1><p>　　以上介绍的Encoder-Decoder模型是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Y中每个单词的生成过程如下：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806141638641-1078830067.png" alt="img"></p><p>　　其中<strong>f是decoder的非线性变换函数</strong>。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，是y1,y2也好，还是y3也好，他们使用的句子X的语义编码C都是一样的，没有任何区别。而语义编码C是由句子X的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实<strong>句子X中任意单词对生成某个目标单词yi来说影响力都是相同的，没有任何区别</strong>（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，估计这也是为何Google提出Sequence to Sequence模型时发现把输入句子逆序输入做翻译效果会更好的小Trick的原因）。这就是为何说这个模型<strong>没有体现出注意力</strong>的缘由。</p><p>　　引入AM模型，以翻译一个英语句子举例：输入X：Tom chase Jerry。 理想输出：汤姆追逐杰瑞。</p><p>　　应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：</p><p>（Tom,0.3）（Chase,0.2）（Jerry,0.5）</p><p>　　每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词Yi的时候，<strong>原先都是相同的中间语义表示C会替换成根据当前生成单词而不断变化的Ci</strong>。<strong>理解AM模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注**</strong>意力模型的变化的<strong>**Ci</strong>。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142115912-1682939089.png" alt="img"></p><p>图2 引入AM模型的Encoder-Decoder框架</p><p>　　即生成目标句子单词的过程成了下面的形式：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142159178-1634092293.png" alt="img"></p><p>　　而<strong>每个Ci可能对应着不同的源语句子单词的注意力分配概率分布</strong>，比如对于上面的英汉翻译来说，其对应的信息可能如下：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142302309-2112006022.png" alt="img"></p><p>　　其中，<strong>f2函数代表Encoder对输入英文单词的某种变换函数</strong>，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；<strong>g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数</strong>，一般的做法中，<strong>g函数就是对构成元素加权求和</strong>，也就是常常在论文里看到的下列公式：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142333000-1492896283.png" alt="img"></p><p>　　假设Ci中那个i就是上面的“汤姆”，那么<strong>Tx就是3，代表输入句子的长度</strong>，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)，对应的注意力模型权值分别是0.6,0.2,0.2，<strong>所以g函数就是个加权求和函数</strong>。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的<strong>中间语义表示Ci的形成过程</strong>类似下图：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142412460-1178080370.png" alt="img"></p><p>图3 Ci的形成过程</p><p>　　这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，<strong>你怎么知道AM模型所需要的输入句子单词注意力分配概率分布值呢？</strong>就是说“汤姆”对应的概率分布：</p><p>划重点(注意力权重获取的过程)（Tom,0.3）（Chase,0.2）（Jerry,0.5）是如何得到的呢？</p><p>　　为了便于说明，我们假设对图1的非AM模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图1的图转换为下图：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142520665-1351011214.png" alt="img"></p><p>图4 RNN作为具体模型的Encoder-Decoder框架</p><p>　　注意力分配概率分布值的通用计算过程：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142634471-1534518198.png" alt="img"></p><p>图5 AM注意力分配概率计算</p><p>　　对于采用RNN的Decoder来说，如果要生成 yi 单词，在时刻 i ，我们是可以知道在生成 Yi 之前的隐层节点i时刻的输出值 Hi 的，而我们的目的是要计算生成 Yi 时的输入句子单词“Tom”、“Chase”、“Jerry”对 Yi 来说的注意力分配概率分布，那么可以用i时刻的<strong>隐层节点状态 Hi 去一一和输入句子中每个单词对应的RNN隐层节点状态 hj 进行对比</strong>，即<strong>通过函数 F(hj,Hi) 来获得目标单词 Yi 和每个输入单词对应的对齐可能性</strong>，这个F函数在不同论文里可能会采取不同的方法，然后<strong>函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值（这就得到了注意力权重）</strong>。图5显示的是当输出单词为“汤姆”时刻对应的输入句子单词的对齐概率。绝大多数AM模型都是<strong>采取上述的计算框架来计算注意力分配概率分布信息</strong>，<strong>区别只是在F的定义上可能有所不同</strong>。</p><p>　　<strong>上述内容就是论文里面常常提到的Soft Attention Model</strong>（任何数据都会给一个权值，没有筛选条件）的基本思想，你能在文献里面看到的大多数AM模型基本就是这个模型，区别很可能只是把这个模型用来解决不同的应用问题。那么<strong>怎么理解AM模型的物理含义呢？</strong>一般文献里会把AM模型看作是<strong>单词对齐模型</strong>，这是非常有道理的。目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的<strong>对齐概率</strong>，这在机器翻译语境下是非常直观的：<strong>传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤</strong>，<strong>而注意力模型其实起的是相同的作用</strong>。在其他应用里面把AM模型理解成输入句子和目标句子单词之间的对齐概率也是很顺畅的想法。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806164619665-1207996587.png" alt="img"></p><p>图6 Google 神经网络机器翻译系统结构图</p><p>　　图6所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。</p><p>当然，从概念上理解的话，<strong>把AM模型理解成影响力模型也是合理的</strong>，就是说生成目标单词的时候，输入句子每个单词对于生成这个单词有多大的影响程度。这种想法也是比较好理解AM模型物理意义的一种思维方式。</p><p>　　图7是论文“<a href="http://www.aclweb.org/anthology/D15-1044" target="_blank" rel="noopener">A Neural Attention Model for Sentence Summarization</a>”中，Rush用AM模型来做生成式摘要给出的一个AM的一个非常直观的例子。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806143753246-1275015223.png" alt="img"></p><p>图7 句子生成式摘要例子</p><p>　　这个例子中，Encoder-Decoder框架的输入句子X是：“russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism”。对应图中纵坐标的句子。系统生成的摘要句子Y是：“russia calls for joint front against terrorism”，对应图中横坐标的句子。可以看出模型已经把句子主体部分正确地抽出来了。<strong>矩阵中每一列代表生成的目标单词对应输入句子每个单词的AM分配概率</strong>，颜色越深代表分配到的概率越大。这个例子对于直观理解AM是很有帮助作用。</p><p>　　《<a href="http://www.aclweb.org/anthology/D15-1044" target="_blank" rel="noopener">A Neural Attention Model for Sentence Summarization</a>》论文提供的实验数据集链接(开放可用)：<a href="https://duc.nist.gov/data.html" target="_blank" rel="noopener">DUC 2004</a>，感兴趣的朋友可以下载看看。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806143957137-1497662277.png" alt="img"></p><p>图8 摘要生成 开放数据集</p><h1 id="4-Attention机制的本质思想"><a href="#4-Attention机制的本质思想" class="headerlink" title="4. Attention机制的本质思想"></a><strong>4. Attention机制的本质思想</strong></h1><p>　　如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806165033943-1072442256.png" alt="img"></p><p>图9 Attention机制的本质思想</p><p>　　我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的<key,value>数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</key,value></p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806165351485-870137528.png" alt="img"></p><p>　　其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p><p>　　当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p><p>　　从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p><p>　　至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806191525966-820975705.png" alt="img"></p><p>图10 三阶段计算Attention过程</p><p>　　在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个 Keyi ，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806191654338-264846698.png" alt="img"></p><p>　　第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><p>  <img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806195333945-948374778.png" alt="img"></p><p>　　第二阶段的计算结果 ai 即为 Valuei 对应的权重系数，然后进行加权求和即可得到Attention数值：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806195421837-251685236.png" alt="img"></p><p>　　通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p><h1 id="5-Self-Attention模型"><a href="#5-Self-Attention模型" class="headerlink" title="5. Self Attention模型"></a><strong>5. Self Attention模型</strong></h1><p>　　通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。<strong>Self Attention也经常被称为intra Attention（内部Attention）</strong>，最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型(抛弃了传统的RNN)。</p><p>　　在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素和Source中的所有元素之间。<strong>而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。</strong>其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。</p><p>　　elf Attention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention不同，它分别在source端和target端进行，仅与source input或者target input自身相关的Self Attention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self Attention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。因此，self Attention Attention比传统的Attention mechanism效果要好，主要原因之一是，传统的Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806200854591-1266493040.png" alt="img"></p><p>图11 可视化Self Attention实例</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806200949175-1322518214.png" alt="img"></p><p>图12 可视化Self Attention实例</p><p>　　从两张图（图11、图12）可以看出，<strong>Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。</strong></p><p>　　很明显，<strong>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征</strong>，<strong>因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</strong></p><p>　　但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以<strong>远距离依赖特征之间的距离被极大缩短</strong>，有利于有效地利用这些特征。除此外，<strong>Self Attention对于增加计算的并行性也有直接帮助作用</strong>。这是为何Self Attention逐渐被广泛使用的主要原因。</p><p>​    <strong>但是attention无法记录词序，所以在self-attention中增加了position embedding</strong></p><h1 id="五种attention模型"><a href="#五种attention模型" class="headerlink" title="五种attention模型"></a>五种attention模型</h1><h2 id="hard-attention-amp-soft-attention"><a href="#hard-attention-amp-soft-attention" class="headerlink" title="hard attention&amp;soft attention"></a>hard attention&amp;soft attention</h2><p><img src="https://image.jiqizhixin.com/uploads/editor/72f889a2-df04-4a85-9c2c-1591c5375537/1528709501819.png" alt="img"></p><p>■ 论文 | Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/812" target="_blank" rel="noopener">https://www.paperweekly.site/papers/812</a></p><p>■ 源码 | <a href="https://github.com/kelvinxu/arctic-captions" target="_blank" rel="noopener">https://github.com/kelvinxu/arctic-captions</a></p><p>文章讨论的场景是图像描述生成（Image Caption Generation），对于这种场景，先放一张图，感受一下 attention 的框架。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/33073f25-48d8-4b17-a179-49191bdb5357/1528709501703.png" alt="img"></p><p>文章提出了两种 attention 模式，即 hard attention 和 soft attention，来感受一下这两种 attention。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/fe4ede76-44de-4055-93e6-3ce99541db21/1528709502108.png" alt="img"></p><p>可以看到，hard attention 会专注于很小的区域，而 soft attention 的注意力相对发散。模型的 encoder 利用 CNN (VGG net)，提取出图像的 L 个 D 维的向量<img src="https://image.jiqizhixin.com/uploads/editor/7a76e6b0-4f08-4f55-8b01-839552bd0de1/1528709501860.png" alt="img">，每个向量表示图像的一部分信息。</p><p>decoder 是一个 LSTM，每个 timestep t 的输入包含三个部分，即 context vector Zt 、前一个 timestep 的 hidden state<img src="https://image.jiqizhixin.com/uploads/editor/a498b30a-f97c-483d-b121-1499bcabe34f/1528709501897.png" alt="img">、前一个 timestep 的 output<img src="https://image.jiqizhixin.com/uploads/editor/89174d88-3afc-4554-ba26-b48065473355/1528709502635.png" alt="img">。 Zt 由 {ai} 和权重 {αti} 通过加权得到。这里的权重 αti 通过attention模型 <em>f</em>att 来计算得到，而本文中的 <em>f</em>att 是一个多层感知机（multilayer perceptron）。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/55f3f8bd-91c7-4d65-917d-292e51d2962f/1528709502715.png" alt="img"></p><p>从而可以计算<img src="https://image.jiqizhixin.com/uploads/editor/b57ff198-4b7c-4e6d-a534-4ca57b825d3b/1528709502754.png" alt="img">。接下来文章重点讨论 hard（也叫 stochastic attention）和 soft（也叫 deterministic）两种 attention 模式。</p><p><strong>1. Stochastic “Hard” Attention</strong> </p><p>记 St 为 decoder 第 t 个时刻的 attention 所关注的位置编号， Sti 表示第 t 时刻 attention 是否关注位置 i ， Sti 服从多元伯努利分布（multinoulli distribution）， 对于任意的 t ，Sti,i=1,2,…,L 中有且只有取 1，其余全部为 0，所以 [St1,St2,…,stL] 是 one-hot 形式。这种 attention 每次只 focus 一个位置的做法，就是“hard”称谓的来源。 Zt 也就被视为一个变量，计算如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/afec742f-d647-43c6-b4dc-9618ac878628/1528709502797.png" alt="img"></p><p>问题是 αti 怎么算呢？把 αti 视为隐变量，研究模型的目标函数，进而研究目标函数对参数的梯度。直观理解，模型要根据 a=(a1,…,aL) 来生成序列 y=(y1,…,yC) ，所以目标可以是最大化 log p(y|a) ，但这里没有显式的包含 s ，所以作者利用著名的 Jensen 不等式（Jensen’s inequality）对目标函数做了转化，得到了目标函数的一个 lower bound，如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/97d7901e-3b6d-4fbc-879a-d81293368368/1528709503022.png" alt="img"></p><p>这里的 s ={ s1,…,sC }，是时间轴上的重点 focus 的序列，理论上这种序列共有个。 然后就用 log p(y|a) 代替原始的目标函数，对模型的参数 W 算 gradient。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/0f5a2605-cea7-4530-8415-e9d3dfb1d3b5/1528709503215.png" alt="img"></p><p>然后利用蒙特卡洛方法对 s 进行抽样，我们做 N 次这样的抽样实验，记每次取到的序列是<img src="https://image.jiqizhixin.com/uploads/editor/8b676888-0fab-4a40-9706-d829823fce0c/1528709503296.png" alt="img">，易知<img src="https://image.jiqizhixin.com/uploads/editor/1adc9914-c26a-47d7-a3e7-3889f9090160/1528709503251.png" alt="img">的概率为<img src="https://image.jiqizhixin.com/uploads/editor/5d82f812-b08a-48fd-bdb9-575fe66c15ce/1528709503333.png" alt="img">，所以上面的求 gradient 的结果即为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/5c7be181-dfdc-4458-b477-765b1613b234/1528709503396.png" alt="img"></p><p>接下来的一些细节涉及reinforcement learning，感兴趣的同学可以去看这篇 paper。</p><p><strong>2. Deterministic “Soft” Attention</strong> </p><p>说完“硬”的 attention，再来说说“软”的 attention。 相对来说 soft attention 很好理解，在 hard attention 里面，每个时刻 t 模型的序列 [ St1,…,StL ] 只有一个取 1，其余全部为 0，也就是说每次只 focus 一个位置，而 soft attention 每次会照顾到全部的位置，只是不同位置的权重不同罢了。这时 Zt 即为 ai 的加权求和：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/419b690d-2d19-4506-8f2e-67edf525fa7c/1528709503873.png" alt="img"></p><p>这样 soft attention 是光滑的且可微的（即目标函数，也就是 LSTM 的目标函数对权重αti 是可微的，原因很简单，因为目标函数对 Zt 可微，而 Zt 对 αti 可微，根据 chain rule 可得目标函数对 αti 可微）。</p><p>文章还对这种 soft attention 做了微调：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/caa7b6b6-ce70-4e19-aa1b-8a77a7914c09/1528709503914.png" alt="img"></p><p>其中<img src="https://image.jiqizhixin.com/uploads/editor/7de8d1a3-3be6-4a66-801e-25cb0bee7b00/1528709504003.png" alt="img">，用来调节 context vector 在 LSTM 中的比重（相对于<img src="https://image.jiqizhixin.com/uploads/editor/ae0fcc90-f8ef-40d8-be2a-8f9721dae24d/1528709501949.png" alt="img"><img src="https://image.jiqizhixin.com/uploads/editor/90abe04d-a4ea-4f7b-9d6b-d5832ad531f1/1528709502672.png" alt="img">的比重）。</p><p>btw，模型的 loss function 加入了 αti 的正则项。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/3193dce5-51b7-4ae2-893d-0696635c9ec9/1528709504051.png" alt="img"></p><h2 id="global-attention-amp-local-attention"><a href="#global-attention-amp-local-attention" class="headerlink" title="global attention &amp; local attention"></a>global attention &amp; local attention</h2><p><img src="https://image.jiqizhixin.com/uploads/editor/e3a3408b-7e64-4752-b8f9-a71563bb06c6/1528709504652.png" alt="img"></p><p>■ 论文 | Effective Approaches to Attention-based Neural Machine Translation</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/806" target="_blank" rel="noopener">https://www.paperweekly.site/papers/806</a></p><p>■ 源码 | <a href="https://github.com/lmthang/nmt.matlab" target="_blank" rel="noopener">https://github.com/lmthang/nmt.matlab</a></p><p>文章提出了两种 attention 的改进版本，即 global attention 和 local attention。先感受一下 global attention 和 local attention 长什么样子。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/55e64daf-5e26-474f-bdea-b99669de205c/1528709504213.png" alt="img"></p><p><strong>▲</strong> Global Attention</p><p><img src="https://image.jiqizhixin.com/uploads/editor/4df6785f-b67e-4a41-9234-401d03c07b59/1528709504569.png" alt="img"></p><p><strong>▲</strong> Local Attention</p><p>文章指出，local attention 可以视为 hard attention 和 soft attention 的混合体（优势上的混合），因为它的计算复杂度要低于 global attention、soft attention，而且与 hard attention 不同的是，local attention 几乎处处可微，易与训练。 文章以机器翻译为场景， x1,…,xn 为 source sentence， y1,…,ym 为 target sentence， c1,…,cm 为 encoder 产生的 context vector，objective function 为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/129e5871-39db-4005-8062-ca744a216335/1528709504768.png" alt="img"></p><p>Ct 来源于 encoder 中多个 source position 所产生的 hidden states，global attention 和 local attention 的主要区别在于 attention 所 forcus 的 source positions 数目的不同：如果 attention forcus 全部的 position，则是 global attention，反之，若只 focus 一部分 position，则为 local attention。 </p><p>由此可见，这里的 global attention、local attention 和 soft attention 并无本质上的区别，两篇 paper 模型的差别只是在 LSTM 结构上有微小的差别。 </p><p>在 decoder 的时刻 t ，在利用 global attention 或 local attention 得到 context vector Ct之后，结合 ht ，对二者做 concatenate 操作，得到 attention hidden state。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/7acdb6ad-0741-4531-a96a-a9e128afda32/1528709505007.png" alt="img"></p><p>最后利用 softmax 产出该时刻的输出：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/078f1af3-0c67-4b13-9359-14f1b48aea4d/1528709505051.png" alt="img"></p><p>下面重点介绍 global attention、local attention。</p><p><strong>1. global attention</strong> </p><p>global attention 在计算 context vector ct 的时候会考虑 encoder 所产生的全部hidden state。记 decoder 时刻 t 的 target hidden为 ht，encoder 的全部 hidden state 为<img src="https://image.jiqizhixin.com/uploads/editor/1a9b3f6f-cd94-4ba5-94d9-96e096dd0802/1528709505151.png" alt="img">，对于其中任意<img src="https://image.jiqizhixin.com/uploads/editor/cab720ef-628c-4af9-b827-f2839ba387a5/1528709505228.png" alt="img">，其权重 αts 为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/ebea9e73-7f59-42a1-975b-9cd9f44ee7ba/1528709505312.png" alt="img"></p><p>而其中的<img src="https://image.jiqizhixin.com/uploads/editor/31b5d177-4123-47f7-b75a-766690049b6f/1528709505690.png" alt="img">，文章给出了四种种计算方法（文章称为 alignment function）：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/5b89594b-f47c-4870-be8a-f3406b38088b/1528709505811.png" alt="img"></p><p><img src="https://image.jiqizhixin.com/uploads/editor/1f8e7063-9c3f-4c96-a528-a841c6117825/1528709505852.png" alt="img"></p><p>四种方法都比较直观、简单。在得到这些权重后， ct 的计算是很自然的，即为<img src="https://image.jiqizhixin.com/uploads/editor/1b37607a-9316-442c-9226-49595d34ff02/1528709505264.png" alt="img">的 weighted summation。</p><p><strong>2. local attention</strong> </p><p>global attention 可能的缺点在于每次都要扫描全部的 source hidden state，计算开销较大，对于长句翻译不利，为了提升效率，提出 local attention，每次只 focus 一小部分的 source position。 </p><p>这里，context vector ct 的计算只 focus 窗口 [pt-D,pt+D] 内的 2D+1 个source hidden states（若发生越界，则忽略界外的 source hidden states）。</p><p>其中 pt 是一个 source position index，可以理解为 attention 的“焦点”，作为模型的参数， D 根据经验来选择（文章选用 10）。 关于 pt 的计算，文章给出了两种计算方案：</p><ul><li><strong>Monotonic alignment (local-m)</strong></li></ul><p><img src="https://image.jiqizhixin.com/uploads/editor/4fe24e96-b61e-4bb4-9cb6-88760dc0f228/1528709505970.png" alt="img"></p><ul><li><strong>Predictive alignment (local-p)</strong></li></ul><p><img src="https://image.jiqizhixin.com/uploads/editor/ec31a794-00f9-4dba-b9d8-89295f1aa1d0/1528709506099.png" alt="img"></p><p>其中 Wp 和 vp 是模型的参数， S 是 source sentence 的长度，易知 pt∈[0,S] 。 权重αt(s) 的计算如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/703a445f-62ed-43dd-a313-256883bac239/1528709506142.png" alt="img"></p><p>可以看出，距离中心 pt 越远的位置，其位置上的 source hidden state 对应的权重就会被压缩地越厉害。</p><h2 id="self-attention-amp-multiple-head-attention"><a href="#self-attention-amp-multiple-head-attention" class="headerlink" title="self-attention &amp; multiple-head attention"></a>self-attention &amp; multiple-head attention</h2><p><img src="https://image.jiqizhixin.com/uploads/editor/d993ae54-145e-4e12-93dd-1f3c94d156f9/1528709506751.png" alt="img"></p><p>■ 论文 | Attention Is All You Need</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/224" target="_blank" rel="noopener">https://www.paperweekly.site/papers/224</a></p><p>■ 源码 | <a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">https://github.com/Kyubyong/transformer</a></p><p><img src="https://image.jiqizhixin.com/uploads/editor/da5cb6b1-aa95-48b4-a728-01bde8de45b4/1528709506930.png" alt="img"></p><p>■ 论文 | Weighted Transformer Network for Machine Translation</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/2013" target="_blank" rel="noopener">https://www.paperweekly.site/papers/2013</a></p><p>■ 源码 | <a href="https://github.com/JayParks/transformer" target="_blank" rel="noopener">https://github.com/JayParks/transformer</a></p><p>作者首先指出，结合了 RNN（及其变体）和注意力机制的模型在序列建模领域取得了不错的成绩，但由于 RNN 的循环特性导致其不利于并行计算，所以模型的训练时间往往较长，在 GPU 上一个大一点的 seq2seq 模型通常要跑上几天，所以作者对 RNN 深恶痛绝，遂决定舍弃 RNN，只用注意力模型来进行序列的建模。 </p><p>作者提出一种新型的网络结构，并起了个名字 Transformer，里面所包含的注意力机制称之为 self-attention。作者骄傲地宣称他这套 Transformer 是能够计算 input 和 output 的 representation 而不借助 RNN 的唯一的 model，所以作者说有 attention 就够了。</p><p>模型同样包含 encoder 和 decoder 两个 stage，encoder 和 decoder 都是抛弃 RNN，而是用堆叠起来的 self-attention，和 fully-connected layer 来完成，模型的架构如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/43ca78f9-024c-4a20-aa8c-681a42b6a734/1528709507321.png" alt="img"></p><p>从图中可以看出，模型共包含三个 attention 成分，分别是 encoder 的 self-attention，decoder 的 self-attention，以及连接 encoder 和 decoder 的 attention。  </p><p>这三个 attention block 都是 multi-head attention 的形式，输入都是 query Q 、key K 、value V 三个元素，只是 Q 、 K 、 V 的取值不同罢了。接下来重点讨论最核心的模块 multi-head attention（多头注意力）。 </p><p>multi-head attention 由多个 scaled dot-product attention 这样的基础单元经过 stack 而成。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/2a179490-64ac-4b22-b42d-1c3de5e08b5c/1528709507410.png" alt="img"></p><p>那重点就变成 scaled dot-product attention 是什么鬼了。按字面意思理解，scaled dot-product attention 即缩放了的点乘注意力，我们来对它进行研究。 </p><p>在这之前，我们先回顾一下上文提到的传统的 attention 方法（例如 global attention，score 采用 dot 形式）。</p><p>记 decoder 时刻 t 的 target hidden state 为 ht，encoder 得到的全部 source hidden state为<img src="https://image.jiqizhixin.com/uploads/editor/9f26e09b-d1f7-4e77-b1e8-2e8e5642ae9c/1528709507675.png" alt="img">，则 decoder 的 context vector ct 的计算过程如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/57b33413-699a-4c04-ae99-0bddaf11ea32/1528709507900.png" alt="img"></p><p>作者先抛出三个名词 query Q、key K、value V，然后计算这三个元素的 attention。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/cbffe9ff-5149-4959-ac8b-f6c4714ff9c9/1528709509122.png" alt="img"></p><p>我的写法与论文有细微差别，但为了接下来说明的简便，我姑且简化成这样。这个 Attention 的计算跟上面的 (*) 式有几分相似。</p><p>那么 Q、K、V 到底是什么？论文里讲的比较晦涩，说说我的理解。encoder 里的 attention 叫 self-attention，顾名思义，就是自己和自己做 attention。</p><p>抛开这篇论文的做法，让我们激活自己的创造力，在传统的 seq2seq 中的 encoder 阶段，我们得到 n 个时刻的 hidden states 之后，可以用每一时刻的 hidden state hi，去分别和任意的 hidden state hj,j=1,2,…,n 计算 attention，这就有点 self-attention 的意思。</p><p>回到当前的模型，由于抛弃了 RNN，encoder 过程就没了 hidden states，那拿什么做 self-attention 来自嗨呢？</p><p>可以想到，假如作为 input 的 sequence 共有 n 个 word，那么我可以先对每一个 word 做 embedding 吧？就得到 n 个 embedding，然后我就可以用 embedding 代替 hidden state 来做 self-attention 了。所以 Q 这个矩阵里面装的就是全部的 word embedding，K、V 也是一样。</p><p>所以为什么管 Q 叫query？就是你每次拿一个 word embedding，去“查询”其和任意的 word embedding 的 match 程度（也就是 attention 的大小），你一共要做 n 轮这样的操作。 </p><p>我们记 word embedding 的 dimension 为 dmodel ，所以 Q 的 shape 就是 n*dmodel， K、V 也是一样，第 i 个 word 的 embedding 为 vi，所以该 word 的 attention 应为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/17ebfb62-21b6-4df4-99ea-08d3bf275d36/1528709509168.png" alt="img"></p><p>那同时做全部 word 的 attention，则是：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/529a6cbf-f2fe-4d8c-a90d-f8a0567f6bf8/1528709508901.png" alt="img"></p><p>scaled dot-product attention 基本就是这样了。基于 RNN 的传统 encoder 在每个时刻会有输入和输出，而现在 encoder 由于抛弃了 RNN 序列模型，所以可以一下子把序列的全部内容输进去，来一次 self-attention 的自嗨。</p><p>理解了 scaled dot-product attention 之后，multi-head attention 就好理解了，因为就是 scaled dot-product attention 的 stacking。</p><p>先把 Q、K、V 做 linear transformation，然后对新生成的 Q’、K’、V’ 算 attention，重复这样的操作 h 次，然后把 h 次的结果做 concat，最后再做一次 linear transformation，就是 multi-head attention 这个小 block 的输出了。 </p><p><img src="https://image.jiqizhixin.com/uploads/editor/b39171e2-f71c-4ea9-a9de-0d6c8d983550/1528709508952.png" alt="img"></p><p>以上介绍了 encoder 的 self-attention。decoder 中的 encoder-decoder attention 道理类似，可以理解为用 decoder 中的每个 vi 对 encoder 中的 vj 做一种交叉 attention。</p><p>decoder 中的 self-attention 也一样的道理，只是要注意一点，decoder 中你在用 vi 对 vj 做 attention 时，有一些 pair 是不合法的。原因在于，虽然 encoder 阶段你可以把序列的全部 word 一次全输入进去，但是 decoder 阶段却并不总是可以，想象一下你在做 inference，decoder 的产出还是按从左至右的顺序，所以你的 vi 是没机会和 vj ( j&gt;i ) 做 attention 的。</p><p>那怎么将这一点体现在 attention 的计算中呢？文中说只需要令 score(vi,vj)=-∞ 即可。为何？因为这样的话：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/13be2088-2463-474a-855b-b2193a08ce44/1528709509073.png" alt="img"></p><p>所以在计算 vi 的 self-attention 的时候，就能够把 vj 屏蔽掉。所以这个问题也就解决了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;br&gt;Attention本质就是一系列注意力分配系数，也就是一系列权重参数罢了。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Attention" scheme="http://kodgv.xyz/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>LSTM</title>
    <link href="http://kodgv.xyz/2019/04/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LSTM/"/>
    <id>http://kodgv.xyz/2019/04/15/神经网络/LSTM/</id>
    <published>2019-04-15T02:38:35.000Z</published>
    <updated>2019-04-20T14:31:12.373Z</updated>
    
    <content type="html"><![CDATA[<p>LSTM详解，把cell转变当作传送带的思想</p><a id="more"></a><p>来源：<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p>[TOC]</p><h2 id="‘递归神经网络"><a href="#‘递归神经网络" class="headerlink" title="‘递归神经网络"></a>‘递归神经网络</h2><p>​    人类不会每一秒钟都从头开始思考。当你阅读这篇文章的时候，你理解每一个单词都是基于你对之前单词的理解。你不会把所有的东西都扔掉，重新开始思考。你的想法是有持久性的。<br>​    传统的神经网络做不到这一点，这似乎是一个主要的缺点。例如，假设您想要对电影中每一秒发生的事件进行分类。目前还不清楚传统的神经网络如何利用其对电影中先前事件的推理来为后来的事件提供信息。<br>​    递归神经网络解决了这个问题。它们是包含循环的网络，允许信息持续存在。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" width="200" hegiht="300" align="center/"></p><p>在上面的图中，一个神经网络块a，观察某个输入xt并输出一个值ht。循环允许信息从网络的一个步骤传递到下一个步骤。</p><p>​    这些循环使得递归神经网络看起来有点神秘。然而，如果你多想一下，就会发现它们和普通的神经网络并没有太大的不同。递归神经网络可以看作是同一网络的多个副本，每个副本都向后继网络传递一条消息。考虑一下如果我们展开循环会发生什么:</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" width="500" hegiht="500" align="center/"></p><p>这种链式性质揭示了递归神经网络与序列和列表密切相关。它们是神经网络用来处理这些数据的自然结构。</p><p>​    它们确实被使用了!在过去的几年里，把RNNs应用到各种各样的问题上取得了令人难以置信的成功:语音识别、语言建模、翻译、图像字幕等等。我将把关于RNNs可以实现的惊人壮举的讨论留给Andrej Karpathy的优秀博客文章《循环神经网络的不合理有效性》。但它们真的很神奇。</p><p>​    这些成功的关键是使用“LSTMs”，这是一种非常特殊的递归神经网络，它在许多任务中都比标准版本运行得好得多。几乎所有基于递归神经网络的激动人心的结果都是用它们实现的。本文将探索这些LSTMs。</p><h2 id="长期依赖的问题"><a href="#长期依赖的问题" class="headerlink" title="长期依赖的问题"></a>长期依赖的问题</h2><p>​    RNNs的一个吸引人的地方就是它们能够将以前的信息与现在的任务联系起来，例如使用以前的视频帧可能有助于理解现在的帧。如果RNN能做到这一点，它们将非常有用。但他们能吗?这可能需要视情况而定。</p><p>​    有时候，我们只需要查看最近的信息就可以执行当前的任务。例如，考虑一个语言模型，它试图根据前面的单词预测下一个单词。如果我们试图预测“云在天空中”中的最后一个单词，我们不需要任何进一步的上下文——很明显下一个单词将是天空。在这种情况下，相关信息和需要信息的地方之间的差距很小，RNNs可以学习使用过去的信息。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png" width="400" hegiht="500" align="center/"></p><p>​    但也有一些情况，我们需要更多的上下文。试着预测文本中的最后一个单词“我在法国长大……我说一口流利的法语。”“最近的信息显示，下一个单词很可能是一种语言的名字，但如果我们想缩小范围，我们需要更早的法语语境。”相关信息与需要它的点之间的差距完全有可能变得非常大。<br>不幸的是，随着这种差距的扩大，RNNs无法学会连接信息。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png" width="400" hegiht="500" align="center/"></p><p>​    理论上，RNNs绝对有能力处理这种“长期依赖”。“一个人可以仔细地为他们选择参数来解决这种形式的玩具问题。遗憾的是，在实践中，RNNs似乎不能学习它们。 [Hochreiter (1991) <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener">German</a>和<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a>对这个问题进行了深入研究，他们发现了一些非常基本的原因，解释了为什么这个问题可能很难。</p><h2 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h2><p>​    Long Short Term Memory networks通常被称为“LSTMs”，是一种特殊的RNN，能够学习长期依赖关系。它们由<a href="http://www.bioinf.jku.at/publications/older/2604.pdf" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber (1997)</a>引入，并在随后的工作中被许多人提炼和推广。他们在各种各样的问题上都做得非常好，现在被广泛使用。</p><p>​    LSTMs的设计是为了避免长期依赖问题。长时间记住信息实际上是他们的默认行为，而不是他们努力学习的东西!</p><p>​    所有的递归神经网络都具有一串重复的神经网络模块的形式。在标准的RNNs中，这个重复模块有一个非常简单的结构，比如一个tanh层。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png" width="400" hegiht="500" align="center/"></p><p>LSTMs也有类似链的结构，但是重复模块有不同的结构。不是只有一个神经网络层，而是有四个，以一种非常特殊的方式相互作用。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" width="400" hegiht="500" align="center/"></p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png" alt="img"></p><p>​    在上面的图中，每一条线都带有一个完整的向量，从一个节点的输出到其他节点的输入。粉色圆圈表示点化操作，比如向量加法，而黄色框表示神经网络学习层。箭头合并表示连接，而箭头分叉表示被正在被复制的内容以及复制到不同位置。</p><h2 id="LSTM输入和输出"><a href="#LSTM输入和输出" class="headerlink" title="LSTM输入和输出"></a>LSTM输入和输出</h2><p><a href="https://www.zhihu.com/question/41949741" target="_blank" rel="noopener">https://www.zhihu.com/question/41949741</a></p><h2 id="LSTM核心思想"><a href="#LSTM核心思想" class="headerlink" title="LSTM核心思想"></a>LSTM核心思想</h2><p>LSTMs的核心是 单元状态，即贯穿图顶部的水平线。</p><p>单元状态有点像传送带。它沿着整个链一直向下，只有一些很小的线性相互作用。信息很容易不加改变地沿着它流动。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" alt="img"></p><p>LSTM有能力删除或添加信息到单元状态，并由称为门的结构小心地控制。</p><p>门是一种选择性地让信息通过的方法。它们由sigmoid神经网络层和点乘运算组成。.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" alt="img"></p><p>sigmoid层输出0到1之间的数字，描述每个组件应该通过的百分比。值0表示“不让任何东西通过”，而值1表示“让所有东西通过!”LSTM有三个这样的门，用来保护和控制单元状态。</p><h2 id="LSTM深入剖析"><a href="#LSTM深入剖析" class="headerlink" title="LSTM深入剖析"></a>LSTM深入剖析</h2><h3 id="忘记阶段"><a href="#忘记阶段" class="headerlink" title="忘记阶段"></a>忘记阶段</h3><p>LSTM的第一步是决定要从单元格状态丢弃什么信息。这个决定是由一个叫做“忘记门”的sigmoid层做出的。“它查看h<sub>t-1</sub>和 x<sub>t</sub>，并为处于单元格状态  C<sub>t-1</sub>的每个值输出一个介于0到1之间的数字。1表示“完全保留这个”，而0表示“完全删除这个”。</p><p>让我们回到语言模型的例子，该模型试图根据前面的所有单词预测下一个单词。在这样的问题中，单元格状态可能包括当前主语的词性，以便使用正确的代词。当我们看到一个新的主语时，我们想要忘记旧主语的词性。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" alt="img"></p><h3 id="选择记忆阶段"><a href="#选择记忆阶段" class="headerlink" title="选择记忆阶段"></a>选择记忆阶段</h3><p>下一步是决定要在单元格状态中存储哪些新信息。它有两部分。首先，一个名为“input gate layer”的sigmoid层决定要更新哪些值。接下来，tanh层创建一个新的候选值向量C<sub> ~t </sub>，可以将其添加到状态中。在下一个步骤中，我们将把这两者结合起来对状态的更新。</p><p>在我们的语言模型示例中，我们希望将新主体的词性添加到单元格状态，以替换我们正在遗忘的旧主体。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" alt="img"></p><p>现在是时候将旧的单元状态C<sub>t - 1</sub>更新为新的单元状态C<sub>t</sub>了。前面的步骤已经决定了要做什么，我们只需要实际去执行。</p><p>我们将旧状态乘以f<sub>t</sub>，忘记我们之前决定忘记的事情。然后我们将它添加到C<sub> ~t </sub>中。这是新的候选值，按我们决定每个状态值的更新进行缩放。</p><p>在语言模型中，这是我们实际删除关于旧信息并添加新信息的地方，正如我们在前面的步骤中描述的那样。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" alt="img"></p><h3 id="输出阶段"><a href="#输出阶段" class="headerlink" title="输出阶段"></a>输出阶段</h3><p>最后，我们需要决定输出什么。这个输出将基于我们的单元格状态，但是是经过过滤的版本。首先，我们运行一个sigmoid层，它决定要输出单元格状态的哪些部分。然后，我们将单元格状态放入tanh(将值缩放到- 1和1之间)，并将其乘以sigmoid层的输出，这样我们只输出我们决定输出的部分。</p><p>对于语言模型的例子，由于它只是看到了一个主语，所以它可能希望输出与动词相关的信息，以防接下来会发生什么。例如，它可以输出主语是单数还是复数，这样我们就知道如果主语是单数或复数，那么动词应该变成什么形式。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" alt="img"></p><h2 id="LSTM变形"><a href="#LSTM变形" class="headerlink" title="LSTM变形"></a>LSTM变形</h2><p>详情看来源网站。变形有很多，包括GRU，数十万种，Which of these variants is best? Do the differences matter? <a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="noopener">Greff, et al. (2015)</a> do a nice comparison of popular variants, finding that they’re all about the same.</p><h3 id="代码以及小例子"><a href="#代码以及小例子" class="headerlink" title="代码以及小例子"></a>代码以及小例子</h3><p><img src="https://cdn-images-1.medium.com/max/1600/1*p2yXhtxmYflEUrTC1rCoUA.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LSTM详解，把cell转变当作传送带的思想&lt;/p&gt;
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="LSTM" scheme="http://kodgv.xyz/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>RNN</title>
    <link href="http://kodgv.xyz/2019/04/14/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RNN/"/>
    <id>http://kodgv.xyz/2019/04/14/神经网络/RNN/</id>
    <published>2019-04-14T08:20:34.000Z</published>
    <updated>2019-04-22T08:20:14.796Z</updated>
    
    <content type="html"><![CDATA[<p>RNN快速学习</p><a id="more"></a><p>[TOC]</p><h2 id="一、从单层网络谈起"><a href="#一、从单层网络谈起" class="headerlink" title="一、从单层网络谈起"></a>一、从单层网络谈起</h2><p>在学习RNN之前，首先要了解一下最基本的单层网络，它的结构如图：</p><p><img src="https://pic1.zhimg.com/80/v2-da9ac1b5e3f91086fd06e6173fed1580_hd.jpg" alt="img"></p><p>输入是x，经过变换Wx+b和激活函数f得到输出y。相信大家对这个已经非常熟悉了。</p><h2 id="二、经典的RNN结构（N-vs-N）"><a href="#二、经典的RNN结构（N-vs-N）" class="headerlink" title="二、经典的RNN结构（N vs N）"></a>二、经典的RNN结构（N vs N）</h2><p>在实际应用中，我们还会遇到很多序列形的数据：</p><p><img src="https://pic3.zhimg.com/80/v2-0f8f8a8313867459d33e902fed97bd16_hd.jpg" alt="img"></p><p>如：</p><ul><li>自然语言处理问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推。</li><li>语音处理。此时，x1、x2、x3……是每帧的声音信号。</li><li>时间序列问题。例如每天的股票价格等等</li></ul><p>序列形的数据就不太好用原始的神经网络处理了。为了建模序列问题，RNN引入了隐状态h（hidden state）的概念，h可以对序列形的数据提取特征，接着再转换为输出。先从h1的计算开始看：</p><p><img src="https://pic1.zhimg.com/80/v2-a5f8bc30bcc2d9eba7470810cb362850_hd.jpg" alt="img"></p><p>图示中记号的含义是：</p><ul><li><strong>圆圈或方块表示的是向量。</strong></li><li><strong>一个箭头就表示对该向量做一次变换。如上图中h0和x1分别有一个箭头连接，就表示对h0和x1各做了一次变换。</strong></li></ul><p><strong>在很多论文中也会出现类似的记号，初学的时候很容易搞乱，但只要把握住以上两点，就可以比较轻松地理解图示背后的含义。</strong></p><p>h2的计算和h1类似。要注意的是，在计算时，<strong>每一步使用的参数U、W、b都是一样的，也就是说每个步骤的参数都是共享的，这是RNN的重要特点，一定要牢记。</strong></p><p><img src="https://pic3.zhimg.com/80/v2-74d7ac80ca83165092579932920d0ffe_hd.jpg" alt="img"></p><p>依次计算剩下来的（使用相同的参数U、W、b）：</p><p><img src="https://pic2.zhimg.com/80/v2-bc9759f8c642208a0f8514ccd0260b31_hd.jpg" alt="img"></p><p>我们这里为了方便起见，只画出序列长度为4的情况，实际上，这个计算过程可以无限地持续下去。</p><p>我们目前的RNN还没有输出，得到输出值的方法就是直接通过h进行计算：</p><p><img src="https://pic1.zhimg.com/80/v2-9f3a921d0d5c1313afa58bd3ef53af48_hd.jpg" alt="img"><br>正如之前所说，<strong>一个箭头就表示对对应的向量做一次类似于f(Wx+b)的变换，这里的这个箭头就表示对h1进行一次变换，得到输出y1。</strong></p><p><img src="https://pic4.zhimg.com/80/v2-9524a28210c98ed130644eb3c3002087_hd.jpg" alt></p><p>剩下的输出类似进行（使用和y1同样的参数V和c）：</p><p><img src="https://pic2.zhimg.com/80/v2-629abbab0d5cc871db396f17e9c58631_hd.jpg" alt="img"></p><p>OK！大功告成！这就是最经典的RNN结构，我们像搭积木一样把它搭好了。它的输入是x1, x2, …..xn，输出为y1, y2, …yn，也就是说，<strong>输入和输出序列必须要是等长的</strong>。</p><p>由于这个限制的存在，经典RNN的适用范围比较小，但也有一些问题适合用经典的RNN结构建模，如：</p><ul><li>计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。</li><li>输入为字符，输出为下一个字符的概率。这就是著名的Char RNN（详细介绍请参考：<a href="https://link.zhihu.com/?target=http%3A//karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a>，Char RNN可以用来生成文章，诗歌，甚至是代码，非常有意思）。</li></ul><h2 id="三、N-VS-1"><a href="#三、N-VS-1" class="headerlink" title="三、N VS 1"></a>三、N VS 1</h2><p>有的时候，我们要处理的问题输入是一个序列，输出是一个单独的值而不是序列，应该怎样建模呢？实际上，我们只在最后一个h上进行输出变换就可以了：</p><p><img src="https://pic1.zhimg.com/80/v2-6caa75392fe47801e605d5e8f2d3a100_hd.jpg" alt="img"></p><p>这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。</p><h2 id="四、1-VS-N"><a href="#四、1-VS-N" class="headerlink" title="四、1 VS N"></a>四、1 VS N</h2><p>输入不是序列而输出为序列的情况怎么处理？我们可以只在序列开始进行输入计算：</p><p><img src="https://pic3.zhimg.com/80/v2-87ebd6a82e32e81657682ffa0ba084ee_hd.jpg" alt="img"></p><p>还有一种结构是把输入信息X作为每个阶段的输入：</p><p><img src="https://pic3.zhimg.com/80/v2-fe054c488bb3a9fbcdfad299b2294266_hd.jpg" alt="img"></p><p>下图省略了一些X的圆圈，是一个等价表示：</p><p><img src="https://pic1.zhimg.com/80/v2-16e626b6e99fb1d23c8a54536f7d28dc_hd.jpg" alt="img"></p><p>这种1 VS N的结构可以处理的问题有：</p><ul><li>从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子</li><li>从类别生成语音或音乐等</li></ul><h2 id="五、N-vs-M"><a href="#五、N-vs-M" class="headerlink" title="五、N vs M"></a>五、N vs M</h2><p>下面我们来介绍RNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。</p><p>原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。</p><p><strong>为此，Encoder-Decoder结构先将输入数据编码成一个上下文向量c：</strong></p><p><img src="https://pic2.zhimg.com/80/v2-03aaa7754bb9992858a05bb9668631a9_hd.jpg" alt="img"></p><p>得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。</p><p><strong>拿到c之后，就用另一个RNN网络对其进行解码</strong>，这部分RNN网络被称为Decoder。具体做法就是将c当做之前的初始状态h0输入到Decoder中：</p><p><img src="https://pic4.zhimg.com/80/v2-77e8a977fc3d43bec8b05633dc52ff9f_hd.jpg" alt="img"></p><p>还有一种做法是将c当做每一步的输入：</p><p><img src="https://pic4.zhimg.com/80/v2-e0fbb46d897400a384873fc100c442db_hd.jpg" alt="img"></p><p>由于这种Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如：</p><ul><li>机器翻译。Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的</li><li>文本摘要。输入是一段文本序列，输出是这段文本序列的摘要序列。</li><li>阅读理解。将输入的文章和问题分别编码，再对其进行解码得到问题的答案。</li><li>语音识别。输入是语音信号序列，输出是文字序列。</li></ul><h2 id="六、Attention机制"><a href="#六、Attention机制" class="headerlink" title="六、Attention机制"></a>六、Attention机制</h2><p>在Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征c再解码，<strong>因此， c中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。</strong>如机器翻译问题，当要翻译的句子较长时，一个c可能存不下那么多信息，就会造成翻译精度的下降。</p><p>Attention机制通过在每个时间输入不同的c来解决这个问题，下图是带有Attention机制的Decoder：</p><p><img src="https://pic2.zhimg.com/80/v2-8da16d429d33b0f2705e47af98e66579_hd.jpg" alt="img"></p><p><strong>每一个c会自动去选取与当前所要输出的y最合适的上下文信息。具体来说，我们用 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="a_{ij}"> 衡量Encoder中第j阶段的hj和解码时第i阶段的相关性，最终Decoder中第i阶段的输入的上下文信息 <img src="https://www.zhihu.com/equation?tex=c_i" alt="c_i"> 就来自于所有 <img src="https://www.zhihu.com/equation?tex=h_j" alt="h_j"> 对 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="a_{ij}"> 的加权和。</strong></p><p>以机器翻译为例（将中文翻译成英文）：</p><p><img src="https://pic1.zhimg.com/80/v2-d266bf48a1d77e7e4db607978574c9fc_hd.jpg" alt="img"></p><p>输入的序列是“我爱中国”，因此，Encoder中的h1、h2、h3、h4就可以分别看做是“我”、“爱”、“中”、“国”所代表的信息。在翻译成英语时，第一个上下文c1应该和“我”这个字最相关，因此对应的 <img src="https://www.zhihu.com/equation?tex=a_%7B11%7D" alt="a_{11}"> 就比较大，而相应的 <img src="https://www.zhihu.com/equation?tex=a_%7B12%7D" alt="a_{12}"> 、 <img src="https://www.zhihu.com/equation?tex=a_%7B13%7D" alt="a_{13}"> 、 <img src="https://www.zhihu.com/equation?tex=a_%7B14%7D" alt="a_{14}"> 就比较小。c2应该和“爱”最相关，因此对应的 <img src="https://www.zhihu.com/equation?tex=a_%7B22%7D" alt="a_{22}"> 就比较大。最后的c3和h3、h4最相关，因此 <img src="https://www.zhihu.com/equation?tex=a_%7B33%7D" alt="a_{33}"> 、 <img src="https://www.zhihu.com/equation?tex=a_%7B34%7D" alt="a_{34}"> 的值就比较大。</p><p>至此，关于Attention模型，我们就只剩最后一个问题了，那就是：<strong>这些权重 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="a_{ij}"> 是怎么来的？</strong></p><p>事实上， <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="a_{ij}"> 同样是从模型中学出的，它实际和Decoder的第i-1阶段的隐状态、Encoder第j个阶段的隐状态有关。</p><p>同样还是拿上面的机器翻译举例， <img src="https://www.zhihu.com/equation?tex=a_%7B1j%7D" alt="a_{1j}"> 的计算（此时箭头就表示对h’和 <img src="https://www.zhihu.com/equation?tex=h_j" alt="h_j"> 同时做变换）：</p><p><img src="https://pic4.zhimg.com/80/v2-5561fa61321f31113043fb9711ee3263_hd.jpg" alt="img"></p><p><img src="https://www.zhihu.com/equation?tex=a_%7B2j%7D" alt="a_{2j}"> 的计算：</p><p><img src="https://pic1.zhimg.com/80/v2-50473aa7b1c20d680abf8ca36d82c9e4_hd.jpg" alt="img"></p><p><img src="https://www.zhihu.com/equation?tex=a_%7B3j%7D" alt="a_{3j}"> 的计算：</p><p><img src="https://pic4.zhimg.com/80/v2-07f7411c77901a7bd913e55884057a63_hd.jpg" alt="img"></p><p>以上就是带有Attention的Encoder-Decoder模型计算的全过程。</p><h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><p>本文主要讲了N vs N，N vs 1、1 vs N、N vs M四种经典的RNN模型，以及如何使用Attention结构。希望能对大家有所帮助。</p><p>可能有小伙伴发现没有LSTM的内容，其实是因为LSTM从外部看和RNN完全一样，因此上面的所有结构对LSTM都是通用的，想了解LSTM内部结构的可以参考这篇文章：<a href="https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a>，写得非常好，推荐阅读。</p><p>所以RNN不是一定是seqtoseq的模型，其一般图为：</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" alt="RNN-rolled"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RNN快速学习&lt;/p&gt;
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="RNN" scheme="http://kodgv.xyz/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>pytorch学习</title>
    <link href="http://kodgv.xyz/2019/04/14/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/pytorch%E5%AD%A6%E4%B9%A0/"/>
    <id>http://kodgv.xyz/2019/04/14/神经网络/pytorch学习/</id>
    <published>2019-04-14T02:23:33.000Z</published>
    <updated>2019-04-22T06:28:29.607Z</updated>
    
    <content type="html"><![CDATA[<p>pytorch学习日记<br><a id="more"></a></p><p>[TOC]</p><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><h3 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h3><p>并行化读取数据<br><strong>len</strong>：代表样本数量。len(obj)等价于obj.<strong>len</strong>()。<br><strong>getitem</strong>：返回一条数据或一个样本。obj[index]等价于obj.<strong>getitem</strong>。</p><p>建议将节奏的图片等高负载的操作放到这里，因为多进程时会并行调用这个函数，这样做可以加速。<br>dataset中应尽量只包含只读对象，避免修改任何可变对象。因为如果使用多进程，可变对象要加锁，但后面讲到的dataloader的设计使其难以加锁。如下面例子中的self.num可能在多进程下出问题</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span><span class="params">(data.Dataset)</span>:</span><span class="comment">#需要继承data.Dataset</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        <span class="comment"># 1. Initialize file path or list of file.</span></span><br><span class="line"> <span class="comment"># 初始化变量</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line"> <span class="comment"># 需要返回batch中的data，即返回index下标的数值</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># the total size of your dataset.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><p>示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNIST</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, train=True, transform=None, target_transform=None, download=False)</span>:</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line">        self.train = train  <span class="comment"># training set or test set</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> download:</span><br><span class="line">            self.download()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._check_exists():</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'Dataset not found.'</span> +</span><br><span class="line">                               <span class="string">' You can use download=True to download it'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            self.train_data, self.train_labels = torch.load(</span><br><span class="line">                os.path.join(root, self.processed_folder, self.training_file))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.test_data, self.test_labels = torch.load(os.path.join(root, self.processed_folder, self.test_file))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            img, target = self.train_data[index], self.train_labels[index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img, target = self.test_data[index], self.test_labels[index]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># doing this so that it is consistent with all other datasets</span></span><br><span class="line">        <span class="comment"># to return a PIL Image</span></span><br><span class="line">        img = Image.fromarray(img.numpy(), mode=<span class="string">'L'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            target = self.target_transform(target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">60000</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure><h3 id="torch-utils-data-Dataload"><a href="#torch-utils-data-Dataload" class="headerlink" title="torch.utils.data.Dataload"></a>torch.utils.data.Dataload</h3><p><code>torch.utils.data.DataLoader</code>(<em>dataset</em>, <em>batch_size=1</em>, <em>shuffle=False</em>, <em>sampler=None</em>, <em>batch_sampler=None</em>, <em>num_workers=0</em>, <em>collate_fn=<function default_collate></function></em>, <em>pin_memory=False</em>, <em>drop_last=False</em>, <em>timeout=0</em>, <em>worker_init_fn=None</em>)<a href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader" target="_blank" rel="noopener">[SOURCE]</a></p><p>Data loader. Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.<br>dataset：加载的数据集(Dataset对象)<br>batch_size：batch size<br>shuffle:：是否将数据打乱<br>sampler： 样本抽样，后续会详细介绍<br>num_workers：使用多进程加载的进程数，0代表不使用多进程<br>collate_fn： 如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可<br>pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些<br>drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃</p><p><strong>重点在于collate_fn</strong></p><p>是不是看着有点头大，没有关系，我们先搞清楚他的输入是什么。这里可以看到他的输入被命名为batch，但是我们还是不知道到底是什么，可以猜测应该是一个batch size的数据。我们继续往后找，可以找到这个<a href="https://link.jianshu.com?t=https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L180" target="_blank" rel="noopener">地方</a>。</p><p><img src="https:////upload-images.jianshu.io/upload_images/3623720-235599af888f7658.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/631/format/webp" alt="img"></p><p>Paste_Image.png</p><p>我们可以从这里看到<code>collate_fn</code>在这里进行了调用，那么他的输入我们就找到了，从这里看这就是一个list，list中的每个元素就是<code>self.data[i]</code>，如果你在往上看，可以看到这个<code>self.data</code>就是我们需要预先定义的Dataset，那么这里<code>self.data[i]</code>就等价于我们在Dataset里面定义的<code>__getitem__</code>这个函数。</p><p>所以我们知道了<code>collate_fn</code>这个函数的输入就是一个list，list的长度是一个batch size，list中的每个元素都是<code>__getitem__</code>得到的结果。</p><p><strong>注意如果dataset里面返回的不是一个值而是元组的话</strong>：[tuple]，这时候就需要用<br>texts, lens, y = zip(*batch)来解开</p><h3 id="squeeze-amp-unsqueeze"><a href="#squeeze-amp-unsqueeze" class="headerlink" title="squeeze&amp;unsqueeze"></a>squeeze&amp;unsqueeze</h3><ol><li>首先初始化一个a<br><img src="https://img-blog.csdn.net/20180812155855509?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZseXNreV9qYXk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt></li></ol><p>可以看出a的维度为（2，3）</p><ol><li><p>在第二维增加一个维度，使其维度变为（2，1，3）<br><img src="https://img-blog.csdn.net/20180812160119403?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZseXNreV9qYXk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt><br>可以看出a的维度已经变为（2，1，3）了，同样如果需要在倒数第二个维度上增加一个维度，那么使用b.unsqueeze(-2)<br>二、squeeze()函数介绍</p></li><li><p>首先得到一个维度为（1，2，3）的tensor（张量）<br><img src="https://img-blog.csdn.net/20180812160833709?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZseXNreV9qYXk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt><br>由图中可以看出c的维度为（1，2，3）<br>2.下面使用squeeze()函数将第一维去掉<br><img src="https://img-blog.csdn.net/20180812161010282?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZseXNreV9qYXk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt><br>可见，维度已经变为（2，3）<br>3.另外<br><img src="https://img-blog.csdn.net/20180812161246184?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZseXNreV9qYXk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt><br>可以看出维度并没有变化，仍然为（1，2，3），这是因为只有维度为1时才会去掉。</p></li></ol><h3 id="pack-padded-sequence-amp-pad-packed-sequence"><a href="#pack-padded-sequence-amp-pad-packed-sequence" class="headerlink" title="pack_padded_sequence&amp;pad_packed_sequence"></a>pack_padded_sequence&amp;pad_packed_sequence</h3><p>踩坑：</p><ul><li>要注意pad回来的会自动补0序列的，注意这些0序列，会对后续的操作产生影响</li><li>要注意0是补在后面，不是补在前面，这一点坑了我两天</li><li>动态padding会快，但是注意pack和pad会额外需要耗时</li><li>注意预测的时候要reverse回来，不然没法做stacking</li></ul><p><strong>RNN处理变长序列参考的的代码</strong>：<a href="https://zhuanlan.zhihu.com/p/40391002" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40391002</a></p><p><strong>pytorch 处理变长序列过程中各个tensor变化情况</strong>：<a href="https://blog.csdn.net/qq_27505047/article/details/78764888" target="_blank" rel="noopener">https://blog.csdn.net/qq_27505047/article/details/78764888</a></p><p><strong>kaggle比赛代码：</strong><a href="https://www.kaggle.com/johnfarrell/plasticc-2018-emb-gru" target="_blank" rel="noopener">https://www.kaggle.com/johnfarrell/plasticc-2018-emb-gru</a></p><p><strong>torch.nn.utils.rnn.pack_padded_sequence()</strong></p><p>这里的<code>pack</code>，理解成压紧比较好。 将一个 填充过的变长序列 压紧。（填充时候，会有冗余，所以压紧一下）</p><p>其中pack的过程为：（注意pack的形式，不是按行压，而是按列压）</p><p><img src="http://image.bubuko.com/info/201810/20181023003228230058.png" alt="技术分享图片"></p><p>（下面方框内为<code>PackedSequence</code>对象，由data和batch_sizes组成）</p><p>输入的形状可以是(T×B×<em> )。<code>T</code>是最长序列长度，<code>B</code>是<code>batch size</code>，`</em><code>代表任意维度(可以是0)。如果</code>batch_first=True<code>的话，那么相应的</code>input size<code>就是</code>(B×T×*)`。</p><p><a href="https://zhuanlan.zhihu.com/p/32103001" target="_blank" rel="noopener">读PyTorch源码学习RNN（1）</a></p><p><img src="https://cdn.nlark.com/yuque/0/2018/png/139777/1540608513263-a57374b7-2448-43f6-931e-4624acc4a146.png" alt="img"></p><p>pack之后得到的每个batch就是一个timestep,也就是说timestep的时候取都是1的状态(a1和b1)，它实际上就是告诉Model为每个序列构建不同数量的状态。</p><p><strong>pack和pad要统一是否使用batch_first=True，否则转换维度会出问题</strong></p><h2 id="问题解答专区"><a href="#问题解答专区" class="headerlink" title="问题解答专区"></a>问题解答专区</h2><h3 id="如何自定义forward函数"><a href="#如何自定义forward函数" class="headerlink" title="如何自定义forward函数"></a>如何自定义forward函数</h3><p>只需要继承nn.Module，然后重写forward函数，注意调用时机为该模块名字 module(参数)，这里得参数跟forward得参数一致</p><h3 id="如何自定义LOSS函数"><a href="#如何自定义LOSS函数" class="headerlink" title="如何自定义LOSS函数"></a>如何自定义LOSS函数</h3><p><a href="https://www.zhihu.com/question/66988664" target="_blank" rel="noopener">https://www.zhihu.com/question/66988664</a></p><h2 id="如何打印网络和参数结构"><a href="#如何打印网络和参数结构" class="headerlink" title="如何打印网络和参数结构"></a>如何打印网络和参数结构</h2><p><a href="https://github.com/nmhkahn/torchsummaryX" target="_blank" rel="noopener">https://github.com/nmhkahn/torchsummaryX</a></p><h2 id="如何释放显存"><a href="#如何释放显存" class="headerlink" title="如何释放显存"></a>如何释放显存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gc.collect()</span><br><span class="line">torch.cuda.empty_cache()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;pytorch学习日记&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="pytorch" scheme="http://kodgv.xyz/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>测试集和训练集分布</title>
    <link href="http://kodgv.xyz/2019/04/13/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/%E6%B5%8B%E8%AF%95%E9%9B%86%E5%92%8C%E8%AE%AD%E7%BB%83%E9%9B%86%E5%88%86%E5%B8%83/"/>
    <id>http://kodgv.xyz/2019/04/13/竞赛经验/测试集和训练集分布/</id>
    <published>2019-04-13T11:40:23.000Z</published>
    <updated>2019-04-13T11:49:03.029Z</updated>
    
    <content type="html"><![CDATA[<p>Train &amp; Test分布主要是为了看数据的分布情况。</p><p>下面的方法是最近kaggle上GM搞的一种极其简单的探索训练集&amp;测试集分布的方案,将一个分布的问题变成一个二分类问题,该方法简单有效，如果AUC低于0.6,则我们可以认为训练集和测试集是分布平衡的,反之我们则可以认为训练集和测试集是分布不一致的,这个时候就需要注意AB榜是否会出现翻车的情况等</p><a id="more"></a><p>[TOC]</p><p>来源：<a href="https://www.kaggle.com/justfor/adversarial-validation-plus-ks-test" target="_blank" rel="noopener">https://www.kaggle.com/justfor/adversarial-validation-plus-ks-test</a></p><h2 id="Adversarial-Validation"><a href="#Adversarial-Validation" class="headerlink" title="Adversarial Validation"></a>Adversarial Validation</h2><p>下面的方法是最近kaggle上GM搞的一种极其简单的探索训练集&amp;测试集分布的方案,将一个分布的问题变成一个二分类问题,该方法简单有效，如果AUC低于0.6,则我们可以认为训练集和测试集是分布平衡的,反之我们则可以认为训练集和测试集是分布不一致的,这个时候就需要注意AB榜是否会出现翻车的情况等</p><p>核心点在于将<strong>y1 = np.array([0] x train.shape[0])y2 = np.array([1] x test.shape[0])</strong></p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Create label array and complete dataset</span></span><br><span class="line"><span class="attr">y1</span> = np.array([<span class="number">0</span>]*train.shape[<span class="number">0</span>])</span><br><span class="line"><span class="attr">y2</span> = np.array([<span class="number">1</span>]*test.shape[<span class="number">0</span>])</span><br><span class="line"><span class="attr">y</span> = np.concatenate((y1, y2))</span><br><span class="line"><span class="attr">X_data</span> = pd.concat([train, test])</span><br><span class="line">X_data.reset_index(<span class="attr">drop=True,</span> <span class="attr">inplace=True)</span></span><br><span class="line"><span class="comment">#Initialize splits&amp;LGBM</span></span><br><span class="line"><span class="attr">skf</span> = StratifiedKFold(<span class="attr">n_splits=5,</span> <span class="attr">shuffle=True,</span> <span class="attr">random_state=13)</span></span><br><span class="line"></span><br><span class="line"><span class="attr">lgb_model</span> = lgb.LGBMClassifier(<span class="attr">max_depth=-1,</span></span><br><span class="line">                                   <span class="attr">n_estimators=500,</span></span><br><span class="line">                                   <span class="attr">learning_rate=0.01,</span></span><br><span class="line">                                   <span class="attr">objective='binary',</span> </span><br><span class="line">                                   <span class="attr">n_jobs=-1)</span></span><br><span class="line">                                   </span><br><span class="line"><span class="attr">counter</span> = <span class="number">1</span></span><br><span class="line"><span class="comment">#Train 5-fold adversarial validation classifier</span></span><br><span class="line">for train_index, test_index <span class="keyword">in</span> skf.split(X_data, y):</span><br><span class="line">    print('\nFold &#123;&#125;'.format(counter))</span><br><span class="line">    X_fit, <span class="attr">X_val</span> = X_data.loc[train_index], X_data.loc[test_index]</span><br><span class="line">    y_fit, <span class="attr">y_val</span> = y[train_index], y[test_index]</span><br><span class="line">    </span><br><span class="line">    lgb_model.fit(X_fit, y_fit, <span class="attr">eval_metric='auc',</span> </span><br><span class="line">              <span class="attr">eval_set=[(X_val,</span> y_val)], </span><br><span class="line">              <span class="attr">verbose=100,</span> <span class="attr">early_stopping_rounds=10)</span></span><br><span class="line">    counter+=<span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="Kolmogorov-Smirnov-Test"><a href="#Kolmogorov-Smirnov-Test" class="headerlink" title="Kolmogorov-Smirnov Test"></a>Kolmogorov-Smirnov Test</h2><p>单独看每个变量是否能过通过KS检验，不仅有图，而且有一种量化的手段</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#Load more packages</span><br><span class="line">from scipy.stats import ks_2samp</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">sns.set_style('whitegrid')</span><br><span class="line">import <span class="built_in">warnings</span></span><br><span class="line"><span class="built_in">warnings</span>.simplefilter(action='ignore', category=FutureWarning)</span><br><span class="line"><span class="built_in">warnings</span>.filterwarnings('ignore')</span><br><span class="line">#Perform KS-Test <span class="keyword">for</span> each <span class="built_in">feature</span> from train/test. Draw its distribution. Count <span class="built_in">features</span> based on statistics.</span><br><span class="line">#Plots are hidden. If you'd like to look <span class="built_in">at</span> them - press <span class="string">"Output"</span> button.</span><br><span class="line">hypothesisnotrejected = []</span><br><span class="line">hypothesisrejected = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">col</span> <span class="keyword">in</span> train.<span class="built_in">columns</span>:</span><br><span class="line">    statistic, pvalue = ks_2samp(train[<span class="built_in">col</span>], test[<span class="built_in">col</span>])</span><br><span class="line">    <span class="keyword">if</span> pvalue&gt;=statistic:</span><br><span class="line">        hypothesisnotrejected.<span class="built_in">append</span>(<span class="built_in">col</span>)</span><br><span class="line">    <span class="keyword">if</span> pvalue&lt;statistic:</span><br><span class="line">        hypothesisrejected.<span class="built_in">append</span>(<span class="built_in">col</span>)</span><br><span class="line">        </span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line">    plt.<span class="built_in">title</span>(<span class="string">"Kolmogorov-Smirnov test for train/test\n"</span></span><br><span class="line">              <span class="string">"feature: &#123;&#125;, statistics: &#123;:.5f&#125;, pvalue: &#123;:5f&#125;"</span>.format(<span class="built_in">col</span>, statistic, pvalue))</span><br><span class="line">    sns.kdeplot(train[<span class="built_in">col</span>], <span class="built_in">color</span>='blue', shade=True, <span class="built_in">label</span>='Train')</span><br><span class="line">    sns.kdeplot(test[<span class="built_in">col</span>], <span class="built_in">color</span>='green', shade=True, <span class="built_in">label</span>='Test')</span><br><span class="line"></span><br><span class="line">    plt.<span class="built_in">show</span>()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Train &amp;amp; Test分布主要是为了看数据的分布情况。&lt;/p&gt;
&lt;p&gt;下面的方法是最近kaggle上GM搞的一种极其简单的探索训练集&amp;amp;测试集分布的方案,将一个分布的问题变成一个二分类问题,该方法简单有效，如果AUC低于0.6,则我们可以认为训练集和测试集是分布平衡的,反之我们则可以认为训练集和测试集是分布不一致的,这个时候就需要注意AB榜是否会出现翻车的情况等&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>比赛规范经验</title>
    <link href="http://kodgv.xyz/2019/04/12/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E6%AF%94%E8%B5%9B%E8%A7%84%E8%8C%83%E7%BB%8F%E9%AA%8C/"/>
    <id>http://kodgv.xyz/2019/04/12/数据竞赛/比赛规范经验/</id>
    <published>2019-04-12T09:12:58.000Z</published>
    <updated>2019-04-13T11:44:31.270Z</updated>
    
    <content type="html"><![CDATA[<p>比赛中的代码规范,代码不规范，亲人两行泪</p><a id="more"></a><p>先杂七杂八写，有一定量了再整理</p><ul><li>思路要记在云笔记里，纸上会丢，而且换环境忘记带</li><li>临近比赛前一个月注意多保存看过的kernel，因为它们做出来之后就会删除。也就是说快结束的时候，那些都不是重要的magic</li><li>要保持一个纯洁统一模型的代码的单独文件，才方便接入</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛中的代码规范,代码不规范，亲人两行泪&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="代码规范" scheme="http://kodgv.xyz/tags/%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/"/>
    
  </entry>
  
  <entry>
    <title>特征构造</title>
    <link href="http://kodgv.xyz/2019/04/11/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E7%89%B9%E5%BE%81%E6%9E%84%E9%80%A0/"/>
    <id>http://kodgv.xyz/2019/04/11/数据竞赛/特征构造/</id>
    <published>2019-04-11T02:47:02.000Z</published>
    <updated>2019-04-13T11:44:48.438Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p>函数统一返回新的df，同时删除原有df</p><h2 id="特征频数"><a href="#特征频数" class="headerlink" title="特征频数"></a>特征频数</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 构造特征时测试集要和训练集一样</span><br><span class="line">def encode_FE(df,<span class="built_in">col</span>,test)：</span><br><span class="line">    <span class="built_in">cv</span> = df[<span class="built_in">col</span>].value_counts()</span><br><span class="line">    <span class="built_in">nm</span> = <span class="built_in">col</span>+'_FE'</span><br><span class="line">    df[<span class="built_in">nm</span>] = df[<span class="built_in">col</span>].<span class="built_in">map</span>(<span class="built_in">cv</span>)</span><br><span class="line">    test[<span class="built_in">nm</span>] = test[<span class="built_in">col</span>].<span class="built_in">map</span>(<span class="built_in">cv</span>)</span><br><span class="line">    <span class="built_in">return</span> df,test</span><br></pre></td></tr></table></figure><h2 id="data-augment"><a href="#data-augment" class="headerlink" title="data augment"></a>data augment</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def augment(x,y,t=<span class="number">2</span>):</span><br><span class="line">    xs,xn = [],[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(t):</span><br><span class="line">        mask = y&gt;<span class="number">0</span></span><br><span class="line">        x1 = x[mask].<span class="built_in">copy</span>()</span><br><span class="line">        ids = <span class="built_in">np</span>.arange(x1.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(x1.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="built_in">np</span>.<span class="built_in">random</span>.shuffle(ids)</span><br><span class="line">            x1[:,c] = x1[ids][:,c]</span><br><span class="line">        xs.<span class="built_in">append</span>(x1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(t//<span class="number">2</span>):</span><br><span class="line">        mask = y==<span class="number">0</span></span><br><span class="line">        x1 = x[mask].<span class="built_in">copy</span>()</span><br><span class="line">        ids = <span class="built_in">np</span>.arange(x1.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(x1.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="built_in">np</span>.<span class="built_in">random</span>.shuffle(ids)</span><br><span class="line">            x1[:,c] = x1[ids][:,c]</span><br><span class="line">        xn.<span class="built_in">append</span>(x1)</span><br><span class="line"></span><br><span class="line">    xs = <span class="built_in">np</span>.vstack(xs)</span><br><span class="line">    xn = <span class="built_in">np</span>.vstack(xn)</span><br><span class="line">    ys = <span class="built_in">np</span>.ones(xs.shape[<span class="number">0</span>])</span><br><span class="line">    yn = <span class="built_in">np</span>.zeros(xn.shape[<span class="number">0</span>])</span><br><span class="line">    x = <span class="built_in">np</span>.vstack([x,xs,xn])</span><br><span class="line">    y = <span class="built_in">np</span>.concatenate([y,ys,yn])</span><br><span class="line">    <span class="built_in">return</span> x,y</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;函数统一返回新的df，同时删除原有df&lt;/p&gt;
&lt;h2 id=&quot;特征频数&quot;&gt;&lt;a href=&quot;#特征频数&quot; class=&quot;headerlink&quot; title=&quot;特征频数&quot;&gt;&lt;/a&gt;特征频数&lt;/h2&gt;&lt;figure class=&quot;highlight 
      
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Santander Customer Transaction Prediction 比赛经验</title>
    <link href="http://kodgv.xyz/2019/04/11/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/Santander%20Customer%20Transaction%20Prediction%20%E6%AF%94%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    <id>http://kodgv.xyz/2019/04/11/竞赛经验/Santander Customer Transaction Prediction 比赛经验/</id>
    <published>2019-04-11T02:46:10.000Z</published>
    <updated>2019-04-22T12:59:36.514Z</updated>
    
    <content type="html"><![CDATA[<p>Santander Customer Transaction Prediction 比赛，01分类问题，200维连续型匿名变量。</p><p>讲述了magic操作，count的新特征构造，特征相关性。。。</p><a id="more"></a><p>[TOC]</p><p>来源：<a href="https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920" target="_blank" rel="noopener">https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920</a></p><h1 id="magic-kernel"><a href="#magic-kernel" class="headerlink" title="magic kernel"></a>magic kernel</h1><h2 id="树模型的缺点"><a href="#树模型的缺点" class="headerlink" title="树模型的缺点"></a>树模型的缺点</h2><h3 id="无法学习到的东西"><a href="#无法学习到的东西" class="headerlink" title="无法学习到的东西"></a>无法学习到的东西</h3><p><img src="http://playagricola.com/Kaggle/198without.png" alt></p><p>LGBM用竖线划分直方图，因为LGBM看不到水平差异。一个直方图会将多个值放置在一个Bin中并且产生一个较为平滑的图。如果你把多个值放在一个bin中，你会得到一个锯齿的图，其中每个bin中有些值是惟一的，有些值出现了几十次，LGBM无法学习到这些事情。</p><p><img src="http://playagricola.com/Kaggle/198zoom3.png" alt></p><p>如上，构造的count图可以看出在相同值的附近存在不同频数的区别。</p><h3 id="对特征敏感的参数设置"><a href="#对特征敏感的参数设置" class="headerlink" title="对特征敏感的参数设置"></a>对特征敏感的参数设置</h3><p>​    仅仅做到上述构造特征还是没有用的，因为你添加新特征到LGBM的时候设置参数feature_fraction=0.05，这会导致特征被随机的采样，破坏了var_1和var_1count之间的依赖关系(目的就是让模型需要同时学习横向和纵向)。所以设置feature_fraction=1，能从0.901到0.910，但是如果要到0.920则需要剔除原始变量之间的<a href="https://en.wikipedia.org/wiki/Spurious_relationship" target="_blank" rel="noopener">spurious effects</a>,因为原始特征对模型敏感，同时相关系数普遍很低，则说明它们虽然有相关但是没有因果关联。</p><ul><li>Use Data Augmentation (as shown in Jiwei’s awesome kernel <a href="https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment" target="_blank" rel="noopener">here</a>). You must keep original and new feature in same row.</li><li>Use 200 separate models as shown in this kernel below.</li><li>Merge new feature and original feature into one feature. In original data, simply add 200 to each unique value. (And don’t add new columns)</li><li>使用数据增强，就是随机打乱特征的时候，保持count和var是一致的，保证count和var的相关性，又去除了var之间相关性</li><li>单独使用count和var预测，然后200个模型再融合(一共200个var），这与第三点的merge我觉得是一致的，将两列合成一列（单纯加减应该是不够的)</li></ul><p>​    <strong>注意计算频数的时候，原则是同分布下越多数据越好。</strong>所以train和test在不在一起取决于它们的分布是否一致，或者如何让它们分布一致。在该比赛中就对test集进行了划分，剔除了和train不一致的数据，再合在一起做频数</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>主要参考链接</p><h1 id="first-kernel"><a href="#first-kernel" class="headerlink" title="first kernel"></a>first kernel</h1><p>来源：<a href="https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920" target="_blank" rel="noopener">https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920</a></p><h2 id="magic-feature"><a href="#magic-feature" class="headerlink" title="magic feature"></a>magic feature</h2><p>不单单是用count ,而是更深入的使用count，告诉模型它所不知道的事情。比如说Unique in train 和test</p><ul><li>This value appears at least another time in data with target==1 and no 0;</li><li>This value appears at least another time in data with target==0 and no 1;</li><li>This value appears at least two more time in data with target==0 &amp; 1;</li><li>This value is unique in data;</li><li>This value is unique in data + test (only including real test samples);</li></ul><p>The other 200 (one per raw feature) features are numerical, let’s call them “not unique feat”, and correspond to the raw feature replacing values that are unique in data + test with the mean of the feature.</p><p>用该列的均值去替换该列的Unique值，使其成为非unique的列，这个操作是尝试出来的，它们也用了nan，median</p><h2 id="magic-insight"><a href="#magic-insight" class="headerlink" title="magic insight"></a>magic insight</h2><p>两个重要的节点：</p><ul><li>I looked at my LGBM trees (with only 3 leafs that’s easy to do) and noticed the trees were using the uniqueness information.通过树画图，看出树当前不能学习的东西</li><li>number of different values in train and test was not the same。 虽然数值上分布是一致的，但是在值的个数上不一致</li></ul><h2 id="technial-part"><a href="#technial-part" class="headerlink" title="technial part:"></a>technial part:</h2><p>匿名变量用NN会更好</p><p>NN的concat可以更好的处理变量之间的关系</p><h2 id="augment-magic"><a href="#augment-magic" class="headerlink" title="augment magic"></a>augment magic</h2><p> using the shuffle augmentation (duplicate and shuffle 16 times samples with target == 1, 4 for target ==0) and added pseudo label (2700 highest predicted test points as 1 and 2000 lowest as 0)</p><p>pseudo label指从测试集中取最高的为1最低的为0，加进去</p><p>shuffle augmentation 指数据增强(eda代码中有)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Santander Customer Transaction Prediction 比赛，01分类问题，200维连续型匿名变量。&lt;/p&gt;
&lt;p&gt;讲述了magic操作，count的新特征构造，特征相关性。。。&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>比赛模型</title>
    <link href="http://kodgv.xyz/2019/04/10/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E6%AF%94%E8%B5%9B%E6%A8%A1%E5%9E%8B/"/>
    <id>http://kodgv.xyz/2019/04/10/数据竞赛/比赛模型/</id>
    <published>2019-04-10T02:20:03.000Z</published>
    <updated>2019-04-12T09:04:51.957Z</updated>
    
    <content type="html"><![CDATA[<p>比赛通用模型代码</p><a id="more"></a><p>[TOC]</p><h2 id="载入模型前操作"><a href="#载入模型前操作" class="headerlink" title="载入模型前操作"></a>载入模型前操作</h2><h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler,StandardScaler</span><br><span class="line">sc = StandardScaler()</span><br><span class="line">train_features = sc.fit_transform(train_features)</span><br><span class="line">test_features = sc.transform(test_features)</span><br></pre></td></tr></table></figure><h3 id="切分训练集和验证集"><a href="#切分训练集和验证集" class="headerlink" title="切分训练集和验证集"></a>切分训练集和验证集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_split</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment"># K-fold</span></span><br><span class="line"><span class="comment"># scikit-learn k-fold cross-validation</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="comment"># data sample</span></span><br><span class="line"><span class="comment"># prepare cross validation</span></span><br><span class="line">kfold = KFold(n_splits=<span class="number">3</span>, shuffle = <span class="literal">True</span>, random_state= <span class="number">1</span>)</span><br><span class="line"><span class="comment"># enumerate splits</span></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kfold.split(data):</span><br><span class="line">    print(<span class="string">'train: %s, test: %s'</span> % (data[train], data[test]))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="comment"># StratifiedKFold</span></span><br><span class="line">folds = StratifiedKFold(n_splits=num_folds, shuffle=<span class="literal">False</span>, random_state=<span class="number">2319</span>)</span><br><span class="line"><span class="keyword">for</span> fold_, (trn_idx, val_idx) <span class="keyword">in</span> enumerate(folds.split(train.values, target.values)):</span><br><span class="line">    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]</span><br><span class="line">   X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]</span><br></pre></td></tr></table></figure><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="xgb回归"><a href="#xgb回归" class="headerlink" title="xgb回归"></a>xgb回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn</span></span><br><span class="line"><span class="comment">########################################################################### 回归</span></span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, StratifiedKFold,GroupKFold</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 别人的自定义损失函数,在parameter里面：object里面赋值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">custom_loss</span><span class="params">(y_true,y_pred)</span>:</span></span><br><span class="line">    penalty=<span class="number">2.0</span></span><br><span class="line">    grad=-y_true/y_pred+penalty*(<span class="number">1</span>-y_true)/(<span class="number">1</span>-y_pred) <span class="comment">#梯度</span></span><br><span class="line">    hess=y_true/(y_pred**<span class="number">2</span>)+penalty*(<span class="number">1</span>-y_true)/(<span class="number">1</span>-y_pred)**<span class="number">2</span> <span class="comment">#2阶导</span></span><br><span class="line">    <span class="keyword">return</span> grad,hess</span><br><span class="line"><span class="comment"># 自定义评价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse</span><span class="params">(y_pred,dtrain)</span>:</span> <span class="comment">#preds是结果（概率值），dtrain是个带label的DMatrix</span></span><br><span class="line">    labels=dtrain.get_label() <span class="comment">#提取label</span></span><br><span class="line">    t=mean_absolute_error(labels, y_pred)</span><br><span class="line">    print(t)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'mse'</span>,t</span><br><span class="line">parameters = &#123;<span class="string">'nthread'</span>:<span class="number">-1</span>, <span class="comment"># cpu 线程数 默认最大</span></span><br><span class="line">              <span class="string">'objective'</span>:<span class="string">'reg:linear'</span>,<span class="comment">#多分类or 回归的问题    若要自定义就替换为custom_loss（不带引号）</span></span><br><span class="line">              <span class="string">'learning_rate'</span>: <span class="number">.01</span>, <span class="comment">#so called `eta` value 如同学习率</span></span><br><span class="line">              <span class="string">'max_depth'</span>: <span class="number">6</span>,<span class="comment"># 构建树的深度，越大越容易过拟合</span></span><br><span class="line">              <span class="string">'min_child_weight'</span>: <span class="number">4</span>,</span><br><span class="line"><span class="comment"># 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言</span></span><br><span class="line"><span class="comment">#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。</span></span><br><span class="line"><span class="comment">#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。</span></span><br><span class="line">              <span class="string">'silent'</span>: <span class="number">1</span>,<span class="comment">#设置成1则没有运行信息输出，最好是设置为0.</span></span><br><span class="line">              <span class="string">'subsample'</span>: <span class="number">0.7</span>, <span class="comment"># 随机采样训练样本</span></span><br><span class="line">              <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,<span class="comment"># 生成树时进行的列采样</span></span><br><span class="line">              <span class="string">'n_estimators'</span>: <span class="number">100</span>,<span class="comment"># 树的个数跟num_boost_round是一样的，所以可以设置无限大，靠early_stop</span></span><br><span class="line">              <span class="string">'gamma'</span>:<span class="number">0.1</span>,<span class="comment"># 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。</span></span><br><span class="line">              <span class="string">'seed'</span>:<span class="number">1000</span> <span class="comment">#随机种子</span></span><br><span class="line">              <span class="comment">#'alpha':0, # L1 正则项参数</span></span><br><span class="line">              <span class="comment">#'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。</span></span><br><span class="line">              <span class="comment">#'num_class':10, # 类别数，多分类与 multisoftmax 并用</span></span><br><span class="line">              &#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergeToOne</span><span class="params">( X, X2)</span>:</span></span><br><span class="line">    X3 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">        tmp = np.array([list(X[i]), list(X2[i])])</span><br><span class="line">        X3.append(list(np.hstack(tmp)))</span><br><span class="line">    X3 = np.array(X3)</span><br><span class="line">    <span class="keyword">return</span> X3</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_XgbRegressor</span><span class="params">(train_data,train_target,test_data,feature_names,parameters,early_stopping_rounds,num_folds,eval_metric,model_name=<span class="string">'model'</span>,stratified=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    :param train_data: 一定是numpy</span></span><br><span class="line"><span class="string">    :param train_target:</span></span><br><span class="line"><span class="string">    :param parameters:</span></span><br><span class="line"><span class="string">    :param round:</span></span><br><span class="line"><span class="string">    :param k:</span></span><br><span class="line"><span class="string">    :param eval_metrics:自定义 or 内置字符串</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    reg=XGBRegressor()</span><br><span class="line">    reg.set_params(**parameters)</span><br><span class="line">    <span class="comment"># 定义一些变量</span></span><br><span class="line">    oof_preds = np.zeros((train_data.shape[<span class="number">0</span>],))</span><br><span class="line">    sub_preds = np.zeros((test_data.shape[<span class="number">0</span>],))</span><br><span class="line">    feature_importance_df = pd.DataFrame()</span><br><span class="line">    cv_result = []</span><br><span class="line">    <span class="comment"># K-flod</span></span><br><span class="line">    <span class="keyword">if</span> stratified:</span><br><span class="line">        folds = StratifiedKFold(n_splits= num_folds, shuffle=<span class="literal">True</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        folds = KFold(n_splits= num_folds, shuffle=<span class="literal">True</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">    X_train_newfeature=np.zeros((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> n_flod, (train_index, val_index) <span class="keyword">in</span> enumerate(folds.split(train_data, train_target)):</span><br><span class="line">        train_X=train_data[train_index]</span><br><span class="line">        val_X=train_data[val_index]</span><br><span class="line">        train_Y=train_target[train_index]</span><br><span class="line">        val_Y=train_target[val_index]</span><br><span class="line">        <span class="comment"># 参数初步定之后划分20%为验证集，准备一个watchlist 给train和validation set ,设置num_round 足够大（比如100000），以至于你能发现每一个round 的验证集预测结果，</span></span><br><span class="line">        <span class="comment"># 如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了。</span></span><br><span class="line">        watchlist= [(train_X, train_Y), (val_X, val_Y)]</span><br><span class="line">        <span class="comment"># early_stop 看validate的eval是否下降，这时候必须传eval_set,并取eval_set的最后一个作为validate</span></span><br><span class="line">        reg.fit(train_X,train_Y,early_stopping_rounds=early_stopping_rounds, eval_set=watchlist,eval_metric=eval_metric)</span><br><span class="line">        <span class="comment">## 生成gbdt新特征</span></span><br><span class="line">        new_feature = reg.apply(val_X)</span><br><span class="line">        <span class="keyword">if</span> X_train_newfeature.shape[<span class="number">0</span>]==<span class="number">1</span>:</span><br><span class="line">            X_train_newfeature=mergeToOne(val_X,new_feature)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X_train_newfeature = mergeToOne(val_X,new_feature)</span><br><span class="line">            X_train_newfeature=np.concatenate((X_train_newfeature,mergeToOne(new_feature, val_X)),axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">print</span> (X_train_newfeature)</span><br><span class="line">       <span class="comment"># 获得每次的预测值补充</span></span><br><span class="line">        oof_preds[val_index]=reg.predict(val_X)</span><br><span class="line">        <span class="comment"># 获得预测的平均值，这里直接加完再除m</span></span><br><span class="line">        sub_preds+= reg.predict(test_data)</span><br><span class="line">        result = mean_absolute_error(val_Y, reg.predict(val_X))</span><br><span class="line">        print(<span class="string">'Fold %2d macro-f1 : %.6f'</span> % (n_flod + <span class="number">1</span>, result))</span><br><span class="line">        cv_result.append(round(result,<span class="number">5</span>))</span><br><span class="line">        gc.collect()</span><br><span class="line">        <span class="comment"># 默认就是gain 如果要修改要再参数定义中修改importance_type</span></span><br><span class="line">        <span class="comment"># 保存特征重要度</span></span><br><span class="line">        gain = reg.feature_importances_</span><br><span class="line">        fold_importance_df = pd.DataFrame(&#123;<span class="string">'feature'</span>: feature_names,</span><br><span class="line">                                           <span class="string">'gain'</span>: <span class="number">100</span> * gain / gain.sum(),</span><br><span class="line">                                           <span class="string">'fold'</span>: n_flod,</span><br><span class="line">                                           &#125;).sort_values(<span class="string">'gain'</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 进行保存</span></span><br><span class="line">    sub_preds=sub_preds/folds.n_splits</span><br><span class="line">    new_feature=reg.apply(test_data)</span><br><span class="line">    X_test_newfeature = mergeToOne(test_data, new_feature)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./sub'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./sub'</span>)</span><br><span class="line">    pd.DataFrame(oof_preds,columns=[<span class="string">'class'</span>]).to_csv(<span class="string">'./sub/val_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>)</span><br><span class="line">    pd.DataFrame(sub_preds, columns=[<span class="string">'class'</span>]).to_csv(<span class="string">'./sub/test_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>)</span><br><span class="line">    print(<span class="string">'cv_result'</span>, cv_result)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./gbdt_newfeature'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./gbdt_newfeature'</span>)</span><br><span class="line"></span><br><span class="line">    np.save(<span class="string">"./gbdt_newfeature/train_newfeature.npy"</span>, X_train_newfeature)</span><br><span class="line">    np.save(<span class="string">"./gbdt_newfeature/test_newfeature.npy"</span>, X_test_newfeature)</span><br><span class="line">    save_importances(feature_importance_df, model_name)</span><br><span class="line">    <span class="keyword">return</span> reg,sub_preds</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_importances</span><span class="params">(feature_importance_df,model_name)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./feature_importance'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./feature_importance'</span>)</span><br><span class="line">    ft = feature_importance_df[[<span class="string">"feature"</span>, <span class="string">"gain"</span>]].groupby(<span class="string">"feature"</span>).mean().sort_values(by=<span class="string">"gain"</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">    ft.to_csv(<span class="string">'./feature_importance/importance_lightgbm_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="xgb回归-1"><a href="#xgb回归-1" class="headerlink" title="xgb回归"></a>xgb回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">########################################################################################## 分类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span>  sklearn.datasets  <span class="keyword">import</span>  make_hastie_10_2</span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line">clf_parameters = &#123;<span class="string">'nthread'</span>:<span class="number">-1</span>, <span class="comment"># cpu 线程数 默认最大</span></span><br><span class="line">              <span class="string">'objective'</span>:<span class="string">'multi:softmax'</span>,<span class="comment">#多分类or 回归的问题    若要自定义就替换为custom_loss（不带引号）</span></span><br><span class="line">              <span class="string">'learning_rate'</span>: <span class="number">.01</span>, <span class="comment">#so called `eta` value 如同学习率</span></span><br><span class="line">              <span class="string">'max_depth'</span>: <span class="number">6</span>,<span class="comment"># 构建树的深度，越大越容易过拟合</span></span><br><span class="line">              <span class="string">'min_child_weight'</span>: <span class="number">4</span>,</span><br><span class="line"><span class="comment"># 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言</span></span><br><span class="line"><span class="comment">#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。</span></span><br><span class="line"><span class="comment">#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。</span></span><br><span class="line">              <span class="string">'silent'</span>: <span class="number">1</span>,<span class="comment">#设置成1则没有运行信息输出，最好是设置为0.</span></span><br><span class="line">              <span class="string">'subsample'</span>: <span class="number">0.7</span>, <span class="comment"># 随机采样训练样本</span></span><br><span class="line">              <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,<span class="comment"># 生成树时进行的列采样</span></span><br><span class="line">              <span class="string">'n_estimators'</span>: <span class="number">500</span>,<span class="comment"># 树的个数跟num_boost_round是一样的，所以可以设置无限大，靠early_stop</span></span><br><span class="line">              <span class="string">'gamma'</span>:<span class="number">0.1</span>,<span class="comment"># 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。</span></span><br><span class="line">              <span class="string">'seed'</span>:<span class="number">1000</span> <span class="comment">#随机种子</span></span><br><span class="line">              <span class="comment">#'alpha':0, # L1 正则项参数</span></span><br><span class="line">              <span class="comment">#'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。</span></span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">n_class=<span class="number">3</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clf_custom_loss</span><span class="params">(y_true,y_pred)</span>:</span></span><br><span class="line">    penalty=<span class="number">2.0</span></span><br><span class="line">    grad=-y_true/y_pred+penalty*(<span class="number">1</span>-y_true)/(<span class="number">1</span>-y_pred) <span class="comment">#梯度</span></span><br><span class="line">    hess=y_true/(y_pred**<span class="number">2</span>)+penalty*(<span class="number">1</span>-y_true)/(<span class="number">1</span>-y_pred)**<span class="number">2</span> <span class="comment">#2阶导</span></span><br><span class="line">    <span class="keyword">return</span> grad,hess</span><br><span class="line"><span class="comment"># 自定义评价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clf_mse</span><span class="params">(y_pred,dtrain)</span>:</span> <span class="comment">#preds是结果（概率值），dtrain是个带label的DMatrix</span></span><br><span class="line">    labels=dtrain.get_label() <span class="comment">#提取label</span></span><br><span class="line">    <span class="comment">######### 分类预测的都是概率哦，所以这里要取一个max类别</span></span><br><span class="line">    y_pred = np.argmax(y_pred.reshape(n_class, <span class="number">-1</span>), axis=<span class="number">0</span>)</span><br><span class="line">    score=mean_absolute_error(labels, y_pred)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'mse'</span>,score</span><br><span class="line"><span class="comment"># 分类的时候要注意！！！！！！！！</span></span><br><span class="line"><span class="comment"># k-flod的时候要按层次拿出来，有一个shuffler我这里就没实现了，否则预测的类别会出现变小甚至报错</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_XgbClassifer</span><span class="params">(train_data,train_target,test_data,feature_names,parameters,early_stopping_rounds,num_folds,eval_metric,model_name=<span class="string">'model'</span>,stratified=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    :param train_data: 一定是numpy</span></span><br><span class="line"><span class="string">    :param train_target:</span></span><br><span class="line"><span class="string">    :param parameters:</span></span><br><span class="line"><span class="string">    :param round:</span></span><br><span class="line"><span class="string">    :param k:</span></span><br><span class="line"><span class="string">    :param eval_metrics:自定义 or 内置字符串</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 如果在param中设置，会莫名报参数不存在的错误</span></span><br><span class="line">    clf=XGBClassifier(num_class=n_class)</span><br><span class="line">    clf.set_params(**parameters)</span><br><span class="line">    <span class="comment"># 定义一些变量</span></span><br><span class="line">    oof_preds = np.zeros((train_data.shape[<span class="number">0</span>],n_class))</span><br><span class="line">    sub_preds = np.zeros((test_data.shape[<span class="number">0</span>],n_class))</span><br><span class="line">    feature_importance_df = pd.DataFrame()</span><br><span class="line">    cv_result = []</span><br><span class="line">    <span class="comment"># K-flod</span></span><br><span class="line">    <span class="keyword">if</span> stratified:</span><br><span class="line">        folds = StratifiedKFold(n_splits= num_folds, shuffle=<span class="literal">True</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        folds = KFold(n_splits= num_folds, shuffle=<span class="literal">True</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">    <span class="keyword">for</span> n_flod,(train_index, val_index) <span class="keyword">in</span> enumerate(folds.split(train_data,train_target)):</span><br><span class="line">        train_X=train_data[train_index]</span><br><span class="line">        val_X=train_data[val_index]</span><br><span class="line">        train_Y=train_target[train_index]</span><br><span class="line">        val_Y=train_target[val_index]</span><br><span class="line">        <span class="comment"># 参数初步定之后划分20%为验证集，准备一个watchlist 给train和validation set ,设置num_round 足够大（比如100000），以至于你能发现每一个round 的验证集预测结果，</span></span><br><span class="line">        <span class="comment"># 如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了。</span></span><br><span class="line">        watchlist= [(train_X, train_Y)]</span><br><span class="line">        <span class="comment"># early_stop 看validate的eval是否下降，这时候必须传eval_set,并取eval_set的最后一个作为validate</span></span><br><span class="line">        clf.fit(train_X,train_Y,early_stopping_rounds=early_stopping_rounds, eval_set=watchlist,eval_metric=eval_metric)</span><br><span class="line">        <span class="comment"># 获得每次的预测值补充</span></span><br><span class="line">        oof_preds[val_index]=clf.predict_proba(val_X)</span><br><span class="line">        <span class="comment"># 获得预测的平均值，这里直接加完再除m</span></span><br><span class="line">        sub_preds+= clf.predict_proba(test_data)</span><br><span class="line">        <span class="comment"># 计算当前准确率</span></span><br><span class="line">        result=mean_absolute_error(val_Y,clf.predict(val_X))</span><br><span class="line">        print(<span class="string">'Fold %2d macro-f1 : %.6f'</span> % (n_flod + <span class="number">1</span>, result))</span><br><span class="line">        print(type(result))</span><br><span class="line">        cv_result.append(round(result,<span class="number">5</span>))</span><br><span class="line">        gc.collect()</span><br><span class="line">        <span class="comment"># 默认就是gain 如果要修改要再参数定义中修改importance_type</span></span><br><span class="line">        <span class="comment"># 保存特征重要度</span></span><br><span class="line">        gain = clf.feature_importances_</span><br><span class="line">        fold_importance_df = pd.DataFrame(&#123;<span class="string">'feature'</span>:feature_names,</span><br><span class="line">                                           <span class="string">'gain'</span>:<span class="number">100</span>*gain/gain.sum(),</span><br><span class="line">                                           <span class="string">'fold'</span>:n_flod,</span><br><span class="line">                                           &#125;).sort_values(<span class="string">'gain'</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 进行保存</span></span><br><span class="line">    sub_preds=sub_preds/folds.n_splits</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./cv'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./cv'</span>)</span><br><span class="line">    pd.DataFrame(oof_preds,columns=[<span class="string">'class_'</span>+ str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_class)]).to_csv(<span class="string">'./cv/val_prob_&#123;&#125;.csv'</span>.format(model_name), index= <span class="literal">False</span>, float_format = <span class="string">'%.4f'</span>)</span><br><span class="line">    pd.DataFrame(sub_preds, columns=[<span class="string">'class_'</span> + str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_class)]).to_csv(<span class="string">'./cv/test_prob_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>, float_format=<span class="string">'%.4f'</span>)</span><br><span class="line">    oof_preds = [np.argmax(x) <span class="keyword">for</span> x <span class="keyword">in</span> oof_preds]</span><br><span class="line">    sub_preds = [np.argmax(x) <span class="keyword">for</span> x <span class="keyword">in</span> sub_preds]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./sub'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./sub'</span>)</span><br><span class="line">    pd.DataFrame(oof_preds,columns=[<span class="string">'class'</span>]).to_csv(<span class="string">'./sub/val_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>)</span><br><span class="line">    pd.DataFrame(sub_preds, columns=[<span class="string">'class'</span>]).to_csv(<span class="string">'./sub/test_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    save_importances(feature_importance_df, model_name)</span><br><span class="line">    <span class="keyword">return</span> clf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_importances</span><span class="params">(feature_importance_df,model_name)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./feature_importance'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./feature_importance'</span>)</span><br><span class="line">    ft = feature_importance_df[[<span class="string">"feature"</span>, <span class="string">"gain"</span>]].groupby(<span class="string">"feature"</span>).mean().sort_values(by=<span class="string">"gain"</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">    ft.to_csv(<span class="string">'./feature_importance/importance_lightgbm_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="lgb回归"><a href="#lgb回归" class="headerlink" title="lgb回归"></a>lgb回归</h3><p>注意Lgb对分类变量会有特殊的支持</p><p>只用看参数和pythonAPI</p><p><a href="http://lightgbm.apachecn.org/" target="_blank" rel="noopener">http://lightgbm.apachecn.org/</a></p><p><a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters" target="_blank" rel="noopener">https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">param = &#123;</span><br><span class="line">    <span class="string">'bagging_freq'</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">'bagging_fraction'</span>: <span class="number">0.335</span>,</span><br><span class="line">    <span class="string">'boost_from_average'</span>:<span class="string">'false'</span>,</span><br><span class="line">    <span class="string">'boost'</span>: <span class="string">'gbdt'</span>,</span><br><span class="line">    <span class="string">'feature_fraction'</span>: <span class="number">0.041</span>,</span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">0.0083</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">-1</span>,</span><br><span class="line">    <span class="string">'metric'</span>:<span class="string">'auc'</span>,</span><br><span class="line">    <span class="string">'min_data_in_leaf'</span>: <span class="number">80</span>,</span><br><span class="line">    <span class="string">'min_sum_hessian_in_leaf'</span>: <span class="number">10.0</span>,</span><br><span class="line">    <span class="string">'num_leaves'</span>: <span class="number">13</span>,</span><br><span class="line">    <span class="string">'num_threads'</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">'tree_learner'</span>: <span class="string">'serial'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'binary'</span>, </span><br><span class="line">    <span class="string">'verbosity'</span>: <span class="number">-1</span></span><br><span class="line">&#125;</span><br><span class="line">num_folds = <span class="number">11</span></span><br><span class="line">features = [c <span class="keyword">for</span> c <span class="keyword">in</span> train.columns <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'ID_code'</span>, <span class="string">'target'</span>]]</span><br><span class="line">folds = KFold(n_splits=num_folds, random_state=<span class="number">2319</span>)</span><br><span class="line">oof = np.zeros(len(train))</span><br><span class="line">getVal = np.zeros(len(train))</span><br><span class="line">predictions = np.zeros(len(target))</span><br><span class="line">feature_importance_df = pd.DataFrame()</span><br><span class="line">print(<span class="string">'Light GBM Model'</span>)</span><br><span class="line"><span class="keyword">for</span> fold_, (trn_idx, val_idx) <span class="keyword">in</span> enumerate(folds.split(train.values, target.values)): </span><br><span class="line">    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]</span><br><span class="line">    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]</span><br><span class="line">    print(<span class="string">"Fold idx:&#123;&#125;"</span>.format(fold_ + <span class="number">1</span>))</span><br><span class="line">    trn_data = lgb.Dataset(X_train, label=y_train)</span><br><span class="line">    val_data = lgb.Dataset(X_valid, label=y_valid)</span><br><span class="line">    clf = lgb.train(param, trn_data, <span class="number">1000000</span>, valid_sets = [trn_data, val_data], verbose_eval=<span class="number">5000</span>, early_stopping_rounds = <span class="number">4000</span>)</span><br><span class="line">    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)</span><br><span class="line">    fold_importance_df = pd.DataFrame()</span><br><span class="line">    fold_importance_df[<span class="string">"feature"</span>] = features</span><br><span class="line">    fold_importance_df[<span class="string">"importance"</span>] = clf.feature_importance()</span><br><span class="line">    fold_importance_df[<span class="string">"fold"</span>] = fold_ + <span class="number">1</span></span><br><span class="line">    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=<span class="number">0</span>)</span><br><span class="line">    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits</span><br><span class="line">print(<span class="string">"CV score: &#123;:&lt;8.5f&#125;"</span>.format(roc_auc_score(target, oof)))</span><br></pre></td></tr></table></figure><h3 id="lgb分类"><a href="#lgb分类" class="headerlink" title="lgb分类"></a>lgb分类</h3><p>回归和分类一致，只是参数不一样而已</p><p>注意Lgb对分类变量会有特殊的支持</p><p>只用看参数和pythonAPI</p><p><a href="http://lightgbm.apachecn.org/" target="_blank" rel="noopener">http://lightgbm.apachecn.org/</a></p><p><a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters" target="_blank" rel="noopener">https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters</a></p><h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><p>比较常用的手工概率blending</p><p><a href="https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899" target="_blank" rel="noopener">https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899</a></p><h2 id="调整参数"><a href="#调整参数" class="headerlink" title="调整参数"></a>调整参数</h2><h3 id="贝叶斯优化"><a href="#贝叶斯优化" class="headerlink" title="贝叶斯优化"></a>贝叶斯优化</h3><p>来源:<a href="https://www.cnblogs.com/yangruiGB2312/p/9374377.html" target="_blank" rel="noopener">https://www.cnblogs.com/yangruiGB2312/p/9374377.html</a></p><p>可以说是目前最好的调参的方法</p><ul><li><p>贝叶斯调参采用高斯过程，<strong>考虑之前的参数信息</strong>，不断地更新先验；网格搜索未考虑之前的参数信息</p></li><li><p>贝叶斯调参<strong>迭代次数少，速度快</strong>；网格搜索速度慢,参数多时易导致维度爆炸</p></li><li><p>贝叶斯调参针对非凸问题依然<strong>稳健</strong>；网格搜索针对非凸问题易得到局部优最</p></li></ul><p>​    公式推导就不在这里展开，主要阐述主要思想：给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程<strong>,直到后验分布基本贴合于真实分布。简单的说，就是</strong>考虑了上一次参数的信息，从而更好的调整当前的参数。</p><p>​    假设这个寻找最优化参数的过程是一个高斯过程。高斯过程有个特点，就是当随机遍历一定的数据点并拿到结果之后，可以大致绘制出整个数据的分布曲线，当然这是也会有一定误差。 </p><p><img src="https://img-blog.csdn.net/20180821135039882?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dzaGVuZ29k/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt></p><p>python代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rf_cv</span><span class="params">(n_estimators, min_samples_split, max_features, max_depth)</span>:</span></span><br><span class="line">    val = cross_val_score(</span><br><span class="line">        RandomForestClassifier(n_estimators=int(n_estimators),</span><br><span class="line">            min_samples_split=int(min_samples_split),</span><br><span class="line">            max_features=min(max_features, <span class="number">0.999</span>), <span class="comment"># float</span></span><br><span class="line">            max_depth=int(max_depth),</span><br><span class="line">            random_state=<span class="number">2</span></span><br><span class="line">        ),</span><br><span class="line">        x, y, scoring=<span class="string">'roc_auc'</span>, cv=<span class="number">5</span></span><br><span class="line">    ).mean()</span><br><span class="line">    <span class="keyword">return</span> val</span><br><span class="line"><span class="comment"># 注意参数名字要对应</span></span><br><span class="line">rf_bo = BayesianOptimization(</span><br><span class="line">        rf_cv,</span><br><span class="line">        &#123;<span class="string">'n_estimators'</span>: (<span class="number">10</span>, <span class="number">250</span>),</span><br><span class="line">        <span class="string">'min_samples_split'</span>: (<span class="number">2</span>, <span class="number">25</span>),</span><br><span class="line">        <span class="string">'max_features'</span>: (<span class="number">0.1</span>, <span class="number">0.999</span>),</span><br><span class="line">        <span class="string">'max_depth'</span>: (<span class="number">5</span>, <span class="number">15</span>)&#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>可以执行的操作</p><ul><li><p>以查看当前最优的参数和结果(同时，我们还可以修改高斯过程的参数，高斯过程主要参数是核函数(<code>kernel</code>)，还有其他参数可以参考<a href="http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html" target="_blank" rel="noopener">sklearn.gaussianprocess</a>)：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gp_param=&#123;<span class="string">'kernel'</span>:None&#125;</span><br><span class="line">rf_bo.maximize(**gp_param)</span><br><span class="line">rf_bo<span class="selector-class">.res</span>[<span class="string">'max'</span>]</span><br></pre></td></tr></table></figure></li><li><p>上面bayes算法得到的参数并不一定最优，当然我们会遇到一种情况，就是我们已经知道有一组或是几组参数是非常好的了，我们想知道其附近有没有更好的。</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rf_bo.explore(</span><br><span class="line">    &#123;<span class="string">'n_estimators'</span>: [<span class="number">10</span>, <span class="number">100</span>, <span class="number">200</span>],</span><br><span class="line">        <span class="string">'min_samples_split'</span>: [<span class="number">2</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">        <span class="string">'max_features'</span>: [<span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.9</span>],</span><br><span class="line">        <span class="string">'max_depth'</span>: [<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li></ul><h2 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛通用模型代码&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型" scheme="http://kodgv.xyz/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Vector Representation</title>
    <link href="http://kodgv.xyz/2019/04/09/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/representation/"/>
    <id>http://kodgv.xyz/2019/04/09/竞赛经验/representation/</id>
    <published>2019-04-09T12:10:18.000Z</published>
    <updated>2019-04-13T11:44:04.024Z</updated>
    
    <content type="html"><![CDATA[<p>​    自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 <code>Id537</code>，“dog”可能表示为 <code>Id143</code>。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。</p><a id="more"></a><p>来源：<a href="https://www.tensorflow.org/tutorials/representation/word2vec" target="_blank" rel="noopener">https://www.tensorflow.org/tutorials/representation/word2vec</a></p><p>​    自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 <code>Id537</code>，“dog”可能表示为 <code>Id143</code>。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。</p><p>​    <a href="https://en.wikipedia.org/wiki/Vector_space_model" target="_blank" rel="noopener">向量空间模型</a> (VSM) 在连续向量空间中表示（嵌入）字词，其中语义相似的字词会映射到附近的点（“在彼此附近嵌入”）。VSM 在 NLP 方面有着悠久而丰富的历史，但所有方法均以某种方式依赖于<a href="https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis" target="_blank" rel="noopener">分布假设</a>，这种假设指明在相同上下文中显示的字词语义相同。利用该原则的不同方法可分为两类：基于计数的方法（例如<a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" target="_blank" rel="noopener">潜在语义分析</a>）以及预测方法（例如<a href="http://www.scholarpedia.org/article/Neural_net_language_models" target="_blank" rel="noopener">神经概率语言模型</a>）。</p><p>​    Word2vec 是一种计算效率特别高的预测模型，用于学习原始文本中的字词嵌入。它分为两种类型：连续词袋模型 (CBOW) 和 Skip-Gram 模型。数据量少的时候使用CBOW，数据量大的时候Skip-gram，一般都用Skip-gram。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">https:</span>//www.tensorflow<span class="meta">.org</span>/tutorials/representation/word2vec</span><br></pre></td></tr></table></figure><h3 id="python"><a href="#python" class="headerlink" title="python"></a>python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.corpora <span class="keyword">import</span> WikiCorpus</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> LineSentence</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">L = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">path = <span class="string">'./'</span></span><br><span class="line">save_path = path + <span class="string">'/w2v'</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(save_path):</span><br><span class="line">    print(save_path)</span><br><span class="line">    os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">train1 = pd.read_csv(path + <span class="string">'/train.csv'</span>)</span><br><span class="line">train = pd.read_csv(path + <span class="string">'/train_old.csv'</span>)</span><br><span class="line">test = pd.read_csv(path + <span class="string">'/test.csv'</span>)</span><br><span class="line"></span><br><span class="line">data = pd.concat([train, test, train1]).reset_index(drop=<span class="literal">True</span>).sample(frac=<span class="number">1</span>, random_state=<span class="number">2018</span>).fillna(<span class="number">0</span>)</span><br><span class="line">data = data.replace(<span class="string">'\\N'</span>, <span class="number">999</span>)</span><br><span class="line">sentence = []</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> list(data[[<span class="string">'1_total_fee'</span>, <span class="string">'2_total_fee'</span>, <span class="string">'3_total_fee'</span>, <span class="string">'4_total_fee'</span>]].values):</span><br><span class="line">    sentence.append([str(float(l)) <span class="keyword">for</span> idx, l <span class="keyword">in</span> enumerate(line)])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'training...'</span>)</span><br><span class="line">model = Word2Vec(sentence, size=L, window=<span class="number">2</span>, min_count=<span class="number">1</span>, workers=multiprocessing.cpu_count(),</span><br><span class="line">                 iter=<span class="number">10</span>)</span><br><span class="line">print(<span class="string">'outputing...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> fea <span class="keyword">in</span> [<span class="string">'1_total_fee'</span>, <span class="string">'2_total_fee'</span>, <span class="string">'3_total_fee'</span>, <span class="string">'4_total_fee'</span>]:</span><br><span class="line">    values = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> list(data[fea].values):</span><br><span class="line">        values.append(line)</span><br><span class="line">    values = set(values)</span><br><span class="line">    print(len(values))</span><br><span class="line">    w2v = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> values:</span><br><span class="line">        a = [i]</span><br><span class="line">        a.extend(model[str(float(i))])</span><br><span class="line">        w2v.append(a)</span><br><span class="line">    out_df = pd.DataFrame(w2v)</span><br><span class="line"></span><br><span class="line">    name = [fea]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">        name.append(name[<span class="number">0</span>] + <span class="string">'W'</span> + str(i))</span><br><span class="line">    out_df.columns = name</span><br><span class="line">    out_df.to_csv(save_path + <span class="string">'/'</span> + fea + <span class="string">'.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="TSNE降维可视化"><a href="#TSNE降维可视化" class="headerlink" title="TSNE降维可视化"></a>TSNE降维可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_with_labels</span><span class="params">(low_dim_embs, labels, filename = <span class="string">'tsne.png'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>] &gt;= len(labels), <span class="string">"More labels than embeddings"</span></span><br><span class="line">    plt.figure(figsize= (<span class="number">10</span>, <span class="number">18</span>))</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">        x, y = low_dim_embs[i, :]</span><br><span class="line">        plt.scatter(x, y)</span><br><span class="line">        plt.annotate(label, xy = (x, y), textcoords = <span class="string">'offset points'</span>, ha = <span class="string">'right'</span>, va = <span class="string">'bottom'</span>)</span><br><span class="line">    plt.savefig(filename)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polt_tnse</span><span class="params">(df,df_target,plot_only=<span class="number">300</span>)</span>:</span></span><br><span class="line">    df_target=list(df_target.astype(<span class="string">'str'</span>))</span><br><span class="line">    tsne = TSNE(perplexity = <span class="number">30</span>, n_components = <span class="number">2</span>, init = <span class="string">'pca'</span>, n_iter = <span class="number">5000</span>)</span><br><span class="line">    low_dim_embs = tsne.fit_transform(df.iloc[:plot_only][:])</span><br><span class="line">    labels = [df_target[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(plot_only)]</span><br><span class="line">    plot_with_labels(low_dim_embs, labels)</span><br></pre></td></tr></table></figure><p><img src="https://www.tensorflow.org/images/tsne.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 &lt;code&gt;Id537&lt;/code&gt;，“dog”可能表示为 &lt;code&gt;Id143&lt;/code&gt;。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="预处理" scheme="http://kodgv.xyz/tags/%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="分类变量" scheme="http://kodgv.xyz/tags/%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>特征选择</title>
    <link href="http://kodgv.xyz/2019/04/09/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    <id>http://kodgv.xyz/2019/04/09/数据竞赛/特征选择/</id>
    <published>2019-04-09T11:52:45.000Z</published>
    <updated>2019-04-12T08:29:21.212Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p>如何进行特征选择</p><a id="more"></a><h2 id="feature-importace"><a href="#feature-importace" class="headerlink" title="feature importace"></a>feature importace</h2><h3 id="特征重要度高"><a href="#特征重要度高" class="headerlink" title="特征重要度高"></a>特征重要度高</h3><p>如果发现特征重要度有明显很高的情况下，可以将这个特征去除再尝试，因为有可能模型过于依赖此特征而导致了过拟合的存在。比如下图中的addr_state</p><p><img src="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/_images/gbm_variable_importance1.png" alt></p><h3 id="特征重要度低"><a href="#特征重要度低" class="headerlink" title="特征重要度低"></a>特征重要度低</h3><p>来源：<a href="https://www.kaggle.com/ogrellier/feature-selection-with-null-importances" target="_blank" rel="noopener">https://www.kaggle.com/ogrellier/feature-selection-with-null-importances</a></p><p>来源：<a href="https://academic.oup.com/bioinformatics/article/26/10/1340/193348" target="_blank" rel="noopener">https://academic.oup.com/bioinformatics/article/26/10/1340/193348</a></p><p>​    传统做法就是根据feature importance然后就跑一个阈值，剔除低于阈值的feature。</p><p>剔除阈值的目的在于：</p><ul><li><p>消除高相关的feature</p></li><li><p>提高model的variance</p></li></ul><p>​    <strong>最好最后的分数剔除分类变量</strong>，因为它主要是平衡分类变量的bias</p><p>​    论文提出的做法是，对于树模型，它的特征重要度会对数量多的分类变量有bias，所以提出了一种基于null importance 的做法。即对target打乱s次，预测s次的特征重要度，然后再利用distribution进行分析，从而得到比较标注的特征重要度。主要的做法请看kaggle上的示例：</p><p>获取重要度，此处需要自定义一个训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shuffle决定是否打乱y值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_feature_importances</span><span class="params">(data, shuffle, seed=None)</span>:</span></span><br><span class="line">    <span class="comment"># Gather real features</span></span><br><span class="line">    train_features = [f <span class="keyword">for</span> f <span class="keyword">in</span> data <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'TARGET'</span>, <span class="string">'SK_ID_CURR'</span>]]</span><br><span class="line">    <span class="comment"># Go over fold and keep track of CV score (train and valid) and feature importances</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle target if required</span></span><br><span class="line">    y = data[<span class="string">'TARGET'</span>].copy()</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># Here you could as well use a binomial distribution</span></span><br><span class="line">        y = data[<span class="string">'TARGET'</span>].copy().sample(frac=<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest</span></span><br><span class="line">    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=<span class="literal">False</span>, silent=<span class="literal">True</span>)</span><br><span class="line">    lgb_params = &#123;</span><br><span class="line">        <span class="string">'objective'</span>: <span class="string">'binary'</span>,</span><br><span class="line">        <span class="string">'boosting_type'</span>: <span class="string">'rf'</span>,</span><br><span class="line">        <span class="string">'subsample'</span>: <span class="number">0.623</span>,</span><br><span class="line">        <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,</span><br><span class="line">        <span class="string">'num_leaves'</span>: <span class="number">127</span>,</span><br><span class="line">        <span class="string">'max_depth'</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">'seed'</span>: seed,</span><br><span class="line">        <span class="string">'bagging_freq'</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">'n_jobs'</span>: <span class="number">4</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fit the model</span></span><br><span class="line">    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=<span class="number">200</span>, categorical_feature=categorical_feats)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get feature importances</span></span><br><span class="line">    imp_df = pd.DataFrame()</span><br><span class="line">    imp_df[<span class="string">"feature"</span>] = list(train_features)</span><br><span class="line">    imp_df[<span class="string">"importance_gain"</span>] = clf.feature_importance(importance_type=<span class="string">'gain'</span>)</span><br><span class="line">    imp_df[<span class="string">"importance_split"</span>] = clf.feature_importance(importance_type=<span class="string">'split'</span>)</span><br><span class="line">    imp_df[<span class="string">'trn_score'</span>] = roc_auc_score(y, clf.predict(data[train_features]))</span><br><span class="line">    <span class="keyword">return</span> imp_df</span><br></pre></td></tr></table></figure><p>跑n轮得到Null importance </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">null_imp_df = pd.DataFrame()</span><br><span class="line">nb_runs = <span class="number">80</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start = time.time()</span><br><span class="line">dsp = <span class="string">''</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_runs):</span><br><span class="line">    <span class="comment"># Get current run importances</span></span><br><span class="line">    imp_df = get_feature_importances(data=data, shuffle=<span class="literal">True</span>)</span><br><span class="line">    imp_df[<span class="string">'run'</span>] = i + <span class="number">1</span> </span><br><span class="line">    <span class="comment"># Concat the latest importances with the old ones</span></span><br><span class="line">    null_imp_df = pd.concat([null_imp_df, imp_df], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>然后画分布图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_distributions</span><span class="params">(actual_imp_df_, null_imp_df_, feature_)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">13</span>, <span class="number">6</span>))</span><br><span class="line">    gs = gridspec.GridSpec(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># Plot Split importances</span></span><br><span class="line">    ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">    a = ax.hist(null_imp_df_.loc[null_imp_df_[<span class="string">'feature'</span>] == feature_, <span class="string">'importance_split'</span>].values, label=<span class="string">'Null importances'</span>)</span><br><span class="line">    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_[<span class="string">'feature'</span>] == feature_, <span class="string">'importance_split'</span>].mean(), </span><br><span class="line">               ymin=<span class="number">0</span>, ymax=np.max(a[<span class="number">0</span>]), color=<span class="string">'r'</span>,linewidth=<span class="number">10</span>, label=<span class="string">'Real Target'</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_title(<span class="string">'Split Importance of %s'</span> % feature_.upper(), fontweight=<span class="string">'bold'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Null Importance (split) Distribution for %s '</span> % feature_.upper())</span><br><span class="line">    <span class="comment"># Plot Gain importances</span></span><br><span class="line">    ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    a = ax.hist(null_imp_df_.loc[null_imp_df_[<span class="string">'feature'</span>] == feature_, <span class="string">'importance_gain'</span>].values, label=<span class="string">'Null importances'</span>)</span><br><span class="line">    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_[<span class="string">'feature'</span>] == feature_, <span class="string">'importance_gain'</span>].mean(), </span><br><span class="line">               ymin=<span class="number">0</span>, ymax=np.max(a[<span class="number">0</span>]), color=<span class="string">'r'</span>,linewidth=<span class="number">10</span>, label=<span class="string">'Real Target'</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_title(<span class="string">'Gain Importance of %s'</span> % feature_.upper(), fontweight=<span class="string">'bold'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Null Importance (gain) Distribution for %s '</span> % feature_.upper())</span><br><span class="line">display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_=<span class="string">'LIVINGAPARTMENTS_AVG'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2019040923253844.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NjYxMjMw,size_16,color_FFFFFF,t_70" alt></p><p><img src="https://img-blog.csdnimg.cn/20190409232626147.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NjYxMjMw,size_16,color_FFFFFF,t_70" alt></p><p>图主要展示了什么叫做好的分布:方差小的。同时远离真实的重要度（红线）</p><p>我们想要的是在与真实目标(即实际重要性)使用时具有很高重要性的特性，而在与噪声训练时得分不高(即null importance得分)。在零假设和正态分布下，如果一个特征的红色实际重要性在蓝色分布内，那么该特征与目标不相关的可能性就很大。如果它在蓝色分布的5%范围内或者在外面，那么它就是相关的。</p><p>然后下一步要做的就是（去掉与target不相关的，去掉自己高相关的）：</p><ul><li>Drop high variance features if they are not really related to the target</li><li>Remove the decaying factor on correlated features, showing their real importance (or unbiased importance)</li></ul><p>结合一些公式，然后进行挑选</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">correlation_scores = []</span><br><span class="line"><span class="keyword">for</span> _f <span class="keyword">in</span> actual_imp_df[<span class="string">'feature'</span>].unique():</span><br><span class="line">    f_null_imps = null_imp_df.loc[null_imp_df[<span class="string">'feature'</span>] == _f, <span class="string">'importance_gain'</span>].values</span><br><span class="line">    f_act_imps = actual_imp_df.loc[actual_imp_df[<span class="string">'feature'</span>] == _f, <span class="string">'importance_gain'</span>].values</span><br><span class="line">    gain_score = <span class="number">100</span> * (f_null_imps &lt; np.percentile(f_act_imps, <span class="number">25</span>)).sum() / f_null_imps.size</span><br><span class="line">    f_null_imps = null_imp_df.loc[null_imp_df[<span class="string">'feature'</span>] == _f, <span class="string">'importance_split'</span>].values</span><br><span class="line">    f_act_imps = actual_imp_df.loc[actual_imp_df[<span class="string">'feature'</span>] == _f, <span class="string">'importance_split'</span>].values</span><br><span class="line">    split_score = <span class="number">100</span> * (f_null_imps &lt; np.percentile(f_act_imps, <span class="number">25</span>)).sum() / f_null_imps.size</span><br><span class="line">    correlation_scores.append((_f, split_score, gain_score))</span><br><span class="line"></span><br><span class="line">corr_scores_df = pd.DataFrame(correlation_scores, columns=[<span class="string">'feature'</span>, <span class="string">'split_score'</span>, <span class="string">'gain_score'</span>])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">score_feature_selection</span><span class="params">(df=None, train_features=None, cat_feats=None, target=None)</span>:</span></span><br><span class="line">    <span class="comment"># Fit LightGBM </span></span><br><span class="line">    dtrain = lgb.Dataset(df[train_features], target, free_raw_data=<span class="literal">False</span>, silent=<span class="literal">True</span>)</span><br><span class="line">    lgb_params = &#123;</span><br><span class="line">        <span class="string">'objective'</span>: <span class="string">'binary'</span>,</span><br><span class="line">        <span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,</span><br><span class="line">        <span class="string">'learning_rate'</span>: <span class="number">.1</span>,</span><br><span class="line">        <span class="string">'subsample'</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">'num_leaves'</span>: <span class="number">31</span>,</span><br><span class="line">        <span class="string">'max_depth'</span>: <span class="number">-1</span>,</span><br><span class="line">        <span class="string">'seed'</span>: <span class="number">13</span>,</span><br><span class="line">        <span class="string">'n_jobs'</span>: <span class="number">4</span>,</span><br><span class="line">        <span class="string">'min_split_gain'</span>: <span class="number">.00001</span>,</span><br><span class="line">        <span class="string">'reg_alpha'</span>: <span class="number">.00001</span>,</span><br><span class="line">        <span class="string">'reg_lambda'</span>: <span class="number">.00001</span>,</span><br><span class="line">        <span class="string">'metric'</span>: <span class="string">'auc'</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fit the model</span></span><br><span class="line">    hist = lgb.cv(</span><br><span class="line">        params=lgb_params, </span><br><span class="line">        train_set=dtrain, </span><br><span class="line">        num_boost_round=<span class="number">2000</span>,</span><br><span class="line">        categorical_feature=cat_feats,</span><br><span class="line">        nfold=<span class="number">5</span>,</span><br><span class="line">        stratified=<span class="literal">True</span>,</span><br><span class="line">        shuffle=<span class="literal">True</span>,</span><br><span class="line">        early_stopping_rounds=<span class="number">50</span>,</span><br><span class="line">        verbose_eval=<span class="number">0</span>,</span><br><span class="line">        seed=<span class="number">17</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># Return the last mean / std values </span></span><br><span class="line">    <span class="keyword">return</span> hist[<span class="string">'auc-mean'</span>][<span class="number">-1</span>], hist[<span class="string">'auc-stdv'</span>][<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># features = [f for f in data.columns if f not in ['SK_ID_CURR', 'TARGET']]</span></span><br><span class="line"><span class="comment"># score_feature_selection(df=data[features], train_features=features, target=data['TARGET'])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> threshold <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span> , <span class="number">40</span>, <span class="number">50</span> ,<span class="number">60</span> , <span class="number">70</span>, <span class="number">80</span> , <span class="number">90</span>, <span class="number">95</span>, <span class="number">99</span>]:</span><br><span class="line">    split_feats = [_f <span class="keyword">for</span> _f, _score, _ <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> _score &gt;= threshold]</span><br><span class="line">    split_cat_feats = [_f <span class="keyword">for</span> _f, _score, _ <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> (_score &gt;= threshold) &amp; (_f <span class="keyword">in</span> categorical_feats)]</span><br><span class="line">    gain_feats = [_f <span class="keyword">for</span> _f, _, _score <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> _score &gt;= threshold]</span><br><span class="line">    gain_cat_feats = [_f <span class="keyword">for</span> _f, _, _score <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> (_score &gt;= threshold) &amp; (_f <span class="keyword">in</span> categorical_feats)]</span><br><span class="line">                                                                                             </span><br><span class="line">    print(<span class="string">'Results for threshold %3d'</span> % threshold)</span><br><span class="line">    split_results = score_feature_selection(df=data, train_features=split_feats, cat_feats=split_cat_feats, target=data[<span class="string">'TARGET'</span>])</span><br><span class="line">    print(<span class="string">'\t SPLIT : %.6f +/- %.6f'</span> % (split_results[<span class="number">0</span>], split_results[<span class="number">1</span>]))</span><br><span class="line">    gain_results = score_feature_selection(df=data, train_features=gain_feats, cat_feats=gain_cat_feats, target=data[<span class="string">'TARGET'</span>])</span><br><span class="line">    print(<span class="string">'\t GAIN  : %.6f +/- %.6f'</span> % (gain_results[<span class="number">0</span>], gain_results[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><h3 id="特征相关性低"><a href="#特征相关性低" class="headerlink" title="特征相关性低"></a>特征相关性低</h3><p>如果特征之间的相关性很低的情况下，可以进一步检测变量之间的独立性，如果变量之间是independent意味着可以单独将这些特征进行多个模型训练再融合</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;如何进行特征选择&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
</feed>
