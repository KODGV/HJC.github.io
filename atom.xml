<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小黑屋</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://kodgv.xyz/"/>
  <updated>2019-05-16T13:14:54.995Z</updated>
  <id>http://kodgv.xyz/</id>
  
  <author>
    <name>KODGV</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux</title>
    <link href="http://kodgv.xyz/2019/05/16/Linux/"/>
    <id>http://kodgv.xyz/2019/05/16/Linux/</id>
    <published>2019-05-16T12:54:54.000Z</published>
    <updated>2019-05-16T13:14:54.995Z</updated>
    
    <content type="html"><![CDATA[<p>Linux学习</p><a id="more"></a><h3 id="查询命令帮助"><a href="#查询命令帮助" class="headerlink" title="查询命令帮助"></a>查询命令帮助</h3><blockquote><p>man 命令<br>man page</p></blockquote><div class="table-container"><table><thead><tr><th>代号</th><th>内容说明</th></tr></thead><tbody><tr><td>NAME</td><td>简短的指令、数据名称说明</td></tr><tr><td>SYNOPSIS</td><td>简短的指令下达语法（syntax）简介</td></tr><tr><td>DESCRIPTION</td><td>较为完整的说明，这部分最好仔细看看！</td></tr><tr><td>OPTIONS</td><td>针对 SYNOPSIS 部分中，有列举的所有可用的选项说明</td></tr><tr><td>COMMANDS</td><td>当这个程序（软件）在执行的时候，可以在此程序（软件）中下达的指令</td></tr><tr><td>FILES</td><td>这个程序或数据所使用或参考或链接到的某些文件</td></tr><tr><td>SEE ALSO</td><td>可以参考的，跟这个指令或数据有相关的其他说明！</td></tr><tr><td>EXAMPLE</td><td>一些可以参考的范例</td></tr></tbody></table></div><blockquote><p>info 命令</p></blockquote><p>​    可读性会强很多，只是可能有些命令查不到，光标移到节点</p><ul><li>回车可进入</li><li>U返回上一节点</li><li>N进入下一节点</li><li>P进入上 一节点</li></ul><h3 id="正确的关机"><a href="#正确的关机" class="headerlink" title="正确的关机"></a>正确的关机</h3><ul><li>who  看谁在线上</li><li>netstat -a 网络连线状态</li><li>ps -aux 查看背景执行程序</li><li>sync 将内存数据写入硬盘（很重要！）</li></ul><h3 id="Linux-快捷键"><a href="#Linux-快捷键" class="headerlink" title="Linux 快捷键"></a>Linux 快捷键</h3><p>编辑器快捷键</p><div class="table-container"><table><thead><tr><th>按键</th><th>进行工作</th></tr></thead><tbody><tr><td>空白键</td><td>向下翻一页</td></tr><tr><td>[Page Down]</td><td>向下翻一页</td></tr><tr><td>[Page Up]</td><td>向上翻一页</td></tr><tr><td>[Home]</td><td>去到第一页</td></tr><tr><td>[End]</td><td>去到最后一页</td></tr><tr><td>/string</td><td>向“下”搜寻 string 这个字串，如果要搜寻 vbird 的话，就输入 /vbird</td></tr><tr><td>?string</td><td>向“上”搜寻 string 这个字串</td></tr><tr><td>n, N</td><td>利用 / 或 ? 来搜寻字串时，可以用 n 来继续下一个搜寻 （不论是 / 或 ?） ，可以利用 N 来进行“反向”搜寻。举例来说，我以 /vbird 搜寻 vbird 字串， 那么可以 n 继续往下查询，用 N 往上查询。若以 ?vbird 向上查询 vbird 字串， 那我可以用 n 继续“向上”查询，用 N 反向查询。</td></tr><tr><td>q</td><td>结束这次的 man page</td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux学习&lt;/p&gt;
    
    </summary>
    
      <category term="linux" scheme="http://kodgv.xyz/categories/linux/"/>
    
    
      <category term="linux" scheme="http://kodgv.xyz/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>查重攻略</title>
    <link href="http://kodgv.xyz/2019/05/02/%E6%AF%95%E4%B8%9A%E9%9C%80%E6%B1%82/%E6%9F%A5%E9%87%8D%E6%94%BB%E7%95%A5/"/>
    <id>http://kodgv.xyz/2019/05/02/毕业需求/查重攻略/</id>
    <published>2019-05-02T11:32:45.000Z</published>
    <updated>2019-05-02T11:36:19.888Z</updated>
    
    <content type="html"><![CDATA[<h2 id="降重技巧"><a href="#降重技巧" class="headerlink" title="降重技巧"></a>降重技巧</h2><ul><li>如果文章字数足够了的话，可以将文字内容转化为图片。目前的查重系统暂时对图片是查不出来的，重复率自然不会高。 </li><li>不要寄希望于标注了参考文献</li><li>主动改被动，被动改主动 </li><li>前后并列的词汇调换</li><li>分析原因一二三四都可以改成1234或①②③④或者直接分出四个段落</li><li>同义近义</li><li>打开谷歌翻译，然后把论文不放心的部分翻译成英文（或日文德文俄文，随你便），再把翻译出来的再复制进去翻译成中文，搞定 <h2 id="查重"><a href="#查重" class="headerlink" title="查重"></a>查重</h2>论文目录检测举个例子正确目录用知网检测系统生成的文本复制比报告单如下图：</li></ul><p><img src="/2019/05/02/毕业需求/查重攻略/1.png" alt></p><p>目录的格式是一个细节问题，微小的错误可能就导致不一样的结果，目录正确知网系统会自动识别目录分章节进行检测，可以得出各个章节的详细重复率；否则目录不完全正确那么知网检测系统不能很好的识别导致不能按照章节检测，就不能得到我们想要的章节重复率结果，甚至知网检测系统会把目录当成正文检测而使得目录全部标红而影响总文字复制比升高。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;降重技巧&quot;&gt;&lt;a href=&quot;#降重技巧&quot; class=&quot;headerlink&quot; title=&quot;降重技巧&quot;&gt;&lt;/a&gt;降重技巧&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;如果文章字数足够了的话，可以将文字内容转化为图片。目前的查重系统暂时对图片是查不出来的，重复率自然不会高。 &lt;/
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>elo比赛心得</title>
    <link href="http://kodgv.xyz/2019/04/30/JAVA%E5%AD%A6%E4%B9%A0/java%E5%8F%82%E8%80%83%E8%B7%AF%E7%BA%BF/"/>
    <id>http://kodgv.xyz/2019/04/30/JAVA学习/java参考路线/</id>
    <published>2019-04-30T11:30:59.000Z</published>
    <updated>2019-05-16T13:13:21.050Z</updated>
    
    <content type="html"><![CDATA[<p>java学习路线与推荐<br><a id="more"></a></p><h2 id="6个月-Java-服务端入门和进阶指南"><a href="#6个月-Java-服务端入门和进阶指南" class="headerlink" title="6个月 Java 服务端入门和进阶指南"></a>6个月 Java 服务端入门和进阶指南</h2><p><a href="https://www.zhihu.com/question/29581524" target="_blank" rel="noopener">https://www.zhihu.com/question/29581524</a><br><img src="/2019/04/30/JAVA学习/java参考路线/1" alt="1557919436614"></p><p><img src="/2019/04/30/JAVA学习/java参考路线/2" alt="1557919469076"></p><p><img src="/2019/04/30/JAVA学习/java参考路线/3" alt="1557919495443"></p><p>资料参考，先学第一阶段，学完再来看第二阶段的任务吧</p><p><a href="https://www.zhihu.com/question/22340525" target="_blank" rel="noopener">https://www.zhihu.com/question/22340525</a></p><p><a href="https://www.zhihu.com/question/307096748/answers/updated" target="_blank" rel="noopener">https://www.zhihu.com/question/307096748/answers/updated</a></p><p><a href="https://mp.weixin.qq.com/s/GxIRqj8s1zrnnPSOJ0203Q" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/GxIRqj8s1zrnnPSOJ0203Q</a></p><p><a href="https://zhuanlan.zhihu.com/p/34880504" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34880504</a></p><p><a href="https://h2pl.github.io/" target="_blank" rel="noopener">后端校招以及大牛成长转折点</a></p><h2 id="自己整理"><a href="#自己整理" class="headerlink" title="自己整理"></a>自己整理</h2><h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3><p><strong>JAVA基础的学习</strong></p><p>《Java 核心技术：卷1 基础知识》(或者《Java 编程思想》)必看</p><p>《Effective Java》</p><p><a href="http://how2j.cn/" target="_blank" rel="noopener">做题网站</a></p><p><strong>代码规范</strong></p><p>《Git 权威指南》 <a href="https://link.zhihu.com/?target=https%3A//learngitbranching.js.org/" target="_blank" rel="noopener">Learn Git Branching通关网站</a></p><p>《Maven 实战》</p><p><a href="https://link.zhihu.com/?target=https%3A//book.douban.com/subject/4262627/" target="_blank" rel="noopener">《重构_改善既有代码的设计》</a></p><p>学习代码规范。我们大致上遵循 oracle 的 Java 语言编码规范，你可以先阅读并熟悉它。Code Formatting 文件在 git@xxx/coding-standard.git，在编写代码之前，请把它导入到 IDE 中。另外，确认 IDE 已经安装 Findbugs 和 CheckStyle 插件。</p><p><strong>开发工具</strong></p><ul><li>熟练使用一种 IDE。Intellij IDEA或者 Eclipse 都可以，推荐使用前者。至少熟悉常用的快捷键，会 debug(包括远程 debug)项目。</li><li>熟悉一种编辑器。比如 Vim/Emacs/Sublime Text，至少学会搜索/替换/代码补全。</li></ul><p><strong>开发环境</strong></p><p>Linux 的基本使用可以通过《鸟哥的Linux私房菜：基础学习篇（第三版）》学习</p><p> bash shell 脚本可以参考《Linux Shell脚本攻略》</p><h2 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h2><p>刷题：</p><p>先从简单的图解算法看起，然后做题的时候不要做太多，做别人整理好的经典题比较重要。</p><p>先servelet和协议。再JAVA进阶，然后框架和数据库并行。</p><p>需要掌握三大框架SSM，需要掌握各类数据库，需要掌握各类协议</p><p>学Unix</p><p><strong>JAVA进阶</strong></p><ul><li><p><strong>《Java并发编程的艺术》《深入理解Java虚拟机》</strong></p></li><li><p>并发编程网：<strong>并发编程网 - ifeve.com</strong> 重点掌握java内存模型，各种锁的原理及应用，JVM GC垃圾回收原理。</p></li></ul><h2 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h2><p>后端校招以及大牛成长转折点</p><p>源码</p><p>微服务</p><p>微架构</p><p>各种组件</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;java学习路线与推荐&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="java" scheme="http://kodgv.xyz/categories/java/"/>
    
    
      <category term="java" scheme="http://kodgv.xyz/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>elo比赛心得</title>
    <link href="http://kodgv.xyz/2019/04/30/elo%E6%AF%94%E8%B5%9B%E5%BF%83%E5%BE%97/"/>
    <id>http://kodgv.xyz/2019/04/30/elo比赛心得/</id>
    <published>2019-04-30T11:30:59.000Z</published>
    <updated>2019-04-30T11:55:32.385Z</updated>
    
    <content type="html"><![CDATA[<h2 id="比赛简介"><a href="#比赛简介" class="headerlink" title="比赛简介"></a>比赛简介</h2><ul><li>回归问题</li><li><p>有异常点</p></li><li><p>商品信息，</p></li><li>每个用户的刷卡时间。</li><li>预测用户的忠诚度。</li></ul><a id="more"></a><p>[TOC]</p><h2 id="11th-place-solution"><a href="#11th-place-solution" class="headerlink" title="11th place solution"></a><a href="https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/82127#latest-502682" target="_blank" rel="noopener">11th place solution</a></h2><p><strong>FEATURE ENGINEERING</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> I refer the Kaggle Rank System Compute Formula（link:[https://www.kaggle.com/progression][<span class="number">4</span>])</span><br><span class="line">    df_data[<span class="string">'duration_sqrt_counts'</span>] = df_data[<span class="string">'durations'</span>]/sqrt(df_data[<span class="string">'card_id_counts'</span>])</span><br><span class="line">    df_data[<span class="string">'duration_log1p_counts'</span>] = df_data[<span class="string">'durations'</span>]/log1p(df_data[<span class="string">'card_id_counts'</span>])</span><br><span class="line">    df_data[<span class="string">'duration_counts'</span>] = df_data[<span class="string">'durations'</span>]/df_data[<span class="string">'card_id_counts'</span>]</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> Categorical features: frequence, Maxfrequence, MaxfrequenceRatio</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> card_id/merchant_id/mechant_category_id/city_id (visit sequence to sequence embedding)</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> purchase_amount:hist/new</span><br><span class="line"></span><br><span class="line"><span class="number">5.</span> features interactions between hist/new</span><br><span class="line">            df[<span class="string">'purchase_amount_ratio_v3'</span>] =                              df[<span class="string">'new_purchase_amount_max'</span>]/df[<span class="string">'hist_purchase_amount_sum'</span>]</span><br><span class="line">            df[<span class="string">'purchase_amount_diff_v1'</span>] = df[<span class="string">'new_purchase_amount_sum'</span>]-df[<span class="string">'hist_purchase_amount_sum'</span>]</span><br><span class="line">            df[<span class="string">'purchase_amount_diff_v2'</span>] = df[<span class="string">'new_purchase_amount_mean'</span>]-df[<span class="string">'hist_purchase_amount_mean'</span>]</span><br><span class="line">            df[<span class="string">'purchase_amount_diff_v3'</span>] = df[<span class="string">'new_purchase_amount_max'</span>]-df[<span class="string">'hist_purchase_amount_max'</span>]</span><br><span class="line">            df[<span class="string">'purchase_amount_diff_v4'</span>] = df[<span class="string">'new_purchase_amount_min'</span>]-df[<span class="string">'hist_purchase_amount_min'</span>]</span><br><span class="line">            df[<span class="string">'pa_mlag_ratio'</span>] = df[<span class="string">'new_purchase_amount_sum'</span>]/(df[<span class="string">'month_lag_mean'</span>] - <span class="number">1</span>)</span><br><span class="line">            df[<span class="string">'pa_new_hist_ratio'</span>] = df[<span class="string">'new_purchase_amount_sum'</span>]/(df[<span class="string">'hist_purchase_amount_sum'</span>])</span><br><span class="line">            df[<span class="string">'pa_new_hist_mean_ratio'</span>] = df[<span class="string">'new_purchase_amount_mean'</span>]/(df[<span class="string">'hist_purchase_amount_mean'</span>] )</span><br><span class="line">            df[<span class="string">'pa_new_hist_min_ratio'</span>] = df[<span class="string">'new_purchase_amount_min'</span>]/(df[<span class="string">'hist_purchase_amount_min'</span>] )</span><br><span class="line">            df[<span class="string">'pa_new_hist_max_ratio'</span>] = df[<span class="string">'new_purchase_amount_max'</span>]/(df[<span class="string">'hist_purchase_amount_max'</span>] )</span><br></pre></td></tr></table></figure><p>我们有两个单独的feature sets。一个具有+1000个feature ，另一个具有+200个feature </p><p>然后，我们取+200特征集的相关矩阵，<strong>将每个特征与其关联最小的特征配对。然后我们对每一对应用了大量的聚合</strong>，结果得到了非常强大的特征。</p><p>所以我们最终得到了两个功能集，每个功能集+1000个功能。”</p><p><strong>STACKING</strong></p><p>We stacked around 32 models using bayesian regression. Our models were well varied that it yielded a score of CV:3.630X LB :3.675</p><p><strong>STUFF THAT DID NOT WORK</strong></p><p>Of course these last two months were not all roses and rainbows. We pulled our hair trying a lot of things and we failed miserably.</p><p>Here are the bloopers of our participation :D :</p><ul><li>NN. We tried designing different architectures with the main focus on having a simple NN with heavy regularization (BatchNorm and Strong Dropout)</li><li>In the middle of the competition, we tried tackling the outliers detection as an anomaly detection problem using AutoEncoders trained only on the non outliers data</li><li>We tried PCA for more features. And it didn’t work</li><li>We tried TSNE. It didn’t work</li><li>We tried FM and FFM. It did not work</li><li>We tried isolation forest. Nope. Did not work.</li><li>We had a Ridge-based pairwise ranker that we intended to use for outliers detection but it didn’t match with the approach we had.</li><li>We tried a lot of weak models in the hope of adding diversity (simple tree-based, linear, svm, etc.). And guess what? It did not work.</li></ul><p>如何识别异常点？</p><p>两个不同的特征集怎么做？</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;比赛简介&quot;&gt;&lt;a href=&quot;#比赛简介&quot; class=&quot;headerlink&quot; title=&quot;比赛简介&quot;&gt;&lt;/a&gt;比赛简介&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;回归问题&lt;/li&gt;
&lt;li&gt;&lt;p&gt;有异常点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;商品信息，&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;每个用户的刷卡时间。&lt;/li&gt;
&lt;li&gt;预测用户的忠诚度。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>java核心技术基础知识</title>
    <link href="http://kodgv.xyz/2019/04/30/JAVA%E5%AD%A6%E4%B9%A0/java%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/"/>
    <id>http://kodgv.xyz/2019/04/30/JAVA学习/java核心技术/</id>
    <published>2019-04-30T11:30:59.000Z</published>
    <updated>2019-05-16T04:15:05.134Z</updated>
    
    <content type="html"><![CDATA[<p>java基础知识<br><a id="more"></a></p><p>[TOC]</p><h2 id="java程序设计环境"><a href="#java程序设计环境" class="headerlink" title="java程序设计环境"></a>java程序设计环境</h2><p><img src="/2019/04/30/JAVA学习/java核心技术/1" alt="1557976698685"></p><ul><li><p>JDK 是 Java Development Kit 的缩写开发人员必须要安装，Java 运行时环境（JRE), 它包含虚拟机 但不包含编译器。</p></li><li><p>Java SE 会大量出现， 相对于 Java EE ( Enterprise Edition) 和 Java ME ( Micro Edition), 它是 Java 的标准版。 </p></li></ul><h2 id="java基本程序设计结构"><a href="#java基本程序设计结构" class="headerlink" title="java基本程序设计结构"></a>java基本程序设计结构</h2><ul><li>类是构建所有 Java 应用程序和 applet 的构建块。Java 应用程序中的全部内容都必须放置在类中。 </li></ul><p><strong>数据类型</strong></p><p>整型</p><p><img src="/2019/04/30/JAVA学习/java核心技术/E:/bolg\source\_posts\JAVA学习\java核心技术\2" alt="1557977861052"></p><p>浮点型</p><p><img src="/2019/04/30/JAVA学习/java核心技术/E:/bolg\source\_posts\JAVA学习\java核心技术\3" alt="1557977876477"></p><p>有三种类型，正无穷大，负无穷大，NAN：</p><ul><li>Double_POSITIVE_INFINITY</li><li>Double.NEGATIVEJNFINITY </li><li>Double.NaN </li></ul><blockquote><p>NAN与任何数字包括NAN都不相等，不能用==Double.NaN,而是要用isNaN()</p></blockquote><p>字符型</p><p><strong>byte</strong></p><p>byte 字节，数据存储容量1byte，byte作为基本数据类型表示的也是一个存储范围上的概念，有别于int、long等专门存数字的类型，这种类型的大小就是1byte,而int是4byte。<br>存数字的话就是1byte=8位，2^8=256 即-128-127。字符的话包括字母和汉字，一个字母是1byte，一个汉字2byte。也就是可以用byte变量去存储一个英文字符，但是却存不下一个中文汉字，因为一个汉字占2byte。</p><p>总结，byte是java中的一个基本数据类型，这个数据类型的长度是1byte，此byte就是彼byte,即是基本数据类型也是存储空间的基本计量单位。</p><p><strong>char</strong></p><p>char是Java中的保留字，与别的语言不同的是，char在Java中是16位的，因为Java用的是Unicode。不过8位的ASCII码包含在Unicode中，是从0~127的。</p><p>Java中使用Unicode的原因是，Java的Applet允许全世界范围内运行，那它就需要一种可以表述人类所有语言的字符编码。Unicode。</p><p>char本质上是一个固定占用两个字节的无符号正整数，这个正整数对应于Unicode编号，用于表示那个Unicode编号对应的字符。</p><p>由于固定占用两个字节，char只能表示Unicode编号在65536以内的字符，而不能表示超出范围的字符。</p><p><strong>Unicode</strong></p><p>需要注意的是，Unicode只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储。</p><p>比如，汉字”严”的unicode是十六进制数4E25，转换成二进制数足足有15位（100111000100101），也就是说这个符号的表示至少需要2个字节。表示其他更大的符号，可能需要3个字节或者4个字节，甚至更多。</p><p>这里就有两个严重的问题，第一个问题是，如何才能区别Unicode和ASCII？计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？第二个问题是，我们已经知道，英文字母只用一个字节表示就够了，如果Unicode统一规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。</p><p>它们造成的结果是：1）出现了Unicode的多种存储方式，也就是说有许多种不同的二进制格式，可以用来表示Unicode。2）Unicode在很长一段时间内无法推广，直到互联网的出现。</p><p><strong>UTF-8</strong></p><p>互联网的普及，强烈要求出现一种统一的编码方式。UTF-8就是在互联网上使用最广的一种Unicode的实现方式。其他实现方式还包括UTF-16（字符用两个字节或四个字节表示）和UTF-32（字符用四个字节表示），不过在互联网上基本不用。重复一遍，这里的关系是，UTF-8是Unicode的实现方式之一。</p><p>以utf8为例，utf8是一个变长编码标准，可以以1~4个字节表示一个字符，而中文占3个字节，ascII字符占1个字节。</p><p>为什么我们在java里面可以用一个char来表示一个中文呢？</p><p>因为java是以unicode作为编码方式的。unicode是一个定长的编码标准，每个字符都是2个字节，也就是1个char类型的空间。</p><p>在编译时会把utf8的中文字符转换成对应的unicode来进行传输运算。</p><p>(<strong>总结</strong>)[<a href="https://www.zhihu.com/question/23374078" target="_blank" rel="noopener">https://www.zhihu.com/question/23374078</a>]</p><ul><li><p>unicode是编码集，UTF8只是实现方式</p></li><li><p>一个char可以储存一个中文字符，因为它是两个byte</p></li><li>UTF8采用8位动长的方式，可以节省内存，区别中英文</li><li>unicode不够的时候，会采取增补代码点，即用2个2位字节来表示增补字符</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;java基础知识&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="java" scheme="http://kodgv.xyz/categories/java/"/>
    
    
      <category term="java" scheme="http://kodgv.xyz/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>比赛常用图</title>
    <link href="http://kodgv.xyz/2019/04/29/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E6%AF%94%E8%B5%9B%E5%B8%B8%E7%94%A8%E5%9B%BE/"/>
    <id>http://kodgv.xyz/2019/04/29/数据竞赛/比赛常用图/</id>
    <published>2019-04-29T08:30:12.934Z</published>
    <updated>2019-04-30T01:29:48.536Z</updated>
    
    <content type="html"><![CDATA[<p>比赛常见的EDA总结</p><a id="more"></a><p><strong>要注意把以后整理函数的时候把图补上</strong></p><p>plt.show()会阻碍当前程序的执行，请再最后执行<br>如果想要不阻碍请执行plt.ion()，但是这样当程序结束时会自动关闭当前图像，所以需要再最后执行plt.ioff()以阻碍图像不被关闭</p><p>[TOC]</p><h2 id="曲线分布图"><a href="#曲线分布图" class="headerlink" title="曲线分布图"></a>曲线分布图</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/gpreda/santander-eda-and-prediction</span></span><br><span class="line">def plot_feature_distribution(df1, df2, label1, label2, features):</span><br><span class="line">    <span class="attr">i</span> = <span class="number">0</span></span><br><span class="line">    sns.set_style('whitegrid')</span><br><span class="line">    plt.figure()</span><br><span class="line">    fig, <span class="attr">ax</span> = plt.subplots(<span class="number">10</span>,<span class="number">10</span>,<span class="attr">figsize=(18,22))</span></span><br><span class="line"></span><br><span class="line">    for feature <span class="keyword">in</span> features:</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        plt.subplot(<span class="number">10</span>,<span class="number">10</span>,i)</span><br><span class="line">        sns.distplot(df1[feature], <span class="attr">hist=False,label=label1)</span></span><br><span class="line">        sns.distplot(df2[feature], <span class="attr">hist=False,label=label2)</span></span><br><span class="line">        plt.xlabel(feature, <span class="attr">fontsize=9)</span></span><br><span class="line">        locs, <span class="attr">labels</span> = plt.xticks()</span><br><span class="line">        plt.tick_params(<span class="attr">axis='x',</span> <span class="attr">which='major',</span> <span class="attr">labelsize=6,</span> <span class="attr">pad=-6)</span></span><br><span class="line">        plt.tick_params(<span class="attr">axis='y',</span> <span class="attr">which='major',</span> <span class="attr">labelsize=6)</span></span><br><span class="line">    plt.show();</span><br><span class="line">    </span><br><span class="line"><span class="attr">t0</span> = train_df.loc[train_df['target'] == <span class="number">0</span>]</span><br><span class="line"><span class="attr">t1</span> = train_df.loc[train_df['target'] == <span class="number">1</span>]</span><br><span class="line"><span class="attr">features</span> = train_df.columns.values[<span class="number">2</span>:<span class="number">102</span>]</span><br><span class="line">plot_feature_distribution(t0, t1, '<span class="number">0</span>', '<span class="number">1</span>', features)</span><br></pre></td></tr></table></figure><p><img src="/2019/04/29/数据竞赛/比赛常用图/2.png" alt></p><h2 id="条形分布图"><a href="#条形分布图" class="headerlink" title="条形分布图"></a>条形分布图</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.<span class="built_in">title</span>(<span class="string">"Distribution of mean values per column in the train and test set"</span>)</span><br><span class="line">sns.distplot(train_df[<span class="built_in">features</span>].<span class="built_in">mean</span>(axis=<span class="number">0</span>),<span class="built_in">color</span>=<span class="string">"magenta"</span>,kde=True,bins=<span class="number">120</span>, <span class="built_in">label</span>='train')</span><br><span class="line">sns.distplot(test_df[<span class="built_in">features</span>].<span class="built_in">mean</span>(axis=<span class="number">0</span>),<span class="built_in">color</span>=<span class="string">"darkblue"</span>, kde=True,bins=<span class="number">120</span>, <span class="built_in">label</span>='test')</span><br><span class="line">plt.<span class="built_in">legend</span>()</span><br><span class="line">plt.<span class="built_in">show</span>()</span><br></pre></td></tr></table></figure><p><img src="/2019/04/29/数据竞赛/比赛常用图/2.png" alt></p><h2 id="条形图"><a href="#条形图" class="headerlink" title="条形图"></a>条形图</h2><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#画条形图</span></span><br><span class="line">sns.barplot<span class="params">(<span class="attr">x</span>='Sex', <span class="attr">y</span>='Survived', <span class="attr">data</span>=train)</span>;<span class="comment"># seaborn 的 barplot() 利用矩阵条的高度反映数值变量的集中趋势，展示的是变量的平均值</span></span><br><span class="line">sns.barplot<span class="params">(<span class="attr">x</span>='Sex', <span class="attr">y</span>='Survived', <span class="attr">hue</span> = 'Pclass', <span class="attr">data</span>=train)</span>;<span class="comment">#加了图例的功能</span></span><br></pre></td></tr></table></figure><h2 id="相关性热度图"><a href="#相关性热度图" class="headerlink" title="　相关性热度图"></a>　相关性热度图</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">colormap</span> = plt.cm.RdBu</span><br><span class="line">plt.figure(<span class="attr">figsize=(14,12))</span></span><br><span class="line">plt.title('Pearson Correlation of Features', <span class="attr">y=1.05,</span> <span class="attr">size=15)</span></span><br><span class="line">sns.heatmap(train.astype(float).corr(),<span class="attr">linewidths=0.1,vmax=1.0,</span> </span><br><span class="line">            <span class="attr">square=True,</span> <span class="attr">cmap=colormap,</span> <span class="attr">linecolor='white',</span> <span class="attr">annot=True)</span></span><br></pre></td></tr></table></figure><h2 id="多变量相关性图"><a href="#多变量相关性图" class="headerlink" title="　多变量相关性图"></a>　多变量相关性图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多变量相关性图 要注意哦这里的变量所有的成对一一对应的</span></span><br><span class="line">g = sns.pairplot(train[[<span class="string">u'Survived'</span>, <span class="string">u'Pclass'</span>, <span class="string">u'Sex'</span>, <span class="string">u'Age'</span>, <span class="string">u'Parch'</span>, <span class="string">u'Fare'</span>, <span class="string">u'Embarked'</span>,</span><br><span class="line">       <span class="string">u'FamilySize'</span>, <span class="string">u'Title'</span>]], hue=<span class="string">'Survived'</span>, palette = <span class="string">'seismic'</span>,size=<span class="number">1.2</span>,diag_kind = <span class="string">'kde'</span>,diag_kws=dict(shade=<span class="literal">True</span>),plot_kws=dict(s=<span class="number">10</span>) )</span><br></pre></td></tr></table></figure><h2 id="箱图"><a href="#箱图" class="headerlink" title="箱图"></a>箱图</h2><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">''<span class="symbol">'Create</span> a <span class="keyword">function</span> <span class="keyword">to</span> count total outliers. <span class="keyword">And</span> plot variables <span class="keyword">with</span> <span class="keyword">and</span> without outliers.'''</span><br><span class="line">def outliers(<span class="keyword">variable</span>):</span><br><span class="line">    # Calculate <span class="number">1</span>st, <span class="number">3</span>rd quartiles <span class="keyword">and</span> iqr.</span><br><span class="line">    q1, q3 = <span class="keyword">variable</span>.quantile(<span class="number">0.25</span>), <span class="keyword">variable</span>.quantile(<span class="number">0.75</span>)</span><br><span class="line">    iqr = q3 - q1</span><br><span class="line">    </span><br><span class="line">    # Calculate lower fence <span class="keyword">and</span> upper fence <span class="keyword">for</span> outliers</span><br><span class="line">    l_fence, u_fence = q1 - <span class="number">1.5</span>*iqr , q3 + <span class="number">1.5</span>*iqr   # Any values less than l_fence <span class="keyword">and</span> greater than u_fence are outliers.</span><br><span class="line">    </span><br><span class="line">    # Observations that are outliers</span><br><span class="line">    outliers = <span class="keyword">variable</span>[(<span class="keyword">variable</span>&lt;l_fence) | (<span class="keyword">variable</span>&gt;u_fence)]</span><br><span class="line">    print(<span class="symbol">'Total</span> Outliers <span class="keyword">of</span>', <span class="keyword">variable</span>.name,':', outliers.count())</span><br><span class="line">    </span><br><span class="line">    # Drop obsevations that are outliers</span><br><span class="line">    filtered = <span class="keyword">variable</span>.drop(outliers.index, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    # Create subplots</span><br><span class="line">    fig, (ax1, ax2) = plt.subplots(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    # Gives space between two subplots</span><br><span class="line">    fig.subplots_adjust(hspace = <span class="number">1</span>) </span><br><span class="line">    </span><br><span class="line">    # Plot <span class="keyword">variable</span> <span class="keyword">with</span> outliers</span><br><span class="line">    <span class="keyword">variable</span>.plot.box(vert = <span class="literal">False</span>, color = <span class="symbol">'coral</span>', grid = <span class="literal">False</span>, ax = ax1, title = <span class="symbol">'Distribution</span> <span class="keyword">with</span> Outliers <span class="keyword">for</span> %s' %<span class="keyword">variable</span>.name)</span><br><span class="line"></span><br><span class="line">    # Plot <span class="keyword">variable</span> without outliers</span><br><span class="line">    filtered.plot.box(vert = <span class="literal">False</span>, color = <span class="symbol">'coral</span>', grid = <span class="literal">False</span>, ax = ax2, title = <span class="symbol">'Distribution</span> without Outliers <span class="keyword">for</span> %s' %<span class="keyword">variable</span>.name)</span><br></pre></td></tr></table></figure><h2 id="刻画变量的不平衡度"><a href="#刻画变量的不平衡度" class="headerlink" title="刻画变量的不平衡度"></a>刻画变量的不平衡度</h2><p>If skewness is less than −1 or greater than +1, the distribution can be considered as highly skewed.</p><p>If skewness is between −1 and −½ or between +½ and +1, the distribution can be considered as moderately skewed.</p><p>And finally if skewness is between −½ and +½, the distribution can be considered as approximately symmetric.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''#2.Density plot with skewness.'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">density_plot_and_skewness</span><span class="params">(variable)</span>:</span></span><br><span class="line">    variable.plot.hist(density = <span class="literal">True</span>)</span><br><span class="line">    variable.plot.kde(style = <span class="string">'k--'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'%s'</span>%variable.name)</span><br><span class="line">    plt.title(<span class="string">'Distribution of %s with Density Plot &amp; Histogram'</span> %variable.name)</span><br><span class="line">    print(<span class="string">'Skewness of '</span>, variable.name, <span class="string">':'</span>)</span><br><span class="line">    skewness = variable.skew()</span><br><span class="line">    <span class="keyword">return</span> display(skewness)</span><br></pre></td></tr></table></figure><h2 id="分类变量和分类变量的关系图"><a href="#分类变量和分类变量的关系图" class="headerlink" title="分类变量和分类变量的关系图"></a>分类变量和分类变量的关系图</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#############################<span class="number">2</span>X2列联表展示########################################</span><br><span class="line"><span class="string">''</span><span class="string">'#1.Create a function that calculates absolute and relative frequency of Survived variable by a categorical variable. And then plots the absolute and relative frequency of Survived by a categorical variable.'</span><span class="string">''</span></span><br><span class="line">def crosstab(cat, cat_target):</span><br><span class="line">    <span class="string">''</span><span class="string">'cat = categorical variable, cat_target = our target categorical variable.'</span><span class="string">''</span></span><br><span class="line">    global ax, ax1</span><br><span class="line">    cat_grouped_by_cat_target = pd.crosstab(index = cat, columns = cat_target)</span><br><span class="line">    cat_grouped_by_cat_target.rename(&#123;<span class="number">0</span>:<span class="string">'Victims'</span>, <span class="number">1</span>:<span class="string">'Survivors'</span>&#125;, axis = <span class="string">'columns'</span>, inplace = <span class="literal">True</span>)  # Renaming the columns</span><br><span class="line">    pct_cat_grouped_by_cat_target = round(pd.crosstab(index = cat, columns = cat_target, normalize = <span class="string">'index'</span>)*<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">    pct_cat_grouped_by_cat_target.rename(&#123;<span class="number">0</span>:<span class="string">'Victims(%)'</span>, <span class="number">1</span>:<span class="string">'Survivors(%)'</span>&#125;, axis = <span class="string">'columns'</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    print(<span class="string">'Survivals and Deaths by'</span>, cat.name,<span class="string">':'</span>, <span class="string">'\n'</span>,cat_grouped_by_cat_target )</span><br><span class="line">    print(<span class="string">'\nPercentage Survivals and Deaths by'</span>, cat.name, <span class="string">':'</span>,<span class="string">'\n'</span>, pct_cat_grouped_by_cat_target)</span><br><span class="line">    </span><br><span class="line">    # Plot absolute frequency <span class="keyword">of</span> Survived by a categorical variable</span><br><span class="line">    ax =  cat_grouped_by_cat_target.plot.bar(color = [<span class="string">'r'</span>, <span class="string">'g'</span>])</span><br><span class="line">    plt.title(<span class="string">'Survival vs Death Count by %s'</span> %cat.name)</span><br><span class="line">    abs_bar_labels()</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    # Plot relative frequrncy <span class="keyword">of</span> Survived by a categorical variable</span><br><span class="line">    ax1 = pct_cat_grouped_by_cat_target.plot.bar(color = [<span class="string">'r'</span>, <span class="string">'g'</span>])</span><br><span class="line">    plt.title(<span class="string">'Percentage Survival vs Death Count by %s'</span> %cat.name)</span><br><span class="line">    pct_bar_labels()</span><br><span class="line">    plt.show()</span><br><span class="line">###########################################卡方检验#######################################</span><br><span class="line"><span class="string">''</span><span class="string">'#2.Create a function to calculate chi_square test between a categorical and target categorical variable.'</span><span class="string">''</span></span><br><span class="line">def chi_square(cat, cat_target):</span><br><span class="line">    cat_grouped_by_cat_target = pd.crosstab(index = cat, columns = cat_target)</span><br><span class="line">    test_result = stats.chi2_contingency (cat_grouped_by_cat_target)</span><br><span class="line">    print(<span class="string">'Chi_square test result between Survived &amp; %s'</span> %cat.name)</span><br><span class="line">    return display(test_result)</span><br><span class="line"></span><br><span class="line">#############################################bonferroni adjusted检验###############################</span><br><span class="line"><span class="string">''</span><span class="string">'#3.Finally create another function to calculate Bonferroni-adjusted pvalue for a categorical and target categorical variable.'</span><span class="string">''</span></span><br><span class="line">def bonferroni_adjusted(cat, cat_target):</span><br><span class="line">    dummies = pd.get_dummies(cat)</span><br><span class="line">    for columns <span class="keyword">in</span> dummies:</span><br><span class="line">        crosstab = pd.crosstab(dummies[columns], cat_target)</span><br><span class="line">        print(stats.chi2_contingency(crosstab))</span><br><span class="line">    print(<span class="string">'\nColumns:'</span>, dummies.columns)</span><br></pre></td></tr></table></figure><h2 id="多个变量组合对因变量的影响图"><a href="#多个变量组合对因变量的影响图" class="headerlink" title="多个变量组合对因变量的影响图"></a>多个变量组合对因变量的影响图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''Create a function that plots the impact of 3 predictor variables at a time on a target variable.'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multivariate_analysis</span><span class="params">(cat1, cat2, cat3, cat_target)</span>:</span></span><br><span class="line">    grouped = round(pd.crosstab(index = [cat1, cat2, cat3], columns = cat_target, normalize = <span class="string">'index'</span>)*<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">    grouped.rename(&#123;<span class="number">0</span>:<span class="string">'Died%'</span>, <span class="number">1</span>:<span class="string">'Survived%'</span>&#125;, axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    ax = grouped.plot.bar(color = [<span class="string">'r'</span>, <span class="string">'g'</span>])</span><br><span class="line">    plt.ylabel(<span class="string">'Relative Frequency (%)'</span>)</span><br></pre></td></tr></table></figure><h2 id="热度图"><a href="#热度图" class="headerlink" title="热度图"></a>热度图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920</span></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">sns.heatmap(x, cmap=<span class="string">'RdBu_r'</span>, center=<span class="number">0.0</span>) </span><br><span class="line">plt.title(<span class="string">'VAR_'</span>+str(j)+<span class="string">' Predictions without Magic'</span>,fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xticks(np.linspace(<span class="number">0</span>,<span class="number">49</span>,<span class="number">5</span>),np.round(np.linspace(mn,mx,<span class="number">5</span>),<span class="number">1</span>))</span><br><span class="line">plt.xlabel(<span class="string">'Var_'</span>+str(j))</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.ylabel(<span class="string">''</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190412150953124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NjYxMjMw,size_16,color_FFFFFF,t_70" alt></p><p>通过热度图可以看出模型是否明显利用了两个特征。虽然可能特征重要度也会说明，但是这样的热度图明显会更具说服力，比如说var_0，模型可以得出竖线15，横线4之类的分界线。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛常见的EDA总结&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="预处理" scheme="http://kodgv.xyz/tags/%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>迁移学习</title>
    <link href="http://kodgv.xyz/2019/04/29/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    <id>http://kodgv.xyz/2019/04/29/数据竞赛/迁移学习/</id>
    <published>2019-04-29T07:56:28.000Z</published>
    <updated>2019-04-29T07:56:36.894Z</updated>
    
    <content type="html"><![CDATA[<p>！！！说不一定可以切分原数据集为新旧数据集</p><ul><li>新旧特征：新数据集为1，旧数据集为0</li><li>合并新旧数据：合并新旧来做特征处理然后有两种操作<br>第一种是用旧训练模型，用模型预测新数据集的train和test概率，然后加在后面做特征，然后在用新数据重新做特征处理然后预测（这个比较好）。 第二种是直接上新旧合并数据训练的模型，然后直接加模型概率做新特征（这个可能会leak，但是在目前的模板上有cv应该会好一点）然后在用新数据重新做特征处理然后预测</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;！！！说不一定可以切分原数据集为新旧数据集&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;新旧特征：新数据集为1，旧数据集为0&lt;/li&gt;
&lt;li&gt;合并新旧数据：合并新旧来做特征处理然后有两种操作&lt;br&gt;第一种是用旧训练模型，用模型预测新数据集的train和test概率，然后加在后面做特征，然后
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>去除重复值</title>
    <link href="http://kodgv.xyz/2019/04/29/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/%E5%8E%BB%E9%99%A4%E9%87%8D%E5%A4%8D%E5%80%BC/"/>
    <id>http://kodgv.xyz/2019/04/29/竞赛经验/去除重复值/</id>
    <published>2019-04-29T07:23:45.000Z</published>
    <updated>2019-04-29T07:24:35.299Z</updated>
    
    <content type="html"><![CDATA[<p>不去重的数据训练的模型 = 80%的非重复样本准确率 + 100%重复样本准确率<br>去重的数据训练的模型 = 85%的非重复样本准确率 + 85%重复样本准确率（直接规则覆盖后100%）<br>深度提取重复样本的真正目的是删除它们以提高模型预测非重复样本的准确率，非重复样本的准确率才是对业务有价值的</p><a id="more"></a><p><strong>1.Giba的home credit 里的重复样本</strong></p><p>如果八月份参加过kaggle的home credit比赛的一定知道，在最后一周才加入比赛的GM Giba通过丰富的比赛经验找到了重复样本trick帮助onodera队伍拿到了比赛的第二名，而另一位GM raddar在最后八个小时加入比赛就直接发现了重复样本问题，并solo到了45名。今天就先来分析一下他们的思路，<a href="https://link.zhihu.com/?target=https%3A//www.kaggle.com/raddar/a-competition-without-a-leak-or-is-it" target="_blank" rel="noopener">A competition without a leak. Or is it?</a></p><p>其实说起来简单，在home credit比赛里，每个客户都有多达两百列特征，但是人的基本特征是不会变的，性别、年龄、生日、银行开户时间等等。打个比方，尽管北京有三千万人，但是如果两样本同一天生日，同一天领驾照，办身份证，同一天结婚，同一天生娃，那么从统计上来说，这两样本肯定就是一个人，而不必再核对身高血型样貌了。radder 就是仅用[DAYS_BIRTH,DAYS_EMPLOYED,DAYS_REGISTRATION,DAYS_ID_PUBLISH,CODE_GENDER,REGION_POPULATION_RELATIVE]六个维度的特征，就把数据集里的重复样本给找出来了，其实其它两百列特征是否一样已经不重要了。</p><p><strong>小结</strong>：虽然我们在比赛里这个称之为trick，但背后是统计学意义的，也可以应用在工作之中。</p><p><strong>2.活学活用</strong></p><p>在我们最近参加的一个关于通讯用户套餐的比赛里，我们的GM piupiu也提到了这个trick。因为出题方把用户的流量消费精确到了byte，金钱消费精确到了分，通讯时间精确到了秒，所以如果用户A上月花了45.32元，用了1545MB 457kb 123byte的流量，打了78分12秒的电话，用户B上月也花了45.32元，用了1545MB 457kb 123byte的流量，打了78分12秒的电话，别管这家通讯公司用户量有多大，A和B肯定是同一用户。所以在其他队伍还在用给定的全部特征判定重复样本的时候，我们用八个特征就可以判定重复样本，把test里和train重复的样本给挑出来了。</p><p><strong>吐槽：</strong>因而我们得到了比其他队伍更多test里的重复样本，然后定义了白名单（piupiu的职业习惯）。在我们代码开源后有一小撮萌新看不懂我们的代码不说，还觉得是piupiu把竞赛网站黑了拿到test的label（滑稽）</p><p><strong>3.进阶</strong></p><p>有人会说了，拿到了test里的重复的样本的label，是不是可以上一波分了～～</p><p>答案是：<strong>并不能。</strong>因为xgb/lgb是具备非常强大的拟合能力，你不刻意找这些重复样本，xgb/lgb也能给你学出来，通过比较你会发现test里的重复样本已经全部给你预测对了。在home credit里是有重复用户的时序信息在里面可以利用，但是在我们这个比赛及大部分比赛就用不上了。</p><p>回到这个比赛，重复样本达到了全部数据的15%，test里的重复样本都预测对了，说明模型把train/test之间的重复样本的特性全学会了，有时候xgb/lgb学习能力太强也不是好事。这时候我们的GM piupiu提到了，test其实可以分为重复样本（15%）和非重复样本两部分（85%）来看待，真正对业务有意义的是非重复样本，可是所有参赛者却把重复样本过拟合的很好，我们真的目的应该是为出题方提供一个有效预测非重复样本的模型，因此提议我们应该把train里的重复样本去掉。打个比方，<strong>valid数据包含重复的和不重复两部分，不去重的数据训练的模型需要1000轮才能达到最优，去重的模型之后500轮最优，那说明有500轮是在过拟合重复样本，但这对于不重复样本来说是一个严重的过拟合，伤害了模型的泛化能力</strong>。而这个通讯比赛的重复样本达到了15%,对模型伤害很大。如果再根据包含重复样本的数据模型调参和特征工程，伤害就更大了。</p><p>简单的打个比方：</p><p>不去重的数据训练的模型 = 80%的非重复样本准确率 + 100%重复样本准确率</p><p>去重的数据训练的模型 = 85%的非重复样本准确率 + 85%重复样本准确率（直接规则覆盖后100%）</p><p>如果你要模型上线，你会上线哪个模型？</p><p>重复样本的准确率并不重要，无论是实际业务还是比赛，一个sql就搞定了。</p><p>根据piupiu的建议，于是我照着做了，果然模型变得特别稳定，也没有其他队伍所提到的抖动也消除了，提升的分数刚好是我们最后领先第二名的差距。</p><p>所以在这个比赛里，<strong>深度提取重复样本的真正目的是删除它们以提高模型预测非重复样本的准确率，非重复样本的准确率才是对业务有价值的。</strong></p><p>有些选手因为看到test里有重复样本就舍不得删的train里的重复样本，难道不用lgb预测的结果就不是结果了吗（流汗）</p><p><strong>小结：</strong>虽然第二节一直在说怎么更多的提取重复样本，但我们真正提升的是模型预测非重复样本的预测能力。</p><p><strong>吐槽：</strong>在我们代码开源后，有一小撮萌新看到我们在lgb输出的结果通过规则覆盖能上好多分，就觉得我们是在用重复样本的leak提升重复样本准确率，却不想想大家既然都能100%预测重复样本的情况下，差距在哪里…</p><p><strong>最后的吐槽：</strong>第一次代码开源就体会了当年plantgo开源携程代码的蛋疼。<a href="https://www.zhihu.com/question/64350623" target="_blank" rel="noopener">如何看待携程举办的大数据比赛？</a> 分享代码有一小撮萌新看不懂学不会清洗重复数据这种常规操作也就算了，还根据自己对代码自己的理解莫名其妙揣测，写本文章主要是帮助我们队友piupiu辟谣，顺便也给大家分享了点有价值的干货。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不去重的数据训练的模型 = 80%的非重复样本准确率 + 100%重复样本准确率&lt;br&gt;去重的数据训练的模型 = 85%的非重复样本准确率 + 85%重复样本准确率（直接规则覆盖后100%）&lt;br&gt;深度提取重复样本的真正目的是删除它们以提高模型预测非重复样本的准确率，非重复样本的准确率才是对业务有价值的&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>异常值检测</title>
    <link href="http://kodgv.xyz/2019/04/29/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B/"/>
    <id>http://kodgv.xyz/2019/04/29/竞赛经验/异常值检测/</id>
    <published>2019-04-29T07:12:19.000Z</published>
    <updated>2019-04-29T07:12:33.534Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/38066650" target="_blank" rel="noopener">https://www.zhihu.com/question/38066650</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/38066650&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.zhihu.com/question/38066650&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>缺失值处理</title>
    <link href="http://kodgv.xyz/2019/04/29/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/"/>
    <id>http://kodgv.xyz/2019/04/29/竞赛经验/缺失值处理/</id>
    <published>2019-04-29T02:52:21.000Z</published>
    <updated>2019-04-29T07:09:25.653Z</updated>
    
    <content type="html"><![CDATA[<p>数据缺失值处理</p><a id="more"></a><p>[TOC]</p><h2 id="数据丢失的原因。"><a href="#数据丢失的原因。" class="headerlink" title="数据丢失的原因。"></a>数据丢失的原因。</h2><ul><li><p>随机缺失(MAR):随机缺失意味着数据点缺失的倾向性与缺失的数据无关，而是与一些观察到的数据相关</p></li><li><p>完全随机缺失(MCAR):某个值缺失的事实与它的假设值以及其他变量的值无关。</p></li><li><p>非随机缺失(MNAR):两个可能的原因是,缺失值取决于假设的值(例如,工资高的人通常不愿透露他们的收入调查)或缺失值依赖于其他变量的值(例如假设女性一般不愿透露他们的年龄!此处年龄变量缺失值受性别变量影响)</p></li></ul><p>在前两种情况下，根据缺失值的出现情况删除缺失值的数据是安全的，而在第三种情况下，删除缺失值的观察值会在模型中产生偏差。所以在移除观测结果之前，我们必须非常小心。注意，归罪法不一定能给出更好的结果。</p><p><img src="/2019/04/29/竞赛经验/缺失值处理/1.png" alt></p><h4 id="删除"><a href="#删除" class="headerlink" title="　删除"></a>　删除</h4><p><strong>成列删除（listwise deletion）</strong></p><p>列表删除(完全案例分析)删除包含一个或多个缺失值的所有数据。特别是如果缺少的数据仅限于少量的观察，您可以选择从分析中删除这些情况。然而，在大多数情况下，使用列表删除通常是不利的。这是因为MCAR的假设(完全随机缺失)通常很少得到支持。因此，列表删除方法产生有偏差的参数和估计。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">newdata &lt;- na.omit(mydata)</span><br><span class="line"><span class="comment"># In python</span></span><br><span class="line">mydata.dropna(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>成对删除（pairwise deletion）</strong></p><p>一般的备选方案，在进行多变量的联立时，只删除掉需要执行的变量的缺失数据。例如在ABC三个变量间，需要计算A和C的协方差，那么只有同时具备A/C的数据会被使用。</p><p>文献指出，当变量间的相关性普遍较低时，成对删除会产生更有效的估计值。然而当变量间的相关性较高时，建议还是使用成列删除。</p><p>理论上成对删除不建议作为成列删除的备选方案。</p><p><strong>虚拟变量调整（哑变量，dummy variables）</strong></p><p>新建两个变量，其中一个变量D为“是否缺失”，缺失值设为0，存在值设为1。</p><p>另一个变量X’，将缺失值设为c（可以是任何常数），存在值设为本身。</p><p>随后，对X’，D和其他变量（因变量和其他预设模型中的自变量）进行回归。这种调整的好处是它利用了所有可用的缺失数据的信息（是否缺失）。为了便利，一个好的c的设置方式是现有非缺失数据X的均数。</p><p>这样做的好处是，D的系数可以被解释成“在控制了其他变量的情况下，X具缺失数据的个体其Y的预测值减去具X平均数的个体于Y的预测值”</p><h4 id="Time-Series-Specific-Methods"><a href="#Time-Series-Specific-Methods" class="headerlink" title="Time-Series Specific Methods"></a>Time-Series Specific Methods</h4><p><strong>Last Observation Carried Forward (LOCF) &amp; Next Observation Carried Backward (NOCB)</strong></p><p><strong>Linear Interpolation</strong></p><p><strong>Seasonal Adjustment + Linear Interpolation</strong></p><h4 id="Mean-Median-and-Mode"><a href="#Mean-Median-and-Mode" class="headerlink" title="Mean, Median and Mode"></a>Mean, Median and Mode</h4><p>计算总体均值、中值或模态是一种非常基本的推算方法，但它没有利用时间序列特征，也没有使用变量之间的关系。它非常快，但有明显的缺点。缺点之一是平均输入减少了数据集中的方差。</p><h4 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h4><p>首先，使用相关矩阵标识值缺失的变量的几个预测器。在回归方程中，选取最优预测因子作为自变量。缺少数据的变量作为因变量。采用预测变量数据完整的案例生成回归方程;然后，该方程用于预测不完全情况下的缺失值。在迭代过程中，插入缺失变量的值，然后用所有的情况来预测因变量。重复这些步骤，直到预测值之间的差值很小，即它们收敛。</p><p>它“理论上”为缺失的值提供了很好的估计。然而，这种模式有几个缺点，往往超过了优点。首先，由于替换后的值是由其他变量预测的，它们往往“太好”地匹配在一起，因此标准误差被缩小了。我们还必须假设回归方程中使用的变量之间存在线性关系，而回归方程中可能没有线性关系。</p><h4 id="Multiple-Imputation"><a href="#Multiple-Imputation" class="headerlink" title="Multiple Imputation"></a>Multiple Imputation</h4><ol><li><strong>Imputation</strong>: Impute the missing entries of the incomplete data sets <em>m</em>times (<em>m</em>=3 in the figure). Note that imputed values are drawn from a distribution. Simulating random draws doesn’t include uncertainty in model parameters. Better approach is to use Markov Chain Monte Carlo (MCMC) simulation. This step results in m complete data sets.</li><li><strong>Analysis</strong>: Analyze each of the <em>m</em> completed data sets.</li><li><strong>Pooling</strong>: Integrate the <em>m</em> analysis results into a final result</li></ol><p><img src="/2019/04/29/竞赛经验/缺失值处理/2.png" alt></p><p>This is by far the most preferred method for imputation for the following reasons:</p><ul><li>Easy to use</li><li>No biases (if imputation model is correct)</li></ul><p>实现的代码包:fancyimpute</p><h4 id="Imputation-of-Categorical-Variables"><a href="#Imputation-of-Categorical-Variables" class="headerlink" title="Imputation of Categorical Variables"></a>Imputation of Categorical Variables</h4><p>把缺失的值作为一个类别进行填补</p><h4 id="KNN-K-Nearest-Neighbors"><a href="#KNN-K-Nearest-Neighbors" class="headerlink" title="KNN (K Nearest Neighbors)"></a>KNN (K Nearest Neighbors)</h4><p> XGBoost and Random Forest 同样也有 data imputation</p><p>该方法基于距离测度选取k个邻域，并以邻域的平均值作为估计的归一化方法。该方法需要选择最近邻的数目和距离度量。KNN既可以预测离散属性(k个近邻中最频繁的值)，也可以预测连续属性(k个近邻中均值)</p><p>距离度量根据数据的类型而变化:</p><ol><li><p>连续数据:连续数据常用的距离度量是欧式、Manhattan和cos</p></li><li><p>分类数据:本例中一般使用汉明距离。它接受所有的分类属性，如果两个点之间的值不相同，则对每个属性进行计数。然后，汉明距离等于值不同的属性的数量。</p></li></ol><p>KNN算法最吸引人的特点之一是易于理解和实现。KNN的非参数特性使其在某些数据可能非常“不寻常”的情况下具有优势。</p><p>KNN算法的一个明显缺点是，在分析大型数据集时非常耗时，因为它在整个数据集中搜索类似的实例。此外，由于最近邻和最近邻之间的距离相差不大，高维数据会严重降低KNN的精度</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据缺失值处理&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>胶囊网络</title>
    <link href="http://kodgv.xyz/2019/04/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C/"/>
    <id>http://kodgv.xyz/2019/04/27/神经网络/胶囊网络/</id>
    <published>2019-04-27T08:30:18.000Z</published>
    <updated>2019-04-28T02:27:06.947Z</updated>
    
    <content type="html"><![CDATA[<p>胶囊网络</p><a id="more"></a><p>来源:<a href="https://spaces.ac.cn/archives/4819" target="_blank" rel="noopener">https://spaces.ac.cn/archives/4819</a></p><p>直接看上面的这个文章，描述的非常详细</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;胶囊网络&lt;/p&gt;
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="网络结构" scheme="http://kodgv.xyz/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>FocalLoss针对不平衡数据</title>
    <link href="http://kodgv.xyz/2019/04/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/FocalLoss%E9%92%88%E5%AF%B9%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE/"/>
    <id>http://kodgv.xyz/2019/04/22/神经网络/FocalLoss针对不平衡数据/</id>
    <published>2019-04-22T10:32:49.000Z</published>
    <updated>2019-04-30T02:52:24.135Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>[TOC]</p><p>参考来源:</p><p><a href="https://zhuanlan.zhihu.com/p/32423092" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32423092</a></p><p><a href="https://www.zhihu.com/question/63581984" target="_blank" rel="noopener">https://www.zhihu.com/question/63581984</a></p><p><a href="https://zhuanlan.zhihu.com/p/28527749" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28527749</a></p><h2 id="讲解"><a href="#讲解" class="headerlink" title="讲解"></a>讲解</h2><p>​    本质上讲，Focal Loss 就是一个解决<strong>分类问题中类别不平衡、分类难度差异</strong>的一个 loss，总之这个工作一片好评就是了。大家还可以看知乎的讨论：<a href="https://www.zhihu.com/question/63581984" target="_blank" rel="noopener">如何评价 Kaiming 的 Focal Loss for Dense Object Detection？</a></p><p><strong>核心思想</strong></p><p>这样的做法就是：<strong>正样本的预测值大于 0.5 的，或者负样本的预测值小于 0.5 的，我都不更新了，把注意力集中在预测不准的那些样本，当然这个阈值可以调整。</strong>这样做能部分地达到目的，但是所需要的迭代次数会大大增加。</p><p>原因是这样的：以正样本为例，<strong>我只告诉模型正样本的预测值大于 0.5 就不更新了，却没有告诉它要“保持”大于 0.5</strong>，所以下一阶段，它的预测值就很有可能变回小于 0.5 了。当然，如果是这样的话，下一回合它又被更新了，这样反复迭代，理论上也能达到目的，但是迭代次数会大大增加。</p><p>所以，要想改进的话，重点就是<strong>“不只是要告诉模型正样本的预测值大于0.5就不更新了，而是要告诉模型当其大于0.5后就只需要保持就好了”</strong>。好比老师看到一个学生及格了就不管了，这显然是不行的。如果学生已经及格，那么应该要想办法要他保持目前这个状态甚至变得更好，而不是不管。</p><p>所以除了单纯的区分外，必须使该loss可导，这样才可以告诉模型。</p><p><strong>目的是通过减少易分类样本的权重，从而使得模型在训练时更专注于难分类的样本</strong>。</p><p>Kaiming 大神的 Focal Loss 形式是：</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f1.jpg" alt></p><p>如果落实到 <em>ŷ =σ(x)</em> 这个预测，那么就有：</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f3.jpg" alt="img"></p><p>特别地，<strong>如果</strong> <strong>K</strong> <strong>和</strong> <strong>γ</strong> <strong>都取 1，那么</strong> <strong>L∗∗=Lfl</strong>。</p><p>事实上 <em>K</em> 和 <em>γ</em> 的作用都是一样的，都是调节权重曲线的陡度，只是调节的方式不一样。注意<em>L∗∗</em>或 <em>Lfl</em> 实际上都已经包含了对不均衡样本的解决方法，或者说，类别不均衡本质上就是分类难度差异的体现。</p><p>​    首先y’的范围是0到1，所以不管γ是多少，这个调制系数都是大于等于0的。易分类的样本再多，你的权重很小，那么对于total loss的共享也就不会太大。那么怎么控制样本权重呢？举个例子，假设一个二分类，样本x1属于类别1的y’=0.9，样本x2属于类别1的y’=0.6，显然前者更可能是类别1，假设γ=1，那么对于y’=0.9，调制系数则为0.1；对于y’=0.6，调制系数则为0.4，这个调制系数就是这个样本对loss的贡献程度，也就是权重，所以难分的样本（pt=0.6）的权重更大。</p><p>​    <strong>比如负样本远比正样本多的话，模型肯定会倾向于数目多的负类（可以想象全部样本都判为负类），这时候，负类的</strong> <strong>*ŷ γ*</strong> <strong>或</strong> <strong>σ(Kx) 都很小，而正类的</strong> <strong>(1−ŷ )γ</strong> <strong>或</strong> <strong>*σ(−Kx)*</strong> <strong>就很大，这时候模型就会开始集中精力关注正样本。</strong></p><p>当然，Kaiming 大神还发现对 <em>Lfl</em> 做个权重调整，结果会有微小提升。</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f2.jpg" alt="img"></p><p>通过一系列调参，得到 <em>α=0.25, γ=2</em>（在他的模型上）的效果最好。注意在他的任务中，正样本是属于少数样本，也就是说，本来正样本难以“匹敌”负样本，但经过 <em>(1−ŷ )γ</em> 和 <em>ŷγ</em> 的“操控”后，也许形势还逆转了，还要对正样本降权。</p><p>不过我认为这样调整只是经验结果，理论上很难有一个指导方案来决定 <em>α</em> 的值，如果没有大算力调参，倒不如直接让 <em>α=0.5</em>（均等）。</p><h2 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a><strong>多分类</strong></h2><p>Focal Loss 在多分类中的形式也很容易得到，其实就是：</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f4.jpg" alt="img"></p><p><em>ŷt</em> 是目标的预测值，一般就是经过 softmax 后的结果。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>多分类</p><p><a href="https://github.com/marvis/pytorch-yolo2/blob/master/FocalLoss.py" target="_blank" rel="noopener">https://github.com/marvis/pytorch-yolo2/blob/master/FocalLoss.py</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">r"""</span></span><br><span class="line"><span class="string">        This criterion is a implemenation of Focal Loss, which is proposed in </span></span><br><span class="line"><span class="string">        Focal Loss for Dense Object Detection.</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            Loss(x, class) = - \alpha (1-softmax(x)[class])^gamma \log(softmax(x)[class])</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        The losses are averaged across observations for each minibatch.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            alpha(1D Tensor, Variable) : the scalar factor for this criterion</span></span><br><span class="line"><span class="string">            gamma(float, double) : gamma &gt; 0; reduces the relative loss for well-classiﬁed examples (p &gt; .5), </span></span><br><span class="line"><span class="string">                                   putting more focus on hard, misclassiﬁed examples</span></span><br><span class="line"><span class="string">            size_average(bool): size_average(bool): By default, the losses are averaged over observations for each minibatch.</span></span><br><span class="line"><span class="string">                                However, if the field size_average is set to False, the losses are</span></span><br><span class="line"><span class="string">                                instead summed for each minibatch.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, class_num, alpha=None, gamma=<span class="number">2</span>, size_average=True)</span>:</span></span><br><span class="line">        super(FocalLoss, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> alpha <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.alpha = Variable(torch.ones(class_num, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> isinstance(alpha, Variable):</span><br><span class="line">                self.alpha = alpha</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.alpha = Variable(alpha)</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.class_num = class_num</span><br><span class="line">        self.size_average = size_average</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets)</span>:</span></span><br><span class="line">        N = inputs.size(<span class="number">0</span>)</span><br><span class="line">        print(N)</span><br><span class="line">        C = inputs.size(<span class="number">1</span>)</span><br><span class="line">        P = F.softmax(inputs)</span><br><span class="line"><span class="comment"># 这是为了获取onehot</span></span><br><span class="line">        class_mask = inputs.data.new(N, C).fill_(<span class="number">0</span>)</span><br><span class="line">        class_mask = Variable(class_mask)</span><br><span class="line">        ids = targets.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        class_mask.scatter_(<span class="number">1</span>, ids.data, <span class="number">1.</span>)</span><br><span class="line">        <span class="comment">#print(class_mask)</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs.is_cuda <span class="keyword">and</span> <span class="keyword">not</span> self.alpha.is_cuda:</span><br><span class="line">            self.alpha = self.alpha.cuda()</span><br><span class="line">        alpha = self.alpha[ids.data.view(<span class="number">-1</span>)]</span><br><span class="line">        </span><br><span class="line">        probs = (P*class_mask).sum(<span class="number">1</span>).view(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        log_p = probs.log()</span><br><span class="line">        <span class="comment">#print('probs size= &#123;&#125;'.format(probs.size()))</span></span><br><span class="line">        <span class="comment">#print(probs)</span></span><br><span class="line"></span><br><span class="line">        batch_loss = -alpha*(torch.pow((<span class="number">1</span>-probs), self.gamma))*log_p </span><br><span class="line">        <span class="comment">#print('-----bacth_loss------')</span></span><br><span class="line">        <span class="comment">#print(batch_loss)</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.size_average:</span><br><span class="line">            loss = batch_loss.mean()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = batch_loss.sum()</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    alpha = torch.rand(<span class="number">21</span>, <span class="number">1</span>)</span><br><span class="line">    print(alpha)</span><br><span class="line">    FL = FocalLoss(class_num=<span class="number">5</span>, gamma=<span class="number">0</span> )</span><br><span class="line">    CE = nn.CrossEntropyLoss()</span><br><span class="line">    N = <span class="number">4</span></span><br><span class="line">    C = <span class="number">5</span></span><br><span class="line">    inputs = torch.rand(N, C)</span><br><span class="line">    targets = torch.LongTensor(N).random_(C)</span><br><span class="line">    inputs_fl = Variable(inputs.clone(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    targets_fl = Variable(targets.clone())</span><br><span class="line"></span><br><span class="line">    inputs_ce = Variable(inputs.clone(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    targets_ce = Variable(targets.clone())</span><br><span class="line">    print(<span class="string">'----inputs----'</span>)</span><br><span class="line">    print(inputs)</span><br><span class="line">    print(<span class="string">'---target-----'</span>)</span><br><span class="line">    print(targets)</span><br><span class="line"></span><br><span class="line">    fl_loss = FL(inputs_fl, targets_fl)</span><br><span class="line">    ce_loss = CE(inputs_ce, targets_ce)</span><br><span class="line">    print(<span class="string">'ce = &#123;&#125;, fl =&#123;&#125;'</span>.format(ce_loss.data[<span class="number">0</span>], fl_loss.data[<span class="number">0</span>]))</span><br><span class="line">    fl_loss.backward()</span><br><span class="line">    ce_loss.backward()</span><br><span class="line">    <span class="comment">#print(inputs_fl.grad.data)</span></span><br><span class="line">    print(inputs_ce.grad.data)</span><br></pre></td></tr></table></figure><p>单分类</p><p><a href="https://www.kaggle.com/aakashnain/diving-deep-into-focal-loss" target="_blank" rel="noopener">https://www.kaggle.com/aakashnain/diving-deep-into-focal-loss</a></p><p><a href="https://www.kaggle.com/sfzero/focal-loss-feature-0-99994/comments" target="_blank" rel="noopener">https://www.kaggle.com/sfzero/focal-loss-feature-0-99994/comments</a></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, alpha=<span class="number">1</span>, gamma=<span class="number">2</span>, logits=True, reduction=<span class="string">'elementwise_mean'</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>(FocalLoss, <span class="keyword">self</span>).__init_<span class="number">_</span>()</span><br><span class="line">        <span class="keyword">self</span>.alpha = alpha</span><br><span class="line">        <span class="keyword">self</span>.gamma = gamma</span><br><span class="line">        <span class="keyword">self</span>.logits = logits</span><br><span class="line">        <span class="keyword">self</span>.reduction = reduction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, inputs, targets)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">logits:</span></span><br><span class="line">            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=<span class="string">'none'</span>)</span><br><span class="line">        <span class="symbol">else:</span></span><br><span class="line">            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction=<span class="string">'none'</span>)</span><br><span class="line">        pt = torch.exp(-BCE_loss)</span><br><span class="line">        F_loss = <span class="keyword">self</span>.alpha * (<span class="number">1</span>-pt)**<span class="keyword">self</span>.gamma * BCE_loss</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.reduction is <span class="symbol">None:</span></span><br><span class="line">            <span class="keyword">return</span> F_loss</span><br><span class="line">        <span class="symbol">else:</span></span><br><span class="line">            <span class="keyword">return</span> torch.mean(F_loss)</span><br></pre></td></tr></table></figure><p>简单版代码</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/83363#486607</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">staticWeightLoss</span><span class="params">(<span class="literal">true</span>,pred)</span></span><span class="symbol">:</span></span><br><span class="line">    loss = K.binary_crossentropy(<span class="literal">true</span>, pred)</span><br><span class="line">    positiveLoss = positiveWeights * loss</span><br><span class="line">    negativeLoss = negativeWeights * loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> K.switch(K.greater(<span class="literal">true</span>, <span class="number">0</span>.<span class="number">5</span>), positiveLoss, negativeLoss)</span><br></pre></td></tr></table></figure><p>!要注意softmax是要有两列以上，sigmod才是一列</p><h2 id="引申"><a href="#引申" class="headerlink" title="引申"></a>引申</h2><p>这就是为什么之前别人做数据增强的时候，把预测很高的数据当作1把预测很低的数据当作0放进去加强训练。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;参考来源:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32423092&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanl
      
    
    </summary>
    
      <category term="神经网络" scheme="http://kodgv.xyz/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="损失函数" scheme="http://kodgv.xyz/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>比赛心得集合</title>
    <link href="http://kodgv.xyz/2019/04/21/%E6%AF%94%E8%B5%9B%E5%BF%83%E5%BE%97%E9%9B%86%E5%90%88/"/>
    <id>http://kodgv.xyz/2019/04/21/比赛心得集合/</id>
    <published>2019-04-21T06:45:42.000Z</published>
    <updated>2019-04-29T07:36:14.462Z</updated>
    
    <content type="html"><![CDATA[<p>好的代码借鉴，必要的时候可以直接抄</p><a id="more"></a><p>[TOC]</p><h1 id="比赛心得"><a href="#比赛心得" class="headerlink" title="比赛心得"></a>比赛心得</h1><p>机器翻译注意力机制及其PyTorch实现</p><p><a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/" target="_blank" rel="noopener">https://plmsmile.github.io/2017/10/12/Attention-based-NMT/</a></p><p>各个NLP模型实现</p><p><a href="http://www.zhongruitech.com/921029206.html" target="_blank" rel="noopener">http://www.zhongruitech.com/921029206.html</a></p><p>苏剑林大神博客</p><p><a href="https://spaces.ac.cn/category/Resources" target="_blank" rel="noopener">https://spaces.ac.cn/category/Resources</a></p><p>QuroaNLP分类比赛心得</p><p><a href="https://www.kaggle.com/c/quora-insincere-questions-classification" target="_blank" rel="noopener">Quroa 识别不良句子</a></p><p><a href="https://www.getit01.com/p20190314357550039/" target="_blank" rel="noopener">https://www.getit01.com/p20190314357550039/</a></p><p>腾讯广告大赛比赛心得</p><p><a href="https://zhuanlan.zhihu.com/p/38341881" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38341881</a></p><p>摩拜杯目的地预测比赛心得</p><p><a href="https://zhuanlan.zhihu.com/p/32151090" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32151090</a></p><h1 id="比赛"><a href="#比赛" class="headerlink" title="比赛"></a>比赛</h1><p>如何不过拟合:<a href="https://www.kaggle.com/c/dont-overfit-ii" target="_blank" rel="noopener">https://www.kaggle.com/c/dont-overfit-ii</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;好的代码借鉴，必要的时候可以直接抄&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>数据挖掘问答汇总</title>
    <link href="http://kodgv.xyz/2019/04/21/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E9%97%AE%E7%AD%94%E6%B1%87%E6%80%BB/"/>
    <id>http://kodgv.xyz/2019/04/21/数据挖掘问答汇总/</id>
    <published>2019-04-21T02:43:51.000Z</published>
    <updated>2019-04-29T14:44:12.802Z</updated>
    
    <content type="html"><![CDATA[<p>数据挖掘竞赛汇总</p><a id="more"></a><p>[TOC]</p><h2 id="如何知道树模型怎么可以提高？"><a href="#如何知道树模型怎么可以提高？" class="headerlink" title="如何知道树模型怎么可以提高？"></a>如何知道树模型怎么可以提高？</h2><p>通过画树的图，分析树当前无法分割的知识是什么，给它补充进数据里面。</p><h2 id="如何提高分数？"><a href="#如何提高分数？" class="headerlink" title="如何提高分数？"></a>如何提高分数？</h2><ul><li>测试集和训练集尽量相似</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据挖掘竞赛汇总&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>NLP代码汇总</title>
    <link href="http://kodgv.xyz/2019/04/20/NLP%E4%BB%A3%E7%A0%81%E6%B1%87%E6%80%BB/"/>
    <id>http://kodgv.xyz/2019/04/20/NLP代码汇总/</id>
    <published>2019-04-20T13:57:19.000Z</published>
    <updated>2019-04-27T07:13:55.279Z</updated>
    
    <content type="html"><![CDATA[<p>NLP汇总</p><a id="more"></a><p>[TOC]</p><h2 id="动态padding，节省时间"><a href="#动态padding，节省时间" class="headerlink" title="动态padding，节省时间"></a>动态padding，节省时间</h2><p>比起一开始固定化padding，动态padding，可以针对当前batch的长度来Padding，明显会增快速度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, text, lens, y=None)</span>:</span></span><br><span class="line">        self.text = text</span><br><span class="line">        self.y = y</span><br><span class="line">        self.lens = lens</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.lens)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self.text[index], self.lens[index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.text[index], self.lens[index], self.y[index]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(batch)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    batch = [dataset[i] for i in N]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    size = len(batch[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> size == <span class="number">3</span>:</span><br><span class="line">        texts, lens, y = zip(*batch)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        texts, lens = zip(*batch)</span><br><span class="line">    lens = np.array(lens)</span><br><span class="line">    sort_idx = np.argsort(<span class="number">-1</span> * lens)</span><br><span class="line">    reverse_idx = np.argsort(sort_idx)</span><br><span class="line">    max_len = min(int(np.percentile(lens, PERCENTILE)), MAX_LEN)</span><br><span class="line">    </span><br><span class="line">    lens = np.clip(lens, <span class="number">0</span>, max_len)[sort_idx]</span><br><span class="line">    texts = torch.tensor(sequence.pad_sequences(texts, maxlen=max_len)[sort_idx], dtype=torch.long).cuda()</span><br><span class="line">    <span class="keyword">if</span> size == <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">return</span> texts, lens, reverse_idx, torch.tensor(y, dtype=torch.float32).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> texts, lens, reverse_idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_data_loader</span><span class="params">(texts, lens, y=None, batch_size=BATCH_SIZE)</span>:</span></span><br><span class="line">    dset = MyDataset(texts, lens, y)</span><br><span class="line">    dloader = data.DataLoader(dset, batch_size=batch_size, shuffle=<span class="literal">False</span>, collate_fn=collate_fn)</span><br><span class="line">    <span class="keyword">return</span> dloader</span><br><span class="line">seqs = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>]]</span><br><span class="line">lens = [len(i) <span class="keyword">for</span> i <span class="keyword">in</span> seqs]</span><br><span class="line"></span><br><span class="line">data_loader = build_data_loader(seqs, lens)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data_loader:</span><br><span class="line">    seq_batch, lens_batch, reverse_idx_batch = batch</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">print(<span class="string">f'original seqs:'</span>)</span><br><span class="line">print(seqs)</span><br><span class="line">print(<span class="string">f'batch seqs, already sort by lens, and padding dynamic in batch:'</span>)</span><br><span class="line">print(seq_batch)</span><br><span class="line">print(<span class="string">f'reverse batch seqs:'</span>)</span><br><span class="line">print(seq_batch[reverse_idx_batch])</span><br><span class="line">h_embedding_pack = pack_padded_sequence(seq_batch,lens_batch,batch_first=<span class="literal">True</span>)</span><br><span class="line">print(h_embedding_pack)</span><br></pre></td></tr></table></figure><h2 id="mask-loss-避免无用结果的求导影响"><a href="#mask-loss-避免无用结果的求导影响" class="headerlink" title="mask loss 避免无用结果的求导影响"></a>mask loss 避免无用结果的求导影响</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> func</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CustomLoss,self).__init__()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, Y_hat, Y)</span>:</span></span><br><span class="line">       <span class="comment"># TRICK 3 ********************************</span></span><br><span class="line">       <span class="comment"># before we calculate the negative log likelihood, we need to mask out the activations</span></span><br><span class="line">       <span class="comment"># this means we don't want to take into account padded items in the output vector</span></span><br><span class="line">       <span class="comment"># simplest way to think about this is to flatten ALL sequences into a REALLY long sequence</span></span><br><span class="line">       <span class="comment"># and calculate the loss on that.</span></span><br><span class="line">       <span class="comment"># flatten all the labels</span></span><br><span class="line">        Y = Y.view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># flatten all predictions</span></span><br><span class="line">        Y_hat = Y_hat.view(<span class="number">-1</span>, self.nb_tags)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># create a mask by filtering out all tokens that ARE NOT the padding token</span></span><br><span class="line">        tag_pad_token = self.tags[<span class="string">'&lt;PAD&gt;'</span>]</span><br><span class="line">        mask = (Y &gt; tag_pad_token).float()</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># count how many tokens we have</span></span><br><span class="line">        nb_tokens = int(torch.sum(mask).data[<span class="number">0</span>])</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># pick the values for the label and zero out the rest with the mask</span></span><br><span class="line">        Y_hat = Y_hat[range(Y_hat.shape[<span class="number">0</span>]), Y] * mask</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># compute cross entropy loss which ignores all &lt;PAD&gt; tokens</span></span><br><span class="line">        ce_loss = -torch.sum(Y_hat) / nb_tokens</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ce_loss</span><br></pre></td></tr></table></figure><h2 id="问题解答专区"><a href="#问题解答专区" class="headerlink" title="问题解答专区"></a>问题解答专区</h2><h3 id="为什么LSTM不同batch的句子长度可以不一致"><a href="#为什么LSTM不同batch的句子长度可以不一致" class="headerlink" title="为什么LSTM不同batch的句子长度可以不一致"></a>为什么LSTM不同batch的句子长度可以不一致</h3><p>LSTM只要保证每个batch的句子长度一致即可，所以可以使用动态padding,但是为什么不同batch的长度可以不一样？</p><p>因为LSTM需要更新的参数是共享，就是每个状态的权重是共享的，所以就无所谓不同长度了。只要保证每个batch内能够更新权重即可。</p><h3 id="深度学习中-number-of-training-epochs-中的-epoch到底指什么？"><a href="#深度学习中-number-of-training-epochs-中的-epoch到底指什么？" class="headerlink" title="深度学习中 number of training epochs 中的 epoch到底指什么？"></a>深度学习中 number of training epochs 中的 epoch到底指什么？</h3><p>对于初学者来讲，有几个概念容易混淆：</p><p>（1）iteration：表示1次迭代（也叫training step），每次迭代更新1次网络结构的参数；</p><p>（2）batch-size：1次迭代所使用的样本量；</p><p>（3）epoch：1个epoch表示过了1遍训练集中的所有样本。</p><p>一次epoch=所有训练数据forward+backward后更新参数的过程。<br>一次iteration=[batch size]个训练数据forward+backward后更新参数过程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NLP汇总&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://kodgv.xyz/categories/NLP/"/>
    
    
      <category term="汇总" scheme="http://kodgv.xyz/tags/%E6%B1%87%E6%80%BB/"/>
    
  </entry>
  
  <entry>
    <title>python多进程</title>
    <link href="http://kodgv.xyz/2019/04/18/%E4%BB%A3%E7%A0%81%E5%9F%BA%E7%A1%80/python%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    <id>http://kodgv.xyz/2019/04/18/代码基础/python多进程/</id>
    <published>2019-04-18T01:46:38.000Z</published>
    <updated>2019-04-18T08:36:10.690Z</updated>
    
    <content type="html"><![CDATA[<p>python multiprocessing模块多进程详解</p><a id="more"></a><p>[TOC]</p><h2 id="multiprocessing模块API"><a href="#multiprocessing模块API" class="headerlink" title="multiprocessing模块API"></a>multiprocessing模块API</h2><p>Pool类用于需要执行的目标很多，而手动限制进程数量又太繁琐时，如果目标少且不用控制进程数量则可以用<a href="http://thief.one/2016/11/24/Multiprocessing-Process" target="_blank" rel="noopener">Process</a>类。</p><p>构造方法</p><ul><li>Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]])</li><li>processes ：使用的工作进程的数量，如果processes是None那么使用 os.cpu_count()返回的数量。</li><li>initializer： 如果initializer是None，那么每一个工作进程在开始的时候会调用initializer(*initargs)。</li><li>maxtasksperchild：工作进程退出之前可以完成的任务数，完成后用一个新的工作进程来替代原进程，来让闲置的资源被释放。maxtasksperchild默认是None，意味着只要Pool存在工作进程就会一直存活。</li><li>context: 用在制定工作进程启动时的上下文，一般使用 multiprocessing.Pool() 或者一个context对象的Pool()方法来创建一个池，两种方法都适当的设置了context。</li></ul><p>实例方法</p><ul><li>apply_async(func[, args[, kwds[, callback]]]) 它是非阻塞。</li><li>apply(func[, args[, kwds]])是阻塞的。</li><li>close() 关闭pool，使其不在接受新的任务。</li><li>terminate() 关闭pool，结束工作进程，不在处理未完成的任务。</li><li>join() 主进程阻塞，等待子进程的退出， join方法要在close或terminate之后使用。</li></ul><h3 id="Pool使用方法"><a href="#Pool使用方法" class="headerlink" title="Pool使用方法"></a>Pool使用方法</h3><p>Pool+map函数</p><p>说明：此写法缺点在于只能通过map向函数传递一个参数。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing import Pool</span><br><span class="line">def test(i):</span><br><span class="line">    <span class="builtin-name">print</span> i</span><br><span class="line"><span class="keyword">if</span> <span class="attribute">__name__</span>=="__main__":</span><br><span class="line">lists=[1,2,3]</span><br><span class="line"><span class="attribute">pool</span>=Pool(processes=2) #定义最大的进程数</span><br><span class="line">pool.map(test,lists)        #p必须是一个可迭代变量。</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure><p>异步进程池（非阻塞）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">pool = Pool(processes=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i  <span class="keyword">in</span> xrange(<span class="number">500</span>):</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">For循环中执行步骤：</span></span><br><span class="line"><span class="string">（1）循环遍历，将500个子进程添加到进程池（相对父进程会阻塞）</span></span><br><span class="line"><span class="string">（2）每次执行10个子进程，等一个子进程执行完后，立马启动新的子进程。（相对父进程不阻塞）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">apply_async为异步进程池写法。</span></span><br><span class="line"><span class="string">异步指的是启动子进程的过程，与父进程本身的执行（print）是异步的，而For循环中往进程池添加子进程的过程，与父进程本身的执行却是同步的。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">    pool.apply_async(test, args=(i,)) <span class="comment">#维持执行的进程总数为10，当一个进程执行完后启动一个新进程.       </span></span><br><span class="line"><span class="keyword">print</span> “test”</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure><p>执行顺序：For循环内执行了2个步骤，第一步：将500个对象放入进程池（阻塞）。第二步：同时执行10个子进程（非阻塞），有结束的就立即添加，维持10个子进程运行。（apply_async方法的会在执行完for循环的添加步骤后，直接执行后面的print语句，而apply方法会等所有进程池中的子进程运行完以后再执行后面的print语句）</p><p>注意：调用join之前，先调用close或者terminate方法，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束。</p><h2 id="多进程示例代码"><a href="#多进程示例代码" class="headerlink" title="多进程示例代码"></a>多进程示例代码</h2><p>纯建立Process<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#encoding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> os, time, random</span><br><span class="line"></span><br><span class="line"><span class="comment">#线程启动后实际执行的代码块</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">r1</span><span class="params">(process_name)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">print</span> process_name, os.getpid()     <span class="comment">#打印出当前进程的id</span></span><br><span class="line">        time.sleep(random.random())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">r2</span><span class="params">(process_name)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">print</span> process_name, os.getpid()     <span class="comment">#打印出当前进程的id</span></span><br><span class="line">        time.sleep(random.random())</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"main process run..."</span></span><br><span class="line">        p1 = Process(target=r1, args=(<span class="string">'process_name1'</span>, ))       <span class="comment">#target:指定进程执行的函数，args:该函数的参数，需要使用tuple,只需要不断往后加参数即可</span></span><br><span class="line">        p2 = Process(target=r2, args=(<span class="string">'process_name2'</span>, ))</span><br><span class="line">        </span><br><span class="line">        p1.start()    <span class="comment">#通过调用start方法启动进程，跟线程差不多。</span></span><br><span class="line">        p2.start()    <span class="comment">#但run方法在哪呢？待会说。。。</span></span><br><span class="line">        p1.join()     <span class="comment">#join方法也很有意思，寻思了一下午，终于理解了。待会演示。</span></span><br><span class="line">        p2.join()</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"main process runned all lines..."</span></span><br></pre></td></tr></table></figure></p><p>POOL池管理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(data, index, size)</span>:</span>  <span class="comment"># data 传入数据，index 数据分片索引，size进程数</span></span><br><span class="line">    size = math.ceil(len(data) / size)</span><br><span class="line">    start = size * index</span><br><span class="line">    end = (index + <span class="number">1</span>) * size <span class="keyword">if</span> (index + <span class="number">1</span>) * size &lt; len(data) <span class="keyword">else</span> len(data)</span><br><span class="line">    temp_data = data[start:end]</span><br><span class="line">    <span class="comment"># do something</span></span><br><span class="line">    <span class="keyword">return</span> data  <span class="comment"># 可以返回数据，在后面收集起来</span></span><br><span class="line"></span><br><span class="line">processor = <span class="number">40</span></span><br><span class="line">res = []</span><br><span class="line">p = Pool(processor)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(processor):</span><br><span class="line">    res.append(p.apply_async(run, args=(data, i, processor,)))</span><br><span class="line">    print(str(i) + <span class="string">' processor started !'</span>)</span><br><span class="line">p.close()</span><br><span class="line">p.join()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> res:</span><br><span class="line">    print(i.get())  <span class="comment"># 使用get获得多进程处理的结果</span></span><br></pre></td></tr></table></figure></p><h2 id="进程注意事项"><a href="#进程注意事项" class="headerlink" title="进程注意事项"></a>进程注意事项</h2><h3 id="进程之间内存独立"><a href="#进程之间内存独立" class="headerlink" title="进程之间内存独立"></a>进程之间内存独立</h3><p>多进程与多线程最大的不同在于，多进程的每一个进程都有一份变量的拷贝，进程之间的操作互不影响，Process进程创建时，子进程会将主进程的Process对象完全复制一份，这样在主进程和子进程各有一个 Process对象，但是p.start()启动的是子进程，主进程中的Process对象作为一个静态对象存在，不执行。</p><p>就是这个地方，要注意的是，对于多进程来说，它是不会复制内存的，所以不用担心main中的变量被复制，但是它会复制main函数以外的变量</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import multiprocessing</span><br><span class="line">import <span class="built_in">time</span></span><br><span class="line"></span><br><span class="line"><span class="literal">zero</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">def change_zero():</span><br><span class="line">    <span class="built_in">global</span> <span class="literal">zero</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        <span class="literal">zero</span> = <span class="literal">zero</span> + <span class="number">1</span></span><br><span class="line">        print(multiprocessing.current_process().name, <span class="literal">zero</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    p1 = multiprocessing.Process(target = change_zero)</span><br><span class="line">    p2 = multiprocessing.Process(target = change_zero)</span><br><span class="line">    p1.<span class="built_in">start</span>()</span><br><span class="line">    p2.<span class="built_in">start</span>()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.join()</span><br><span class="line">    print(<span class="literal">zero</span>)</span><br></pre></td></tr></table></figure><p>运行结果如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Process-1 1</span><br><span class="line">Process-1 2</span><br><span class="line">Process-1 3</span><br><span class="line">Process-2 1</span><br><span class="line">Process-2 2</span><br><span class="line">Process-2 3</span><br><span class="line">0</span><br></pre></td></tr></table></figure><h3 id="共享变量Queue"><a href="#共享变量Queue" class="headerlink" title="共享变量Queue"></a>共享变量Queue</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">下面我们要讨论第一种情况，如果真的要在两个进程之间共享变量需要怎么办</span><br><span class="line"></span><br><span class="line">队列</span><br><span class="line">这里介绍进程之间的第一种交流方式——队列。multiprocessing模块中提供了multiprocessing.Queue，它和Queue.Queue的区别在于，它里面封装了进程之间的数据交流，不同进程可以操作同一个multiprocessing.Queue。</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Queue</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addone</span><span class="params">(q)</span>:</span></span><br><span class="line">    q.put(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addtwo</span><span class="params">(q)</span>:</span></span><br><span class="line">    q.put(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    q = Queue()</span><br><span class="line">    p1 = Process(target=addone, args = (q, ))</span><br><span class="line">    p2 = Process(target=addtwo, args = (q, ))</span><br><span class="line">    p1.start()</span><br><span class="line">    p2.start()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.join()</span><br><span class="line">    print(q.get())</span><br><span class="line">    print(q.get())</span><br><span class="line">运行结果如下</span><br><span class="line"></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">这个队列是线程、进程安全的，即对队列的每一次修改中间不会被中断从而造成结果错误。</span><br></pre></td></tr></table></figure><h3 id="进程锁"><a href="#进程锁" class="headerlink" title="进程锁"></a>进程锁</h3><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">进程锁</span><br><span class="line">既然变量在进程之间可以共享了，那么同时操作一个变量导致的不安全也随之出现。同多线程一样，进程也是通过锁来解决，而且使用方法都和多线程里相同。</span><br><span class="line"><span class="built_in">lock</span> = multiprocessing.<span class="built_in">Lock</span>()</span><br><span class="line"><span class="built_in">lock</span>.acquire()</span><br><span class="line"><span class="built_in">lock</span>.release()</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">lock</span>:</span><br><span class="line">这些用法和功能都和多线程是一样的</span><br><span class="line">另外，multiprocessing.Semaphore Condition Event RLock也和多线程相同</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python multiprocessing模块多进程详解&lt;/p&gt;
    
    </summary>
    
    
      <category term="多进程" scheme="http://kodgv.xyz/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>CNN</title>
    <link href="http://kodgv.xyz/2019/04/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN/"/>
    <id>http://kodgv.xyz/2019/04/17/神经网络/CNN/</id>
    <published>2019-04-17T02:32:46.000Z</published>
    <updated>2019-04-27T14:57:16.609Z</updated>
    
    <content type="html"><![CDATA[<p>CNN学习</p><a id="more"></a><p>[TOC]</p><h1 id="CNN结构基础"><a href="#CNN结构基础" class="headerlink" title="CNN结构基础"></a>CNN结构基础</h1><p>首先，提出这样一个问题：识别一幅图片是包含有字母”X”还是字母”O”？</p><p>为了帮助指导你理解卷积神经网络，我们讲采用一个非常简化的例子：确定一幅图像是包含有”X”还是”O”？<br><img src="/2019/04/17/神经网络/CNN/p1" alt="img"><br>这个例子足够说明CNN背后的原理，同时它足够简单，能够避免陷入不必要的细节。</p><p>在CNN中有这样一个问题，就是每次给你一张图，你需要判断它是否含有”X”或者”O”。并且假设必须两者选其一，不是”X”就是”O”。理想的情况就像下面这个样子：<br><img src="/2019/04/17/神经网络/CNN/p2" alt="img"><br>标准的”X”和”O”，字母位于图像的正中央，并且比例合适，无变形</p><p>对于计算机来说，只要图像稍稍有一点变化，不是标准的，那么要解决这个问题还是不是那么容易的：<br><img src="/2019/04/17/神经网络/CNN/p3" alt="img"></p><p>计算机要解决上面这个问题，一个比较天真的做法就是先保存一张”X”和”O”的标准图像（就像前面给出的例子），然后将其他的新给出的图像来和这两张标准图像进行对比，看看到底和哪一张图更匹配，就判断为哪个字母。</p><p>但是这么做的话，其实是非常不可靠的，因为计算机还是比较死板的。在计算机的“视觉”中，一幅图看起来就像是一个二维的像素数组（可以想象成一个棋盘），每一个位置对应一个数字。在我们这个例子当中，像素值”1”代表白色，像素值”-1”代表黑色。<br><img src="/2019/04/17/神经网络/CNN/p4" alt="img"></p><p>当比较两幅图的时候，如果有任何一个像素值不匹配，那么这两幅图就不匹配，至少对于计算机来说是这样的。</p><p>对于这个例子，计算机认为上述两幅图中的白色像素除了中间的3*3的小方格里面是相同的，其他四个角上都不同：<br><img src="/2019/04/17/神经网络/CNN/p5" alt="img"></p><p>因此，从表面上看，计算机判别右边那幅图不是”X”，两幅图不同，得出结论：<br><img src="/2019/04/17/神经网络/CNN/p6" alt="img"></p><p>但是这么做，显得太不合理了。理想的情况下，我们希望，对于那些仅仅只是做了一些像平移，缩放，旋转，微变形等简单变换的图像，计算机仍然能够识别出图中的”X”和”O”。就像下面这些情况，我们希望计算机依然能够很快并且很准的识别出来：<br><img src="/2019/04/17/神经网络/CNN/p7" alt="img"></p><p>这也就是CNN出现所要解决的问题。</p><p>Features<br><img src="/2019/04/17/神经网络/CNN/p8" alt="img"></p><p>对于CNN来说，它是一块一块地来进行比对。它拿来比对的这个“小块”我们称之为Features（特征）。在两幅图中大致相同的位置找到一些粗糙的特征进行匹配，CNN能够更好的看到两幅图的相似性，相比起传统的整幅图逐一比对的方法。</p><p>每一个feature就像是一个小图（就是一个比较小的有值的二维数组）。不同的Feature匹配图像中不同的特征。在字母”X”的例子中，那些由对角线和交叉线组成的features基本上能够识别出大多数”X”所具有的重要特征。<br><img src="/2019/04/17/神经网络/CNN/p" alt="img"></p><p>这些features很有可能就是匹配任何含有字母”X”的图中字母X的四个角和它的中心。那么具体到底是怎么匹配的呢？如下：<br><img src="/2019/04/17/神经网络/CNN/p9" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p10" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p11" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p12" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p13" alt="img"></p><p>看到这里是不是有了一点头目呢。但其实这只是第一步，你知道了这些Features是怎么在原图上面进行匹配的。但是你还不知道在这里面究竟进行的是怎样的数学计算，比如这个下面3*3的小块到底干了什么？<br><img src="/2019/04/17/神经网络/CNN/p14" alt="img"><br>接下来就跟进介绍里面的数学操作，也就是我们常说的“卷积”操作。</p><p>卷积(Convolution)<br><img src="/2019/04/17/神经网络/CNN/p15" alt="img"><br>Convolution</p><p><img src="/2019/04/17/神经网络/CNN/p16" alt="img"><br>当给你一张新的图时，CNN并不能准确地知道这些features到底要匹配原图的哪些部分，所以它会在原图中每一个可能的位置进行尝试。这样在原始整幅图上每一个位置进行匹配计算，我们相当于<strong>把这个feature变成了一个过滤器</strong>。这个我们用来匹配的过程就被称为卷积操作，这也就是卷积神经网络名字的由来。</p><p>这个卷积操作背后的数学知识其实非常的简单。要计算一个feature和其在原图上对应的某一小块的结果，只需要简单地将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可。</p><p>如果两个像素点都是白色（也就是值均为1），那么1<em>1 = 1，如果均为黑色，那么(-1)</em>(-1) = 1。不管哪种情况，每一对能够匹配上的像素，其相乘结果为1。类似地，任何不匹配的像素相乘结果为-1。如果一个feature（比如n<em>n）内部所有的像素都和原图中对应一小块（n</em>n）匹配上了，那么它们对应像素值相乘再累加就等于n2，然后除以像素点总个数n2，结果就是1。同理，如果每一个像素都不匹配，那么结果就是-1。</p><p>具体过程如下：</p><p><img src="/2019/04/17/神经网络/CNN/p17" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p18" alt="img"><img src="https://img-blog.csdn.net/2018030618132177" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p19" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p20" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p21" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p22" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p23" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p24" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p25" alt="img"></p><p>对于中间部分，也是一样的操作：</p><p><img src="/2019/04/17/神经网络/CNN/p26" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p27" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p28" alt="img"><br><img src="https://img-blog.csdn.net/20180306181612616" alt="img"><br>最后整张图算完，大概就像下面这个样子：<br><img src="/2019/04/17/神经网络/CNN/p29" alt="img"><br>然后换用其他feature进行同样的操作，最后得到的结果就是这样了：<br><img src="/2019/04/17/神经网络/CNN/p30" alt="img"><br>为了完成我们的卷积，我们不断地重复着上述过程，将feature和图中每一块进行卷积操作。最后通过每一个feature的卷积操作，我们会得到一个新的二维数组。</p><p>这也可以理解为对原始图像进行过滤的结果，我们称之为feature map，它是每一个feature从原始图像中提取出来的“特征”。其中的值，越接近为1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。<br><img src="/2019/04/17/神经网络/CNN/p31" alt="img"><br>这样我们的原始图，经过不同feature的卷积操作就变成了一系列的feature map。我们可以很方便，直观地将这整个操作视为一个单独的处理过程。在CNN中，我们称之为卷积层(convolution layer)，这样你可能很快就会想到后面肯定还有其他的layer。没错，后面会提到。</p><p>我们可以将卷积层看成下面这个样子：<br><img src="/2019/04/17/神经网络/CNN/p32" alt="img"><br>因此可想而知，CNN其实做的操作也没什么复杂的。但是尽管我们能够以这一点篇幅就描述了CNN的工作，其内部的加法，乘法和除法操作的次数其实会增加地很快。从数学的角度来说，它们会随着图像的大小，每一个filter的大小和filter的数目呈线性增长。由于有这么多因素的影响，很容易使得这个问题的计算量变得相当的庞大，这也难怪很多微处理器制造商现在都在生产制造专业的芯片来跟上CNN计算的需求。</p><p>池化(Pooling)<br><img src="/2019/04/17/神经网络/CNN/p33" alt="img"><br>Pooling</p><p>CNN中使用的另一个有效的工具被称为“池化(Pooling)”。池化可以将一幅大的图像缩小，同时又保留其中的重要信息。池化背后的数学顶多也就是小学二年级水平。它就是将输入图像进行缩小，减少像素信息，只保留重要信息。通常情况下，池化都是2<em>2大小，比如对于max-pooling来说，就是取输入图像中2</em>2大小的块中的最大值，作为结果的像素值，相当于将原始图像缩小了4倍(注：同理，对于average-pooling来说，就是取2*2大小块的平均值作为结果的像素值)。</p><p>对于本文的这个例子，池化操作具体如下：</p><p><img src="/2019/04/17/神经网络/CNN/p34" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p35" alt="img"><br><img src="/2019/04/17/神经网络/CNN/p37" alt="img"></p><p>不足的外面补”0”：</p><p><img src="/2019/04/17/神经网络/CNN/p36" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p38" alt="img"><br>经过最大池化操作（比如2*2大小）之后，一幅图就缩小为原来的四分之一了：<br><img src="/2019/04/17/神经网络/CNN/p39" alt="img"><br>然后对所有的feature map执行同样的操作，得到如下结果：<br><img src="/2019/04/17/神经网络/CNN/p40" alt="img"><br>因为最大池化（max-pooling）保留了每一个小块内的最大值，所以它相当于保留了这一块最佳的匹配结果（因为值越接近1表示匹配越好）。这也就意味着它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。这也就能够看出，CNN能够发现图像中是否具有某种特征，而不用在意到底在哪里具有这种特征。这也就能够帮助解决之前提到的计算机逐一像素匹配的死板做法。</p><p>当对所有的feature map执行池化操作之后，相当于一系列输入的大图变成了一系列小图。同样地，我们可以将这整个操作看作是一个操作，这也就是CNN中的池化层(pooling layer)，如下：<br><img src="/2019/04/17/神经网络/CNN/p41" alt="img"><br>通过加入池化层，可以很大程度上减少计算量，降低机器负载。</p><p>Normalization<br>激活函数Relu (Rectified Linear Units)<br>这是一个很小但是很重要的操作，叫做Relu(Rectified Linear Units)，或者修正线性单元。它的数学公式也很简单:<br>f(x) = max(0, x)</p><p>对于输入的负值，输出全为0，对于正值，原样输出。关于其功能，更多详见这里。</p><p>下面我们看一下本文的离例子中relu激活函数具体操作：<br><img src="/2019/04/17/神经网络/CNN/p42" alt="img"><br><img src="/2019/04/17/神经网络/CNN/p43" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p44" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p45" alt="img"></p><p>最后，对整幅图操作之后，结果如下：<br><img src="/2019/04/17/神经网络/CNN/p46" alt="img"><br>同样地，在CNN中，我们这一系列操作视为一个操作，那么就得到Relu Layer，如下：<br><img src="/2019/04/17/神经网络/CNN/p47" alt="img"><br>Deep Learning<br>最后，我们将上面所提到的卷积，池化，激活放在一起，就是下面这个样子：<br><img src="/2019/04/17/神经网络/CNN/p48" alt="img"><br>然后，我们加大网络的深度，增加更多的层，就得到深度神经网络了：<br><img src="/2019/04/17/神经网络/CNN/p49" alt="img"><br>然后在不同的层，我们进行可视化，就可以看到本文开头提到的先验知识里面的结果了：<br><img src="/2019/04/17/神经网络/CNN/p50" alt="img"><br>全连接层(Fully connected layers)<br><img src="/2019/04/17/神经网络/CNN/p51" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p52" alt="img"><img src="/2019/04/17/神经网络/CNN/p53" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p54" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p55" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p56" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p57" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p58" alt="img"></p><p>根据结果判定为”X”：<br><img src="/2019/04/17/神经网络/CNN/p59" alt="img"></p><p>在这个过程中，我们定义这一系列操作为”全连接层“(Fully connected layers)：<br><img src="/2019/04/17/神经网络/CNN/p60" alt="img"><br>全连接层也能够有很多个，如下：<br><img src="/2019/04/17/神经网络/CNN/p61" alt="img"><br>【综合上述所有结构】<br><img src="/2019/04/17/神经网络/CNN/p62" alt="img"></p><h1 id="CNN三大核心思想"><a href="#CNN三大核心思想" class="headerlink" title="CNN三大核心思想"></a><strong>CNN三大核心思想</strong></h1><p>卷积神经网络CNN的出现是为了解决MLP多层感知器全连接和梯度发散的问题。其引入三个核心思想：1.局部感知(local field)，2.权值共享(Shared Weights)，3.下采样(subsampling)。极大地提升了计算速度，减少了连接数量。</p><p><strong>2.1 局部感知</strong></p><p>形象地说，就是模仿你的眼睛，想想看，你在看东西的时候，目光是聚焦在一个相对很小的局部的吧？严格一些说，普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上，而在卷积神经网络中，每个隐层节点只连接到图像某个足够小局部的像素点上，从而大大减少需要训练的权值参数。</p><p>对于一个 1000∗1000 的输入图像而言，如果下一个隐藏层的神经元数目为 106 个，采用全连接则有 1000∗1000∗106=1012 个权值参数，如此数目巨大的参数几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中 10∗10的局部图像相连接，那么此时的权值参数数量为 10∗10∗106=108，将直接减少4个数量级。</p><p>如下图所示，左边是每个像素的全连接，右边是每行隔两个像素作为局部连接，因此在数量上，少了很多权值参数数量（每一条连接每一条线需要有一个权值参数，具体忘记了的可以回顾单个[神经元模型]。因此 局部感知就是：<br>通过卷积操作，把 全连接变成局部连接 ，因为多层网络能够抽取高阶统计特性， 即使网络为局部连接，由于格外的突触连接和额外的神经交互作用，也可以使网络在不十分严格的意义下获得一个全局关系。</p><p><img src="/2019/04/17/神经网络/CNN/p63.jpg" alt="img"></p><p>下面我们来详细看一下到底局部感知如何使 全连接变成局部连接，按照人工神经网络的方法，把输入图像的像素一字排开之后，每一个像素值就是一个神经元输入，需要对隐层或者输出层做全连接，如上图左侧所示。卷积神经网络引入卷积概念后，卷积核通过原图像，然后卷积核对原图像上符合卷积核大小的像素进行加权求和，每一次只是对符合卷积核的图像像素做卷积，这就是局部感知的概念，使 全连接变成局部连接。</p><p><img src="/2019/04/17/神经网络/CNN/p64.png" alt="img"></p><p><strong>2.2 权值共享</strong></p><p>尽管局部感知使计算量减少了几个数量级，但权重参数数量依然很多。能不能再进一步减少呢？方法就是权值共享。</p><p>权值共享：不同的图像或者同一张图像共用一个卷积核，减少重复的卷积核。同一张图像当中可能会出现相同的特征，共享卷积核能够进一步减少权值参数。</p><p>如下图所示，为了找到鸟嘴，一个激活函数A需要检测图像左侧有没有鸟嘴，另外一个激活函数B需要检测另外一张图像中间有没有类似的鸟嘴。其实，鸟嘴都可能具有同样的特征，只需要一个激活函数C就可以了，这个时候，就可以共享同样的权值参数（也就是卷积核）。</p><p><img src="/2019/04/17/神经网络/CNN/p65.png" alt="img"></p><p>如果使用了权值共享（共同使用一个卷积核），那么将可以大大减少卷积核的数量，加快运算速度。天下武功，唯快不破。</p><p><img src="/2019/04/17/神经网络/CNN/p66.png" alt="img"></p><p>举个栗子，在局部连接中隐藏层的每一个神经元连接的是一个 10∗10 的局部图像，因此有 10∗10 个权值参数，将这 10∗10 个权值参数共享给剩下的神经元，也就是说隐藏层中 106 个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10∗10 个权值参数（也就是卷积核(也称滤波器)的大小。</p><p>尽管只有这么少的参数，依旧有出色的性能。但是，这样仅提取了图像的一种特征，如果要多提取出一些特征，需要增加多个卷积核，不同的卷积核能够得到图像的不同映射下的特征，称之为 Feature Map。如果有100个卷积核，最终的权值参数也仅为 100∗100=104 个而已。另外，偏置参数b也是共享的，同一种滤波器共享一个。</p><p><strong>2.3 池化</strong></p><p>在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。</p><p>pooling的好处有什么？<br>\1. 这些统计特征能够有更低的维度，减少计算量。<br>\2. 不容易过拟合，当参数过多的时候很容易造成过度拟合。<br>\3. 缩小图像的规模，提升计算速度。</p><p>如下图所示，原图是一张500∗500 的图像，经过subsampling之后哦，变成了一张 250∗250 的图像。这样操作的好处非常明显，虽然经过权值共享和局部连接后的图像权值参数已经大大减少，但是对于计算量来说，还是非常巨大，需要消费很大的计算时间，于是为了进一步减少计算量，于是加入了subsampling这个概念，不仅仅使图像像素减少了， 同时也减少计算时间。</p><p><img src="/2019/04/17/神经网络/CNN/p67.png" alt="img"></p><p>举个栗子：以最大池化（Max Pooling）为例，1000×1000的图像经过10×10的卷积核卷积后，得到的是991×991的特征图，然后使用2×2的池化规模，即每4个点组成的小方块中，取最大的一个作为输出，最终得到的是496×496大小的特征图。</p><p>下采样，即池化，目的是减小特征图，池化规模一般为2×2。常用的池化方法有：</p><p>Pooling算法</p><p>最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。<br>均值池化（Mean Pooling）。取4个点的均值。<br>可训练池化。训练函数 f ，接受4个点为输入，出入1个点。</p><p>由于特征图的变长不一定是2的倍数，所以在边缘处理上也有两种方案：</p><p>保留边缘。将特征图的变长用0填充为2的倍数，然后再池化。<br>忽略边缘。将多出来的边缘直接省去。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CNN学习&lt;/p&gt;
    
    </summary>
    
      <category term="神经网络" scheme="http://kodgv.xyz/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="网络结构" scheme="http://kodgv.xyz/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Attention</title>
    <link href="http://kodgv.xyz/2019/04/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Attention/"/>
    <id>http://kodgv.xyz/2019/04/15/神经网络/Attention/</id>
    <published>2019-04-15T08:07:35.000Z</published>
    <updated>2019-04-27T15:28:31.924Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]<br>Attention本质就是一系列注意力分配系数，也就是一系列权重参数罢了。<br><a id="more"></a></p><p>来源：<a href="https://www.cnblogs.com/guoyaohua/p/9429924.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/9429924.html</a></p><h1 id="1-什么是Attention机制？"><a href="#1-什么是Attention机制？" class="headerlink" title="1. 什么是Attention机制？"></a>1. 什么是Attention机制？</h1><p>　　最近两年，注意力模型（Attention Model）被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，是深度学习技术中最值得关注与深入了解的核心技术之一。</p><p>　　当我们人在看一样东西的时候，我们当前时刻关注的一定是我们当前正在看的这样东西的某一地方，换句话说，当我们目光移到别处时，注意力随着目光的移动也在转移，这意味着，当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。————-（思考：对于图片，会有些特别显眼的场景会率先吸引住注意力，那是因为脑袋中对这类东西很敏感。对于文本，我们大都是带目的性的去读，顺序查找，顺序读，但是在理解的过程中，我们是根据我们自带的目的去理解，去关注的。 注意力模型应该与具体的目的(或者任务)相结合。）</p><p>　　从Attention的作用角度出发，我们就可以从两个角度来分类Attention种类：<strong>Spatial Attention 空间注意力</strong>和<strong>Temporal Attention 时间注意力</strong>。更具实际的应用，也可以将Attention分为<strong>Soft Attention</strong>和<strong>Hard Attention</strong>。<strong>Soft Attention是所有的数据都会注意，都会计算出相应的注意力权值，不会设置筛选条件。Hard Attention会在生成注意力权重后筛选掉一部分不符合条件的注意力，让它的注意力权值为0，即可以理解为不再注意这些不符合条件的部分。</strong></p><h1 id="2-先了解编码-解码框架：Encoder-Decoder框架"><a href="#2-先了解编码-解码框架：Encoder-Decoder框架" class="headerlink" title="2. 先了解编码-解码框架：Encoder-Decoder框架"></a>2. 先了解编码-解码框架：Encoder-Decoder框架</h1><p>　　目前绝大多数文献中出现的AM模型是附着在Encoder-Decoder框架下的，当然，其实AM模型可以看作一种通用的思想，本身并不依赖于Encoder-Decoder模型，这点需要注意。<strong>Encoder-Decoder框架可以看作是一种文本处理领域的研究模式</strong>，应用场景异常广泛，本身就值得细谈。</p><p><img src="/2019/04/15/神经网络/Attention/p1.png" alt="img"></p><p>图1 抽象的Encoder-Decoder框架</p><p>　　Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对<x,y>。 ————（思考：<x,y>对很通用，X是一个问句，Y是答案；X是一个句子，Y是抽取的关系三元组；X是汉语句子，Y是汉语句子的英文翻译。等等），我们的目标是给定输入句子X，期待通过Encoder-Decoder框架来生成目标句子Y。X和Y可以是同一种语言，也可以是两种不同的语言。而X和Y分别由各自的单词序列构成：</x,y></x,y></p><p><img src="/2019/04/15/神经网络/Attention/f1.png" alt="img"></p><p>　　Encoder顾名思义就是对输入句子X进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p><p><img src="/2019/04/15/神经网络/Attention/f2.png" alt="img"></p><p>　　对于解码器Decoder来说，其任务是根据句子X的中间语义表示C和之前已经生成的历史信息y1,y2….yi-1来生成i时刻要生成的单词yi ：</p><p><img src="/2019/04/15/神经网络/Attention/f3.png" alt="img"></p><p>　　每个yi都依次这么产生，那么看起来就是整个系统根据输入句子X生成了目标句子Y。 ———（思考：其实这里的Encoder-Decoder是一个序列到序列的模型seq2seq，这个模型是对顺序有依赖的。）</p><p>　　Encoder-Decoder是个非常通用的计算框架，至于Encoder和Decoder具体使用什么模型都是由研究者自己定的，常见的比如 CNN / RNN / BiRNN / GRU / LSTM / Deep LSTM 等，这里的变化组合非常多。</p><hr><p>（思考：人的学习过程包括输入、输出、外界评价。Encoder模型类似于人的输入学习过程，Decoder模型类似于人的输出学习过程，对输出的内容进行评价就类似于损失函数。英语老师给我上了几堂英语课，我在不断的输入Encoder；突然有一个随堂测试，我得做题输出Decoder；最后英语老师改卷子，给我一个分数，不对的地方我得反思调整我对输入数据的加工方式。）</p><hr><p>（再思考：关于英语翻译。课本上的单词和课文是原始数据输入，相当于X；我在大脑里加工这些数据，相当于Encoder模型，我的脑子里有很多加工后的数据，相当于C；现在要让我翻译一个英语句子，这个任务相当于Y，我不能翻课本，所以我只能借助我脑袋里加工的数据C去翻译这个句子，即我得动脑子，相当于Decoder。 学习的过程是什么都要学，要分类整理，要增加线索，并不知道未来的某天能用到什么，所以Encoder-Decoder是一个泛泛学习的框架）</p><h1 id="3-Attention-Model"><a href="#3-Attention-Model" class="headerlink" title="3. Attention Model"></a>3. Attention Model</h1><p>　　以上介绍的Encoder-Decoder模型是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Y中每个单词的生成过程如下：</p><p><img src="/2019/04/15/神经网络/Attention/f4.png" alt="img"></p><p>　　其中<strong>f是decoder的非线性变换函数</strong>。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，是y1,y2也好，还是y3也好，他们使用的句子X的语义编码C都是一样的，没有任何区别。而语义编码C是由句子X的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实<strong>句子X中任意单词对生成某个目标单词yi来说影响力都是相同的，没有任何区别</strong>（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，估计这也是为何Google提出Sequence to Sequence模型时发现把输入句子逆序输入做翻译效果会更好的小Trick的原因）。这就是为何说这个模型<strong>没有体现出注意力</strong>的缘由。</p><p>　　引入AM模型，以翻译一个英语句子举例：输入X：Tom chase Jerry。 理想输出：汤姆追逐杰瑞。</p><p>　　应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：</p><p>（Tom,0.3）（Chase,0.2）（Jerry,0.5）</p><p>　　每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词Yi的时候，<strong>原先都是相同的中间语义表示C会替换成根据当前生成单词而不断变化的Ci。理解AM模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci</strong>。</p><p><img src="/2019/04/15/神经网络/Attention/p2.png" alt="img"></p><p>图2 引入AM模型的Encoder-Decoder框架</p><p>　　即生成目标句子单词的过程成了下面的形式：</p><p><img src="/2019/04/15/神经网络/Attention/p3.png" alt="img"></p><p>　　而<strong>每个Ci可能对应着不同的源语句子单词的注意力分配概率分布</strong>，比如对于上面的英汉翻译来说，其对应的信息可能如下：</p><p><img src="/2019/04/15/神经网络/Attention/p4.png" alt="img"></p><p>　　其中，<strong>f2函数代表Encoder对输入英文单词的某种变换函数</strong>，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；<strong>g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数</strong>，一般的做法中，<strong>g函数就是对构成元素加权求和</strong>，也就是常常在论文里看到的下列公式：</p><p><img src="/2019/04/15/神经网络/Attention/p5.png" alt="img"></p><p>　　假设Ci中那个i就是上面的“汤姆”，那么<strong>Tx就是3，代表输入句子的长度</strong>，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)，对应的注意力模型权值分别是0.6,0.2,0.2，<strong>所以g函数就是个加权求和函数</strong>。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的<strong>中间语义表示Ci的形成过程</strong>类似下图：</p><p><img src="/2019/04/15/神经网络/Attention/p6.png" alt="img"></p><p>图3 Ci的形成过程</p><p>　　这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，<strong>你怎么知道AM模型所需要的输入句子单词注意力分配概率分布值呢？</strong>就是说“汤姆”对应的概率分布：</p><p>划重点(注意力权重获取的过程)（Tom,0.3）（Chase,0.2）（Jerry,0.5）是如何得到的呢？</p><p>　　为了便于说明，我们假设对图1的非AM模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图1的图转换为下图：</p><p><img src="/2019/04/15/神经网络/Attention/p7.png" alt="img"></p><p>图4 RNN作为具体模型的Encoder-Decoder框架</p><p>　　注意力分配概率分布值的通用计算过程：</p><p><img src="/2019/04/15/神经网络/Attention/p8.png" alt="img"></p><p>图5 AM注意力分配概率计算</p><p>　　对于采用RNN的Decoder来说，如果要生成 yi 单词，在时刻 i ，我们是可以知道在生成 Yi 之前的隐层节点i时刻的输出值 Hi 的，而我们的目的是要计算生成 Yi 时的输入句子单词“Tom”、“Chase”、“Jerry”对 Yi 来说的注意力分配概率分布，那么可以用i时刻的<strong>隐层节点状态 Hi 去一一和输入句子中每个单词对应的RNN隐层节点状态 hj 进行对比</strong>，即<strong>通过函数 F(hj,Hi) 来获得目标单词 Yi 和每个输入单词对应的对齐可能性</strong>，这个F函数在不同论文里可能会采取不同的方法，然后<strong>函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值（这就得到了注意力权重）</strong>。图5显示的是当输出单词为“汤姆”时刻对应的输入句子单词的对齐概率。绝大多数AM模型都是<strong>采取上述的计算框架来计算注意力分配概率分布信息</strong>，<strong>区别只是在F的定义上可能有所不同</strong>。</p><p>　　<strong>上述内容就是论文里面常常提到的Soft Attention Model</strong>（任何数据都会给一个权值，没有筛选条件）的基本思想，你能在文献里面看到的大多数AM模型基本就是这个模型，区别很可能只是把这个模型用来解决不同的应用问题。那么<strong>怎么理解AM模型的物理含义呢？</strong>一般文献里会把AM模型看作是<strong>单词对齐模型</strong>，这是非常有道理的。目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的<strong>对齐概率</strong>，这在机器翻译语境下是非常直观的：<strong>传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤</strong>，<strong>而注意力模型其实起的是相同的作用</strong>。在其他应用里面把AM模型理解成输入句子和目标句子单词之间的对齐概率也是很顺畅的想法。</p><p><img src="/2019/04/15/神经网络/Attention/p9.png" alt="img"></p><p>图6 Google 神经网络机器翻译系统结构图</p><p>　　图6所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。</p><p>当然，从概念上理解的话，<strong>把AM模型理解成影响力模型也是合理的</strong>，就是说生成目标单词的时候，输入句子每个单词对于生成这个单词有多大的影响程度。这种想法也是比较好理解AM模型物理意义的一种思维方式。</p><p>　　图7是论文“<a href="http://www.aclweb.org/anthology/D15-1044" target="_blank" rel="noopener">A Neural Attention Model for Sentence Summarization</a>”中，Rush用AM模型来做生成式摘要给出的一个AM的一个非常直观的例子。</p><p><img src="/2019/04/15/神经网络/Attention/怕0.png" alt="img"></p><p>图7 句子生成式摘要例子</p><p>　　这个例子中，Encoder-Decoder框架的输入句子X是：“russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism”。对应图中纵坐标的句子。系统生成的摘要句子Y是：“russia calls for joint front against terrorism”，对应图中横坐标的句子。可以看出模型已经把句子主体部分正确地抽出来了。<strong>矩阵中每一列代表生成的目标单词对应输入句子每个单词的AM分配概率</strong>，颜色越深代表分配到的概率越大。这个例子对于直观理解AM是很有帮助作用。</p><p>　　《<a href="http://www.aclweb.org/anthology/D15-1044" target="_blank" rel="noopener">A Neural Attention Model for Sentence Summarization</a>》论文提供的实验数据集链接(开放可用)：<a href="https://duc.nist.gov/data.html" target="_blank" rel="noopener">DUC 2004</a>，感兴趣的朋友可以下载看看。</p><p><img src="/2019/04/15/神经网络/Attention/p11.png" alt="img"></p><p>图8 摘要生成 开放数据集</p><h1 id="4-Attention机制的本质思想"><a href="#4-Attention机制的本质思想" class="headerlink" title="4. Attention机制的本质思想"></a><strong>4. Attention机制的本质思想</strong></h1><p>　　如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806165033943-1072442256.png" alt="img"></p><p>图9 Attention机制的本质思想</p><p>　　我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的<key,value>数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</key,value></p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806165351485-870137528.png" alt="img"></p><p>　　其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p><p>　　当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p><p>　　从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p><p>　　至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806191525966-820975705.png" alt="img"></p><p>图10 三阶段计算Attention过程</p><p>　　在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个 Keyi ，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806191654338-264846698.png" alt="img"></p><p>　　第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><p>  <img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806195333945-948374778.png" alt="img"></p><p>　　第二阶段的计算结果 ai 即为 Valuei 对应的权重系数，然后进行加权求和即可得到Attention数值：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806195421837-251685236.png" alt="img"></p><p>　　通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p><h1 id="5-Self-Attention模型"><a href="#5-Self-Attention模型" class="headerlink" title="5. Self Attention模型"></a><strong>5. Self Attention模型</strong></h1><p>　　通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。<strong>Self Attention也经常被称为intra Attention（内部Attention）</strong>，最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型(抛弃了传统的RNN)。</p><p>　　在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素和Source中的所有元素之间。<strong>而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。</strong>其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。</p><p>　　elf Attention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention不同，它分别在source端和target端进行，仅与source input或者target input自身相关的Self Attention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self Attention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。因此，self Attention Attention比传统的Attention mechanism效果要好，主要原因之一是，传统的Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806200854591-1266493040.png" alt="img"></p><p>图11 可视化Self Attention实例</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806200949175-1322518214.png" alt="img"></p><p>图12 可视化Self Attention实例</p><p>　　从两张图（图11、图12）可以看出，<strong>Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。</strong></p><p>　　很明显，<strong>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征</strong>，<strong>因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</strong></p><p>　　但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以<strong>远距离依赖特征之间的距离被极大缩短</strong>，有利于有效地利用这些特征。除此外，<strong>Self Attention对于增加计算的并行性也有直接帮助作用</strong>。这是为何Self Attention逐渐被广泛使用的主要原因。</p><p>​    <strong>但是attention无法记录词序，所以在self-attention中增加了position embedding</strong></p><h1 id="五种attention模型"><a href="#五种attention模型" class="headerlink" title="五种attention模型"></a>五种attention模型</h1><h2 id="hard-attention-amp-soft-attention"><a href="#hard-attention-amp-soft-attention" class="headerlink" title="hard attention&amp;soft attention"></a>hard attention&amp;soft attention</h2><p><img src="https://image.jiqizhixin.com/uploads/editor/72f889a2-df04-4a85-9c2c-1591c5375537/1528709501819.png" alt="img"></p><p>■ 论文 | Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/812" target="_blank" rel="noopener">https://www.paperweekly.site/papers/812</a></p><p>■ 源码 | <a href="https://github.com/kelvinxu/arctic-captions" target="_blank" rel="noopener">https://github.com/kelvinxu/arctic-captions</a></p><p>文章讨论的场景是图像描述生成（Image Caption Generation），对于这种场景，先放一张图，感受一下 attention 的框架。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/33073f25-48d8-4b17-a179-49191bdb5357/1528709501703.png" alt="img"></p><p>文章提出了两种 attention 模式，即 hard attention 和 soft attention，来感受一下这两种 attention。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/fe4ede76-44de-4055-93e6-3ce99541db21/1528709502108.png" alt="img"></p><p>可以看到，hard attention 会专注于很小的区域，而 soft attention 的注意力相对发散。模型的 encoder 利用 CNN (VGG net)，提取出图像的 L 个 D 维的向量<img src="https://image.jiqizhixin.com/uploads/editor/7a76e6b0-4f08-4f55-8b01-839552bd0de1/1528709501860.png" alt="img">，每个向量表示图像的一部分信息。</p><p>decoder 是一个 LSTM，每个 timestep t 的输入包含三个部分，即 context vector Zt 、前一个 timestep 的 hidden state<img src="https://image.jiqizhixin.com/uploads/editor/a498b30a-f97c-483d-b121-1499bcabe34f/1528709501897.png" alt="img">、前一个 timestep 的 output<img src="https://image.jiqizhixin.com/uploads/editor/89174d88-3afc-4554-ba26-b48065473355/1528709502635.png" alt="img">。 Zt 由 {ai} 和权重 {αti} 通过加权得到。这里的权重 αti 通过attention模型 <em>f</em>att 来计算得到，而本文中的 <em>f</em>att 是一个多层感知机（multilayer perceptron）。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/55f3f8bd-91c7-4d65-917d-292e51d2962f/1528709502715.png" alt="img"></p><p>从而可以计算<img src="https://image.jiqizhixin.com/uploads/editor/b57ff198-4b7c-4e6d-a534-4ca57b825d3b/1528709502754.png" alt="img">。接下来文章重点讨论 hard（也叫 stochastic attention）和 soft（也叫 deterministic）两种 attention 模式。</p><p><strong>1. Stochastic “Hard” Attention</strong> </p><p>记 St 为 decoder 第 t 个时刻的 attention 所关注的位置编号， Sti 表示第 t 时刻 attention 是否关注位置 i ， Sti 服从多元伯努利分布（multinoulli distribution）， 对于任意的 t ，Sti,i=1,2,…,L 中有且只有取 1，其余全部为 0，所以 [St1,St2,…,stL] 是 one-hot 形式。这种 attention 每次只 focus 一个位置的做法，就是“hard”称谓的来源。 Zt 也就被视为一个变量，计算如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/afec742f-d647-43c6-b4dc-9618ac878628/1528709502797.png" alt="img"></p><p>问题是 αti 怎么算呢？把 αti 视为隐变量，研究模型的目标函数，进而研究目标函数对参数的梯度。直观理解，模型要根据 a=(a1,…,aL) 来生成序列 y=(y1,…,yC) ，所以目标可以是最大化 log p(y|a) ，但这里没有显式的包含 s ，所以作者利用著名的 Jensen 不等式（Jensen’s inequality）对目标函数做了转化，得到了目标函数的一个 lower bound，如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/97d7901e-3b6d-4fbc-879a-d81293368368/1528709503022.png" alt="img"></p><p>这里的 s ={ s1,…,sC }，是时间轴上的重点 focus 的序列，理论上这种序列共有个。 然后就用 log p(y|a) 代替原始的目标函数，对模型的参数 W 算 gradient。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/0f5a2605-cea7-4530-8415-e9d3dfb1d3b5/1528709503215.png" alt="img"></p><p>然后利用蒙特卡洛方法对 s 进行抽样，我们做 N 次这样的抽样实验，记每次取到的序列是<img src="https://image.jiqizhixin.com/uploads/editor/8b676888-0fab-4a40-9706-d829823fce0c/1528709503296.png" alt="img">，易知<img src="https://image.jiqizhixin.com/uploads/editor/1adc9914-c26a-47d7-a3e7-3889f9090160/1528709503251.png" alt="img">的概率为<img src="https://image.jiqizhixin.com/uploads/editor/5d82f812-b08a-48fd-bdb9-575fe66c15ce/1528709503333.png" alt="img">，所以上面的求 gradient 的结果即为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/5c7be181-dfdc-4458-b477-765b1613b234/1528709503396.png" alt="img"></p><p>接下来的一些细节涉及reinforcement learning，感兴趣的同学可以去看这篇 paper。</p><p><strong>2. Deterministic “Soft” Attention</strong> </p><p>说完“硬”的 attention，再来说说“软”的 attention。 相对来说 soft attention 很好理解，在 hard attention 里面，每个时刻 t 模型的序列 [ St1,…,StL ] 只有一个取 1，其余全部为 0，也就是说每次只 focus 一个位置，而 soft attention 每次会照顾到全部的位置，只是不同位置的权重不同罢了。这时 Zt 即为 ai 的加权求和：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/419b690d-2d19-4506-8f2e-67edf525fa7c/1528709503873.png" alt="img"></p><p>这样 soft attention 是光滑的且可微的（即目标函数，也就是 LSTM 的目标函数对权重αti 是可微的，原因很简单，因为目标函数对 Zt 可微，而 Zt 对 αti 可微，根据 chain rule 可得目标函数对 αti 可微）。</p><p>文章还对这种 soft attention 做了微调：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/caa7b6b6-ce70-4e19-aa1b-8a77a7914c09/1528709503914.png" alt="img"></p><p>其中<img src="https://image.jiqizhixin.com/uploads/editor/7de8d1a3-3be6-4a66-801e-25cb0bee7b00/1528709504003.png" alt="img">，用来调节 context vector 在 LSTM 中的比重（相对于<img src="https://image.jiqizhixin.com/uploads/editor/ae0fcc90-f8ef-40d8-be2a-8f9721dae24d/1528709501949.png" alt="img"><img src="https://image.jiqizhixin.com/uploads/editor/90abe04d-a4ea-4f7b-9d6b-d5832ad531f1/1528709502672.png" alt="img">的比重）。</p><p>btw，模型的 loss function 加入了 αti 的正则项。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/3193dce5-51b7-4ae2-893d-0696635c9ec9/1528709504051.png" alt="img"></p><h2 id="global-attention-amp-local-attention"><a href="#global-attention-amp-local-attention" class="headerlink" title="global attention &amp; local attention"></a>global attention &amp; local attention</h2><p><img src="https://image.jiqizhixin.com/uploads/editor/e3a3408b-7e64-4752-b8f9-a71563bb06c6/1528709504652.png" alt="img"></p><p>■ 论文 | Effective Approaches to Attention-based Neural Machine Translation</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/806" target="_blank" rel="noopener">https://www.paperweekly.site/papers/806</a></p><p>■ 源码 | <a href="https://github.com/lmthang/nmt.matlab" target="_blank" rel="noopener">https://github.com/lmthang/nmt.matlab</a></p><p>文章提出了两种 attention 的改进版本，即 global attention 和 local attention。先感受一下 global attention 和 local attention 长什么样子。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/55e64daf-5e26-474f-bdea-b99669de205c/1528709504213.png" alt="img"></p><p><strong>▲</strong> Global Attention</p><p><img src="https://image.jiqizhixin.com/uploads/editor/4df6785f-b67e-4a41-9234-401d03c07b59/1528709504569.png" alt="img"></p><p><strong>▲</strong> Local Attention</p><p>文章指出，local attention 可以视为 hard attention 和 soft attention 的混合体（优势上的混合），因为它的计算复杂度要低于 global attention、soft attention，而且与 hard attention 不同的是，local attention 几乎处处可微，易与训练。 文章以机器翻译为场景， x1,…,xn 为 source sentence， y1,…,ym 为 target sentence， c1,…,cm 为 encoder 产生的 context vector，objective function 为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/129e5871-39db-4005-8062-ca744a216335/1528709504768.png" alt="img"></p><p>Ct 来源于 encoder 中多个 source position 所产生的 hidden states，global attention 和 local attention 的主要区别在于 attention 所 forcus 的 source positions 数目的不同：如果 attention forcus 全部的 position，则是 global attention，反之，若只 focus 一部分 position，则为 local attention。 </p><p>由此可见，这里的 global attention、local attention 和 soft attention 并无本质上的区别，两篇 paper 模型的差别只是在 LSTM 结构上有微小的差别。 </p><p>在 decoder 的时刻 t ，在利用 global attention 或 local attention 得到 context vector Ct之后，结合 ht ，对二者做 concatenate 操作，得到 attention hidden state。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/7acdb6ad-0741-4531-a96a-a9e128afda32/1528709505007.png" alt="img"></p><p>最后利用 softmax 产出该时刻的输出：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/078f1af3-0c67-4b13-9359-14f1b48aea4d/1528709505051.png" alt="img"></p><p>下面重点介绍 global attention、local attention。</p><p><strong>1. global attention</strong> </p><p>global attention 在计算 context vector ct 的时候会考虑 encoder 所产生的全部hidden state。记 decoder 时刻 t 的 target hidden为 ht，encoder 的全部 hidden state 为<img src="https://image.jiqizhixin.com/uploads/editor/1a9b3f6f-cd94-4ba5-94d9-96e096dd0802/1528709505151.png" alt="img">，对于其中任意<img src="https://image.jiqizhixin.com/uploads/editor/cab720ef-628c-4af9-b827-f2839ba387a5/1528709505228.png" alt="img">，其权重 αts 为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/ebea9e73-7f59-42a1-975b-9cd9f44ee7ba/1528709505312.png" alt="img"></p><p>而其中的<img src="https://image.jiqizhixin.com/uploads/editor/31b5d177-4123-47f7-b75a-766690049b6f/1528709505690.png" alt="img">，文章给出了四种种计算方法（文章称为 alignment function）：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/5b89594b-f47c-4870-be8a-f3406b38088b/1528709505811.png" alt="img"></p><p><img src="https://image.jiqizhixin.com/uploads/editor/1f8e7063-9c3f-4c96-a528-a841c6117825/1528709505852.png" alt="img"></p><p>四种方法都比较直观、简单。在得到这些权重后， ct 的计算是很自然的，即为<img src="https://image.jiqizhixin.com/uploads/editor/1b37607a-9316-442c-9226-49595d34ff02/1528709505264.png" alt="img">的 weighted summation。</p><p><strong>2. local attention</strong> </p><p>global attention 可能的缺点在于每次都要扫描全部的 source hidden state，计算开销较大，对于长句翻译不利，为了提升效率，提出 local attention，每次只 focus 一小部分的 source position。 </p><p>这里，context vector ct 的计算只 focus 窗口 [pt-D,pt+D] 内的 2D+1 个source hidden states（若发生越界，则忽略界外的 source hidden states）。</p><p>其中 pt 是一个 source position index，可以理解为 attention 的“焦点”，作为模型的参数， D 根据经验来选择（文章选用 10）。 关于 pt 的计算，文章给出了两种计算方案：</p><ul><li><strong>Monotonic alignment (local-m)</strong></li></ul><p><img src="https://image.jiqizhixin.com/uploads/editor/4fe24e96-b61e-4bb4-9cb6-88760dc0f228/1528709505970.png" alt="img"></p><ul><li><strong>Predictive alignment (local-p)</strong></li></ul><p><img src="https://image.jiqizhixin.com/uploads/editor/ec31a794-00f9-4dba-b9d8-89295f1aa1d0/1528709506099.png" alt="img"></p><p>其中 Wp 和 vp 是模型的参数， S 是 source sentence 的长度，易知 pt∈[0,S] 。 权重αt(s) 的计算如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/703a445f-62ed-43dd-a313-256883bac239/1528709506142.png" alt="img"></p><p>可以看出，距离中心 pt 越远的位置，其位置上的 source hidden state 对应的权重就会被压缩地越厉害。</p><h2 id="self-attention-amp-multiple-head-attention"><a href="#self-attention-amp-multiple-head-attention" class="headerlink" title="self-attention &amp; multiple-head attention"></a>self-attention &amp; multiple-head attention</h2><p><img src="https://image.jiqizhixin.com/uploads/editor/d993ae54-145e-4e12-93dd-1f3c94d156f9/1528709506751.png" alt="img"></p><p>■ 论文 | Attention Is All You Need</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/224" target="_blank" rel="noopener">https://www.paperweekly.site/papers/224</a></p><p>■ 源码 | <a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">https://github.com/Kyubyong/transformer</a></p><p><img src="https://image.jiqizhixin.com/uploads/editor/da5cb6b1-aa95-48b4-a728-01bde8de45b4/1528709506930.png" alt="img"></p><p>■ 论文 | Weighted Transformer Network for Machine Translation</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/2013" target="_blank" rel="noopener">https://www.paperweekly.site/papers/2013</a></p><p>■ 源码 | <a href="https://github.com/JayParks/transformer" target="_blank" rel="noopener">https://github.com/JayParks/transformer</a></p><p>作者首先指出，结合了 RNN（及其变体）和注意力机制的模型在序列建模领域取得了不错的成绩，但由于 RNN 的循环特性导致其不利于并行计算，所以模型的训练时间往往较长，在 GPU 上一个大一点的 seq2seq 模型通常要跑上几天，所以作者对 RNN 深恶痛绝，遂决定舍弃 RNN，只用注意力模型来进行序列的建模。 </p><p>作者提出一种新型的网络结构，并起了个名字 Transformer，里面所包含的注意力机制称之为 self-attention。作者骄傲地宣称他这套 Transformer 是能够计算 input 和 output 的 representation 而不借助 RNN 的唯一的 model，所以作者说有 attention 就够了。</p><p>模型同样包含 encoder 和 decoder 两个 stage，encoder 和 decoder 都是抛弃 RNN，而是用堆叠起来的 self-attention，和 fully-connected layer 来完成，模型的架构如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/43ca78f9-024c-4a20-aa8c-681a42b6a734/1528709507321.png" alt="img"></p><p>从图中可以看出，模型共包含三个 attention 成分，分别是 encoder 的 self-attention，decoder 的 self-attention，以及连接 encoder 和 decoder 的 attention。  </p><p>这三个 attention block 都是 multi-head attention 的形式，输入都是 query Q 、key K 、value V 三个元素，只是 Q 、 K 、 V 的取值不同罢了。接下来重点讨论最核心的模块 multi-head attention（多头注意力）。 </p><p>multi-head attention 由多个 scaled dot-product attention 这样的基础单元经过 stack 而成。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/2a179490-64ac-4b22-b42d-1c3de5e08b5c/1528709507410.png" alt="img"></p><p>那重点就变成 scaled dot-product attention 是什么鬼了。按字面意思理解，scaled dot-product attention 即缩放了的点乘注意力，我们来对它进行研究。 </p><p>在这之前，我们先回顾一下上文提到的传统的 attention 方法（例如 global attention，score 采用 dot 形式）。</p><p>记 decoder 时刻 t 的 target hidden state 为 ht，encoder 得到的全部 source hidden state为<img src="https://image.jiqizhixin.com/uploads/editor/9f26e09b-d1f7-4e77-b1e8-2e8e5642ae9c/1528709507675.png" alt="img">，则 decoder 的 context vector ct 的计算过程如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/57b33413-699a-4c04-ae99-0bddaf11ea32/1528709507900.png" alt="img"></p><p>作者先抛出三个名词 query Q、key K、value V，然后计算这三个元素的 attention。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/cbffe9ff-5149-4959-ac8b-f6c4714ff9c9/1528709509122.png" alt="img"></p><p>我的写法与论文有细微差别，但为了接下来说明的简便，我姑且简化成这样。这个 Attention 的计算跟上面的 (*) 式有几分相似。</p><p>那么 Q、K、V 到底是什么？论文里讲的比较晦涩，说说我的理解。encoder 里的 attention 叫 self-attention，顾名思义，就是自己和自己做 attention。</p><p>抛开这篇论文的做法，让我们激活自己的创造力，在传统的 seq2seq 中的 encoder 阶段，我们得到 n 个时刻的 hidden states 之后，可以用每一时刻的 hidden state hi，去分别和任意的 hidden state hj,j=1,2,…,n 计算 attention，这就有点 self-attention 的意思。</p><p>回到当前的模型，由于抛弃了 RNN，encoder 过程就没了 hidden states，那拿什么做 self-attention 来自嗨呢？</p><p>可以想到，假如作为 input 的 sequence 共有 n 个 word，那么我可以先对每一个 word 做 embedding 吧？就得到 n 个 embedding，然后我就可以用 embedding 代替 hidden state 来做 self-attention 了。所以 Q 这个矩阵里面装的就是全部的 word embedding，K、V 也是一样。</p><p>所以为什么管 Q 叫query？就是你每次拿一个 word embedding，去“查询”其和任意的 word embedding 的 match 程度（也就是 attention 的大小），你一共要做 n 轮这样的操作。 </p><p>我们记 word embedding 的 dimension 为 dmodel ，所以 Q 的 shape 就是 n*dmodel， K、V 也是一样，第 i 个 word 的 embedding 为 vi，所以该 word 的 attention 应为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/17ebfb62-21b6-4df4-99ea-08d3bf275d36/1528709509168.png" alt="img"></p><p>那同时做全部 word 的 attention，则是：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/529a6cbf-f2fe-4d8c-a90d-f8a0567f6bf8/1528709508901.png" alt="img"></p><p>scaled dot-product attention 基本就是这样了。基于 RNN 的传统 encoder 在每个时刻会有输入和输出，而现在 encoder 由于抛弃了 RNN 序列模型，所以可以一下子把序列的全部内容输进去，来一次 self-attention 的自嗨。</p><p>理解了 scaled dot-product attention 之后，multi-head attention 就好理解了，因为就是 scaled dot-product attention 的 stacking。</p><p>先把 Q、K、V 做 linear transformation，然后对新生成的 Q’、K’、V’ 算 attention，重复这样的操作 h 次，然后把 h 次的结果做 concat，最后再做一次 linear transformation，就是 multi-head attention 这个小 block 的输出了。 </p><p><img src="https://image.jiqizhixin.com/uploads/editor/b39171e2-f71c-4ea9-a9de-0d6c8d983550/1528709508952.png" alt="img"></p><p>以上介绍了 encoder 的 self-attention。decoder 中的 encoder-decoder attention 道理类似，可以理解为用 decoder 中的每个 vi 对 encoder 中的 vj 做一种交叉 attention。</p><p>decoder 中的 self-attention 也一样的道理，只是要注意一点，decoder 中你在用 vi 对 vj 做 attention 时，有一些 pair 是不合法的。原因在于，虽然 encoder 阶段你可以把序列的全部 word 一次全输入进去，但是 decoder 阶段却并不总是可以，想象一下你在做 inference，decoder 的产出还是按从左至右的顺序，所以你的 vi 是没机会和 vj ( j&gt;i ) 做 attention 的。</p><p>那怎么将这一点体现在 attention 的计算中呢？文中说只需要令 score(vi,vj)=-∞ 即可。为何？因为这样的话：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/13be2088-2463-474a-855b-b2193a08ce44/1528709509073.png" alt="img"></p><p>所以在计算 vi 的 self-attention 的时候，就能够把 vj 屏蔽掉。所以这个问题也就解决了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;br&gt;Attention本质就是一系列注意力分配系数，也就是一系列权重参数罢了。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="神经网络" scheme="http://kodgv.xyz/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="网络结构" scheme="http://kodgv.xyz/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>LSTM</title>
    <link href="http://kodgv.xyz/2019/04/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LSTM/"/>
    <id>http://kodgv.xyz/2019/04/15/神经网络/LSTM/</id>
    <published>2019-04-15T02:38:35.000Z</published>
    <updated>2019-04-27T12:41:46.547Z</updated>
    
    <content type="html"><![CDATA[<p>LSTM详解，把cell转变当作传送带的思想</p><a id="more"></a><p>来源：<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p>[TOC]</p><h2 id="‘递归神经网络"><a href="#‘递归神经网络" class="headerlink" title="‘递归神经网络"></a>‘递归神经网络</h2><p>​    人类不会每一秒钟都从头开始思考。当你阅读这篇文章的时候，你理解每一个单词都是基于你对之前单词的理解。你不会把所有的东西都扔掉，重新开始思考。你的想法是有持久性的。<br>​    传统的神经网络做不到这一点，这似乎是一个主要的缺点。例如，假设您想要对电影中每一秒发生的事件进行分类。目前还不清楚传统的神经网络如何利用其对电影中先前事件的推理来为后来的事件提供信息。<br>​    递归神经网络解决了这个问题。它们是包含循环的网络，允许信息持续存在。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" width="200" hegiht="300" align="center/"></p><p>在上面的图中，一个神经网络块a，观察某个输入xt并输出一个值ht。循环允许信息从网络的一个步骤传递到下一个步骤。</p><p>​    这些循环使得递归神经网络看起来有点神秘。然而，如果你多想一下，就会发现它们和普通的神经网络并没有太大的不同。递归神经网络可以看作是同一网络的多个副本，每个副本都向后继网络传递一条消息。考虑一下如果我们展开循环会发生什么:</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" width="500" hegiht="500" align="center/"></p><p>这种链式性质揭示了递归神经网络与序列和列表密切相关。它们是神经网络用来处理这些数据的自然结构。</p><p>​    它们确实被使用了!在过去的几年里，把RNNs应用到各种各样的问题上取得了令人难以置信的成功:语音识别、语言建模、翻译、图像字幕等等。我将把关于RNNs可以实现的惊人壮举的讨论留给Andrej Karpathy的优秀博客文章《循环神经网络的不合理有效性》。但它们真的很神奇。</p><p>​    这些成功的关键是使用“LSTMs”，这是一种非常特殊的递归神经网络，它在许多任务中都比标准版本运行得好得多。几乎所有基于递归神经网络的激动人心的结果都是用它们实现的。本文将探索这些LSTMs。</p><h2 id="长期依赖的问题"><a href="#长期依赖的问题" class="headerlink" title="长期依赖的问题"></a>长期依赖的问题</h2><p>​    RNNs的一个吸引人的地方就是它们能够将以前的信息与现在的任务联系起来，例如使用以前的视频帧可能有助于理解现在的帧。如果RNN能做到这一点，它们将非常有用。但他们能吗?这可能需要视情况而定。</p><p>​    有时候，我们只需要查看最近的信息就可以执行当前的任务。例如，考虑一个语言模型，它试图根据前面的单词预测下一个单词。如果我们试图预测“云在天空中”中的最后一个单词，我们不需要任何进一步的上下文——很明显下一个单词将是天空。在这种情况下，相关信息和需要信息的地方之间的差距很小，RNNs可以学习使用过去的信息。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png" width="400" hegiht="500" align="center/"></p><p>​    但也有一些情况，我们需要更多的上下文。试着预测文本中的最后一个单词“我在法国长大……我说一口流利的法语。”“最近的信息显示，下一个单词很可能是一种语言的名字，但如果我们想缩小范围，我们需要更早的法语语境。”相关信息与需要它的点之间的差距完全有可能变得非常大。<br>不幸的是，随着这种差距的扩大，RNNs无法学会连接信息。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png" width="400" hegiht="500" align="center/"></p><p>​    理论上，RNNs绝对有能力处理这种“长期依赖”。“一个人可以仔细地为他们选择参数来解决这种形式的玩具问题。遗憾的是，在实践中，RNNs似乎不能学习它们。 [Hochreiter (1991) <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener">German</a>和<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a>对这个问题进行了深入研究，他们发现了一些非常基本的原因，解释了为什么这个问题可能很难。</p><h2 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h2><p>​    Long Short Term Memory networks通常被称为“LSTMs”，是一种特殊的RNN，能够学习长期依赖关系。它们由<a href="http://www.bioinf.jku.at/publications/older/2604.pdf" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber (1997)</a>引入，并在随后的工作中被许多人提炼和推广。他们在各种各样的问题上都做得非常好，现在被广泛使用。</p><p>​    LSTMs的设计是为了避免长期依赖问题。长时间记住信息实际上是他们的默认行为，而不是他们努力学习的东西!</p><p>​    所有的递归神经网络都具有一串重复的神经网络模块的形式。在标准的RNNs中，这个重复模块有一个非常简单的结构，比如一个tanh层。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png" width="400" hegiht="500" align="center/"></p><p>LSTMs也有类似链的结构，但是重复模块有不同的结构。不是只有一个神经网络层，而是有四个，以一种非常特殊的方式相互作用。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" width="400" hegiht="500" align="center/"></p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png" alt="img"></p><p>​    在上面的图中，每一条线都带有一个完整的向量，从一个节点的输出到其他节点的输入。粉色圆圈表示点化操作，比如向量加法，而黄色框表示神经网络学习层。箭头合并表示连接，而箭头分叉表示被正在被复制的内容以及复制到不同位置。</p><h2 id="LSTM输入和输出"><a href="#LSTM输入和输出" class="headerlink" title="LSTM输入和输出"></a>LSTM输入和输出</h2><p><a href="https://www.zhihu.com/question/41949741" target="_blank" rel="noopener">https://www.zhihu.com/question/41949741</a></p><h2 id="LSTM核心思想"><a href="#LSTM核心思想" class="headerlink" title="LSTM核心思想"></a>LSTM核心思想</h2><p>LSTMs的核心是 单元状态，即贯穿图顶部的水平线。</p><p>单元状态有点像传送带。它沿着整个链一直向下，只有一些很小的线性相互作用。信息很容易不加改变地沿着它流动。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" alt="img"></p><p>LSTM有能力删除或添加信息到单元状态，并由称为门的结构小心地控制。</p><p>门是一种选择性地让信息通过的方法。它们由sigmoid神经网络层和点乘运算组成。.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" alt="img"></p><p>sigmoid层输出0到1之间的数字，描述每个组件应该通过的百分比。值0表示“不让任何东西通过”，而值1表示“让所有东西通过!”LSTM有三个这样的门，用来保护和控制单元状态。</p><h2 id="LSTM深入剖析"><a href="#LSTM深入剖析" class="headerlink" title="LSTM深入剖析"></a>LSTM深入剖析</h2><h3 id="忘记阶段"><a href="#忘记阶段" class="headerlink" title="忘记阶段"></a>忘记阶段</h3><p>LSTM的第一步是决定要从单元格状态丢弃什么信息。这个决定是由一个叫做“忘记门”的sigmoid层做出的。“它查看h<sub>t-1</sub>和 x<sub>t</sub>，并为处于单元格状态  C<sub>t-1</sub>的每个值输出一个介于0到1之间的数字。1表示“完全保留这个”，而0表示“完全删除这个”。</p><p>让我们回到语言模型的例子，该模型试图根据前面的所有单词预测下一个单词。在这样的问题中，单元格状态可能包括当前主语的词性，以便使用正确的代词。当我们看到一个新的主语时，我们想要忘记旧主语的词性。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" alt="img"></p><h3 id="选择记忆阶段"><a href="#选择记忆阶段" class="headerlink" title="选择记忆阶段"></a>选择记忆阶段</h3><p>下一步是决定要在单元格状态中存储哪些新信息。它有两部分。首先，一个名为“input gate layer”的sigmoid层决定要更新哪些值。接下来，tanh层创建一个新的候选值向量C<sub> ~t </sub>，可以将其添加到状态中。在下一个步骤中，我们将把这两者结合起来对状态的更新。</p><p>在我们的语言模型示例中，我们希望将新主体的词性添加到单元格状态，以替换我们正在遗忘的旧主体。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" alt="img"></p><p>现在是时候将旧的单元状态C<sub>t - 1</sub>更新为新的单元状态C<sub>t</sub>了。前面的步骤已经决定了要做什么，我们只需要实际去执行。</p><p>我们将旧状态乘以f<sub>t</sub>，忘记我们之前决定忘记的事情。然后我们将它添加到C<sub> ~t </sub>中。这是新的候选值，按我们决定每个状态值的更新进行缩放。</p><p>在语言模型中，这是我们实际删除关于旧信息并添加新信息的地方，正如我们在前面的步骤中描述的那样。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" alt="img"></p><h3 id="输出阶段"><a href="#输出阶段" class="headerlink" title="输出阶段"></a>输出阶段</h3><p>最后，我们需要决定输出什么。这个输出将基于我们的单元格状态，但是是经过过滤的版本。首先，我们运行一个sigmoid层，它决定要输出单元格状态的哪些部分。然后，我们将单元格状态放入tanh(将值缩放到- 1和1之间)，并将其乘以sigmoid层的输出，这样我们只输出我们决定输出的部分。</p><p>对于语言模型的例子，由于它只是看到了一个主语，所以它可能希望输出与动词相关的信息，以防接下来会发生什么。例如，它可以输出主语是单数还是复数，这样我们就知道如果主语是单数或复数，那么动词应该变成什么形式。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" alt="img"></p><h2 id="LSTM变形"><a href="#LSTM变形" class="headerlink" title="LSTM变形"></a>LSTM变形</h2><p>详情看来源网站。变形有很多，包括GRU，数十万种，Which of these variants is best? Do the differences matter? <a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="noopener">Greff, et al. (2015)</a> do a nice comparison of popular variants, finding that they’re all about the same.</p><h3 id="代码以及小例子"><a href="#代码以及小例子" class="headerlink" title="代码以及小例子"></a>代码以及小例子</h3><p><img src="https://cdn-images-1.medium.com/max/1600/1*p2yXhtxmYflEUrTC1rCoUA.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LSTM详解，把cell转变当作传送带的思想&lt;/p&gt;
    
    </summary>
    
      <category term="神经网络" scheme="http://kodgv.xyz/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="网络结构" scheme="http://kodgv.xyz/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>神经网络迭代更新</title>
    <link href="http://kodgv.xyz/2019/04/14/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    <id>http://kodgv.xyz/2019/04/14/神经网络/神经网络学习率/</id>
    <published>2019-04-14T08:20:34.000Z</published>
    <updated>2019-04-30T12:50:08.035Z</updated>
    
    <content type="html"><![CDATA[<p>神经网络学习率以及参数的更新</p><a id="more"></a><hr><p>要注意区别，梯度下降计算的是参数，cyc计算的是学习率，自适应优化器学习率本身就会衰减，所以要注意两者是否冲突。</p><p><strong>因为kernel跑不超过5epcho，所以其实学习率不会太影响，因为它更新也只能更新5次</strong></p><p>[TOC]</p><p>来源:<a href="http://ruder.io/optimizing-gradient-descent/index.html#momentum" target="_blank" rel="noopener">http://ruder.io/optimizing-gradient-descent/index.html#momentum</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>​    调参就是指调学习率</p><p>​    学习速率是指导我们该如何通过损失函数的梯度调整网络权重的超参数。学习率越低，损失函数的变化速度就越慢。虽然使用低学习率可以确保我们不会错过任何局部极小值，但也意味着我们将花费更长的时间来进行收敛，特别是在被困在高原区域的情况下。</p><p>​    一般而言，用户可以利用过去的经验（或其他类型的学习资料）直观地设定学习率的最佳值。</p><p>因此，想得到最佳学习速率是很难做到的。下图演示了配置学习速率时可能遇到的不同情况。</p><p><img src="/2019/04/14/神经网络/神经网络学习率/p1.png" alt></p><p>对于太慢的学习速率来说，损失函数可能减小，但是按照非常浅薄的速率减小的。当进入了最优学习率区域，你将会观察到在损失函数上一次非常大的下降。进一步增加学习速率会造成损失函数值「跳来跳去」甚至在最低点附近发散。记住，最好的学习速率搭配着损失函数上最陡的下降，所以我们主要关注分析图的坡度。</p><p><img src="/2019/04/14/神经网络/神经网络学习率/p2.jpeg" alt></p><h2 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h2><p><strong>make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.</strong></p><h3 id="Batch-gradient-descent"><a href="#Batch-gradient-descent" class="headerlink" title="Batch gradient descent"></a>Batch gradient descent</h3><script type="math/tex; mode=display">\theta = \theta - \eta \cdot \nabla_\theta J( \theta)</script><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="title">range</span>(<span class="params">nb_epochs</span>):</span></span><br><span class="line"><span class="function">  params_grad</span> = evaluate_gradient(loss_function, data, <span class="keyword">params</span>)</span><br><span class="line">  <span class="keyword">params</span> = <span class="keyword">params</span> - learning_rate * params_grad</span><br></pre></td></tr></table></figure><p>优点：保证批量梯度下降收敛于凸误差曲面的全局最小值和非凸曲面的局部最小值。</p><p>缺点：下降可能非常慢，而且要求每次计算整个epcho的数据，对内存要求比较高，而且它不允许在线更新。</p><h3 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h3><script type="math/tex; mode=display">\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})</script><p>注意每次都需要对数据进行shuffle打乱</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_epochs):</span><br><span class="line">  <span class="built_in">np</span>.<span class="built_in">random</span>.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> <span class="built_in">example</span> <span class="keyword">in</span> data:</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, <span class="built_in">example</span>, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure><p>优点：减少计算的冗余，由于每次只更新一次，速度快，对在线更新友好。</p><p>缺点：会拥有比较高的方差，会震荡比较明显，如果学习率不足够小的话可能会收敛于局部点。</p><p>SGD的学习率一般要配合退火，不断变小</p><h3 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h3><script type="math/tex; mode=display">\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})</script><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_epochs):</span><br><span class="line">  <span class="built_in">np</span>.<span class="built_in">random</span>.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> <span class="built_in">batch</span> <span class="keyword">in</span> get_batches(data, batch_size=<span class="number">50</span>):</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, <span class="built_in">batch</span>, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure><p>优点：减少方差</p><p>缺点：batch需要调节，在内存和速度上trade-off</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>简而言之，虽然有mini-batch，但是学习率仍然需要精心设计学习速率过小会导致收敛异常缓慢，而学习速率过大则会阻碍收敛，导致损失函数在最小值附近波动，甚至出现发散。</p><h2 id="Adaptive-Optimizers"><a href="#Adaptive-Optimizers" class="headerlink" title="Adaptive Optimizers"></a>Adaptive Optimizers</h2><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><script type="math/tex; mode=display">\begin{align} \begin{split} v_t &= \gamma v_{t-1} + \eta \nabla_\theta J( \theta) \\ \theta &= \theta - v_t \end{split} \end{align}</script><p>要理解vt-1是上一次的更新，如果符号一致就会加速，如果符号不一致说明震荡，就会减速，这就是命名动量的意义，就像一个ball向下滚，它向下加速，向上却会受到空气阻力</p><p>优点：动量项对于梯度指向相同方向的维度增加，对于梯度改变方向的维度减少更新。因此，我们获得更快的收敛速度和减少振荡。</p><h3 id="Nesterov-accelerated-gradient"><a href="#Nesterov-accelerated-gradient" class="headerlink" title="Nesterov accelerated gradient"></a>Nesterov accelerated gradient</h3><p>然而，一个从山上滚下来的球，盲目地跟着斜坡走，是非常令人不满意的。我们想要一个更聪明的球，一个知道它要去哪里的球，这样它就知道在山再次倾斜之前减速。</p><script type="math/tex; mode=display">\begin{align} \begin{split} v_t &= \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} ) \\ \theta &= \theta - v_t \end{split} \end{align}</script><p>跟上面Momentum公式的唯一区别在于，梯度不是根据当前参数位置<img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bi-1%7D" alt="\theta_{i-1}">，而是根据先走了本来计划要走的一步后，达到的参数位置<img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bi-1%7D+-+%5Calpha%5Cbeta+d_%7Bi-1%7D" alt="\theta_{i-1} - \alpha\beta d_{i-1}">计算出来的。</p><p>对于这个改动，很多文章给出的解释是，能够让算法提前看到前方的地形梯度，如果前面的梯度比当前位置的梯度大，那我就可以把步子迈得比原来大一些，如果前面的梯度比现在的梯度小，那我就可以把步子迈得小一些。这个大一些、小一些，都是相对于原来不看前方梯度、只看当前位置梯度的情况来说的。</p><p>有很复杂的推导过程:<a href="https://zhuanlan.zhihu.com/p/22810533" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22810533</a></p><p>在原始形式中，Nesterov Accelerated Gradient（NAG）算法相对于Momentum的改进在于，以“向前看”看到的梯度而不是当前位置梯度去更新。经过变换之后的等效形式中，NAG算法相对于Momentum多了一个本次梯度相对上次梯度的变化量，这个变化量本质上是对目标函数二阶导的近似。由于利用了二阶导的信息，NAG算法才会比Momentum具有更快的收敛速度。</p><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>针对不同的参数设置不同的学习率，根据频繁更新的学习率小，不频繁更新的学习率大。</p><p>For brevity, we use gtgt to denote the gradient at time step tt. gt,igt,i is then the partial derivative of the objective function w.r.t. to the parameter θiθi at time step tt:</p><script type="math/tex; mode=display">g_{t, i} = \nabla_\theta J( \theta_{t, i})</script><p>The SGD update for every parameter θiθi at each time step tt then becomes:</p><script type="math/tex; mode=display">\theta_{t+1, i} = \theta_{t, i} - \eta \cdot g_{t, i}</script><p>In its update rule, Adagrad modifies the general learning rate ηη at each time step tt for every parameter θiθi based on the past gradients that have been computed for θiθi:</p><script type="math/tex; mode=display">\theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}</script><p>Gt∈Rd×d是一个对角矩阵，每个对角线位置i,i的值累加到t次迭代的对应参数 θi 梯度平方和。ϵ是平滑项，防止除零操作，一般取值1e−8。为什么分母要进行平方根的原因是去掉平方根操作算法的表现会大打折扣。</p><p>缺点：G_t会了累积越来越大，分母累积的和会越来越大。这样学习到后来的阶段，网络的更新能力会越来越弱，能学到的更多知识的能力也越来越弱</p><p>优点：因为有G_t的存在，所以n不用调，一般设0.01默认就可以了。</p><h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><p>修复上面的缺点，梯度和是递归的定义成历史梯度平方的衰减平均值。动态平均值E[g^2]_t<br>仅仅取决于当前的梯度值与上一时刻的平均值.这样就不需要算累积的量</p><script type="math/tex; mode=display">E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t</script><script type="math/tex; mode=display">\Delta \theta_t = - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}\\\Delta \theta_t = - \dfrac{\eta}{RMS[g]_{t}} g_t</script><script type="math/tex; mode=display">RMS[\Delta \theta]_{t} = \sqrt{E[\Delta \theta^2]_t + \epsilon}</script><p>在此处Adadelta其实还是依赖于全局学习率的，但是作者做了一定处理，经过近似牛顿迭代法之后：</p><script type="math/tex; mode=display">\begin{align} \begin{split} \Delta \theta_t &= - \dfrac{RMS[\Delta \theta]_{t-1}}{RMS[g]_{t}} g_{t} \\ \theta_{t+1} &= \theta_t + \Delta \theta_t \end{split} \end{align}</script><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p><em>相比于AdaGrad的历史梯度：</em></p><p><img src="/2019/04/14/神经网络/神经网络学习率/f6.jpg" alt="img"></p><p><em>RMSProp增加了一个衰减系数来控制历史信息的获取多少：</em></p><p><img src="/2019/04/14/神经网络/神经网络学习率/f7.jpg" alt="img"></p><p><strong>简单来讲，设置全局学习率之后，每次通过，全局学习率逐参数的除以经过衰减系数控制的历史梯度平方和的平方根，使得每个参数的学习率不同</strong></p><p>那么它起到的作用是什么呢？</p><p><strong>起到的效果是在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。</strong></p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p><strong>其实就是Momentum+RMSProp的结合，然后再修正其偏差</strong></p><script type="math/tex; mode=display">\begin{align} \begin{split} m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \end{split} \end{align}</script><p>修正偏差：</p><script type="math/tex; mode=display">\begin{align} \begin{split} \hat{m}_t &= \dfrac{m_t}{1 - \beta^t_1} \\ \hat{v}_t &= \dfrac{v_t}{1 - \beta^t_2} \end{split} \end{align}</script><p>计算梯度：</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t</script><p>The authors propose default values of 0.9 for β1, 0.999 for β2, and 10−8for ϵ.</p><p><strong>1.Adams可能不收敛</strong></p><p>文中各大优化算法的学习率：其中，SGD没有用到二阶动量，因此学习率是恒定的（实际使用过程中会采用学习率衰减策略，因此学习率递减）。AdaGrad的二阶动量不断累积，单调递增，因此学习率是单调递减的。因此，这两类算法会使得学习率不断递减，最终收敛到0，模型也得以收敛。</p><p>但AdaDelta和Adam则不然。二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得 <img src="https://www.zhihu.com/equation?tex=V_t" alt="V_t"> 可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。</p><p><strong>2.Adams可能错失全局最优解</strong></p><p>​       吐槽Adam最狠的 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1705.08292" target="_blank" rel="noopener">The Marginal Value of Adaptive Gradient Methods in Machine Learning</a> 。文中说到，同样的一个优化问题，不同的优化算法可能会找到不同的答案，但自适应学习率的算法往往找到非常差的答案。他们通过一个特定的数据例子说明，自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>那么，现在应该使用哪个优化器呢?如果您的输入数据是稀疏的，那么您可能使用自适应学习率方法之一获得最佳结果。另一个好处是，您不需要调优学习率，但是可以使用缺省值获得最佳结果。</p><p>总之，RMSprop是Adagrad的一个扩展，它处理的是学习速度的急剧下降。它与Adadelta相同，只是Adadelta在numinator update规则中使用参数更新的RMS。最后，Adam为RMSprop添加了偏差修正和动量。到目前为止，RMSprop、Adadelta和Adam都是非常相似的算法，它们在相似的环境中都表现得很好。Kingma等人[14:1]表明，当梯度变得更稀疏时，它的偏倚校正帮助Adam在优化的最后略微优于RMSprop。到目前为止，Adam可能是最好的选择。</p><p>有趣的是，许多最近的论文使用SGD和一个简单的学习速率退火时间表。正如所示，SGD通常能够找到最小值，但是它可能比一些优化器花费的时间要长得多，更依赖于健壮的初始化和退火调度，并且可能会陷入鞍点而不是局部极小值。因此，如果你关心快速收敛和训练一个深度或复杂的神经网络，你应该选择一种自适应学习速率方法。</p><h2 id="Additional-strategies"><a href="#Additional-strategies" class="headerlink" title="Additional strategies"></a>Additional strategies</h2><h3 id="Shuffling-and-Curriculum-Learning"><a href="#Shuffling-and-Curriculum-Learning" class="headerlink" title="Shuffling and Curriculum Learning"></a>Shuffling and Curriculum Learning</h3><p>​    通常，我们希望避免以有意义的顺序为模型提供训练示例，因为这可能会影响优化算法。因此，在每个epoch之后重新整理训练数据通常是一个好主意。</p><p>​    另一方面，在某些情况下，我们的目标是逐步解决更难的问题，按有意义的顺序提供训练示例实际上可能会提高性能和更好的收敛性。建立这种有意义顺序的方法称为课程学习</p><h3 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h3><p>​    为了便于学习，我们通常用零均值和单位方差初始化参数的初始值，从而对参数的初始值进行标准化。随着训练的进展，我们在不同程度上更新参数，我们失去了这种标准化，这减慢了训练的速度，并随着网络变得更深而放大了变化。</p><p>​    批处理规范化[27]为每个小批处理重新建立这些规范化，并通过操作反向传播更改。通过将标准化作为模型体系结构的一部分，我们可以使用更高的学习率，而不太关注初始化参数。批处理规范化还可以作为一个正则化器，减少(有时甚至消除)退出的需要。</p><h3 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h3><p>​    你应该观察验证集上的误差，并且停止如果它没有足够的提高了。</p><h2 id="newest-algorithns"><a href="#newest-algorithns" class="headerlink" title="newest algorithns"></a>newest algorithns</h2><h3 id="Cyclical-Learning-Rates"><a href="#Cyclical-Learning-Rates" class="headerlink" title="Cyclical Learning Rates"></a>Cyclical Learning Rates</h3><p>两个特点：</p><ul><li>它为我们提供了一种在训练过程中有效地控制学习率的方法，方法是在上下界之间以三角形的方式改变学习率</li><li>它为我们提供了一个非常不错的估计，即哪种学习率的范围适合您的特定网络。</li></ul><p><img src="/2019/04/14/神经网络/神经网络学习率/p2.png" alt></p><p>There are a number of parameters to play around with here:</p><ul><li><strong>step size</strong>: during how many epochs will the LR go up from the lower bound, up to the upper bound.</li><li><strong>max_lr</strong>: the highest LR in the schedule.</li><li><strong>base_lr</strong>: the lowest LR in the schedule, in practice: the author of the paper suggests to take this a factor R smaller than the <strong>max_lr</strong>. Our used factor was 6.</li></ul><p>它的核心思想:这个学习率策略的本质来自于一个观察，增加学习率会有短暂的负面影响，但是长远来看有好处。这个观察启发了我们的想法，让学习率在一个范围内变化，而不是用常值或指数递减啥的。所以只需要设置上下界和周期变化就可以了。大量的实验尝试了各种形式，triangular window (linear), a Welch window (parabolic) and a Hann window (sinusoidal)，他们的结果差不多。就采用triangular窗吧</p><p><strong>代码实现</strong></p><h4 id="Step-1-find-the-upper-LR"><a href="#Step-1-find-the-upper-LR" class="headerlink" title="Step 1: find the upper LR"></a>Step 1: find the upper LR</h4><ul><li>define an initial learning rate, the lower boundary of the range you want to test (let’s say 1e-7)</li><li>define an upper boundary of the range (let’s say 0.1)</li><li>define an exponential scheme to run through this step by step:</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (lr_find_epochs * len( dataloaders[<span class="string">"train"</span>])))</span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)</span><br></pre></td></tr></table></figure><ul><li>Next, do a run (I used two epochs) through your network. At each step (each batch size): capture the LR, capture the loss and optimize the gradients:</li></ul><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the model</span></span><br><span class="line"><span class="attr">model</span> = CNN().to(device)</span><br><span class="line"></span><br><span class="line"><span class="attr">optimizer</span> = torch.optim.SGD(model.parameters(), start_lr)</span><br><span class="line"><span class="attr">criterion</span> = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make lists to capture the logs</span></span><br><span class="line"></span><br><span class="line"><span class="attr">lr_find_loss</span> = []</span><br><span class="line"><span class="attr">lr_find_lr</span> = []</span><br><span class="line"></span><br><span class="line"><span class="attr">iter</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="attr">smoothing</span> = <span class="number">0.05</span></span><br><span class="line"></span><br><span class="line">for i <span class="keyword">in</span> range(lr_find_epochs):</span><br><span class="line">  print(<span class="string">"epoch &#123;&#125;"</span>.format(i))</span><br><span class="line">  for inputs, labels <span class="keyword">in</span> dataloaders[<span class="string">"train"</span>]:</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Send to device</span></span><br><span class="line">    <span class="attr">inputs</span> = inputs.to(device)</span><br><span class="line">    <span class="attr">labels</span> = labels.to(device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Training mode and zero gradients</span></span><br><span class="line">    model.train()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get outputs to calc loss</span></span><br><span class="line">    <span class="attr">outputs</span> = model(inputs)</span><br><span class="line">    <span class="attr">loss</span> = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update LR</span></span><br><span class="line">    scheduler.step()</span><br><span class="line">    <span class="attr">lr_step</span> = optimizer.state_dict()[<span class="string">"param_groups"</span>][<span class="number">0</span>][<span class="string">"lr"</span>]</span><br><span class="line">    lr_find_lr.append(lr_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># smooth the loss</span></span><br><span class="line">    <span class="keyword">if</span> <span class="attr">iter==0:</span></span><br><span class="line">      lr_find_loss.append(loss)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="attr">loss</span> = smoothing  * loss + (<span class="number">1</span> - smoothing) * lr_find_loss[-<span class="number">1</span>]</span><br><span class="line">      lr_find_loss.append(loss)    </span><br><span class="line">    iter += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>观察图，确定一个范围。根据fast.ai的课程描述，一个好的上界不是在最低点，而是左移10倍左右，一个好的下界是上界，除以一个因子6</p><p><img src="/2019/04/14/神经网络/神经网络学习率/p3.png" alt></p><h4 id="Step-2-CLR-scheduler"><a href="#Step-2-CLR-scheduler" class="headerlink" title="Step 2: CLR scheduler"></a>Step 2: CLR scheduler</h4><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def cyclical_lr(stepsize, min_lr=<span class="number">3e-2</span>, max_lr=<span class="number">3e-3</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Scaler: we can adapt this if we do not want the triangular CLR</span></span><br><span class="line">    scaler = lambda x: <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Lambda function to calculate the LR</span></span><br><span class="line">    lr_lambda = lambda <span class="keyword">it</span>: min_lr + (max_lr - min_lr) * <span class="built_in">relative</span>(<span class="keyword">it</span>, stepsize)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Additional function to see where on the cycle we are</span></span><br><span class="line">    def <span class="built_in">relative</span>(<span class="keyword">it</span>, stepsize):</span><br><span class="line">        cycle = math.floor(<span class="number">1</span> + <span class="keyword">it</span> / (<span class="number">2</span> * stepsize))</span><br><span class="line">        x = <span class="built_in">abs</span>(<span class="keyword">it</span> / stepsize - <span class="number">2</span> * cycle + <span class="number">1</span>)</span><br><span class="line">        <span class="literal">return</span> <span class="built_in">max</span>(<span class="number">0</span>, (<span class="number">1</span> - x)) * scaler(cycle)</span><br><span class="line"></span><br><span class="line">    <span class="literal">return</span> lr_lambda</span><br></pre></td></tr></table></figure><h4 id="Step-3-wrap-it"><a href="#Step-3-wrap-it" class="headerlink" title="Step 3: wrap it"></a>Step 3: wrap it</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = CNN().<span class="keyword">to</span>(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">    </span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), <span class="attribute">lr</span>=1.)</span><br><span class="line">step_size = 4*len(train_loader)</span><br><span class="line">clr = cyclical_lr(step_size, <span class="attribute">min_lr</span>=end_lr/factor, <span class="attribute">max_lr</span>=end_lr)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr])</span><br></pre></td></tr></table></figure><h4 id="Step-4-train"><a href="#Step-4-train" class="headerlink" title="Step 4: train"></a>Step 4: train</h4><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (images, labels) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">  images, labels = images.<span class="keyword">to</span>(device), labels.<span class="keyword">to</span>(device)</span><br><span class="line">  <span class="comment">#Clear the gradients</span></span><br><span class="line">  optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">  <span class="comment">#Forward propagation </span></span><br><span class="line">  outputs = model(images)   </span><br><span class="line">            </span><br><span class="line">  <span class="comment">#Calculating loss with softmax to obtain cross entropy loss</span></span><br><span class="line">  loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#Backward propation</span></span><br><span class="line">  loss.backward()</span><br><span class="line">  scheduler.step() <span class="comment"># &gt; Where the magic happens</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">#Updating gradients</span></span><br><span class="line">  optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="AdamW"><a href="#AdamW" class="headerlink" title="AdamW"></a>AdamW</h2><p><a href="https://github.com/mpyrozhok/adamwr" target="_blank" rel="noopener">https://github.com/mpyrozhok/adamwr</a></p><p><a href="https://zhuanlan.zhihu.com/p/52084949" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/52084949</a></p><p>所以在 2018 年，你应该做什么来代替 3e-4 Adam 工作流程呢？有很多东西需要考虑，如批量大小、动量等。但是，更好的工作流程将是：</p><ul><li><p>使用 LR Range Test 找到最佳学习率，并完整地检查当前模型和数据。</p></li><li><p>始终使用学习率调度器，该调度器会改变上一步中找到的学习率，可以是 CLR 或 Restart。</p></li><li><p>如果需要 Adam，请使用具有适当权值衰减的 AdamW，而不是当前流行框架中使用的默认权值衰减。</p></li><li><p>如果想实现超收敛，可以进一步尝试一周期策略。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;神经网络学习率以及参数的更新&lt;/p&gt;
    
    </summary>
    
      <category term="神经网络" scheme="http://kodgv.xyz/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="学习率" scheme="http://kodgv.xyz/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
  </entry>
  
</feed>
