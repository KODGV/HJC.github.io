<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小黑屋</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://kodgv.xyz/"/>
  <updated>2019-04-21T12:35:37.523Z</updated>
  <id>http://kodgv.xyz/</id>
  
  <author>
    <name>KODGV</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>好的代码参考汇总</title>
    <link href="http://kodgv.xyz/2019/04/21/%E5%A5%BD%E7%9A%84%E4%BB%A3%E7%A0%81%E5%8F%82%E8%80%83%E6%B1%87%E6%80%BB/"/>
    <id>http://kodgv.xyz/2019/04/21/好的代码参考汇总/</id>
    <published>2019-04-21T06:45:42.000Z</published>
    <updated>2019-04-21T12:35:37.523Z</updated>
    
    <content type="html"><![CDATA[<p>好的代码借鉴，必要的时候可以直接抄</p><a id="more"></a><p>[TOC]</p><h2 id="机器翻译注意力机制及其PyTorch实现"><a href="#机器翻译注意力机制及其PyTorch实现" class="headerlink" title="机器翻译注意力机制及其PyTorch实现"></a>机器翻译注意力机制及其PyTorch实现</h2><p><a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/" target="_blank" rel="noopener">https://plmsmile.github.io/2017/10/12/Attention-based-NMT/</a></p><h2 id="各个NLP模型实现"><a href="#各个NLP模型实现" class="headerlink" title="各个NLP模型实现"></a>各个NLP模型实现</h2><p><a href="http://www.zhongruitech.com/921029206.html" target="_blank" rel="noopener">http://www.zhongruitech.com/921029206.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;好的代码借鉴，必要的时候可以直接抄&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>数据挖掘问答汇总</title>
    <link href="http://kodgv.xyz/2019/04/21/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E9%97%AE%E7%AD%94%E6%B1%87%E6%80%BB/"/>
    <id>http://kodgv.xyz/2019/04/21/数据挖掘问答汇总/</id>
    <published>2019-04-21T02:43:51.000Z</published>
    <updated>2019-04-21T02:45:22.116Z</updated>
    
    <content type="html"><![CDATA[<p>数据挖掘竞赛汇总</p><a id="more"></a><p>[TOC]</p><h2 id="如何知道树模型怎么可以提高？"><a href="#如何知道树模型怎么可以提高？" class="headerlink" title="如何知道树模型怎么可以提高？"></a>如何知道树模型怎么可以提高？</h2><p>通过画树的图，分析树当前无法分割的知识是什么，给它补充进数据里面。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据挖掘竞赛汇总&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>NLP知识汇总</title>
    <link href="http://kodgv.xyz/2019/04/20/NLP%E7%9F%A5%E8%AF%86%E6%B1%87%E6%80%BB/"/>
    <id>http://kodgv.xyz/2019/04/20/NLP知识汇总/</id>
    <published>2019-04-20T13:57:19.000Z</published>
    <updated>2019-04-20T14:29:27.279Z</updated>
    
    <content type="html"><![CDATA[<p>NLP汇总</p><a id="more"></a><p>[TOC]</p><h2 id="动态padding，节省时间"><a href="#动态padding，节省时间" class="headerlink" title="动态padding，节省时间"></a>动态padding，节省时间</h2><p>比起一开始固定化padding，动态padding，可以针对当前batch的长度来Padding，明显会增快速度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, text, lens, y=None)</span>:</span></span><br><span class="line">        self.text = text</span><br><span class="line">        self.y = y</span><br><span class="line">        self.lens = lens</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.lens)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self.text[index], self.lens[index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.text[index], self.lens[index], self.y[index]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(batch)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    batch = [dataset[i] for i in N]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    size = len(batch[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> size == <span class="number">3</span>:</span><br><span class="line">        texts, lens, y = zip(*batch)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        texts, lens = zip(*batch)</span><br><span class="line">    lens = np.array(lens)</span><br><span class="line">    sort_idx = np.argsort(<span class="number">-1</span> * lens)</span><br><span class="line">    reverse_idx = np.argsort(sort_idx)</span><br><span class="line">    max_len = min(int(np.percentile(lens, PERCENTILE)), MAX_LEN)</span><br><span class="line">    </span><br><span class="line">    lens = np.clip(lens, <span class="number">0</span>, max_len)[sort_idx]</span><br><span class="line">    texts = torch.tensor(sequence.pad_sequences(texts, maxlen=max_len)[sort_idx], dtype=torch.long).cuda()</span><br><span class="line">    <span class="keyword">if</span> size == <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">return</span> texts, lens, reverse_idx, torch.tensor(y, dtype=torch.float32).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> texts, lens, reverse_idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_data_loader</span><span class="params">(texts, lens, y=None, batch_size=BATCH_SIZE)</span>:</span></span><br><span class="line">    dset = MyDataset(texts, lens, y)</span><br><span class="line">    dloader = data.DataLoader(dset, batch_size=batch_size, shuffle=<span class="literal">False</span>, collate_fn=collate_fn)</span><br><span class="line">    <span class="keyword">return</span> dloader</span><br><span class="line">seqs = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>]]</span><br><span class="line">lens = [len(i) <span class="keyword">for</span> i <span class="keyword">in</span> seqs]</span><br><span class="line"></span><br><span class="line">data_loader = build_data_loader(seqs, lens)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data_loader:</span><br><span class="line">    seq_batch, lens_batch, reverse_idx_batch = batch</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">print(<span class="string">f'original seqs:'</span>)</span><br><span class="line">print(seqs)</span><br><span class="line">print(<span class="string">f'batch seqs, already sort by lens, and padding dynamic in batch:'</span>)</span><br><span class="line">print(seq_batch)</span><br><span class="line">print(<span class="string">f'reverse batch seqs:'</span>)</span><br><span class="line">print(seq_batch[reverse_idx_batch])</span><br><span class="line">h_embedding_pack = pack_padded_sequence(seq_batch,lens_batch,batch_first=<span class="literal">True</span>)</span><br><span class="line">print(h_embedding_pack)</span><br></pre></td></tr></table></figure><h2 id="mask-loss-避免无用结果的求导影响"><a href="#mask-loss-避免无用结果的求导影响" class="headerlink" title="mask loss 避免无用结果的求导影响"></a>mask loss 避免无用结果的求导影响</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> func</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CustomLoss,self).__init__()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, Y_hat, Y)</span>:</span></span><br><span class="line">       <span class="comment"># TRICK 3 ********************************</span></span><br><span class="line">       <span class="comment"># before we calculate the negative log likelihood, we need to mask out the activations</span></span><br><span class="line">       <span class="comment"># this means we don't want to take into account padded items in the output vector</span></span><br><span class="line">       <span class="comment"># simplest way to think about this is to flatten ALL sequences into a REALLY long sequence</span></span><br><span class="line">       <span class="comment"># and calculate the loss on that.</span></span><br><span class="line">       <span class="comment"># flatten all the labels</span></span><br><span class="line">        Y = Y.view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># flatten all predictions</span></span><br><span class="line">        Y_hat = Y_hat.view(<span class="number">-1</span>, self.nb_tags)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># create a mask by filtering out all tokens that ARE NOT the padding token</span></span><br><span class="line">        tag_pad_token = self.tags[<span class="string">'&lt;PAD&gt;'</span>]</span><br><span class="line">        mask = (Y &gt; tag_pad_token).float()</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># count how many tokens we have</span></span><br><span class="line">        nb_tokens = int(torch.sum(mask).data[<span class="number">0</span>])</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># pick the values for the label and zero out the rest with the mask</span></span><br><span class="line">        Y_hat = Y_hat[range(Y_hat.shape[<span class="number">0</span>]), Y] * mask</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># compute cross entropy loss which ignores all &lt;PAD&gt; tokens</span></span><br><span class="line">        ce_loss = -torch.sum(Y_hat) / nb_tokens</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ce_loss</span><br></pre></td></tr></table></figure><h2 id="问题解答专区"><a href="#问题解答专区" class="headerlink" title="问题解答专区"></a>问题解答专区</h2><h3 id="为什么LSTM不同batch的句子长度可以不一致"><a href="#为什么LSTM不同batch的句子长度可以不一致" class="headerlink" title="为什么LSTM不同batch的句子长度可以不一致"></a>为什么LSTM不同batch的句子长度可以不一致</h3><p>LSTM只要保证每个batch的句子长度一致即可，所以可以使用动态padding,但是为什么不同batch的长度可以不一样？</p><p>因为LSTM需要更新的参数是共享，就是每个状态的权重是共享的，所以就无所谓不同长度了。只要保证每个batch内能够更新权重即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NLP汇总&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://kodgv.xyz/categories/NLP/"/>
    
    
      <category term="汇总" scheme="http://kodgv.xyz/tags/%E6%B1%87%E6%80%BB/"/>
    
  </entry>
  
  <entry>
    <title>python多进程</title>
    <link href="http://kodgv.xyz/2019/04/18/python%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    <id>http://kodgv.xyz/2019/04/18/python多进程/</id>
    <published>2019-04-18T01:46:38.000Z</published>
    <updated>2019-04-18T08:36:10.690Z</updated>
    
    <content type="html"><![CDATA[<p>python multiprocessing模块多进程详解</p><a id="more"></a><p>[TOC]</p><h2 id="multiprocessing模块API"><a href="#multiprocessing模块API" class="headerlink" title="multiprocessing模块API"></a>multiprocessing模块API</h2><p>Pool类用于需要执行的目标很多，而手动限制进程数量又太繁琐时，如果目标少且不用控制进程数量则可以用<a href="http://thief.one/2016/11/24/Multiprocessing-Process" target="_blank" rel="noopener">Process</a>类。</p><p>构造方法</p><ul><li>Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]])</li><li>processes ：使用的工作进程的数量，如果processes是None那么使用 os.cpu_count()返回的数量。</li><li>initializer： 如果initializer是None，那么每一个工作进程在开始的时候会调用initializer(*initargs)。</li><li>maxtasksperchild：工作进程退出之前可以完成的任务数，完成后用一个新的工作进程来替代原进程，来让闲置的资源被释放。maxtasksperchild默认是None，意味着只要Pool存在工作进程就会一直存活。</li><li>context: 用在制定工作进程启动时的上下文，一般使用 multiprocessing.Pool() 或者一个context对象的Pool()方法来创建一个池，两种方法都适当的设置了context。</li></ul><p>实例方法</p><ul><li>apply_async(func[, args[, kwds[, callback]]]) 它是非阻塞。</li><li>apply(func[, args[, kwds]])是阻塞的。</li><li>close() 关闭pool，使其不在接受新的任务。</li><li>terminate() 关闭pool，结束工作进程，不在处理未完成的任务。</li><li>join() 主进程阻塞，等待子进程的退出， join方法要在close或terminate之后使用。</li></ul><h3 id="Pool使用方法"><a href="#Pool使用方法" class="headerlink" title="Pool使用方法"></a>Pool使用方法</h3><p>Pool+map函数</p><p>说明：此写法缺点在于只能通过map向函数传递一个参数。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing import Pool</span><br><span class="line">def test(i):</span><br><span class="line">    <span class="builtin-name">print</span> i</span><br><span class="line"><span class="keyword">if</span> <span class="attribute">__name__</span>=="__main__":</span><br><span class="line">lists=[1,2,3]</span><br><span class="line"><span class="attribute">pool</span>=Pool(processes=2) #定义最大的进程数</span><br><span class="line">pool.map(test,lists)        #p必须是一个可迭代变量。</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure><p>异步进程池（非阻塞）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">pool = Pool(processes=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i  <span class="keyword">in</span> xrange(<span class="number">500</span>):</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">For循环中执行步骤：</span></span><br><span class="line"><span class="string">（1）循环遍历，将500个子进程添加到进程池（相对父进程会阻塞）</span></span><br><span class="line"><span class="string">（2）每次执行10个子进程，等一个子进程执行完后，立马启动新的子进程。（相对父进程不阻塞）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">apply_async为异步进程池写法。</span></span><br><span class="line"><span class="string">异步指的是启动子进程的过程，与父进程本身的执行（print）是异步的，而For循环中往进程池添加子进程的过程，与父进程本身的执行却是同步的。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">    pool.apply_async(test, args=(i,)) <span class="comment">#维持执行的进程总数为10，当一个进程执行完后启动一个新进程.       </span></span><br><span class="line"><span class="keyword">print</span> “test”</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure><p>执行顺序：For循环内执行了2个步骤，第一步：将500个对象放入进程池（阻塞）。第二步：同时执行10个子进程（非阻塞），有结束的就立即添加，维持10个子进程运行。（apply_async方法的会在执行完for循环的添加步骤后，直接执行后面的print语句，而apply方法会等所有进程池中的子进程运行完以后再执行后面的print语句）</p><p>注意：调用join之前，先调用close或者terminate方法，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束。</p><h2 id="多进程示例代码"><a href="#多进程示例代码" class="headerlink" title="多进程示例代码"></a>多进程示例代码</h2><p>纯建立Process<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#encoding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> os, time, random</span><br><span class="line"></span><br><span class="line"><span class="comment">#线程启动后实际执行的代码块</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">r1</span><span class="params">(process_name)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">print</span> process_name, os.getpid()     <span class="comment">#打印出当前进程的id</span></span><br><span class="line">        time.sleep(random.random())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">r2</span><span class="params">(process_name)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">print</span> process_name, os.getpid()     <span class="comment">#打印出当前进程的id</span></span><br><span class="line">        time.sleep(random.random())</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"main process run..."</span></span><br><span class="line">        p1 = Process(target=r1, args=(<span class="string">'process_name1'</span>, ))       <span class="comment">#target:指定进程执行的函数，args:该函数的参数，需要使用tuple,只需要不断往后加参数即可</span></span><br><span class="line">        p2 = Process(target=r2, args=(<span class="string">'process_name2'</span>, ))</span><br><span class="line">        </span><br><span class="line">        p1.start()    <span class="comment">#通过调用start方法启动进程，跟线程差不多。</span></span><br><span class="line">        p2.start()    <span class="comment">#但run方法在哪呢？待会说。。。</span></span><br><span class="line">        p1.join()     <span class="comment">#join方法也很有意思，寻思了一下午，终于理解了。待会演示。</span></span><br><span class="line">        p2.join()</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"main process runned all lines..."</span></span><br></pre></td></tr></table></figure></p><p>POOL池管理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(data, index, size)</span>:</span>  <span class="comment"># data 传入数据，index 数据分片索引，size进程数</span></span><br><span class="line">    size = math.ceil(len(data) / size)</span><br><span class="line">    start = size * index</span><br><span class="line">    end = (index + <span class="number">1</span>) * size <span class="keyword">if</span> (index + <span class="number">1</span>) * size &lt; len(data) <span class="keyword">else</span> len(data)</span><br><span class="line">    temp_data = data[start:end]</span><br><span class="line">    <span class="comment"># do something</span></span><br><span class="line">    <span class="keyword">return</span> data  <span class="comment"># 可以返回数据，在后面收集起来</span></span><br><span class="line"></span><br><span class="line">processor = <span class="number">40</span></span><br><span class="line">res = []</span><br><span class="line">p = Pool(processor)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(processor):</span><br><span class="line">    res.append(p.apply_async(run, args=(data, i, processor,)))</span><br><span class="line">    print(str(i) + <span class="string">' processor started !'</span>)</span><br><span class="line">p.close()</span><br><span class="line">p.join()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> res:</span><br><span class="line">    print(i.get())  <span class="comment"># 使用get获得多进程处理的结果</span></span><br></pre></td></tr></table></figure></p><h2 id="进程注意事项"><a href="#进程注意事项" class="headerlink" title="进程注意事项"></a>进程注意事项</h2><h3 id="进程之间内存独立"><a href="#进程之间内存独立" class="headerlink" title="进程之间内存独立"></a>进程之间内存独立</h3><p>多进程与多线程最大的不同在于，多进程的每一个进程都有一份变量的拷贝，进程之间的操作互不影响，Process进程创建时，子进程会将主进程的Process对象完全复制一份，这样在主进程和子进程各有一个 Process对象，但是p.start()启动的是子进程，主进程中的Process对象作为一个静态对象存在，不执行。</p><p>就是这个地方，要注意的是，对于多进程来说，它是不会复制内存的，所以不用担心main中的变量被复制，但是它会复制main函数以外的变量</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import multiprocessing</span><br><span class="line">import <span class="built_in">time</span></span><br><span class="line"></span><br><span class="line"><span class="literal">zero</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">def change_zero():</span><br><span class="line">    <span class="built_in">global</span> <span class="literal">zero</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        <span class="literal">zero</span> = <span class="literal">zero</span> + <span class="number">1</span></span><br><span class="line">        print(multiprocessing.current_process().name, <span class="literal">zero</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    p1 = multiprocessing.Process(target = change_zero)</span><br><span class="line">    p2 = multiprocessing.Process(target = change_zero)</span><br><span class="line">    p1.<span class="built_in">start</span>()</span><br><span class="line">    p2.<span class="built_in">start</span>()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.join()</span><br><span class="line">    print(<span class="literal">zero</span>)</span><br></pre></td></tr></table></figure><p>运行结果如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Process-1 1</span><br><span class="line">Process-1 2</span><br><span class="line">Process-1 3</span><br><span class="line">Process-2 1</span><br><span class="line">Process-2 2</span><br><span class="line">Process-2 3</span><br><span class="line">0</span><br></pre></td></tr></table></figure><h3 id="共享变量Queue"><a href="#共享变量Queue" class="headerlink" title="共享变量Queue"></a>共享变量Queue</h3><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">下面我们要讨论第一种情况，如果真的要在两个进程之间共享变量需要怎么办</span><br><span class="line"></span><br><span class="line">队列</span><br><span class="line">这里介绍进程之间的第一种交流方式——队列。multiprocessing模块中提供了multiprocessing.Queue，它和Queue.Queue的区别在于，它里面封装了进程之间的数据交流，不同进程可以操作同一个multiprocessing.Queue。</span><br><span class="line"></span><br><span class="line">from multiprocessing <span class="keyword">import</span> <span class="built_in">Process</span>, Queue</span><br><span class="line"></span><br><span class="line">def addone(q):</span><br><span class="line">    q.<span class="built_in">put</span>(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">def addtwo(q):</span><br><span class="line">    q.<span class="built_in">put</span>(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    q = Queue()</span><br><span class="line">    p1 = <span class="built_in">Process</span>(target=addone, args = (q, ))</span><br><span class="line">    p2 = <span class="built_in">Process</span>(target=addtwo, args = (q, ))</span><br><span class="line">    p1.start()</span><br><span class="line">    p2.start()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.join()</span><br><span class="line">    <span class="built_in">print</span>(q.<span class="built_in">get</span>())</span><br><span class="line">    <span class="built_in">print</span>(q.<span class="built_in">get</span>())</span><br><span class="line">运行结果如下</span><br><span class="line"></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">这个队列是线程、进程安全的，即对队列的每一次修改中间不会被中断从而造成结果错误。</span><br></pre></td></tr></table></figure><h3 id="进程锁"><a href="#进程锁" class="headerlink" title="进程锁"></a>进程锁</h3><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">进程锁</span><br><span class="line">既然变量在进程之间可以共享了，那么同时操作一个变量导致的不安全也随之出现。同多线程一样，进程也是通过锁来解决，而且使用方法都和多线程里相同。</span><br><span class="line"><span class="built_in">lock</span> = multiprocessing.<span class="built_in">Lock</span>()</span><br><span class="line"><span class="built_in">lock</span>.acquire()</span><br><span class="line"><span class="built_in">lock</span>.release()</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">lock</span>:</span><br><span class="line">这些用法和功能都和多线程是一样的</span><br><span class="line">另外，multiprocessing.Semaphore Condition Event RLock也和多线程相同</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python multiprocessing模块多进程详解&lt;/p&gt;
    
    </summary>
    
    
      <category term="多进程" scheme="http://kodgv.xyz/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>CNN</title>
    <link href="http://kodgv.xyz/2019/04/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN/"/>
    <id>http://kodgv.xyz/2019/04/17/神经网络/CNN/</id>
    <published>2019-04-17T02:32:46.000Z</published>
    <updated>2019-04-17T03:50:38.490Z</updated>
    
    <content type="html"><![CDATA[<p>CNN学习</p><a id="more"></a><p>[TOC]</p><h1 id="CNN结构基础"><a href="#CNN结构基础" class="headerlink" title="CNN结构基础"></a>CNN结构基础</h1><p>首先，提出这样一个问题：识别一幅图片是包含有字母”X”还是字母”O”？</p><p>为了帮助指导你理解卷积神经网络，我们讲采用一个非常简化的例子：确定一幅图像是包含有”X”还是”O”？<br><img src="https://img-blog.csdn.net/20180306180334512" alt="img"><br>这个例子足够说明CNN背后的原理，同时它足够简单，能够避免陷入不必要的细节。</p><p>在CNN中有这样一个问题，就是每次给你一张图，你需要判断它是否含有”X”或者”O”。并且假设必须两者选其一，不是”X”就是”O”。理想的情况就像下面这个样子：<br><img src="https://img-blog.csdn.net/20180306180403855" alt="img"><br>标准的”X”和”O”，字母位于图像的正中央，并且比例合适，无变形</p><p>对于计算机来说，只要图像稍稍有一点变化，不是标准的，那么要解决这个问题还是不是那么容易的：<br><img src="https://img-blog.csdn.net/20180306180420908" alt="img"></p><p>计算机要解决上面这个问题，一个比较天真的做法就是先保存一张”X”和”O”的标准图像（就像前面给出的例子），然后将其他的新给出的图像来和这两张标准图像进行对比，看看到底和哪一张图更匹配，就判断为哪个字母。</p><p>但是这么做的话，其实是非常不可靠的，因为计算机还是比较死板的。在计算机的“视觉”中，一幅图看起来就像是一个二维的像素数组（可以想象成一个棋盘），每一个位置对应一个数字。在我们这个例子当中，像素值”1”代表白色，像素值”-1”代表黑色。<br><img src="https://img-blog.csdn.net/20180306180438461" alt="img"></p><p>当比较两幅图的时候，如果有任何一个像素值不匹配，那么这两幅图就不匹配，至少对于计算机来说是这样的。</p><p>对于这个例子，计算机认为上述两幅图中的白色像素除了中间的3*3的小方格里面是相同的，其他四个角上都不同：<br><img src="https://img-blog.csdn.net/20180306180450259" alt="img"></p><p>因此，从表面上看，计算机判别右边那幅图不是”X”，两幅图不同，得出结论：<br><img src="https://img-blog.csdn.net/20180306180500462" alt="img"></p><p>但是这么做，显得太不合理了。理想的情况下，我们希望，对于那些仅仅只是做了一些像平移，缩放，旋转，微变形等简单变换的图像，计算机仍然能够识别出图中的”X”和”O”。就像下面这些情况，我们希望计算机依然能够很快并且很准的识别出来：<br><img src="https://img-blog.csdn.net/201803061805080" alt="img"></p><p>这也就是CNN出现所要解决的问题。</p><p>Features<br><img src="https://img-blog.csdn.net/20180306180518406" alt="img"></p><p>对于CNN来说，它是一块一块地来进行比对。它拿来比对的这个“小块”我们称之为Features（特征）。在两幅图中大致相同的位置找到一些粗糙的特征进行匹配，CNN能够更好的看到两幅图的相似性，相比起传统的整幅图逐一比对的方法。</p><p>每一个feature就像是一个小图（就是一个比较小的有值的二维数组）。不同的Feature匹配图像中不同的特征。在字母”X”的例子中，那些由对角线和交叉线组成的features基本上能够识别出大多数”X”所具有的重要特征。<br><img src="https://img-blog.csdn.net/20180306180528299" alt="img"></p><p>这些features很有可能就是匹配任何含有字母”X”的图中字母X的四个角和它的中心。那么具体到底是怎么匹配的呢？如下：<br><img src="https://img-blog.csdn.net/20180306180835951" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306180847317" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306180853914" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306180901666" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306180908456" alt="img"></p><p>看到这里是不是有了一点头目呢。但其实这只是第一步，你知道了这些Features是怎么在原图上面进行匹配的。但是你还不知道在这里面究竟进行的是怎样的数学计算，比如这个下面3*3的小块到底干了什么？<br><img src="https://img-blog.csdn.net/20180306180937780" alt="img"><br>接下来就跟进介绍里面的数学操作，也就是我们常说的“卷积”操作。</p><p>卷积(Convolution)<br><img src="https://img-blog.csdn.net/20180306180949227" alt="img"><br>Convolution</p><p><img src="https://img-blog.csdn.net/20180306181142569" alt="img"><br>当给你一张新的图时，CNN并不能准确地知道这些features到底要匹配原图的哪些部分，所以它会在原图中每一个可能的位置进行尝试。这样在原始整幅图上每一个位置进行匹配计算，我们相当于<strong>把这个feature变成了一个过滤器</strong>。这个我们用来匹配的过程就被称为卷积操作，这也就是卷积神经网络名字的由来。</p><p>这个卷积操作背后的数学知识其实非常的简单。要计算一个feature和其在原图上对应的某一小块的结果，只需要简单地将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可。</p><p>如果两个像素点都是白色（也就是值均为1），那么1<em>1 = 1，如果均为黑色，那么(-1)</em>(-1) = 1。不管哪种情况，每一对能够匹配上的像素，其相乘结果为1。类似地，任何不匹配的像素相乘结果为-1。如果一个feature（比如n<em>n）内部所有的像素都和原图中对应一小块（n</em>n）匹配上了，那么它们对应像素值相乘再累加就等于n2，然后除以像素点总个数n2，结果就是1。同理，如果每一个像素都不匹配，那么结果就是-1。</p><p>具体过程如下：</p><p><img src="https://img-blog.csdn.net/20180306181259497" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181307663" alt="img"><img src="https://img-blog.csdn.net/2018030618132177" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181348268" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181401547" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181407914" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181416401" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181427106" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181438929" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181456531" alt="img"></p><p>对于中间部分，也是一样的操作：</p><p><img src="https://img-blog.csdn.net/20180306181538273" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306190609622" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181558447" alt="img"><br><img src="https://img-blog.csdn.net/20180306181612616" alt="img"><br>最后整张图算完，大概就像下面这个样子：<br><img src="https://img-blog.csdn.net/20180306181626302" alt="img"><br>然后换用其他feature进行同样的操作，最后得到的结果就是这样了：<br><img src="https://img-blog.csdn.net/20180306181640996" alt="img"><br>为了完成我们的卷积，我们不断地重复着上述过程，将feature和图中每一块进行卷积操作。最后通过每一个feature的卷积操作，我们会得到一个新的二维数组。</p><p>这也可以理解为对原始图像进行过滤的结果，我们称之为feature map，它是每一个feature从原始图像中提取出来的“特征”。其中的值，越接近为1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。<br><img src="https://img-blog.csdn.net/20180306181719344" alt="img"><br>这样我们的原始图，经过不同feature的卷积操作就变成了一系列的feature map。我们可以很方便，直观地将这整个操作视为一个单独的处理过程。在CNN中，我们称之为卷积层(convolution layer)，这样你可能很快就会想到后面肯定还有其他的layer。没错，后面会提到。</p><p>我们可以将卷积层看成下面这个样子：<br><img src="https://img-blog.csdn.net/20180306181740905" alt="img"><br>因此可想而知，CNN其实做的操作也没什么复杂的。但是尽管我们能够以这一点篇幅就描述了CNN的工作，其内部的加法，乘法和除法操作的次数其实会增加地很快。从数学的角度来说，它们会随着图像的大小，每一个filter的大小和filter的数目呈线性增长。由于有这么多因素的影响，很容易使得这个问题的计算量变得相当的庞大，这也难怪很多微处理器制造商现在都在生产制造专业的芯片来跟上CNN计算的需求。</p><p>池化(Pooling)<br><img src="https://img-blog.csdn.net/20180306181756444" alt="img"><br>Pooling</p><p>CNN中使用的另一个有效的工具被称为“池化(Pooling)”。池化可以将一幅大的图像缩小，同时又保留其中的重要信息。池化背后的数学顶多也就是小学二年级水平。它就是将输入图像进行缩小，减少像素信息，只保留重要信息。通常情况下，池化都是2<em>2大小，比如对于max-pooling来说，就是取输入图像中2</em>2大小的块中的最大值，作为结果的像素值，相当于将原始图像缩小了4倍(注：同理，对于average-pooling来说，就是取2*2大小块的平均值作为结果的像素值)。</p><p>对于本文的这个例子，池化操作具体如下：</p><p><img src="https://img-blog.csdn.net/20180306181913201" alt="img"></p><p><img src="https://img-blog.csdn.net/2018030618192589" alt="img"><br><img src="https://img-blog.csdn.net/20180306181939786" alt="img"></p><p>不足的外面补”0”：</p><p><img src="https://img-blog.csdn.net/20180306185831170" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306181956608" alt="img"><br>经过最大池化操作（比如2*2大小）之后，一幅图就缩小为原来的四分之一了：<br><img src="https://img-blog.csdn.net/20180306182032388" alt="img"><br>然后对所有的feature map执行同样的操作，得到如下结果：<br><img src="https://img-blog.csdn.net/20180306182045131" alt="img"><br>因为最大池化（max-pooling）保留了每一个小块内的最大值，所以它相当于保留了这一块最佳的匹配结果（因为值越接近1表示匹配越好）。这也就意味着它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。这也就能够看出，CNN能够发现图像中是否具有某种特征，而不用在意到底在哪里具有这种特征。这也就能够帮助解决之前提到的计算机逐一像素匹配的死板做法。</p><p>当对所有的feature map执行池化操作之后，相当于一系列输入的大图变成了一系列小图。同样地，我们可以将这整个操作看作是一个操作，这也就是CNN中的池化层(pooling layer)，如下：<br><img src="https://img-blog.csdn.net/20180306182105596" alt="img"><br>通过加入池化层，可以很大程度上减少计算量，降低机器负载。</p><p>Normalization<br>激活函数Relu (Rectified Linear Units)<br>这是一个很小但是很重要的操作，叫做Relu(Rectified Linear Units)，或者修正线性单元。它的数学公式也很简单:<br>![][01]<br>[01]:<a href="http://latex.codecogs.com/png.latex?f(x" target="_blank" rel="noopener">http://latex.codecogs.com/png.latex?f(x</a>) = max(0, x)</p><p>对于输入的负值，输出全为0，对于正值，原样输出。关于其功能，更多详见这里。</p><p>下面我们看一下本文的离例子中relu激活函数具体操作：<br><img src="https://img-blog.csdn.net/20180306182143253" alt="img"><br><img src="https://img-blog.csdn.net/20180306182153402" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182210933" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182232154" alt="img"></p><p>最后，对整幅图操作之后，结果如下：<br><img src="https://img-blog.csdn.net/20180306182300573" alt="img"><br>同样地，在CNN中，我们这一系列操作视为一个操作，那么就得到Relu Layer，如下：<br><img src="https://img-blog.csdn.net/20180306182321381" alt="img"><br>Deep Learning<br>最后，我们将上面所提到的卷积，池化，激活放在一起，就是下面这个样子：<br><img src="https://img-blog.csdn.net/20180306182433190" alt="img"><br>然后，我们加大网络的深度，增加更多的层，就得到深度神经网络了：<br><img src="https://img-blog.csdn.net/20180306182451356" alt="img"><br>然后在不同的层，我们进行可视化，就可以看到本文开头提到的先验知识里面的结果了：<br><img src="https://img-blog.csdn.net/20180306182513523" alt="img"><br>全连接层(Fully connected layers)<br><img src="https://img-blog.csdn.net/20180306182530284" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182546940" alt="img"><img src="https://img-blog.csdn.net/20180306182538949" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182605783" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182622832" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182633541" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182741524" alt="img"></p><p><img src="https://img-blog.csdn.net/20180306182723860" alt="img"></p><p>根据结果判定为”X”：<br><img src="https://img-blog.csdn.net/20180306182758331" alt="img"></p><p>在这个过程中，我们定义这一系列操作为”全连接层“(Fully connected layers)：<br><img src="https://img-blog.csdn.net/20180306182819200" alt="img"><br>全连接层也能够有很多个，如下：<br><img src="https://img-blog.csdn.net/20180306182839210" alt="img"><br>【综合上述所有结构】<br><img src="https://img-blog.csdn.net/20180306182901787" alt="img"></p><h1 id="CNN三大核心思想"><a href="#CNN三大核心思想" class="headerlink" title="CNN三大核心思想"></a><strong>CNN三大核心思想</strong></h1><p>卷积神经网络CNN的出现是为了解决MLP多层感知器全连接和梯度发散的问题。其引入三个核心思想：1.局部感知(local field)，2.权值共享(Shared Weights)，3.下采样(subsampling)。极大地提升了计算速度，减少了连接数量。</p><p><strong>2.1 局部感知</strong></p><p>形象地说，就是模仿你的眼睛，想想看，你在看东西的时候，目光是聚焦在一个相对很小的局部的吧？严格一些说，普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上，而在卷积神经网络中，每个隐层节点只连接到图像某个足够小局部的像素点上，从而大大减少需要训练的权值参数。</p><p>对于一个 1000∗1000 的输入图像而言，如果下一个隐藏层的神经元数目为 106 个，采用全连接则有 1000∗1000∗106=1012 个权值参数，如此数目巨大的参数几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中 10∗10的局部图像相连接，那么此时的权值参数数量为 10∗10∗106=108，将直接减少4个数量级。</p><p>如下图所示，左边是每个像素的全连接，右边是每行隔两个像素作为局部连接，因此在数量上，少了很多权值参数数量（每一条连接每一条线需要有一个权值参数，具体忘记了的可以回顾单个[神经元模型]。因此 局部感知就是：<br>通过卷积操作，把 全连接变成局部连接 ，因为多层网络能够抽取高阶统计特性， 即使网络为局部连接，由于格外的突触连接和额外的神经交互作用，也可以使网络在不十分严格的意义下获得一个全局关系。</p><p><img src="http://xilinx.eetrend.com/files-eetrend-xilinx/article/201612/10827-27690-06.jpg" alt="img"></p><p>下面我们来详细看一下到底局部感知如何使 全连接变成局部连接，按照人工神经网络的方法，把输入图像的像素一字排开之后，每一个像素值就是一个神经元输入，需要对隐层或者输出层做全连接，如上图左侧所示。卷积神经网络引入卷积概念后，卷积核通过原图像，然后卷积核对原图像上符合卷积核大小的像素进行加权求和，每一次只是对符合卷积核的图像像素做卷积，这就是局部感知的概念，使 全连接变成局部连接。</p><p><img src="http://xilinx.eetrend.com/files-eetrend-xilinx/article/201612/10827-27691-07.png" alt="img"></p><p><strong>2.2 权值共享</strong></p><p>尽管局部感知使计算量减少了几个数量级，但权重参数数量依然很多。能不能再进一步减少呢？方法就是权值共享。</p><p>权值共享：不同的图像或者同一张图像共用一个卷积核，减少重复的卷积核。同一张图像当中可能会出现相同的特征，共享卷积核能够进一步减少权值参数。</p><p>如下图所示，为了找到鸟嘴，一个激活函数A需要检测图像左侧有没有鸟嘴，另外一个激活函数B需要检测另外一张图像中间有没有类似的鸟嘴。其实，鸟嘴都可能具有同样的特征，只需要一个激活函数C就可以了，这个时候，就可以共享同样的权值参数（也就是卷积核）。</p><p><img src="http://xilinx.eetrend.com/files-eetrend-xilinx/article/201612/10827-27692-08.png" alt="img"></p><p>如果使用了权值共享（共同使用一个卷积核），那么将可以大大减少卷积核的数量，加快运算速度。天下武功，唯快不破。</p><p><img src="http://xilinx.eetrend.com/files-eetrend-xilinx/article/201612/10827-27693-09.png" alt="img"></p><p>举个栗子，在局部连接中隐藏层的每一个神经元连接的是一个 10∗10 的局部图像，因此有 10∗10 个权值参数，将这 10∗10 个权值参数共享给剩下的神经元，也就是说隐藏层中 106 个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10∗10 个权值参数（也就是卷积核(也称滤波器)的大小。</p><p>尽管只有这么少的参数，依旧有出色的性能。但是，这样仅提取了图像的一种特征，如果要多提取出一些特征，需要增加多个卷积核，不同的卷积核能够得到图像的不同映射下的特征，称之为 Feature Map。如果有100个卷积核，最终的权值参数也仅为 100∗100=104 个而已。另外，偏置参数b也是共享的，同一种滤波器共享一个。</p><p><strong>2.3 池化</strong></p><p>在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。</p><p>pooling的好处有什么？<br>\1. 这些统计特征能够有更低的维度，减少计算量。<br>\2. 不容易过拟合，当参数过多的时候很容易造成过度拟合。<br>\3. 缩小图像的规模，提升计算速度。</p><p>如下图所示，原图是一张500∗500 的图像，经过subsampling之后哦，变成了一张 250∗250 的图像。这样操作的好处非常明显，虽然经过权值共享和局部连接后的图像权值参数已经大大减少，但是对于计算量来说，还是非常巨大，需要消费很大的计算时间，于是为了进一步减少计算量，于是加入了subsampling这个概念，不仅仅使图像像素减少了， 同时也减少计算时间。</p><p><img src="http://xilinx.eetrend.com/files-eetrend-xilinx/article/201612/10827-27694-10.png" alt="img"></p><p>举个栗子：以最大池化（Max Pooling）为例，1000×1000的图像经过10×10的卷积核卷积后，得到的是991×991的特征图，然后使用2×2的池化规模，即每4个点组成的小方块中，取最大的一个作为输出，最终得到的是496×496大小的特征图。</p><p>下采样，即池化，目的是减小特征图，池化规模一般为2×2。常用的池化方法有：</p><p>Pooling算法</p><p>最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。<br>均值池化（Mean Pooling）。取4个点的均值。<br>可训练池化。训练函数 f ，接受4个点为输入，出入1个点。</p><p>由于特征图的变长不一定是2的倍数，所以在边缘处理上也有两种方案：</p><p>保留边缘。将特征图的变长用0填充为2的倍数，然后再池化。<br>忽略边缘。将多出来的边缘直接省去。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CNN学习&lt;/p&gt;
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="CNN" scheme="http://kodgv.xyz/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>Attention</title>
    <link href="http://kodgv.xyz/2019/04/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Attention/"/>
    <id>http://kodgv.xyz/2019/04/15/神经网络/Attention/</id>
    <published>2019-04-15T08:07:35.000Z</published>
    <updated>2019-04-16T14:45:16.960Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]<br>Attention本质就是一系列注意力分配系数，也就是一系列权重参数罢了。<br><a id="more"></a></p><p>来源：<a href="https://www.cnblogs.com/guoyaohua/p/9429924.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/9429924.html</a></p><h1 id="1-什么是Attention机制？"><a href="#1-什么是Attention机制？" class="headerlink" title="1. 什么是Attention机制？"></a>1. 什么是Attention机制？</h1><p>　　最近两年，注意力模型（Attention Model）被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，是深度学习技术中最值得关注与深入了解的核心技术之一。</p><p>　　当我们人在看一样东西的时候，我们当前时刻关注的一定是我们当前正在看的这样东西的某一地方，换句话说，当我们目光移到别处时，注意力随着目光的移动也在转移，这意味着，当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。————-（思考：对于图片，会有些特别显眼的场景会率先吸引住注意力，那是因为脑袋中对这类东西很敏感。对于文本，我们大都是带目的性的去读，顺序查找，顺序读，但是在理解的过程中，我们是根据我们自带的目的去理解，去关注的。 注意力模型应该与具体的目的(或者任务)相结合。）</p><p>　　从Attention的作用角度出发，我们就可以从两个角度来分类Attention种类：<strong>Spatial Attention 空间注意力</strong>和<strong>Temporal Attention 时间注意力</strong>。更具实际的应用，也可以将Attention分为<strong>Soft Attention</strong>和<strong>Hard Attention</strong>。<strong>Soft Attention是所有的数据都会注意，都会计算出相应的注意力权值，不会设置筛选条件。Hard Attention会在生成注意力权重后筛选掉一部分不符合条件的注意力，让它的注意力权值为0，即可以理解为不再注意这些不符合条件的部分。</strong></p><h1 id="2-先了解编码-解码框架：Encoder-Decoder框架"><a href="#2-先了解编码-解码框架：Encoder-Decoder框架" class="headerlink" title="2. 先了解编码-解码框架：Encoder-Decoder框架"></a>2. 先了解编码-解码框架：Encoder-Decoder框架</h1><p>　　目前绝大多数文献中出现的AM模型是附着在Encoder-Decoder框架下的，当然，其实AM模型可以看作一种通用的思想，本身并不依赖于Encoder-Decoder模型，这点需要注意。<strong>Encoder-Decoder框架可以看作是一种文本处理领域的研究模式</strong>，应用场景异常广泛，本身就值得细谈。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806135448102-667913176.png" alt="img"></p><p>图1 抽象的Encoder-Decoder框架</p><p>　　Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对<x,y>。 ————（思考：<x,y>对很通用，X是一个问句，Y是答案；X是一个句子，Y是抽取的关系三元组；X是汉语句子，Y是汉语句子的英文翻译。等等），我们的目标是给定输入句子X，期待通过Encoder-Decoder框架来生成目标句子Y。X和Y可以是同一种语言，也可以是两种不同的语言。而X和Y分别由各自的单词序列构成：</x,y></x,y></p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806141247005-858346593.png" alt="img"></p><p>　　Encoder顾名思义就是对输入句子X进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806141320980-818442456.png" alt="img"></p><p>　　对于解码器Decoder来说，其任务是根据句子X的中间语义表示C和之前已经生成的历史信息y1,y2….yi-1来生成i时刻要生成的单词yi ：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806141350194-1066590310.png" alt="img"></p><p>　　每个yi都依次这么产生，那么看起来就是整个系统根据输入句子X生成了目标句子Y。 ———（思考：其实这里的Encoder-Decoder是一个序列到序列的模型seq2seq，这个模型是对顺序有依赖的。）</p><p>　　Encoder-Decoder是个非常通用的计算框架，至于Encoder和Decoder具体使用什么模型都是由研究者自己定的，常见的比如 CNN / RNN / BiRNN / GRU / LSTM / Deep LSTM 等，这里的变化组合非常多。 ———（思考：人的学习过程包括输入、输出、外界评价。Encoder模型类似于人的输入学习过程，Decoder模型类似于人的输出学习过程，对输出的内容进行评价就类似于损失函数。英语老师给我上了几堂英语课，我在不断的输入Encoder；突然有一个随堂测试，我得做题输出Decoder；最后英语老师改卷子，给我一个分数，不对的地方我得反思调整我对输入数据的加工方式。）———-（再思考：关于英语翻译。课本上的单词和课文是原始数据输入，相当于X；我在大脑里加工这些数据，相当于Encoder模型，我的脑子里有很多加工后的数据，相当于C；现在要让我翻译一个英语句子，这个任务相当于Y，我不能翻课本，所以我只能借助我脑袋里加工的数据C去翻译这个句子，即我得动脑子，相当于Decoder。 学习的过程是什么都要学，要分类整理，要增加线索，并不知道未来的某天能用到什么，所以Encoder-Decoder是一个泛泛学习的框架）</p><h1 id="3-Attention-Model"><a href="#3-Attention-Model" class="headerlink" title="3. Attention Model"></a>3. Attention Model</h1><p>　　以上介绍的Encoder-Decoder模型是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Y中每个单词的生成过程如下：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806141638641-1078830067.png" alt="img"></p><p>　　其中<strong>f是decoder的非线性变换函数</strong>。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，是y1,y2也好，还是y3也好，他们使用的句子X的语义编码C都是一样的，没有任何区别。而语义编码C是由句子X的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实<strong>句子X中任意单词对生成某个目标单词yi来说影响力都是相同的，没有任何区别</strong>（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，估计这也是为何Google提出Sequence to Sequence模型时发现把输入句子逆序输入做翻译效果会更好的小Trick的原因）。这就是为何说这个模型<strong>没有体现出注意力</strong>的缘由。</p><p>　　引入AM模型，以翻译一个英语句子举例：输入X：Tom chase Jerry。 理想输出：汤姆追逐杰瑞。</p><p>　　应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：</p><p>（Tom,0.3）（Chase,0.2）（Jerry,0.5）</p><p>　　每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词Yi的时候，<strong>原先都是相同的中间语义表示C会替换成根据当前生成单词而不断变化的Ci</strong>。<strong>理解AM模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注**</strong>意力模型的变化的<strong>**Ci</strong>。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142115912-1682939089.png" alt="img"></p><p>图2 引入AM模型的Encoder-Decoder框架</p><p>　　即生成目标句子单词的过程成了下面的形式：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142159178-1634092293.png" alt="img"></p><p>　　而<strong>每个Ci可能对应着不同的源语句子单词的注意力分配概率分布</strong>，比如对于上面的英汉翻译来说，其对应的信息可能如下：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142302309-2112006022.png" alt="img"></p><p>　　其中，<strong>f2函数代表Encoder对输入英文单词的某种变换函数</strong>，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；<strong>g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数</strong>，一般的做法中，<strong>g函数就是对构成元素加权求和</strong>，也就是常常在论文里看到的下列公式：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142333000-1492896283.png" alt="img"></p><p>　　假设Ci中那个i就是上面的“汤姆”，那么<strong>Tx就是3，代表输入句子的长度</strong>，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)，对应的注意力模型权值分别是0.6,0.2,0.2，<strong>所以g函数就是个加权求和函数</strong>。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的<strong>中间语义表示Ci的形成过程</strong>类似下图：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142412460-1178080370.png" alt="img"></p><p>图3 Ci的形成过程</p><p>　　这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，<strong>你怎么知道AM模型所需要的输入句子单词注意力分配概率分布值呢？</strong>就是说“汤姆”对应的概率分布：</p><h4 id="划重点-注意力权重获取的过程-（Tom-0-3）（Chase-0-2）（Jerry-0-5）是如何得到的呢？"><a href="#划重点-注意力权重获取的过程-（Tom-0-3）（Chase-0-2）（Jerry-0-5）是如何得到的呢？" class="headerlink" title="　　划重点(注意力权重获取的过程)（Tom,0.3）（Chase,0.2）（Jerry,0.5）是如何得到的呢？"></a>　　划重点(注意力权重获取的过程)（Tom,0.3）（Chase,0.2）（Jerry,0.5）是如何得到的呢？</h4><p>　　为了便于说明，我们假设对图1的非AM模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图1的图转换为下图：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142520665-1351011214.png" alt="img"></p><p>图4 RNN作为具体模型的Encoder-Decoder框架</p><p>　　注意力分配概率分布值的通用计算过程：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142634471-1534518198.png" alt="img"></p><p>图5 AM注意力分配概率计算</p><p>　　对于采用RNN的Decoder来说，如果要生成 yi 单词，在时刻 i ，我们是可以知道在生成 Yi 之前的隐层节点i时刻的输出值 Hi 的，而我们的目的是要计算生成 Yi 时的输入句子单词“Tom”、“Chase”、“Jerry”对 Yi 来说的注意力分配概率分布，那么可以用i时刻的<strong>隐层节点状态 Hi 去一一和输入句子中每个单词对应的RNN隐层节点状态 hj 进行对比</strong>，即<strong>通过函数 F(hj,Hi) 来获得目标单词 Yi 和每个输入单词对应的对齐可能性</strong>，这个F函数在不同论文里可能会采取不同的方法，然后<strong>函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值（这就得到了注意力权重）</strong>。图5显示的是当输出单词为“汤姆”时刻对应的输入句子单词的对齐概率。绝大多数AM模型都是<strong>采取上述的计算框架来计算注意力分配概率分布信息</strong>，<strong>区别只是在F的定义上可能有所不同</strong>。</p><p>　　<strong>上述内容就是论文里面常常提到的Soft Attention Model</strong>（任何数据都会给一个权值，没有筛选条件）的基本思想，你能在文献里面看到的大多数AM模型基本就是这个模型，区别很可能只是把这个模型用来解决不同的应用问题。那么<strong>怎么理解AM模型的物理含义呢？</strong>一般文献里会把AM模型看作是<strong>单词对齐模型</strong>，这是非常有道理的。目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的<strong>对齐概率</strong>，这在机器翻译语境下是非常直观的：<strong>传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤</strong>，<strong>而注意力模型其实起的是相同的作用</strong>。在其他应用里面把AM模型理解成输入句子和目标句子单词之间的对齐概率也是很顺畅的想法。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806164619665-1207996587.png" alt="img"></p><p>图6 Google 神经网络机器翻译系统结构图</p><p>　　图6所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。</p><p>当然，从概念上理解的话，<strong>把AM模型理解成影响力模型也是合理的</strong>，就是说生成目标单词的时候，输入句子每个单词对于生成这个单词有多大的影响程度。这种想法也是比较好理解AM模型物理意义的一种思维方式。</p><p>　　图7是论文“<a href="http://www.aclweb.org/anthology/D15-1044" target="_blank" rel="noopener">A Neural Attention Model for Sentence Summarization</a>”中，Rush用AM模型来做生成式摘要给出的一个AM的一个非常直观的例子。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806143753246-1275015223.png" alt="img"></p><p>图7 句子生成式摘要例子</p><p>　　这个例子中，Encoder-Decoder框架的输入句子X是：“russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism”。对应图中纵坐标的句子。系统生成的摘要句子Y是：“russia calls for joint front against terrorism”，对应图中横坐标的句子。可以看出模型已经把句子主体部分正确地抽出来了。<strong>矩阵中每一列代表生成的目标单词对应输入句子每个单词的AM分配概率</strong>，颜色越深代表分配到的概率越大。这个例子对于直观理解AM是很有帮助作用。</p><p>　　《<a href="http://www.aclweb.org/anthology/D15-1044" target="_blank" rel="noopener">A Neural Attention Model for Sentence Summarization</a>》论文提供的实验数据集链接(开放可用)：<a href="https://duc.nist.gov/data.html" target="_blank" rel="noopener">DUC 2004</a>，感兴趣的朋友可以下载看看。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806143957137-1497662277.png" alt="img"></p><p>图8 摘要生成 开放数据集</p><h1 id="4-Attention机制的本质思想"><a href="#4-Attention机制的本质思想" class="headerlink" title="4. Attention机制的本质思想"></a><strong>4. Attention机制的本质思想</strong></h1><p>　　如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806165033943-1072442256.png" alt="img"></p><p>图9 Attention机制的本质思想</p><p>　　我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的<key,value>数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</key,value></p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806165351485-870137528.png" alt="img"></p><p>　　其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p><p>　　当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p><p>　　从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p><p>　　至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806191525966-820975705.png" alt="img"></p><p>图10 三阶段计算Attention过程</p><p>　　在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个 Keyi ，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806191654338-264846698.png" alt="img"></p><p>　　第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><p>  <img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806195333945-948374778.png" alt="img"></p><p>　　第二阶段的计算结果 ai 即为 Valuei 对应的权重系数，然后进行加权求和即可得到Attention数值：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806195421837-251685236.png" alt="img"></p><p>　　通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p><h1 id="5-Self-Attention模型"><a href="#5-Self-Attention模型" class="headerlink" title="5. Self Attention模型"></a><strong>5. Self Attention模型</strong></h1><p>　　通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。<strong>Self Attention也经常被称为intra Attention（内部Attention）</strong>，最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型(抛弃了传统的RNN)。</p><p>　　在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素和Source中的所有元素之间。<strong>而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。</strong>其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。</p><p>　　elf Attention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention不同，它分别在source端和target端进行，仅与source input或者target input自身相关的Self Attention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self Attention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。因此，self Attention Attention比传统的Attention mechanism效果要好，主要原因之一是，传统的Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806200854591-1266493040.png" alt="img"></p><p>图11 可视化Self Attention实例</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806200949175-1322518214.png" alt="img"></p><p>图12 可视化Self Attention实例</p><p>　　从两张图（图11、图12）可以看出，<strong>Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。</strong></p><p>　　很明显，<strong>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征</strong>，<strong>因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</strong></p><p>　　但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以<strong>远距离依赖特征之间的距离被极大缩短</strong>，有利于有效地利用这些特征。除此外，<strong>Self Attention对于增加计算的并行性也有直接帮助作用</strong>。这是为何Self Attention逐渐被广泛使用的主要原因。</p><p>​    <strong>但是attention无法记录词序，所以在self-attention中增加了position embedding</strong></p><h1 id="五种attention模型"><a href="#五种attention模型" class="headerlink" title="五种attention模型"></a>五种attention模型</h1><h2 id="hard-attention-amp-soft-attention"><a href="#hard-attention-amp-soft-attention" class="headerlink" title="hard attention&amp;soft attention"></a>hard attention&amp;soft attention</h2><p><img src="https://image.jiqizhixin.com/uploads/editor/72f889a2-df04-4a85-9c2c-1591c5375537/1528709501819.png" alt="img"></p><p>■ 论文 | Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/812" target="_blank" rel="noopener">https://www.paperweekly.site/papers/812</a></p><p>■ 源码 | <a href="https://github.com/kelvinxu/arctic-captions" target="_blank" rel="noopener">https://github.com/kelvinxu/arctic-captions</a></p><p>文章讨论的场景是图像描述生成（Image Caption Generation），对于这种场景，先放一张图，感受一下 attention 的框架。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/33073f25-48d8-4b17-a179-49191bdb5357/1528709501703.png" alt="img"></p><p>文章提出了两种 attention 模式，即 hard attention 和 soft attention，来感受一下这两种 attention。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/fe4ede76-44de-4055-93e6-3ce99541db21/1528709502108.png" alt="img"></p><p>可以看到，hard attention 会专注于很小的区域，而 soft attention 的注意力相对发散。模型的 encoder 利用 CNN (VGG net)，提取出图像的 L 个 D 维的向量<img src="https://image.jiqizhixin.com/uploads/editor/7a76e6b0-4f08-4f55-8b01-839552bd0de1/1528709501860.png" alt="img">，每个向量表示图像的一部分信息。</p><p>decoder 是一个 LSTM，每个 timestep t 的输入包含三个部分，即 context vector Zt 、前一个 timestep 的 hidden state<img src="https://image.jiqizhixin.com/uploads/editor/a498b30a-f97c-483d-b121-1499bcabe34f/1528709501897.png" alt="img">、前一个 timestep 的 output<img src="https://image.jiqizhixin.com/uploads/editor/89174d88-3afc-4554-ba26-b48065473355/1528709502635.png" alt="img">。 Zt 由 {ai} 和权重 {αti} 通过加权得到。这里的权重 αti 通过attention模型 <em>f</em>att 来计算得到，而本文中的 <em>f</em>att 是一个多层感知机（multilayer perceptron）。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/55f3f8bd-91c7-4d65-917d-292e51d2962f/1528709502715.png" alt="img"></p><p>从而可以计算<img src="https://image.jiqizhixin.com/uploads/editor/b57ff198-4b7c-4e6d-a534-4ca57b825d3b/1528709502754.png" alt="img">。接下来文章重点讨论 hard（也叫 stochastic attention）和 soft（也叫 deterministic）两种 attention 模式。</p><p><strong>1. Stochastic “Hard” Attention</strong> </p><p>记 St 为 decoder 第 t 个时刻的 attention 所关注的位置编号， Sti 表示第 t 时刻 attention 是否关注位置 i ， Sti 服从多元伯努利分布（multinoulli distribution）， 对于任意的 t ，Sti,i=1,2,…,L 中有且只有取 1，其余全部为 0，所以 [St1,St2,…,stL] 是 one-hot 形式。这种 attention 每次只 focus 一个位置的做法，就是“hard”称谓的来源。 Zt 也就被视为一个变量，计算如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/afec742f-d647-43c6-b4dc-9618ac878628/1528709502797.png" alt="img"></p><p>问题是 αti 怎么算呢？把 αti 视为隐变量，研究模型的目标函数，进而研究目标函数对参数的梯度。直观理解，模型要根据 a=(a1,…,aL) 来生成序列 y=(y1,…,yC) ，所以目标可以是最大化 log p(y|a) ，但这里没有显式的包含 s ，所以作者利用著名的 Jensen 不等式（Jensen’s inequality）对目标函数做了转化，得到了目标函数的一个 lower bound，如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/97d7901e-3b6d-4fbc-879a-d81293368368/1528709503022.png" alt="img"></p><p>这里的 s ={ s1,…,sC }，是时间轴上的重点 focus 的序列，理论上这种序列共有个。 然后就用 log p(y|a) 代替原始的目标函数，对模型的参数 W 算 gradient。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/0f5a2605-cea7-4530-8415-e9d3dfb1d3b5/1528709503215.png" alt="img"></p><p>然后利用蒙特卡洛方法对 s 进行抽样，我们做 N 次这样的抽样实验，记每次取到的序列是<img src="https://image.jiqizhixin.com/uploads/editor/8b676888-0fab-4a40-9706-d829823fce0c/1528709503296.png" alt="img">，易知<img src="https://image.jiqizhixin.com/uploads/editor/1adc9914-c26a-47d7-a3e7-3889f9090160/1528709503251.png" alt="img">的概率为<img src="https://image.jiqizhixin.com/uploads/editor/5d82f812-b08a-48fd-bdb9-575fe66c15ce/1528709503333.png" alt="img">，所以上面的求 gradient 的结果即为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/5c7be181-dfdc-4458-b477-765b1613b234/1528709503396.png" alt="img"></p><p>接下来的一些细节涉及reinforcement learning，感兴趣的同学可以去看这篇 paper。</p><p><strong>2. Deterministic “Soft” Attention</strong> </p><p>说完“硬”的 attention，再来说说“软”的 attention。 相对来说 soft attention 很好理解，在 hard attention 里面，每个时刻 t 模型的序列 [ St1,…,StL ] 只有一个取 1，其余全部为 0，也就是说每次只 focus 一个位置，而 soft attention 每次会照顾到全部的位置，只是不同位置的权重不同罢了。这时 Zt 即为 ai 的加权求和：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/419b690d-2d19-4506-8f2e-67edf525fa7c/1528709503873.png" alt="img"></p><p>这样 soft attention 是光滑的且可微的（即目标函数，也就是 LSTM 的目标函数对权重αti 是可微的，原因很简单，因为目标函数对 Zt 可微，而 Zt 对 αti 可微，根据 chain rule 可得目标函数对 αti 可微）。</p><p>文章还对这种 soft attention 做了微调：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/caa7b6b6-ce70-4e19-aa1b-8a77a7914c09/1528709503914.png" alt="img"></p><p>其中<img src="https://image.jiqizhixin.com/uploads/editor/7de8d1a3-3be6-4a66-801e-25cb0bee7b00/1528709504003.png" alt="img">，用来调节 context vector 在 LSTM 中的比重（相对于<img src="https://image.jiqizhixin.com/uploads/editor/ae0fcc90-f8ef-40d8-be2a-8f9721dae24d/1528709501949.png" alt="img"><img src="https://image.jiqizhixin.com/uploads/editor/90abe04d-a4ea-4f7b-9d6b-d5832ad531f1/1528709502672.png" alt="img">的比重）。</p><p>btw，模型的 loss function 加入了 αti 的正则项。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/3193dce5-51b7-4ae2-893d-0696635c9ec9/1528709504051.png" alt="img"></p><h2 id="global-attention-amp-local-attention"><a href="#global-attention-amp-local-attention" class="headerlink" title="global attention &amp; local attention"></a>global attention &amp; local attention</h2><p><img src="https://image.jiqizhixin.com/uploads/editor/e3a3408b-7e64-4752-b8f9-a71563bb06c6/1528709504652.png" alt="img"></p><p>■ 论文 | Effective Approaches to Attention-based Neural Machine Translation</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/806" target="_blank" rel="noopener">https://www.paperweekly.site/papers/806</a></p><p>■ 源码 | <a href="https://github.com/lmthang/nmt.matlab" target="_blank" rel="noopener">https://github.com/lmthang/nmt.matlab</a></p><p>文章提出了两种 attention 的改进版本，即 global attention 和 local attention。先感受一下 global attention 和 local attention 长什么样子。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/55e64daf-5e26-474f-bdea-b99669de205c/1528709504213.png" alt="img"></p><p><strong>▲</strong> Global Attention</p><p><img src="https://image.jiqizhixin.com/uploads/editor/4df6785f-b67e-4a41-9234-401d03c07b59/1528709504569.png" alt="img"></p><p><strong>▲</strong> Local Attention</p><p>文章指出，local attention 可以视为 hard attention 和 soft attention 的混合体（优势上的混合），因为它的计算复杂度要低于 global attention、soft attention，而且与 hard attention 不同的是，local attention 几乎处处可微，易与训练。 文章以机器翻译为场景， x1,…,xn 为 source sentence， y1,…,ym 为 target sentence， c1,…,cm 为 encoder 产生的 context vector，objective function 为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/129e5871-39db-4005-8062-ca744a216335/1528709504768.png" alt="img"></p><p>Ct 来源于 encoder 中多个 source position 所产生的 hidden states，global attention 和 local attention 的主要区别在于 attention 所 forcus 的 source positions 数目的不同：如果 attention forcus 全部的 position，则是 global attention，反之，若只 focus 一部分 position，则为 local attention。 </p><p>由此可见，这里的 global attention、local attention 和 soft attention 并无本质上的区别，两篇 paper 模型的差别只是在 LSTM 结构上有微小的差别。 </p><p>在 decoder 的时刻 t ，在利用 global attention 或 local attention 得到 context vector Ct之后，结合 ht ，对二者做 concatenate 操作，得到 attention hidden state。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/7acdb6ad-0741-4531-a96a-a9e128afda32/1528709505007.png" alt="img"></p><p>最后利用 softmax 产出该时刻的输出：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/078f1af3-0c67-4b13-9359-14f1b48aea4d/1528709505051.png" alt="img"></p><p>下面重点介绍 global attention、local attention。</p><p><strong>1. global attention</strong> </p><p>global attention 在计算 context vector ct 的时候会考虑 encoder 所产生的全部hidden state。记 decoder 时刻 t 的 target hidden为 ht，encoder 的全部 hidden state 为<img src="https://image.jiqizhixin.com/uploads/editor/1a9b3f6f-cd94-4ba5-94d9-96e096dd0802/1528709505151.png" alt="img">，对于其中任意<img src="https://image.jiqizhixin.com/uploads/editor/cab720ef-628c-4af9-b827-f2839ba387a5/1528709505228.png" alt="img">，其权重 αts 为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/ebea9e73-7f59-42a1-975b-9cd9f44ee7ba/1528709505312.png" alt="img"></p><p>而其中的<img src="https://image.jiqizhixin.com/uploads/editor/31b5d177-4123-47f7-b75a-766690049b6f/1528709505690.png" alt="img">，文章给出了四种种计算方法（文章称为 alignment function）：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/5b89594b-f47c-4870-be8a-f3406b38088b/1528709505811.png" alt="img"></p><p><img src="https://image.jiqizhixin.com/uploads/editor/1f8e7063-9c3f-4c96-a528-a841c6117825/1528709505852.png" alt="img"></p><p>四种方法都比较直观、简单。在得到这些权重后， ct 的计算是很自然的，即为<img src="https://image.jiqizhixin.com/uploads/editor/1b37607a-9316-442c-9226-49595d34ff02/1528709505264.png" alt="img">的 weighted summation。</p><p><strong>2. local attention</strong> </p><p>global attention 可能的缺点在于每次都要扫描全部的 source hidden state，计算开销较大，对于长句翻译不利，为了提升效率，提出 local attention，每次只 focus 一小部分的 source position。 </p><p>这里，context vector ct 的计算只 focus 窗口 [pt-D,pt+D] 内的 2D+1 个source hidden states（若发生越界，则忽略界外的 source hidden states）。</p><p>其中 pt 是一个 source position index，可以理解为 attention 的“焦点”，作为模型的参数， D 根据经验来选择（文章选用 10）。 关于 pt 的计算，文章给出了两种计算方案：</p><ul><li><strong>Monotonic alignment (local-m)</strong></li></ul><p><img src="https://image.jiqizhixin.com/uploads/editor/4fe24e96-b61e-4bb4-9cb6-88760dc0f228/1528709505970.png" alt="img"></p><ul><li><strong>Predictive alignment (local-p)</strong></li></ul><p><img src="https://image.jiqizhixin.com/uploads/editor/ec31a794-00f9-4dba-b9d8-89295f1aa1d0/1528709506099.png" alt="img"></p><p>其中 Wp 和 vp 是模型的参数， S 是 source sentence 的长度，易知 pt∈[0,S] 。 权重αt(s) 的计算如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/703a445f-62ed-43dd-a313-256883bac239/1528709506142.png" alt="img"></p><p>可以看出，距离中心 pt 越远的位置，其位置上的 source hidden state 对应的权重就会被压缩地越厉害。</p><h2 id="self-attention-amp-multiple-head-attention"><a href="#self-attention-amp-multiple-head-attention" class="headerlink" title="self-attention &amp; multiple-head attention"></a>self-attention &amp; multiple-head attention</h2><p><img src="https://image.jiqizhixin.com/uploads/editor/d993ae54-145e-4e12-93dd-1f3c94d156f9/1528709506751.png" alt="img"></p><p>■ 论文 | Attention Is All You Need</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/224" target="_blank" rel="noopener">https://www.paperweekly.site/papers/224</a></p><p>■ 源码 | <a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">https://github.com/Kyubyong/transformer</a></p><p><img src="https://image.jiqizhixin.com/uploads/editor/da5cb6b1-aa95-48b4-a728-01bde8de45b4/1528709506930.png" alt="img"></p><p>■ 论文 | Weighted Transformer Network for Machine Translation</p><p>■ 链接 | <a href="https://www.paperweekly.site/papers/2013" target="_blank" rel="noopener">https://www.paperweekly.site/papers/2013</a></p><p>■ 源码 | <a href="https://github.com/JayParks/transformer" target="_blank" rel="noopener">https://github.com/JayParks/transformer</a></p><p>作者首先指出，结合了 RNN（及其变体）和注意力机制的模型在序列建模领域取得了不错的成绩，但由于 RNN 的循环特性导致其不利于并行计算，所以模型的训练时间往往较长，在 GPU 上一个大一点的 seq2seq 模型通常要跑上几天，所以作者对 RNN 深恶痛绝，遂决定舍弃 RNN，只用注意力模型来进行序列的建模。 </p><p>作者提出一种新型的网络结构，并起了个名字 Transformer，里面所包含的注意力机制称之为 self-attention。作者骄傲地宣称他这套 Transformer 是能够计算 input 和 output 的 representation 而不借助 RNN 的唯一的 model，所以作者说有 attention 就够了。</p><p>模型同样包含 encoder 和 decoder 两个 stage，encoder 和 decoder 都是抛弃 RNN，而是用堆叠起来的 self-attention，和 fully-connected layer 来完成，模型的架构如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/43ca78f9-024c-4a20-aa8c-681a42b6a734/1528709507321.png" alt="img"></p><p>从图中可以看出，模型共包含三个 attention 成分，分别是 encoder 的 self-attention，decoder 的 self-attention，以及连接 encoder 和 decoder 的 attention。  </p><p>这三个 attention block 都是 multi-head attention 的形式，输入都是 query Q 、key K 、value V 三个元素，只是 Q 、 K 、 V 的取值不同罢了。接下来重点讨论最核心的模块 multi-head attention（多头注意力）。 </p><p>multi-head attention 由多个 scaled dot-product attention 这样的基础单元经过 stack 而成。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/2a179490-64ac-4b22-b42d-1c3de5e08b5c/1528709507410.png" alt="img"></p><p>那重点就变成 scaled dot-product attention 是什么鬼了。按字面意思理解，scaled dot-product attention 即缩放了的点乘注意力，我们来对它进行研究。 </p><p>在这之前，我们先回顾一下上文提到的传统的 attention 方法（例如 global attention，score 采用 dot 形式）。</p><p>记 decoder 时刻 t 的 target hidden state 为 ht，encoder 得到的全部 source hidden state为<img src="https://image.jiqizhixin.com/uploads/editor/9f26e09b-d1f7-4e77-b1e8-2e8e5642ae9c/1528709507675.png" alt="img">，则 decoder 的 context vector ct 的计算过程如下：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/57b33413-699a-4c04-ae99-0bddaf11ea32/1528709507900.png" alt="img"></p><p>作者先抛出三个名词 query Q、key K、value V，然后计算这三个元素的 attention。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/cbffe9ff-5149-4959-ac8b-f6c4714ff9c9/1528709509122.png" alt="img"></p><p>我的写法与论文有细微差别，但为了接下来说明的简便，我姑且简化成这样。这个 Attention 的计算跟上面的 (*) 式有几分相似。</p><p>那么 Q、K、V 到底是什么？论文里讲的比较晦涩，说说我的理解。encoder 里的 attention 叫 self-attention，顾名思义，就是自己和自己做 attention。</p><p>抛开这篇论文的做法，让我们激活自己的创造力，在传统的 seq2seq 中的 encoder 阶段，我们得到 n 个时刻的 hidden states 之后，可以用每一时刻的 hidden state hi，去分别和任意的 hidden state hj,j=1,2,…,n 计算 attention，这就有点 self-attention 的意思。</p><p>回到当前的模型，由于抛弃了 RNN，encoder 过程就没了 hidden states，那拿什么做 self-attention 来自嗨呢？</p><p>可以想到，假如作为 input 的 sequence 共有 n 个 word，那么我可以先对每一个 word 做 embedding 吧？就得到 n 个 embedding，然后我就可以用 embedding 代替 hidden state 来做 self-attention 了。所以 Q 这个矩阵里面装的就是全部的 word embedding，K、V 也是一样。</p><p>所以为什么管 Q 叫query？就是你每次拿一个 word embedding，去“查询”其和任意的 word embedding 的 match 程度（也就是 attention 的大小），你一共要做 n 轮这样的操作。 </p><p>我们记 word embedding 的 dimension 为 dmodel ，所以 Q 的 shape 就是 n*dmodel， K、V 也是一样，第 i 个 word 的 embedding 为 vi，所以该 word 的 attention 应为：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/17ebfb62-21b6-4df4-99ea-08d3bf275d36/1528709509168.png" alt="img"></p><p>那同时做全部 word 的 attention，则是：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/529a6cbf-f2fe-4d8c-a90d-f8a0567f6bf8/1528709508901.png" alt="img"></p><p>scaled dot-product attention 基本就是这样了。基于 RNN 的传统 encoder 在每个时刻会有输入和输出，而现在 encoder 由于抛弃了 RNN 序列模型，所以可以一下子把序列的全部内容输进去，来一次 self-attention 的自嗨。</p><p>理解了 scaled dot-product attention 之后，multi-head attention 就好理解了，因为就是 scaled dot-product attention 的 stacking。</p><p>先把 Q、K、V 做 linear transformation，然后对新生成的 Q’、K’、V’ 算 attention，重复这样的操作 h 次，然后把 h 次的结果做 concat，最后再做一次 linear transformation，就是 multi-head attention 这个小 block 的输出了。 </p><p><img src="https://image.jiqizhixin.com/uploads/editor/b39171e2-f71c-4ea9-a9de-0d6c8d983550/1528709508952.png" alt="img"></p><p>以上介绍了 encoder 的 self-attention。decoder 中的 encoder-decoder attention 道理类似，可以理解为用 decoder 中的每个 vi 对 encoder 中的 vj 做一种交叉 attention。</p><p>decoder 中的 self-attention 也一样的道理，只是要注意一点，decoder 中你在用 vi 对 vj 做 attention 时，有一些 pair 是不合法的。原因在于，虽然 encoder 阶段你可以把序列的全部 word 一次全输入进去，但是 decoder 阶段却并不总是可以，想象一下你在做 inference，decoder 的产出还是按从左至右的顺序，所以你的 vi 是没机会和 vj ( j&gt;i ) 做 attention 的。</p><p>那怎么将这一点体现在 attention 的计算中呢？文中说只需要令 score(vi,vj)=-∞ 即可。为何？因为这样的话：</p><p><img src="https://image.jiqizhixin.com/uploads/editor/13be2088-2463-474a-855b-b2193a08ce44/1528709509073.png" alt="img"></p><p>所以在计算 vi 的 self-attention 的时候，就能够把 vj 屏蔽掉。所以这个问题也就解决了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;br&gt;Attention本质就是一系列注意力分配系数，也就是一系列权重参数罢了。&lt;br&gt;
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Attention" scheme="http://kodgv.xyz/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>LSTM</title>
    <link href="http://kodgv.xyz/2019/04/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LSTM/"/>
    <id>http://kodgv.xyz/2019/04/15/神经网络/LSTM/</id>
    <published>2019-04-15T02:38:35.000Z</published>
    <updated>2019-04-20T14:31:12.373Z</updated>
    
    <content type="html"><![CDATA[<p>LSTM详解，把cell转变当作传送带的思想</p><a id="more"></a><p>来源：<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p>[TOC]</p><h2 id="‘递归神经网络"><a href="#‘递归神经网络" class="headerlink" title="‘递归神经网络"></a>‘递归神经网络</h2><p>​    人类不会每一秒钟都从头开始思考。当你阅读这篇文章的时候，你理解每一个单词都是基于你对之前单词的理解。你不会把所有的东西都扔掉，重新开始思考。你的想法是有持久性的。<br>​    传统的神经网络做不到这一点，这似乎是一个主要的缺点。例如，假设您想要对电影中每一秒发生的事件进行分类。目前还不清楚传统的神经网络如何利用其对电影中先前事件的推理来为后来的事件提供信息。<br>​    递归神经网络解决了这个问题。它们是包含循环的网络，允许信息持续存在。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" width="200" hegiht="300" align="center/"></p><p>在上面的图中，一个神经网络块a，观察某个输入xt并输出一个值ht。循环允许信息从网络的一个步骤传递到下一个步骤。</p><p>​    这些循环使得递归神经网络看起来有点神秘。然而，如果你多想一下，就会发现它们和普通的神经网络并没有太大的不同。递归神经网络可以看作是同一网络的多个副本，每个副本都向后继网络传递一条消息。考虑一下如果我们展开循环会发生什么:</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" width="500" hegiht="500" align="center/"></p><p>这种链式性质揭示了递归神经网络与序列和列表密切相关。它们是神经网络用来处理这些数据的自然结构。</p><p>​    它们确实被使用了!在过去的几年里，把RNNs应用到各种各样的问题上取得了令人难以置信的成功:语音识别、语言建模、翻译、图像字幕等等。我将把关于RNNs可以实现的惊人壮举的讨论留给Andrej Karpathy的优秀博客文章《循环神经网络的不合理有效性》。但它们真的很神奇。</p><p>​    这些成功的关键是使用“LSTMs”，这是一种非常特殊的递归神经网络，它在许多任务中都比标准版本运行得好得多。几乎所有基于递归神经网络的激动人心的结果都是用它们实现的。本文将探索这些LSTMs。</p><h2 id="长期依赖的问题"><a href="#长期依赖的问题" class="headerlink" title="长期依赖的问题"></a>长期依赖的问题</h2><p>​    RNNs的一个吸引人的地方就是它们能够将以前的信息与现在的任务联系起来，例如使用以前的视频帧可能有助于理解现在的帧。如果RNN能做到这一点，它们将非常有用。但他们能吗?这可能需要视情况而定。</p><p>​    有时候，我们只需要查看最近的信息就可以执行当前的任务。例如，考虑一个语言模型，它试图根据前面的单词预测下一个单词。如果我们试图预测“云在天空中”中的最后一个单词，我们不需要任何进一步的上下文——很明显下一个单词将是天空。在这种情况下，相关信息和需要信息的地方之间的差距很小，RNNs可以学习使用过去的信息。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png" width="400" hegiht="500" align="center/"></p><p>​    但也有一些情况，我们需要更多的上下文。试着预测文本中的最后一个单词“我在法国长大……我说一口流利的法语。”“最近的信息显示，下一个单词很可能是一种语言的名字，但如果我们想缩小范围，我们需要更早的法语语境。”相关信息与需要它的点之间的差距完全有可能变得非常大。<br>不幸的是，随着这种差距的扩大，RNNs无法学会连接信息。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png" width="400" hegiht="500" align="center/"></p><p>​    理论上，RNNs绝对有能力处理这种“长期依赖”。“一个人可以仔细地为他们选择参数来解决这种形式的玩具问题。遗憾的是，在实践中，RNNs似乎不能学习它们。 [Hochreiter (1991) <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener">German</a>和<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a>对这个问题进行了深入研究，他们发现了一些非常基本的原因，解释了为什么这个问题可能很难。</p><h2 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h2><p>​    Long Short Term Memory networks通常被称为“LSTMs”，是一种特殊的RNN，能够学习长期依赖关系。它们由<a href="http://www.bioinf.jku.at/publications/older/2604.pdf" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber (1997)</a>引入，并在随后的工作中被许多人提炼和推广。他们在各种各样的问题上都做得非常好，现在被广泛使用。</p><p>​    LSTMs的设计是为了避免长期依赖问题。长时间记住信息实际上是他们的默认行为，而不是他们努力学习的东西!</p><p>​    所有的递归神经网络都具有一串重复的神经网络模块的形式。在标准的RNNs中，这个重复模块有一个非常简单的结构，比如一个tanh层。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png" width="400" hegiht="500" align="center/"></p><p>LSTMs也有类似链的结构，但是重复模块有不同的结构。不是只有一个神经网络层，而是有四个，以一种非常特殊的方式相互作用。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" width="400" hegiht="500" align="center/"></p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png" alt="img"></p><p>​    在上面的图中，每一条线都带有一个完整的向量，从一个节点的输出到其他节点的输入。粉色圆圈表示点化操作，比如向量加法，而黄色框表示神经网络学习层。箭头合并表示连接，而箭头分叉表示被正在被复制的内容以及复制到不同位置。</p><h2 id="LSTM输入和输出"><a href="#LSTM输入和输出" class="headerlink" title="LSTM输入和输出"></a>LSTM输入和输出</h2><p><a href="https://www.zhihu.com/question/41949741" target="_blank" rel="noopener">https://www.zhihu.com/question/41949741</a></p><h2 id="LSTM核心思想"><a href="#LSTM核心思想" class="headerlink" title="LSTM核心思想"></a>LSTM核心思想</h2><p>LSTMs的核心是 单元状态，即贯穿图顶部的水平线。</p><p>单元状态有点像传送带。它沿着整个链一直向下，只有一些很小的线性相互作用。信息很容易不加改变地沿着它流动。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" alt="img"></p><p>LSTM有能力删除或添加信息到单元状态，并由称为门的结构小心地控制。</p><p>门是一种选择性地让信息通过的方法。它们由sigmoid神经网络层和点乘运算组成。.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" alt="img"></p><p>sigmoid层输出0到1之间的数字，描述每个组件应该通过的百分比。值0表示“不让任何东西通过”，而值1表示“让所有东西通过!”LSTM有三个这样的门，用来保护和控制单元状态。</p><h2 id="LSTM深入剖析"><a href="#LSTM深入剖析" class="headerlink" title="LSTM深入剖析"></a>LSTM深入剖析</h2><h3 id="忘记阶段"><a href="#忘记阶段" class="headerlink" title="忘记阶段"></a>忘记阶段</h3><p>LSTM的第一步是决定要从单元格状态丢弃什么信息。这个决定是由一个叫做“忘记门”的sigmoid层做出的。“它查看h<sub>t-1</sub>和 x<sub>t</sub>，并为处于单元格状态  C<sub>t-1</sub>的每个值输出一个介于0到1之间的数字。1表示“完全保留这个”，而0表示“完全删除这个”。</p><p>让我们回到语言模型的例子，该模型试图根据前面的所有单词预测下一个单词。在这样的问题中，单元格状态可能包括当前主语的词性，以便使用正确的代词。当我们看到一个新的主语时，我们想要忘记旧主语的词性。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" alt="img"></p><h3 id="选择记忆阶段"><a href="#选择记忆阶段" class="headerlink" title="选择记忆阶段"></a>选择记忆阶段</h3><p>下一步是决定要在单元格状态中存储哪些新信息。它有两部分。首先，一个名为“input gate layer”的sigmoid层决定要更新哪些值。接下来，tanh层创建一个新的候选值向量C<sub> ~t </sub>，可以将其添加到状态中。在下一个步骤中，我们将把这两者结合起来对状态的更新。</p><p>在我们的语言模型示例中，我们希望将新主体的词性添加到单元格状态，以替换我们正在遗忘的旧主体。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" alt="img"></p><p>现在是时候将旧的单元状态C<sub>t - 1</sub>更新为新的单元状态C<sub>t</sub>了。前面的步骤已经决定了要做什么，我们只需要实际去执行。</p><p>我们将旧状态乘以f<sub>t</sub>，忘记我们之前决定忘记的事情。然后我们将它添加到C<sub> ~t </sub>中。这是新的候选值，按我们决定每个状态值的更新进行缩放。</p><p>在语言模型中，这是我们实际删除关于旧信息并添加新信息的地方，正如我们在前面的步骤中描述的那样。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" alt="img"></p><h3 id="输出阶段"><a href="#输出阶段" class="headerlink" title="输出阶段"></a>输出阶段</h3><p>最后，我们需要决定输出什么。这个输出将基于我们的单元格状态，但是是经过过滤的版本。首先，我们运行一个sigmoid层，它决定要输出单元格状态的哪些部分。然后，我们将单元格状态放入tanh(将值缩放到- 1和1之间)，并将其乘以sigmoid层的输出，这样我们只输出我们决定输出的部分。</p><p>对于语言模型的例子，由于它只是看到了一个主语，所以它可能希望输出与动词相关的信息，以防接下来会发生什么。例如，它可以输出主语是单数还是复数，这样我们就知道如果主语是单数或复数，那么动词应该变成什么形式。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" alt="img"></p><h2 id="LSTM变形"><a href="#LSTM变形" class="headerlink" title="LSTM变形"></a>LSTM变形</h2><p>详情看来源网站。变形有很多，包括GRU，数十万种，Which of these variants is best? Do the differences matter? <a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="noopener">Greff, et al. (2015)</a> do a nice comparison of popular variants, finding that they’re all about the same.</p><h3 id="代码以及小例子"><a href="#代码以及小例子" class="headerlink" title="代码以及小例子"></a>代码以及小例子</h3><p><img src="https://cdn-images-1.medium.com/max/1600/1*p2yXhtxmYflEUrTC1rCoUA.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LSTM详解，把cell转变当作传送带的思想&lt;/p&gt;
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="LSTM" scheme="http://kodgv.xyz/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>RNN</title>
    <link href="http://kodgv.xyz/2019/04/14/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RNN/"/>
    <id>http://kodgv.xyz/2019/04/14/神经网络/RNN/</id>
    <published>2019-04-14T08:20:34.000Z</published>
    <updated>2019-04-15T02:43:38.845Z</updated>
    
    <content type="html"><![CDATA[<p>RNN快速学习</p><a id="more"></a><p>[TOC]</p><h2 id="一、从单层网络谈起"><a href="#一、从单层网络谈起" class="headerlink" title="一、从单层网络谈起"></a>一、从单层网络谈起</h2><p>在学习RNN之前，首先要了解一下最基本的单层网络，它的结构如图：</p><p><img src="https://pic1.zhimg.com/80/v2-da9ac1b5e3f91086fd06e6173fed1580_hd.jpg" alt="img"></p><p>输入是x，经过变换Wx+b和激活函数f得到输出y。相信大家对这个已经非常熟悉了。</p><h2 id="二、经典的RNN结构（N-vs-N）"><a href="#二、经典的RNN结构（N-vs-N）" class="headerlink" title="二、经典的RNN结构（N vs N）"></a>二、经典的RNN结构（N vs N）</h2><p>在实际应用中，我们还会遇到很多序列形的数据：</p><p><img src="https://pic3.zhimg.com/80/v2-0f8f8a8313867459d33e902fed97bd16_hd.jpg" alt="img"></p><p>如：</p><ul><li>自然语言处理问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推。</li><li>语音处理。此时，x1、x2、x3……是每帧的声音信号。</li><li>时间序列问题。例如每天的股票价格等等</li></ul><p>序列形的数据就不太好用原始的神经网络处理了。为了建模序列问题，RNN引入了隐状态h（hidden state）的概念，h可以对序列形的数据提取特征，接着再转换为输出。先从h1的计算开始看：</p><p><img src="https://pic1.zhimg.com/80/v2-a5f8bc30bcc2d9eba7470810cb362850_hd.jpg" alt="img"></p><p>图示中记号的含义是：</p><ul><li><strong>圆圈或方块表示的是向量。</strong></li><li><strong>一个箭头就表示对该向量做一次变换。如上图中h0和x1分别有一个箭头连接，就表示对h0和x1各做了一次变换。</strong></li></ul><p><strong>在很多论文中也会出现类似的记号，初学的时候很容易搞乱，但只要把握住以上两点，就可以比较轻松地理解图示背后的含义。</strong></p><p>h2的计算和h1类似。要注意的是，在计算时，<strong>每一步使用的参数U、W、b都是一样的，也就是说每个步骤的参数都是共享的，这是RNN的重要特点，一定要牢记。</strong></p><p><img src="https://pic3.zhimg.com/80/v2-74d7ac80ca83165092579932920d0ffe_hd.jpg" alt="img"></p><p>依次计算剩下来的（使用相同的参数U、W、b）：</p><p><img src="https://pic2.zhimg.com/80/v2-bc9759f8c642208a0f8514ccd0260b31_hd.jpg" alt="img"></p><p>我们这里为了方便起见，只画出序列长度为4的情况，实际上，这个计算过程可以无限地持续下去。</p><p>我们目前的RNN还没有输出，得到输出值的方法就是直接通过h进行计算：</p><p><img src="https://pic1.zhimg.com/80/v2-9f3a921d0d5c1313afa58bd3ef53af48_hd.jpg" alt="img"><br>正如之前所说，<strong>一个箭头就表示对对应的向量做一次类似于f(Wx+b)的变换，这里的这个箭头就表示对h1进行一次变换，得到输出y1。</strong></p><p><img src="https://pic4.zhimg.com/80/v2-9524a28210c98ed130644eb3c3002087_hd.jpg" alt></p><p>剩下的输出类似进行（使用和y1同样的参数V和c）：</p><p><img src="https://pic2.zhimg.com/80/v2-629abbab0d5cc871db396f17e9c58631_hd.jpg" alt="img"></p><p>OK！大功告成！这就是最经典的RNN结构，我们像搭积木一样把它搭好了。它的输入是x1, x2, …..xn，输出为y1, y2, …yn，也就是说，<strong>输入和输出序列必须要是等长的</strong>。</p><p>由于这个限制的存在，经典RNN的适用范围比较小，但也有一些问题适合用经典的RNN结构建模，如：</p><ul><li>计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。</li><li>输入为字符，输出为下一个字符的概率。这就是著名的Char RNN（详细介绍请参考：<a href="https://link.zhihu.com/?target=http%3A//karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a>，Char RNN可以用来生成文章，诗歌，甚至是代码，非常有意思）。</li></ul><h2 id="三、N-VS-1"><a href="#三、N-VS-1" class="headerlink" title="三、N VS 1"></a>三、N VS 1</h2><p>有的时候，我们要处理的问题输入是一个序列，输出是一个单独的值而不是序列，应该怎样建模呢？实际上，我们只在最后一个h上进行输出变换就可以了：</p><p><img src="https://pic1.zhimg.com/80/v2-6caa75392fe47801e605d5e8f2d3a100_hd.jpg" alt="img"></p><p>这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。</p><h2 id="四、1-VS-N"><a href="#四、1-VS-N" class="headerlink" title="四、1 VS N"></a>四、1 VS N</h2><p>输入不是序列而输出为序列的情况怎么处理？我们可以只在序列开始进行输入计算：</p><p><img src="https://pic3.zhimg.com/80/v2-87ebd6a82e32e81657682ffa0ba084ee_hd.jpg" alt="img"></p><p>还有一种结构是把输入信息X作为每个阶段的输入：</p><p><img src="https://pic3.zhimg.com/80/v2-fe054c488bb3a9fbcdfad299b2294266_hd.jpg" alt="img"></p><p>下图省略了一些X的圆圈，是一个等价表示：</p><p><img src="https://pic1.zhimg.com/80/v2-16e626b6e99fb1d23c8a54536f7d28dc_hd.jpg" alt="img"></p><p>这种1 VS N的结构可以处理的问题有：</p><ul><li>从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子</li><li>从类别生成语音或音乐等</li></ul><h2 id="五、N-vs-M"><a href="#五、N-vs-M" class="headerlink" title="五、N vs M"></a>五、N vs M</h2><p>下面我们来介绍RNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。</p><p>原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。</p><p><strong>为此，Encoder-Decoder结构先将输入数据编码成一个上下文向量c：</strong></p><p><img src="https://pic2.zhimg.com/80/v2-03aaa7754bb9992858a05bb9668631a9_hd.jpg" alt="img"></p><p>得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。</p><p><strong>拿到c之后，就用另一个RNN网络对其进行解码</strong>，这部分RNN网络被称为Decoder。具体做法就是将c当做之前的初始状态h0输入到Decoder中：</p><p><img src="https://pic4.zhimg.com/80/v2-77e8a977fc3d43bec8b05633dc52ff9f_hd.jpg" alt="img"></p><p>还有一种做法是将c当做每一步的输入：</p><p><img src="https://pic4.zhimg.com/80/v2-e0fbb46d897400a384873fc100c442db_hd.jpg" alt="img"></p><p>由于这种Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如：</p><ul><li>机器翻译。Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的</li><li>文本摘要。输入是一段文本序列，输出是这段文本序列的摘要序列。</li><li>阅读理解。将输入的文章和问题分别编码，再对其进行解码得到问题的答案。</li><li>语音识别。输入是语音信号序列，输出是文字序列。</li></ul><h2 id="六、Attention机制"><a href="#六、Attention机制" class="headerlink" title="六、Attention机制"></a>六、Attention机制</h2><p>在Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征c再解码，<strong>因此， c中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。</strong>如机器翻译问题，当要翻译的句子较长时，一个c可能存不下那么多信息，就会造成翻译精度的下降。</p><p>Attention机制通过在每个时间输入不同的c来解决这个问题，下图是带有Attention机制的Decoder：</p><p><img src="https://pic2.zhimg.com/80/v2-8da16d429d33b0f2705e47af98e66579_hd.jpg" alt="img"></p><p><strong>每一个c会自动去选取与当前所要输出的y最合适的上下文信息。具体来说，我们用 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="a_{ij}"> 衡量Encoder中第j阶段的hj和解码时第i阶段的相关性，最终Decoder中第i阶段的输入的上下文信息 <img src="https://www.zhihu.com/equation?tex=c_i" alt="c_i"> 就来自于所有 <img src="https://www.zhihu.com/equation?tex=h_j" alt="h_j"> 对 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="a_{ij}"> 的加权和。</strong></p><p>以机器翻译为例（将中文翻译成英文）：</p><p><img src="https://pic1.zhimg.com/80/v2-d266bf48a1d77e7e4db607978574c9fc_hd.jpg" alt="img"></p><p>输入的序列是“我爱中国”，因此，Encoder中的h1、h2、h3、h4就可以分别看做是“我”、“爱”、“中”、“国”所代表的信息。在翻译成英语时，第一个上下文c1应该和“我”这个字最相关，因此对应的 <img src="https://www.zhihu.com/equation?tex=a_%7B11%7D" alt="a_{11}"> 就比较大，而相应的 <img src="https://www.zhihu.com/equation?tex=a_%7B12%7D" alt="a_{12}"> 、 <img src="https://www.zhihu.com/equation?tex=a_%7B13%7D" alt="a_{13}"> 、 <img src="https://www.zhihu.com/equation?tex=a_%7B14%7D" alt="a_{14}"> 就比较小。c2应该和“爱”最相关，因此对应的 <img src="https://www.zhihu.com/equation?tex=a_%7B22%7D" alt="a_{22}"> 就比较大。最后的c3和h3、h4最相关，因此 <img src="https://www.zhihu.com/equation?tex=a_%7B33%7D" alt="a_{33}"> 、 <img src="https://www.zhihu.com/equation?tex=a_%7B34%7D" alt="a_{34}"> 的值就比较大。</p><p>至此，关于Attention模型，我们就只剩最后一个问题了，那就是：<strong>这些权重 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="a_{ij}"> 是怎么来的？</strong></p><p>事实上， <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="a_{ij}"> 同样是从模型中学出的，它实际和Decoder的第i-1阶段的隐状态、Encoder第j个阶段的隐状态有关。</p><p>同样还是拿上面的机器翻译举例， <img src="https://www.zhihu.com/equation?tex=a_%7B1j%7D" alt="a_{1j}"> 的计算（此时箭头就表示对h’和 <img src="https://www.zhihu.com/equation?tex=h_j" alt="h_j"> 同时做变换）：</p><p><img src="https://pic4.zhimg.com/80/v2-5561fa61321f31113043fb9711ee3263_hd.jpg" alt="img"></p><p><img src="https://www.zhihu.com/equation?tex=a_%7B2j%7D" alt="a_{2j}"> 的计算：</p><p><img src="https://pic1.zhimg.com/80/v2-50473aa7b1c20d680abf8ca36d82c9e4_hd.jpg" alt="img"></p><p><img src="https://www.zhihu.com/equation?tex=a_%7B3j%7D" alt="a_{3j}"> 的计算：</p><p><img src="https://pic4.zhimg.com/80/v2-07f7411c77901a7bd913e55884057a63_hd.jpg" alt="img"></p><p>以上就是带有Attention的Encoder-Decoder模型计算的全过程。</p><h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><p>本文主要讲了N vs N，N vs 1、1 vs N、N vs M四种经典的RNN模型，以及如何使用Attention结构。希望能对大家有所帮助。</p><p>可能有小伙伴发现没有LSTM的内容，其实是因为LSTM从外部看和RNN完全一样，因此上面的所有结构对LSTM都是通用的，想了解LSTM内部结构的可以参考这篇文章：<a href="https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a>，写得非常好，推荐阅读。</p><p>所以RNN不是一定是seqtoseq的模型，其一般图为：</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" alt="RNN-rolled"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RNN快速学习&lt;/p&gt;
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="RNN" scheme="http://kodgv.xyz/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>pytorch学习</title>
    <link href="http://kodgv.xyz/2019/04/14/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/pytorch%E5%AD%A6%E4%B9%A0/"/>
    <id>http://kodgv.xyz/2019/04/14/神经网络/pytorch学习/</id>
    <published>2019-04-14T02:23:33.000Z</published>
    <updated>2019-04-22T06:28:29.607Z</updated>
    
    <content type="html"><![CDATA[<p>pytorch学习日记<br><a id="more"></a></p><p>[TOC]</p><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><h3 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h3><p>并行化读取数据<br><strong>len</strong>：代表样本数量。len(obj)等价于obj.<strong>len</strong>()。<br><strong>getitem</strong>：返回一条数据或一个样本。obj[index]等价于obj.<strong>getitem</strong>。</p><p>建议将节奏的图片等高负载的操作放到这里，因为多进程时会并行调用这个函数，这样做可以加速。<br>dataset中应尽量只包含只读对象，避免修改任何可变对象。因为如果使用多进程，可变对象要加锁，但后面讲到的dataloader的设计使其难以加锁。如下面例子中的self.num可能在多进程下出问题</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span><span class="params">(data.Dataset)</span>:</span><span class="comment">#需要继承data.Dataset</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        <span class="comment"># 1. Initialize file path or list of file.</span></span><br><span class="line"> <span class="comment"># 初始化变量</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line"> <span class="comment"># 需要返回batch中的data，即返回index下标的数值</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># the total size of your dataset.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><p>示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNIST</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, train=True, transform=None, target_transform=None, download=False)</span>:</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line">        self.train = train  <span class="comment"># training set or test set</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> download:</span><br><span class="line">            self.download()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._check_exists():</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'Dataset not found.'</span> +</span><br><span class="line">                               <span class="string">' You can use download=True to download it'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            self.train_data, self.train_labels = torch.load(</span><br><span class="line">                os.path.join(root, self.processed_folder, self.training_file))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.test_data, self.test_labels = torch.load(os.path.join(root, self.processed_folder, self.test_file))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            img, target = self.train_data[index], self.train_labels[index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img, target = self.test_data[index], self.test_labels[index]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># doing this so that it is consistent with all other datasets</span></span><br><span class="line">        <span class="comment"># to return a PIL Image</span></span><br><span class="line">        img = Image.fromarray(img.numpy(), mode=<span class="string">'L'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            target = self.target_transform(target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">60000</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure><h3 id="torch-utils-data-Dataload"><a href="#torch-utils-data-Dataload" class="headerlink" title="torch.utils.data.Dataload"></a>torch.utils.data.Dataload</h3><p><code>torch.utils.data.DataLoader</code>(<em>dataset</em>, <em>batch_size=1</em>, <em>shuffle=False</em>, <em>sampler=None</em>, <em>batch_sampler=None</em>, <em>num_workers=0</em>, <em>collate_fn=<function default_collate></function></em>, <em>pin_memory=False</em>, <em>drop_last=False</em>, <em>timeout=0</em>, <em>worker_init_fn=None</em>)<a href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader" target="_blank" rel="noopener">[SOURCE]</a></p><p>Data loader. Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.<br>dataset：加载的数据集(Dataset对象)<br>batch_size：batch size<br>shuffle:：是否将数据打乱<br>sampler： 样本抽样，后续会详细介绍<br>num_workers：使用多进程加载的进程数，0代表不使用多进程<br>collate_fn： 如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可<br>pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些<br>drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃</p><p><strong>重点在于collate_fn</strong></p><p>是不是看着有点头大，没有关系，我们先搞清楚他的输入是什么。这里可以看到他的输入被命名为batch，但是我们还是不知道到底是什么，可以猜测应该是一个batch size的数据。我们继续往后找，可以找到这个<a href="https://link.jianshu.com?t=https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L180" target="_blank" rel="noopener">地方</a>。</p><p><img src="https:////upload-images.jianshu.io/upload_images/3623720-235599af888f7658.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/631/format/webp" alt="img"></p><p>Paste_Image.png</p><p>我们可以从这里看到<code>collate_fn</code>在这里进行了调用，那么他的输入我们就找到了，从这里看这就是一个list，list中的每个元素就是<code>self.data[i]</code>，如果你在往上看，可以看到这个<code>self.data</code>就是我们需要预先定义的Dataset，那么这里<code>self.data[i]</code>就等价于我们在Dataset里面定义的<code>__getitem__</code>这个函数。</p><p>所以我们知道了<code>collate_fn</code>这个函数的输入就是一个list，list的长度是一个batch size，list中的每个元素都是<code>__getitem__</code>得到的结果。</p><p><strong>注意如果dataset里面返回的不是一个值而是元组的话</strong>：[tuple]，这时候就需要用<br>texts, lens, y = zip(*batch)来解开</p><h3 id="squeeze-amp-unsqueeze"><a href="#squeeze-amp-unsqueeze" class="headerlink" title="squeeze&amp;unsqueeze"></a>squeeze&amp;unsqueeze</h3><ol><li>首先初始化一个a<br><img src="https://img-blog.csdn.net/20180812155855509?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZseXNreV9qYXk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt></li></ol><p>可以看出a的维度为（2，3）</p><ol><li><p>在第二维增加一个维度，使其维度变为（2，1，3）<br><img src="https://img-blog.csdn.net/20180812160119403?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZseXNreV9qYXk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt><br>可以看出a的维度已经变为（2，1，3）了，同样如果需要在倒数第二个维度上增加一个维度，那么使用b.unsqueeze(-2)<br>二、squeeze()函数介绍</p></li><li><p>首先得到一个维度为（1，2，3）的tensor（张量）<br><img src="https://img-blog.csdn.net/20180812160833709?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZseXNreV9qYXk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt><br>由图中可以看出c的维度为（1，2，3）<br>2.下面使用squeeze()函数将第一维去掉<br><img src="https://img-blog.csdn.net/20180812161010282?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZseXNreV9qYXk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt><br>可见，维度已经变为（2，3）<br>3.另外<br><img src="https://img-blog.csdn.net/20180812161246184?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZseXNreV9qYXk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt><br>可以看出维度并没有变化，仍然为（1，2，3），这是因为只有维度为1时才会去掉。</p></li></ol><h3 id="pack-padded-sequence-amp-pad-packed-sequence"><a href="#pack-padded-sequence-amp-pad-packed-sequence" class="headerlink" title="pack_padded_sequence&amp;pad_packed_sequence"></a>pack_padded_sequence&amp;pad_packed_sequence</h3><p>踩坑：</p><ul><li>要注意pad回来的会自动补0序列的，注意这些0序列，会对后续的操作产生影响</li><li>要注意0是补在后面，不是补在前面，这一点坑了我两天</li><li>动态padding会快，但是注意pack和pad会额外需要耗时</li><li>注意预测的时候要reverse回来，不然没法做stacking</li></ul><p><strong>RNN处理变长序列参考的的代码</strong>：<a href="https://zhuanlan.zhihu.com/p/40391002" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40391002</a></p><p><strong>pytorch 处理变长序列过程中各个tensor变化情况</strong>：<a href="https://blog.csdn.net/qq_27505047/article/details/78764888" target="_blank" rel="noopener">https://blog.csdn.net/qq_27505047/article/details/78764888</a></p><p><strong>kaggle比赛代码：</strong><a href="https://www.kaggle.com/johnfarrell/plasticc-2018-emb-gru" target="_blank" rel="noopener">https://www.kaggle.com/johnfarrell/plasticc-2018-emb-gru</a></p><p><strong>torch.nn.utils.rnn.pack_padded_sequence()</strong></p><p>这里的<code>pack</code>，理解成压紧比较好。 将一个 填充过的变长序列 压紧。（填充时候，会有冗余，所以压紧一下）</p><p>其中pack的过程为：（注意pack的形式，不是按行压，而是按列压）</p><p><img src="http://image.bubuko.com/info/201810/20181023003228230058.png" alt="技术分享图片"></p><p>（下面方框内为<code>PackedSequence</code>对象，由data和batch_sizes组成）</p><p>输入的形状可以是(T×B×<em> )。<code>T</code>是最长序列长度，<code>B</code>是<code>batch size</code>，`</em><code>代表任意维度(可以是0)。如果</code>batch_first=True<code>的话，那么相应的</code>input size<code>就是</code>(B×T×*)`。</p><p><a href="https://zhuanlan.zhihu.com/p/32103001" target="_blank" rel="noopener">读PyTorch源码学习RNN（1）</a></p><p><img src="https://cdn.nlark.com/yuque/0/2018/png/139777/1540608513263-a57374b7-2448-43f6-931e-4624acc4a146.png" alt="img"></p><p>pack之后得到的每个batch就是一个timestep,也就是说timestep的时候取都是1的状态(a1和b1)，它实际上就是告诉Model为每个序列构建不同数量的状态。</p><p><strong>pack和pad要统一是否使用batch_first=True，否则转换维度会出问题</strong></p><h2 id="问题解答专区"><a href="#问题解答专区" class="headerlink" title="问题解答专区"></a>问题解答专区</h2><h3 id="如何自定义forward函数"><a href="#如何自定义forward函数" class="headerlink" title="如何自定义forward函数"></a>如何自定义forward函数</h3><p>只需要继承nn.Module，然后重写forward函数，注意调用时机为该模块名字 module(参数)，这里得参数跟forward得参数一致</p><h3 id="如何自定义LOSS函数"><a href="#如何自定义LOSS函数" class="headerlink" title="如何自定义LOSS函数"></a>如何自定义LOSS函数</h3><p><a href="https://www.zhihu.com/question/66988664" target="_blank" rel="noopener">https://www.zhihu.com/question/66988664</a></p><h2 id="如何打印网络和参数结构"><a href="#如何打印网络和参数结构" class="headerlink" title="如何打印网络和参数结构"></a>如何打印网络和参数结构</h2><p><a href="https://github.com/nmhkahn/torchsummaryX" target="_blank" rel="noopener">https://github.com/nmhkahn/torchsummaryX</a></p><h2 id="如何释放显存"><a href="#如何释放显存" class="headerlink" title="如何释放显存"></a>如何释放显存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gc.collect()</span><br><span class="line">torch.cuda.empty_cache()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;pytorch学习日记&lt;br&gt;
    
    </summary>
    
      <category term="nn学习" scheme="http://kodgv.xyz/categories/nn%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="pytorch" scheme="http://kodgv.xyz/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>测试集和训练集分布</title>
    <link href="http://kodgv.xyz/2019/04/13/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/%E6%B5%8B%E8%AF%95%E9%9B%86%E5%92%8C%E8%AE%AD%E7%BB%83%E9%9B%86%E5%88%86%E5%B8%83/"/>
    <id>http://kodgv.xyz/2019/04/13/竞赛经验/测试集和训练集分布/</id>
    <published>2019-04-13T11:40:23.000Z</published>
    <updated>2019-04-13T11:49:03.029Z</updated>
    
    <content type="html"><![CDATA[<p>Train &amp; Test分布主要是为了看数据的分布情况。</p><p>下面的方法是最近kaggle上GM搞的一种极其简单的探索训练集&amp;测试集分布的方案,将一个分布的问题变成一个二分类问题,该方法简单有效，如果AUC低于0.6,则我们可以认为训练集和测试集是分布平衡的,反之我们则可以认为训练集和测试集是分布不一致的,这个时候就需要注意AB榜是否会出现翻车的情况等</p><a id="more"></a><p>[TOC]</p><p>来源：<a href="https://www.kaggle.com/justfor/adversarial-validation-plus-ks-test" target="_blank" rel="noopener">https://www.kaggle.com/justfor/adversarial-validation-plus-ks-test</a></p><h2 id="Adversarial-Validation"><a href="#Adversarial-Validation" class="headerlink" title="Adversarial Validation"></a>Adversarial Validation</h2><p>下面的方法是最近kaggle上GM搞的一种极其简单的探索训练集&amp;测试集分布的方案,将一个分布的问题变成一个二分类问题,该方法简单有效，如果AUC低于0.6,则我们可以认为训练集和测试集是分布平衡的,反之我们则可以认为训练集和测试集是分布不一致的,这个时候就需要注意AB榜是否会出现翻车的情况等</p><p>核心点在于将<strong>y1 = np.array([0] x train.shape[0])y2 = np.array([1] x test.shape[0])</strong></p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Create label array and complete dataset</span></span><br><span class="line"><span class="attr">y1</span> = np.array([<span class="number">0</span>]*train.shape[<span class="number">0</span>])</span><br><span class="line"><span class="attr">y2</span> = np.array([<span class="number">1</span>]*test.shape[<span class="number">0</span>])</span><br><span class="line"><span class="attr">y</span> = np.concatenate((y1, y2))</span><br><span class="line"><span class="attr">X_data</span> = pd.concat([train, test])</span><br><span class="line">X_data.reset_index(<span class="attr">drop=True,</span> <span class="attr">inplace=True)</span></span><br><span class="line"><span class="comment">#Initialize splits&amp;LGBM</span></span><br><span class="line"><span class="attr">skf</span> = StratifiedKFold(<span class="attr">n_splits=5,</span> <span class="attr">shuffle=True,</span> <span class="attr">random_state=13)</span></span><br><span class="line"></span><br><span class="line"><span class="attr">lgb_model</span> = lgb.LGBMClassifier(<span class="attr">max_depth=-1,</span></span><br><span class="line">                                   <span class="attr">n_estimators=500,</span></span><br><span class="line">                                   <span class="attr">learning_rate=0.01,</span></span><br><span class="line">                                   <span class="attr">objective='binary',</span> </span><br><span class="line">                                   <span class="attr">n_jobs=-1)</span></span><br><span class="line">                                   </span><br><span class="line"><span class="attr">counter</span> = <span class="number">1</span></span><br><span class="line"><span class="comment">#Train 5-fold adversarial validation classifier</span></span><br><span class="line">for train_index, test_index <span class="keyword">in</span> skf.split(X_data, y):</span><br><span class="line">    print('\nFold &#123;&#125;'.format(counter))</span><br><span class="line">    X_fit, <span class="attr">X_val</span> = X_data.loc[train_index], X_data.loc[test_index]</span><br><span class="line">    y_fit, <span class="attr">y_val</span> = y[train_index], y[test_index]</span><br><span class="line">    </span><br><span class="line">    lgb_model.fit(X_fit, y_fit, <span class="attr">eval_metric='auc',</span> </span><br><span class="line">              <span class="attr">eval_set=[(X_val,</span> y_val)], </span><br><span class="line">              <span class="attr">verbose=100,</span> <span class="attr">early_stopping_rounds=10)</span></span><br><span class="line">    counter+=<span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="Kolmogorov-Smirnov-Test"><a href="#Kolmogorov-Smirnov-Test" class="headerlink" title="Kolmogorov-Smirnov Test"></a>Kolmogorov-Smirnov Test</h2><p>单独看每个变量是否能过通过KS检验，不仅有图，而且有一种量化的手段</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#Load more packages</span><br><span class="line">from scipy.stats import ks_2samp</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">sns.set_style('whitegrid')</span><br><span class="line">import <span class="built_in">warnings</span></span><br><span class="line"><span class="built_in">warnings</span>.simplefilter(action='ignore', category=FutureWarning)</span><br><span class="line"><span class="built_in">warnings</span>.filterwarnings('ignore')</span><br><span class="line">#Perform KS-Test <span class="keyword">for</span> each <span class="built_in">feature</span> from train/test. Draw its distribution. Count <span class="built_in">features</span> based on statistics.</span><br><span class="line">#Plots are hidden. If you'd like to look <span class="built_in">at</span> them - press <span class="string">"Output"</span> button.</span><br><span class="line">hypothesisnotrejected = []</span><br><span class="line">hypothesisrejected = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">col</span> <span class="keyword">in</span> train.<span class="built_in">columns</span>:</span><br><span class="line">    statistic, pvalue = ks_2samp(train[<span class="built_in">col</span>], test[<span class="built_in">col</span>])</span><br><span class="line">    <span class="keyword">if</span> pvalue&gt;=statistic:</span><br><span class="line">        hypothesisnotrejected.<span class="built_in">append</span>(<span class="built_in">col</span>)</span><br><span class="line">    <span class="keyword">if</span> pvalue&lt;statistic:</span><br><span class="line">        hypothesisrejected.<span class="built_in">append</span>(<span class="built_in">col</span>)</span><br><span class="line">        </span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line">    plt.<span class="built_in">title</span>(<span class="string">"Kolmogorov-Smirnov test for train/test\n"</span></span><br><span class="line">              <span class="string">"feature: &#123;&#125;, statistics: &#123;:.5f&#125;, pvalue: &#123;:5f&#125;"</span>.format(<span class="built_in">col</span>, statistic, pvalue))</span><br><span class="line">    sns.kdeplot(train[<span class="built_in">col</span>], <span class="built_in">color</span>='blue', shade=True, <span class="built_in">label</span>='Train')</span><br><span class="line">    sns.kdeplot(test[<span class="built_in">col</span>], <span class="built_in">color</span>='green', shade=True, <span class="built_in">label</span>='Test')</span><br><span class="line"></span><br><span class="line">    plt.<span class="built_in">show</span>()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Train &amp;amp; Test分布主要是为了看数据的分布情况。&lt;/p&gt;
&lt;p&gt;下面的方法是最近kaggle上GM搞的一种极其简单的探索训练集&amp;amp;测试集分布的方案,将一个分布的问题变成一个二分类问题,该方法简单有效，如果AUC低于0.6,则我们可以认为训练集和测试集是分布平衡的,反之我们则可以认为训练集和测试集是分布不一致的,这个时候就需要注意AB榜是否会出现翻车的情况等&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>比赛规范经验</title>
    <link href="http://kodgv.xyz/2019/04/12/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E6%AF%94%E8%B5%9B%E8%A7%84%E8%8C%83%E7%BB%8F%E9%AA%8C/"/>
    <id>http://kodgv.xyz/2019/04/12/数据竞赛/比赛规范经验/</id>
    <published>2019-04-12T09:12:58.000Z</published>
    <updated>2019-04-13T11:44:31.270Z</updated>
    
    <content type="html"><![CDATA[<p>比赛中的代码规范,代码不规范，亲人两行泪</p><a id="more"></a><p>先杂七杂八写，有一定量了再整理</p><ul><li>思路要记在云笔记里，纸上会丢，而且换环境忘记带</li><li>临近比赛前一个月注意多保存看过的kernel，因为它们做出来之后就会删除。也就是说快结束的时候，那些都不是重要的magic</li><li>要保持一个纯洁统一模型的代码的单独文件，才方便接入</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛中的代码规范,代码不规范，亲人两行泪&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="代码规范" scheme="http://kodgv.xyz/tags/%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/"/>
    
  </entry>
  
  <entry>
    <title>特征构造</title>
    <link href="http://kodgv.xyz/2019/04/11/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E7%89%B9%E5%BE%81%E6%9E%84%E9%80%A0/"/>
    <id>http://kodgv.xyz/2019/04/11/数据竞赛/特征构造/</id>
    <published>2019-04-11T02:47:02.000Z</published>
    <updated>2019-04-13T11:44:48.438Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p>函数统一返回新的df，同时删除原有df</p><h2 id="特征频数"><a href="#特征频数" class="headerlink" title="特征频数"></a>特征频数</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 构造特征时测试集要和训练集一样</span><br><span class="line">def encode_FE(df,<span class="built_in">col</span>,test)：</span><br><span class="line">    <span class="built_in">cv</span> = df[<span class="built_in">col</span>].value_counts()</span><br><span class="line">    <span class="built_in">nm</span> = <span class="built_in">col</span>+'_FE'</span><br><span class="line">    df[<span class="built_in">nm</span>] = df[<span class="built_in">col</span>].<span class="built_in">map</span>(<span class="built_in">cv</span>)</span><br><span class="line">    test[<span class="built_in">nm</span>] = test[<span class="built_in">col</span>].<span class="built_in">map</span>(<span class="built_in">cv</span>)</span><br><span class="line">    <span class="built_in">return</span> df,test</span><br></pre></td></tr></table></figure><h2 id="data-augment"><a href="#data-augment" class="headerlink" title="data augment"></a>data augment</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def augment(x,y,t=<span class="number">2</span>):</span><br><span class="line">    xs,xn = [],[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(t):</span><br><span class="line">        mask = y&gt;<span class="number">0</span></span><br><span class="line">        x1 = x[mask].<span class="built_in">copy</span>()</span><br><span class="line">        ids = <span class="built_in">np</span>.arange(x1.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(x1.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="built_in">np</span>.<span class="built_in">random</span>.shuffle(ids)</span><br><span class="line">            x1[:,c] = x1[ids][:,c]</span><br><span class="line">        xs.<span class="built_in">append</span>(x1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(t//<span class="number">2</span>):</span><br><span class="line">        mask = y==<span class="number">0</span></span><br><span class="line">        x1 = x[mask].<span class="built_in">copy</span>()</span><br><span class="line">        ids = <span class="built_in">np</span>.arange(x1.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(x1.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="built_in">np</span>.<span class="built_in">random</span>.shuffle(ids)</span><br><span class="line">            x1[:,c] = x1[ids][:,c]</span><br><span class="line">        xn.<span class="built_in">append</span>(x1)</span><br><span class="line"></span><br><span class="line">    xs = <span class="built_in">np</span>.vstack(xs)</span><br><span class="line">    xn = <span class="built_in">np</span>.vstack(xn)</span><br><span class="line">    ys = <span class="built_in">np</span>.ones(xs.shape[<span class="number">0</span>])</span><br><span class="line">    yn = <span class="built_in">np</span>.zeros(xn.shape[<span class="number">0</span>])</span><br><span class="line">    x = <span class="built_in">np</span>.vstack([x,xs,xn])</span><br><span class="line">    y = <span class="built_in">np</span>.concatenate([y,ys,yn])</span><br><span class="line">    <span class="built_in">return</span> x,y</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;函数统一返回新的df，同时删除原有df&lt;/p&gt;
&lt;h2 id=&quot;特征频数&quot;&gt;&lt;a href=&quot;#特征频数&quot; class=&quot;headerlink&quot; title=&quot;特征频数&quot;&gt;&lt;/a&gt;特征频数&lt;/h2&gt;&lt;figure class=&quot;highlight 
      
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Santander Customer Transaction Prediction 比赛经验</title>
    <link href="http://kodgv.xyz/2019/04/11/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/Santander%20Customer%20Transaction%20Prediction%20%E6%AF%94%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    <id>http://kodgv.xyz/2019/04/11/竞赛经验/Santander Customer Transaction Prediction 比赛经验/</id>
    <published>2019-04-11T02:46:10.000Z</published>
    <updated>2019-04-13T11:44:10.007Z</updated>
    
    <content type="html"><![CDATA[<p>Santander Customer Transaction Prediction 比赛，01分类问题，200维连续型匿名变量。</p><p>讲述了magic操作，count的新特征构造，特征相关性。。。</p><a id="more"></a><p>[TOC]</p><p>来源：<a href="https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920" target="_blank" rel="noopener">https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920</a></p><h1 id="magic-kernel"><a href="#magic-kernel" class="headerlink" title="magic kernel"></a>magic kernel</h1><h2 id="树模型的缺点"><a href="#树模型的缺点" class="headerlink" title="树模型的缺点"></a>树模型的缺点</h2><h3 id="无法学习到的东西"><a href="#无法学习到的东西" class="headerlink" title="无法学习到的东西"></a>无法学习到的东西</h3><p><img src="http://playagricola.com/Kaggle/198without.png" alt></p><p>LGBM用竖线划分直方图，因为LGBM看不到水平差异。一个直方图会将多个值放置在一个Bin中并且产生一个较为平滑的图。如果你把多个值放在一个bin中，你会得到一个锯齿的图，其中每个bin中有些值是惟一的，有些值出现了几十次，LGBM无法学习到这些事情。</p><p><img src="http://playagricola.com/Kaggle/198zoom3.png" alt></p><p>如上，构造的count图可以看出在相同值的附近存在不同频数的区别。</p><h3 id="对特征敏感的参数设置"><a href="#对特征敏感的参数设置" class="headerlink" title="对特征敏感的参数设置"></a>对特征敏感的参数设置</h3><p>​    仅仅做到上述构造特征还是没有用的，因为你添加新特征到LGBM的时候设置参数feature_fraction=0.05，这会导致特征被随机的采样，破坏了var_1和var_1count之间的依赖关系(目的就是让模型需要同时学习横向和纵向)。所以设置feature_fraction=1，能从0.901到0.910，但是如果要到0.920则需要剔除原始变量之间的<a href="https://en.wikipedia.org/wiki/Spurious_relationship" target="_blank" rel="noopener">spurious effects</a>,因为原始特征对模型敏感，同时相关系数普遍很低，则说明它们虽然有相关但是没有因果关联。</p><ul><li>Use Data Augmentation (as shown in Jiwei’s awesome kernel <a href="https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment" target="_blank" rel="noopener">here</a>). You must keep original and new feature in same row.</li><li>Use 200 separate models as shown in this kernel below.</li><li>Merge new feature and original feature into one feature. In original data, simply add 200 to each unique value. (And don’t add new columns)</li><li>使用数据增强，就是随机打乱特征的时候，保持count和var是一致的，保证count和var的相关性，又去除了var之间相关性</li><li>单独使用count和var预测，然后200个模型再融合(一共200个var），这与第三点的merge我觉得是一致的，将两列合成一列（单纯加减应该是不够的)</li></ul><p>​    <strong>注意计算频数的时候，原则是同分布下越多数据越好。</strong>所以train和test在不在一起取决于它们的分布是否一致，或者如何让它们分布一致。在该比赛中就对test集进行了划分，剔除了和train不一致的数据，再合在一起做频数</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>主要参考链接</p><h1 id="first-kernel"><a href="#first-kernel" class="headerlink" title="first kernel"></a>first kernel</h1><p>来源：<a href="https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920" target="_blank" rel="noopener">https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920</a></p><h2 id="magic-feature"><a href="#magic-feature" class="headerlink" title="magic feature"></a>magic feature</h2><p>不单单是用count ,而是更深入的使用count，告诉模型它所不知道的事情。比如说Unique in train 和test</p><ul><li>This value appears at least another time in data with target==1 and no 0;</li><li>This value appears at least another time in data with target==0 and no 1;</li><li>This value appears at least two more time in data with target==0 &amp; 1;</li><li>This value is unique in data;</li><li>This value is unique in data + test (only including real test samples);</li></ul><p>The other 200 (one per raw feature) features are numerical, let’s call them “not unique feat”, and correspond to the raw feature replacing values that are unique in data + test with the mean of the feature.</p><p>用该列的均值去替换该列的Unique值，使其成为非unique的列，这个操作是尝试出来的，它们也用了nan，median</p><h2 id="magic-insight"><a href="#magic-insight" class="headerlink" title="magic insight"></a>magic insight</h2><p>两个重要的节点：</p><ul><li>I looked at my LGBM trees (with only 3 leafs that’s easy to do) and noticed the trees were using the uniqueness information.通过树画图，看出树当前不能学习的东西</li><li>number of different values in train and test was not the same。 虽然数值上分布是一致的，但是在值的个数上不一致</li></ul><h2 id="technial-part"><a href="#technial-part" class="headerlink" title="technial part:"></a>technial part:</h2><p>匿名变量用NN会更好</p><p>NN的concat可以更好的处理变量之间的关系</p><h2 id="rest-magic"><a href="#rest-magic" class="headerlink" title="rest magic"></a>rest magic</h2><p> using the shuffle augmentation (duplicate and shuffle 16 times samples with target == 1, 4 for target ==0) and added pseudo label (2700 highest predicted test points as 1 and 2000 lowest as 0)</p><p>pseudo label指从测试集中取最高的为1最低的为0，加进去</p><p>shuffle augmentation 指数据增强(eda代码中有)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Santander Customer Transaction Prediction 比赛，01分类问题，200维连续型匿名变量。&lt;/p&gt;
&lt;p&gt;讲述了magic操作，count的新特征构造，特征相关性。。。&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>比赛模型</title>
    <link href="http://kodgv.xyz/2019/04/10/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E6%AF%94%E8%B5%9B%E6%A8%A1%E5%9E%8B/"/>
    <id>http://kodgv.xyz/2019/04/10/数据竞赛/比赛模型/</id>
    <published>2019-04-10T02:20:03.000Z</published>
    <updated>2019-04-12T09:04:51.957Z</updated>
    
    <content type="html"><![CDATA[<p>比赛通用模型代码</p><a id="more"></a><p>[TOC]</p><h2 id="载入模型前操作"><a href="#载入模型前操作" class="headerlink" title="载入模型前操作"></a>载入模型前操作</h2><h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler,StandardScaler</span><br><span class="line">sc = StandardScaler()</span><br><span class="line">train_features = sc.fit_transform(train_features)</span><br><span class="line">test_features = sc.transform(test_features)</span><br></pre></td></tr></table></figure><h3 id="切分训练集和验证集"><a href="#切分训练集和验证集" class="headerlink" title="切分训练集和验证集"></a>切分训练集和验证集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_split</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment"># K-fold</span></span><br><span class="line"><span class="comment"># scikit-learn k-fold cross-validation</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="comment"># data sample</span></span><br><span class="line"><span class="comment"># prepare cross validation</span></span><br><span class="line">kfold = KFold(n_splits=<span class="number">3</span>, shuffle = <span class="literal">True</span>, random_state= <span class="number">1</span>)</span><br><span class="line"><span class="comment"># enumerate splits</span></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kfold.split(data):</span><br><span class="line">    print(<span class="string">'train: %s, test: %s'</span> % (data[train], data[test]))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="comment"># StratifiedKFold</span></span><br><span class="line">folds = StratifiedKFold(n_splits=num_folds, shuffle=<span class="literal">False</span>, random_state=<span class="number">2319</span>)</span><br><span class="line"><span class="keyword">for</span> fold_, (trn_idx, val_idx) <span class="keyword">in</span> enumerate(folds.split(train.values, target.values)):</span><br><span class="line">    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]</span><br><span class="line">   X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]</span><br></pre></td></tr></table></figure><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="xgb回归"><a href="#xgb回归" class="headerlink" title="xgb回归"></a>xgb回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn</span></span><br><span class="line"><span class="comment">########################################################################### 回归</span></span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, StratifiedKFold,GroupKFold</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 别人的自定义损失函数,在parameter里面：object里面赋值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">custom_loss</span><span class="params">(y_true,y_pred)</span>:</span></span><br><span class="line">    penalty=<span class="number">2.0</span></span><br><span class="line">    grad=-y_true/y_pred+penalty*(<span class="number">1</span>-y_true)/(<span class="number">1</span>-y_pred) <span class="comment">#梯度</span></span><br><span class="line">    hess=y_true/(y_pred**<span class="number">2</span>)+penalty*(<span class="number">1</span>-y_true)/(<span class="number">1</span>-y_pred)**<span class="number">2</span> <span class="comment">#2阶导</span></span><br><span class="line">    <span class="keyword">return</span> grad,hess</span><br><span class="line"><span class="comment"># 自定义评价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse</span><span class="params">(y_pred,dtrain)</span>:</span> <span class="comment">#preds是结果（概率值），dtrain是个带label的DMatrix</span></span><br><span class="line">    labels=dtrain.get_label() <span class="comment">#提取label</span></span><br><span class="line">    t=mean_absolute_error(labels, y_pred)</span><br><span class="line">    print(t)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'mse'</span>,t</span><br><span class="line">parameters = &#123;<span class="string">'nthread'</span>:<span class="number">-1</span>, <span class="comment"># cpu 线程数 默认最大</span></span><br><span class="line">              <span class="string">'objective'</span>:<span class="string">'reg:linear'</span>,<span class="comment">#多分类or 回归的问题    若要自定义就替换为custom_loss（不带引号）</span></span><br><span class="line">              <span class="string">'learning_rate'</span>: <span class="number">.01</span>, <span class="comment">#so called `eta` value 如同学习率</span></span><br><span class="line">              <span class="string">'max_depth'</span>: <span class="number">6</span>,<span class="comment"># 构建树的深度，越大越容易过拟合</span></span><br><span class="line">              <span class="string">'min_child_weight'</span>: <span class="number">4</span>,</span><br><span class="line"><span class="comment"># 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言</span></span><br><span class="line"><span class="comment">#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。</span></span><br><span class="line"><span class="comment">#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。</span></span><br><span class="line">              <span class="string">'silent'</span>: <span class="number">1</span>,<span class="comment">#设置成1则没有运行信息输出，最好是设置为0.</span></span><br><span class="line">              <span class="string">'subsample'</span>: <span class="number">0.7</span>, <span class="comment"># 随机采样训练样本</span></span><br><span class="line">              <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,<span class="comment"># 生成树时进行的列采样</span></span><br><span class="line">              <span class="string">'n_estimators'</span>: <span class="number">100</span>,<span class="comment"># 树的个数跟num_boost_round是一样的，所以可以设置无限大，靠early_stop</span></span><br><span class="line">              <span class="string">'gamma'</span>:<span class="number">0.1</span>,<span class="comment"># 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。</span></span><br><span class="line">              <span class="string">'seed'</span>:<span class="number">1000</span> <span class="comment">#随机种子</span></span><br><span class="line">              <span class="comment">#'alpha':0, # L1 正则项参数</span></span><br><span class="line">              <span class="comment">#'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。</span></span><br><span class="line">              <span class="comment">#'num_class':10, # 类别数，多分类与 multisoftmax 并用</span></span><br><span class="line">              &#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergeToOne</span><span class="params">( X, X2)</span>:</span></span><br><span class="line">    X3 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">        tmp = np.array([list(X[i]), list(X2[i])])</span><br><span class="line">        X3.append(list(np.hstack(tmp)))</span><br><span class="line">    X3 = np.array(X3)</span><br><span class="line">    <span class="keyword">return</span> X3</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_XgbRegressor</span><span class="params">(train_data,train_target,test_data,feature_names,parameters,early_stopping_rounds,num_folds,eval_metric,model_name=<span class="string">'model'</span>,stratified=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    :param train_data: 一定是numpy</span></span><br><span class="line"><span class="string">    :param train_target:</span></span><br><span class="line"><span class="string">    :param parameters:</span></span><br><span class="line"><span class="string">    :param round:</span></span><br><span class="line"><span class="string">    :param k:</span></span><br><span class="line"><span class="string">    :param eval_metrics:自定义 or 内置字符串</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    reg=XGBRegressor()</span><br><span class="line">    reg.set_params(**parameters)</span><br><span class="line">    <span class="comment"># 定义一些变量</span></span><br><span class="line">    oof_preds = np.zeros((train_data.shape[<span class="number">0</span>],))</span><br><span class="line">    sub_preds = np.zeros((test_data.shape[<span class="number">0</span>],))</span><br><span class="line">    feature_importance_df = pd.DataFrame()</span><br><span class="line">    cv_result = []</span><br><span class="line">    <span class="comment"># K-flod</span></span><br><span class="line">    <span class="keyword">if</span> stratified:</span><br><span class="line">        folds = StratifiedKFold(n_splits= num_folds, shuffle=<span class="literal">True</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        folds = KFold(n_splits= num_folds, shuffle=<span class="literal">True</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">    X_train_newfeature=np.zeros((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> n_flod, (train_index, val_index) <span class="keyword">in</span> enumerate(folds.split(train_data, train_target)):</span><br><span class="line">        train_X=train_data[train_index]</span><br><span class="line">        val_X=train_data[val_index]</span><br><span class="line">        train_Y=train_target[train_index]</span><br><span class="line">        val_Y=train_target[val_index]</span><br><span class="line">        <span class="comment"># 参数初步定之后划分20%为验证集，准备一个watchlist 给train和validation set ,设置num_round 足够大（比如100000），以至于你能发现每一个round 的验证集预测结果，</span></span><br><span class="line">        <span class="comment"># 如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了。</span></span><br><span class="line">        watchlist= [(train_X, train_Y), (val_X, val_Y)]</span><br><span class="line">        <span class="comment"># early_stop 看validate的eval是否下降，这时候必须传eval_set,并取eval_set的最后一个作为validate</span></span><br><span class="line">        reg.fit(train_X,train_Y,early_stopping_rounds=early_stopping_rounds, eval_set=watchlist,eval_metric=eval_metric)</span><br><span class="line">        <span class="comment">## 生成gbdt新特征</span></span><br><span class="line">        new_feature = reg.apply(val_X)</span><br><span class="line">        <span class="keyword">if</span> X_train_newfeature.shape[<span class="number">0</span>]==<span class="number">1</span>:</span><br><span class="line">            X_train_newfeature=mergeToOne(val_X,new_feature)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X_train_newfeature = mergeToOne(val_X,new_feature)</span><br><span class="line">            X_train_newfeature=np.concatenate((X_train_newfeature,mergeToOne(new_feature, val_X)),axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">print</span> (X_train_newfeature)</span><br><span class="line">       <span class="comment"># 获得每次的预测值补充</span></span><br><span class="line">        oof_preds[val_index]=reg.predict(val_X)</span><br><span class="line">        <span class="comment"># 获得预测的平均值，这里直接加完再除m</span></span><br><span class="line">        sub_preds+= reg.predict(test_data)</span><br><span class="line">        result = mean_absolute_error(val_Y, reg.predict(val_X))</span><br><span class="line">        print(<span class="string">'Fold %2d macro-f1 : %.6f'</span> % (n_flod + <span class="number">1</span>, result))</span><br><span class="line">        cv_result.append(round(result,<span class="number">5</span>))</span><br><span class="line">        gc.collect()</span><br><span class="line">        <span class="comment"># 默认就是gain 如果要修改要再参数定义中修改importance_type</span></span><br><span class="line">        <span class="comment"># 保存特征重要度</span></span><br><span class="line">        gain = reg.feature_importances_</span><br><span class="line">        fold_importance_df = pd.DataFrame(&#123;<span class="string">'feature'</span>: feature_names,</span><br><span class="line">                                           <span class="string">'gain'</span>: <span class="number">100</span> * gain / gain.sum(),</span><br><span class="line">                                           <span class="string">'fold'</span>: n_flod,</span><br><span class="line">                                           &#125;).sort_values(<span class="string">'gain'</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 进行保存</span></span><br><span class="line">    sub_preds=sub_preds/folds.n_splits</span><br><span class="line">    new_feature=reg.apply(test_data)</span><br><span class="line">    X_test_newfeature = mergeToOne(test_data, new_feature)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./sub'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./sub'</span>)</span><br><span class="line">    pd.DataFrame(oof_preds,columns=[<span class="string">'class'</span>]).to_csv(<span class="string">'./sub/val_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>)</span><br><span class="line">    pd.DataFrame(sub_preds, columns=[<span class="string">'class'</span>]).to_csv(<span class="string">'./sub/test_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>)</span><br><span class="line">    print(<span class="string">'cv_result'</span>, cv_result)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./gbdt_newfeature'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./gbdt_newfeature'</span>)</span><br><span class="line"></span><br><span class="line">    np.save(<span class="string">"./gbdt_newfeature/train_newfeature.npy"</span>, X_train_newfeature)</span><br><span class="line">    np.save(<span class="string">"./gbdt_newfeature/test_newfeature.npy"</span>, X_test_newfeature)</span><br><span class="line">    save_importances(feature_importance_df, model_name)</span><br><span class="line">    <span class="keyword">return</span> reg,sub_preds</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_importances</span><span class="params">(feature_importance_df,model_name)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./feature_importance'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./feature_importance'</span>)</span><br><span class="line">    ft = feature_importance_df[[<span class="string">"feature"</span>, <span class="string">"gain"</span>]].groupby(<span class="string">"feature"</span>).mean().sort_values(by=<span class="string">"gain"</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">    ft.to_csv(<span class="string">'./feature_importance/importance_lightgbm_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="xgb回归-1"><a href="#xgb回归-1" class="headerlink" title="xgb回归"></a>xgb回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">########################################################################################## 分类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span>  sklearn.datasets  <span class="keyword">import</span>  make_hastie_10_2</span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line">clf_parameters = &#123;<span class="string">'nthread'</span>:<span class="number">-1</span>, <span class="comment"># cpu 线程数 默认最大</span></span><br><span class="line">              <span class="string">'objective'</span>:<span class="string">'multi:softmax'</span>,<span class="comment">#多分类or 回归的问题    若要自定义就替换为custom_loss（不带引号）</span></span><br><span class="line">              <span class="string">'learning_rate'</span>: <span class="number">.01</span>, <span class="comment">#so called `eta` value 如同学习率</span></span><br><span class="line">              <span class="string">'max_depth'</span>: <span class="number">6</span>,<span class="comment"># 构建树的深度，越大越容易过拟合</span></span><br><span class="line">              <span class="string">'min_child_weight'</span>: <span class="number">4</span>,</span><br><span class="line"><span class="comment"># 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言</span></span><br><span class="line"><span class="comment">#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。</span></span><br><span class="line"><span class="comment">#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。</span></span><br><span class="line">              <span class="string">'silent'</span>: <span class="number">1</span>,<span class="comment">#设置成1则没有运行信息输出，最好是设置为0.</span></span><br><span class="line">              <span class="string">'subsample'</span>: <span class="number">0.7</span>, <span class="comment"># 随机采样训练样本</span></span><br><span class="line">              <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,<span class="comment"># 生成树时进行的列采样</span></span><br><span class="line">              <span class="string">'n_estimators'</span>: <span class="number">500</span>,<span class="comment"># 树的个数跟num_boost_round是一样的，所以可以设置无限大，靠early_stop</span></span><br><span class="line">              <span class="string">'gamma'</span>:<span class="number">0.1</span>,<span class="comment"># 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。</span></span><br><span class="line">              <span class="string">'seed'</span>:<span class="number">1000</span> <span class="comment">#随机种子</span></span><br><span class="line">              <span class="comment">#'alpha':0, # L1 正则项参数</span></span><br><span class="line">              <span class="comment">#'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。</span></span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">n_class=<span class="number">3</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clf_custom_loss</span><span class="params">(y_true,y_pred)</span>:</span></span><br><span class="line">    penalty=<span class="number">2.0</span></span><br><span class="line">    grad=-y_true/y_pred+penalty*(<span class="number">1</span>-y_true)/(<span class="number">1</span>-y_pred) <span class="comment">#梯度</span></span><br><span class="line">    hess=y_true/(y_pred**<span class="number">2</span>)+penalty*(<span class="number">1</span>-y_true)/(<span class="number">1</span>-y_pred)**<span class="number">2</span> <span class="comment">#2阶导</span></span><br><span class="line">    <span class="keyword">return</span> grad,hess</span><br><span class="line"><span class="comment"># 自定义评价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clf_mse</span><span class="params">(y_pred,dtrain)</span>:</span> <span class="comment">#preds是结果（概率值），dtrain是个带label的DMatrix</span></span><br><span class="line">    labels=dtrain.get_label() <span class="comment">#提取label</span></span><br><span class="line">    <span class="comment">######### 分类预测的都是概率哦，所以这里要取一个max类别</span></span><br><span class="line">    y_pred = np.argmax(y_pred.reshape(n_class, <span class="number">-1</span>), axis=<span class="number">0</span>)</span><br><span class="line">    score=mean_absolute_error(labels, y_pred)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'mse'</span>,score</span><br><span class="line"><span class="comment"># 分类的时候要注意！！！！！！！！</span></span><br><span class="line"><span class="comment"># k-flod的时候要按层次拿出来，有一个shuffler我这里就没实现了，否则预测的类别会出现变小甚至报错</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_XgbClassifer</span><span class="params">(train_data,train_target,test_data,feature_names,parameters,early_stopping_rounds,num_folds,eval_metric,model_name=<span class="string">'model'</span>,stratified=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    :param train_data: 一定是numpy</span></span><br><span class="line"><span class="string">    :param train_target:</span></span><br><span class="line"><span class="string">    :param parameters:</span></span><br><span class="line"><span class="string">    :param round:</span></span><br><span class="line"><span class="string">    :param k:</span></span><br><span class="line"><span class="string">    :param eval_metrics:自定义 or 内置字符串</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 如果在param中设置，会莫名报参数不存在的错误</span></span><br><span class="line">    clf=XGBClassifier(num_class=n_class)</span><br><span class="line">    clf.set_params(**parameters)</span><br><span class="line">    <span class="comment"># 定义一些变量</span></span><br><span class="line">    oof_preds = np.zeros((train_data.shape[<span class="number">0</span>],n_class))</span><br><span class="line">    sub_preds = np.zeros((test_data.shape[<span class="number">0</span>],n_class))</span><br><span class="line">    feature_importance_df = pd.DataFrame()</span><br><span class="line">    cv_result = []</span><br><span class="line">    <span class="comment"># K-flod</span></span><br><span class="line">    <span class="keyword">if</span> stratified:</span><br><span class="line">        folds = StratifiedKFold(n_splits= num_folds, shuffle=<span class="literal">True</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        folds = KFold(n_splits= num_folds, shuffle=<span class="literal">True</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">    <span class="keyword">for</span> n_flod,(train_index, val_index) <span class="keyword">in</span> enumerate(folds.split(train_data,train_target)):</span><br><span class="line">        train_X=train_data[train_index]</span><br><span class="line">        val_X=train_data[val_index]</span><br><span class="line">        train_Y=train_target[train_index]</span><br><span class="line">        val_Y=train_target[val_index]</span><br><span class="line">        <span class="comment"># 参数初步定之后划分20%为验证集，准备一个watchlist 给train和validation set ,设置num_round 足够大（比如100000），以至于你能发现每一个round 的验证集预测结果，</span></span><br><span class="line">        <span class="comment"># 如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了。</span></span><br><span class="line">        watchlist= [(train_X, train_Y)]</span><br><span class="line">        <span class="comment"># early_stop 看validate的eval是否下降，这时候必须传eval_set,并取eval_set的最后一个作为validate</span></span><br><span class="line">        clf.fit(train_X,train_Y,early_stopping_rounds=early_stopping_rounds, eval_set=watchlist,eval_metric=eval_metric)</span><br><span class="line">        <span class="comment"># 获得每次的预测值补充</span></span><br><span class="line">        oof_preds[val_index]=clf.predict_proba(val_X)</span><br><span class="line">        <span class="comment"># 获得预测的平均值，这里直接加完再除m</span></span><br><span class="line">        sub_preds+= clf.predict_proba(test_data)</span><br><span class="line">        <span class="comment"># 计算当前准确率</span></span><br><span class="line">        result=mean_absolute_error(val_Y,clf.predict(val_X))</span><br><span class="line">        print(<span class="string">'Fold %2d macro-f1 : %.6f'</span> % (n_flod + <span class="number">1</span>, result))</span><br><span class="line">        print(type(result))</span><br><span class="line">        cv_result.append(round(result,<span class="number">5</span>))</span><br><span class="line">        gc.collect()</span><br><span class="line">        <span class="comment"># 默认就是gain 如果要修改要再参数定义中修改importance_type</span></span><br><span class="line">        <span class="comment"># 保存特征重要度</span></span><br><span class="line">        gain = clf.feature_importances_</span><br><span class="line">        fold_importance_df = pd.DataFrame(&#123;<span class="string">'feature'</span>:feature_names,</span><br><span class="line">                                           <span class="string">'gain'</span>:<span class="number">100</span>*gain/gain.sum(),</span><br><span class="line">                                           <span class="string">'fold'</span>:n_flod,</span><br><span class="line">                                           &#125;).sort_values(<span class="string">'gain'</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 进行保存</span></span><br><span class="line">    sub_preds=sub_preds/folds.n_splits</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./cv'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./cv'</span>)</span><br><span class="line">    pd.DataFrame(oof_preds,columns=[<span class="string">'class_'</span>+ str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_class)]).to_csv(<span class="string">'./cv/val_prob_&#123;&#125;.csv'</span>.format(model_name), index= <span class="literal">False</span>, float_format = <span class="string">'%.4f'</span>)</span><br><span class="line">    pd.DataFrame(sub_preds, columns=[<span class="string">'class_'</span> + str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_class)]).to_csv(<span class="string">'./cv/test_prob_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>, float_format=<span class="string">'%.4f'</span>)</span><br><span class="line">    oof_preds = [np.argmax(x) <span class="keyword">for</span> x <span class="keyword">in</span> oof_preds]</span><br><span class="line">    sub_preds = [np.argmax(x) <span class="keyword">for</span> x <span class="keyword">in</span> sub_preds]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./sub'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./sub'</span>)</span><br><span class="line">    pd.DataFrame(oof_preds,columns=[<span class="string">'class'</span>]).to_csv(<span class="string">'./sub/val_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>)</span><br><span class="line">    pd.DataFrame(sub_preds, columns=[<span class="string">'class'</span>]).to_csv(<span class="string">'./sub/test_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    save_importances(feature_importance_df, model_name)</span><br><span class="line">    <span class="keyword">return</span> clf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_importances</span><span class="params">(feature_importance_df,model_name)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./feature_importance'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./feature_importance'</span>)</span><br><span class="line">    ft = feature_importance_df[[<span class="string">"feature"</span>, <span class="string">"gain"</span>]].groupby(<span class="string">"feature"</span>).mean().sort_values(by=<span class="string">"gain"</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">    ft.to_csv(<span class="string">'./feature_importance/importance_lightgbm_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="lgb回归"><a href="#lgb回归" class="headerlink" title="lgb回归"></a>lgb回归</h3><p>注意Lgb对分类变量会有特殊的支持</p><p>只用看参数和pythonAPI</p><p><a href="http://lightgbm.apachecn.org/" target="_blank" rel="noopener">http://lightgbm.apachecn.org/</a></p><p><a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters" target="_blank" rel="noopener">https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">param = &#123;</span><br><span class="line">    <span class="string">'bagging_freq'</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">'bagging_fraction'</span>: <span class="number">0.335</span>,</span><br><span class="line">    <span class="string">'boost_from_average'</span>:<span class="string">'false'</span>,</span><br><span class="line">    <span class="string">'boost'</span>: <span class="string">'gbdt'</span>,</span><br><span class="line">    <span class="string">'feature_fraction'</span>: <span class="number">0.041</span>,</span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">0.0083</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">-1</span>,</span><br><span class="line">    <span class="string">'metric'</span>:<span class="string">'auc'</span>,</span><br><span class="line">    <span class="string">'min_data_in_leaf'</span>: <span class="number">80</span>,</span><br><span class="line">    <span class="string">'min_sum_hessian_in_leaf'</span>: <span class="number">10.0</span>,</span><br><span class="line">    <span class="string">'num_leaves'</span>: <span class="number">13</span>,</span><br><span class="line">    <span class="string">'num_threads'</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">'tree_learner'</span>: <span class="string">'serial'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'binary'</span>, </span><br><span class="line">    <span class="string">'verbosity'</span>: <span class="number">-1</span></span><br><span class="line">&#125;</span><br><span class="line">num_folds = <span class="number">11</span></span><br><span class="line">features = [c <span class="keyword">for</span> c <span class="keyword">in</span> train.columns <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'ID_code'</span>, <span class="string">'target'</span>]]</span><br><span class="line">folds = KFold(n_splits=num_folds, random_state=<span class="number">2319</span>)</span><br><span class="line">oof = np.zeros(len(train))</span><br><span class="line">getVal = np.zeros(len(train))</span><br><span class="line">predictions = np.zeros(len(target))</span><br><span class="line">feature_importance_df = pd.DataFrame()</span><br><span class="line">print(<span class="string">'Light GBM Model'</span>)</span><br><span class="line"><span class="keyword">for</span> fold_, (trn_idx, val_idx) <span class="keyword">in</span> enumerate(folds.split(train.values, target.values)): </span><br><span class="line">    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]</span><br><span class="line">    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]</span><br><span class="line">    print(<span class="string">"Fold idx:&#123;&#125;"</span>.format(fold_ + <span class="number">1</span>))</span><br><span class="line">    trn_data = lgb.Dataset(X_train, label=y_train)</span><br><span class="line">    val_data = lgb.Dataset(X_valid, label=y_valid)</span><br><span class="line">    clf = lgb.train(param, trn_data, <span class="number">1000000</span>, valid_sets = [trn_data, val_data], verbose_eval=<span class="number">5000</span>, early_stopping_rounds = <span class="number">4000</span>)</span><br><span class="line">    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)</span><br><span class="line">    fold_importance_df = pd.DataFrame()</span><br><span class="line">    fold_importance_df[<span class="string">"feature"</span>] = features</span><br><span class="line">    fold_importance_df[<span class="string">"importance"</span>] = clf.feature_importance()</span><br><span class="line">    fold_importance_df[<span class="string">"fold"</span>] = fold_ + <span class="number">1</span></span><br><span class="line">    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=<span class="number">0</span>)</span><br><span class="line">    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits</span><br><span class="line">print(<span class="string">"CV score: &#123;:&lt;8.5f&#125;"</span>.format(roc_auc_score(target, oof)))</span><br></pre></td></tr></table></figure><h3 id="lgb分类"><a href="#lgb分类" class="headerlink" title="lgb分类"></a>lgb分类</h3><p>回归和分类一致，只是参数不一样而已</p><p>注意Lgb对分类变量会有特殊的支持</p><p>只用看参数和pythonAPI</p><p><a href="http://lightgbm.apachecn.org/" target="_blank" rel="noopener">http://lightgbm.apachecn.org/</a></p><p><a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters" target="_blank" rel="noopener">https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters</a></p><h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><p>比较常用的手工概率blending</p><p><a href="https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899" target="_blank" rel="noopener">https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899</a></p><h2 id="调整参数"><a href="#调整参数" class="headerlink" title="调整参数"></a>调整参数</h2><h3 id="贝叶斯优化"><a href="#贝叶斯优化" class="headerlink" title="贝叶斯优化"></a>贝叶斯优化</h3><p>来源:<a href="https://www.cnblogs.com/yangruiGB2312/p/9374377.html" target="_blank" rel="noopener">https://www.cnblogs.com/yangruiGB2312/p/9374377.html</a></p><p>可以说是目前最好的调参的方法</p><ul><li><p>贝叶斯调参采用高斯过程，<strong>考虑之前的参数信息</strong>，不断地更新先验；网格搜索未考虑之前的参数信息</p></li><li><p>贝叶斯调参<strong>迭代次数少，速度快</strong>；网格搜索速度慢,参数多时易导致维度爆炸</p></li><li><p>贝叶斯调参针对非凸问题依然<strong>稳健</strong>；网格搜索针对非凸问题易得到局部优最</p></li></ul><p>​    公式推导就不在这里展开，主要阐述主要思想：给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程<strong>,直到后验分布基本贴合于真实分布。简单的说，就是</strong>考虑了上一次参数的信息，从而更好的调整当前的参数。</p><p>​    假设这个寻找最优化参数的过程是一个高斯过程。高斯过程有个特点，就是当随机遍历一定的数据点并拿到结果之后，可以大致绘制出整个数据的分布曲线，当然这是也会有一定误差。 </p><p><img src="https://img-blog.csdn.net/20180821135039882?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dzaGVuZ29k/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt></p><p>python代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rf_cv</span><span class="params">(n_estimators, min_samples_split, max_features, max_depth)</span>:</span></span><br><span class="line">    val = cross_val_score(</span><br><span class="line">        RandomForestClassifier(n_estimators=int(n_estimators),</span><br><span class="line">            min_samples_split=int(min_samples_split),</span><br><span class="line">            max_features=min(max_features, <span class="number">0.999</span>), <span class="comment"># float</span></span><br><span class="line">            max_depth=int(max_depth),</span><br><span class="line">            random_state=<span class="number">2</span></span><br><span class="line">        ),</span><br><span class="line">        x, y, scoring=<span class="string">'roc_auc'</span>, cv=<span class="number">5</span></span><br><span class="line">    ).mean()</span><br><span class="line">    <span class="keyword">return</span> val</span><br><span class="line"><span class="comment"># 注意参数名字要对应</span></span><br><span class="line">rf_bo = BayesianOptimization(</span><br><span class="line">        rf_cv,</span><br><span class="line">        &#123;<span class="string">'n_estimators'</span>: (<span class="number">10</span>, <span class="number">250</span>),</span><br><span class="line">        <span class="string">'min_samples_split'</span>: (<span class="number">2</span>, <span class="number">25</span>),</span><br><span class="line">        <span class="string">'max_features'</span>: (<span class="number">0.1</span>, <span class="number">0.999</span>),</span><br><span class="line">        <span class="string">'max_depth'</span>: (<span class="number">5</span>, <span class="number">15</span>)&#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>可以执行的操作</p><ul><li><p>以查看当前最优的参数和结果(同时，我们还可以修改高斯过程的参数，高斯过程主要参数是核函数(<code>kernel</code>)，还有其他参数可以参考<a href="http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html" target="_blank" rel="noopener">sklearn.gaussianprocess</a>)：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gp_param=&#123;<span class="string">'kernel'</span>:None&#125;</span><br><span class="line">rf_bo.maximize(**gp_param)</span><br><span class="line">rf_bo<span class="selector-class">.res</span>[<span class="string">'max'</span>]</span><br></pre></td></tr></table></figure></li><li><p>上面bayes算法得到的参数并不一定最优，当然我们会遇到一种情况，就是我们已经知道有一组或是几组参数是非常好的了，我们想知道其附近有没有更好的。</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rf_bo.explore(</span><br><span class="line">    &#123;<span class="string">'n_estimators'</span>: [<span class="number">10</span>, <span class="number">100</span>, <span class="number">200</span>],</span><br><span class="line">        <span class="string">'min_samples_split'</span>: [<span class="number">2</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">        <span class="string">'max_features'</span>: [<span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.9</span>],</span><br><span class="line">        <span class="string">'max_depth'</span>: [<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li></ul><h2 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛通用模型代码&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型" scheme="http://kodgv.xyz/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Vector Representation</title>
    <link href="http://kodgv.xyz/2019/04/09/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/representation/"/>
    <id>http://kodgv.xyz/2019/04/09/竞赛经验/representation/</id>
    <published>2019-04-09T12:10:18.000Z</published>
    <updated>2019-04-13T11:44:04.024Z</updated>
    
    <content type="html"><![CDATA[<p>​    自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 <code>Id537</code>，“dog”可能表示为 <code>Id143</code>。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。</p><a id="more"></a><p>来源：<a href="https://www.tensorflow.org/tutorials/representation/word2vec" target="_blank" rel="noopener">https://www.tensorflow.org/tutorials/representation/word2vec</a></p><p>​    自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 <code>Id537</code>，“dog”可能表示为 <code>Id143</code>。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。</p><p>​    <a href="https://en.wikipedia.org/wiki/Vector_space_model" target="_blank" rel="noopener">向量空间模型</a> (VSM) 在连续向量空间中表示（嵌入）字词，其中语义相似的字词会映射到附近的点（“在彼此附近嵌入”）。VSM 在 NLP 方面有着悠久而丰富的历史，但所有方法均以某种方式依赖于<a href="https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis" target="_blank" rel="noopener">分布假设</a>，这种假设指明在相同上下文中显示的字词语义相同。利用该原则的不同方法可分为两类：基于计数的方法（例如<a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" target="_blank" rel="noopener">潜在语义分析</a>）以及预测方法（例如<a href="http://www.scholarpedia.org/article/Neural_net_language_models" target="_blank" rel="noopener">神经概率语言模型</a>）。</p><p>​    Word2vec 是一种计算效率特别高的预测模型，用于学习原始文本中的字词嵌入。它分为两种类型：连续词袋模型 (CBOW) 和 Skip-Gram 模型。数据量少的时候使用CBOW，数据量大的时候Skip-gram，一般都用Skip-gram。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">https:</span>//www.tensorflow<span class="meta">.org</span>/tutorials/representation/word2vec</span><br></pre></td></tr></table></figure><h3 id="python"><a href="#python" class="headerlink" title="python"></a>python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.corpora <span class="keyword">import</span> WikiCorpus</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> LineSentence</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">L = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">path = <span class="string">'./'</span></span><br><span class="line">save_path = path + <span class="string">'/w2v'</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(save_path):</span><br><span class="line">    print(save_path)</span><br><span class="line">    os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">train1 = pd.read_csv(path + <span class="string">'/train.csv'</span>)</span><br><span class="line">train = pd.read_csv(path + <span class="string">'/train_old.csv'</span>)</span><br><span class="line">test = pd.read_csv(path + <span class="string">'/test.csv'</span>)</span><br><span class="line"></span><br><span class="line">data = pd.concat([train, test, train1]).reset_index(drop=<span class="literal">True</span>).sample(frac=<span class="number">1</span>, random_state=<span class="number">2018</span>).fillna(<span class="number">0</span>)</span><br><span class="line">data = data.replace(<span class="string">'\\N'</span>, <span class="number">999</span>)</span><br><span class="line">sentence = []</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> list(data[[<span class="string">'1_total_fee'</span>, <span class="string">'2_total_fee'</span>, <span class="string">'3_total_fee'</span>, <span class="string">'4_total_fee'</span>]].values):</span><br><span class="line">    sentence.append([str(float(l)) <span class="keyword">for</span> idx, l <span class="keyword">in</span> enumerate(line)])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'training...'</span>)</span><br><span class="line">model = Word2Vec(sentence, size=L, window=<span class="number">2</span>, min_count=<span class="number">1</span>, workers=multiprocessing.cpu_count(),</span><br><span class="line">                 iter=<span class="number">10</span>)</span><br><span class="line">print(<span class="string">'outputing...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> fea <span class="keyword">in</span> [<span class="string">'1_total_fee'</span>, <span class="string">'2_total_fee'</span>, <span class="string">'3_total_fee'</span>, <span class="string">'4_total_fee'</span>]:</span><br><span class="line">    values = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> list(data[fea].values):</span><br><span class="line">        values.append(line)</span><br><span class="line">    values = set(values)</span><br><span class="line">    print(len(values))</span><br><span class="line">    w2v = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> values:</span><br><span class="line">        a = [i]</span><br><span class="line">        a.extend(model[str(float(i))])</span><br><span class="line">        w2v.append(a)</span><br><span class="line">    out_df = pd.DataFrame(w2v)</span><br><span class="line"></span><br><span class="line">    name = [fea]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">        name.append(name[<span class="number">0</span>] + <span class="string">'W'</span> + str(i))</span><br><span class="line">    out_df.columns = name</span><br><span class="line">    out_df.to_csv(save_path + <span class="string">'/'</span> + fea + <span class="string">'.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="TSNE降维可视化"><a href="#TSNE降维可视化" class="headerlink" title="TSNE降维可视化"></a>TSNE降维可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_with_labels</span><span class="params">(low_dim_embs, labels, filename = <span class="string">'tsne.png'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>] &gt;= len(labels), <span class="string">"More labels than embeddings"</span></span><br><span class="line">    plt.figure(figsize= (<span class="number">10</span>, <span class="number">18</span>))</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">        x, y = low_dim_embs[i, :]</span><br><span class="line">        plt.scatter(x, y)</span><br><span class="line">        plt.annotate(label, xy = (x, y), textcoords = <span class="string">'offset points'</span>, ha = <span class="string">'right'</span>, va = <span class="string">'bottom'</span>)</span><br><span class="line">    plt.savefig(filename)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polt_tnse</span><span class="params">(df,df_target,plot_only=<span class="number">300</span>)</span>:</span></span><br><span class="line">    df_target=list(df_target.astype(<span class="string">'str'</span>))</span><br><span class="line">    tsne = TSNE(perplexity = <span class="number">30</span>, n_components = <span class="number">2</span>, init = <span class="string">'pca'</span>, n_iter = <span class="number">5000</span>)</span><br><span class="line">    low_dim_embs = tsne.fit_transform(df.iloc[:plot_only][:])</span><br><span class="line">    labels = [df_target[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(plot_only)]</span><br><span class="line">    plot_with_labels(low_dim_embs, labels)</span><br></pre></td></tr></table></figure><p><img src="https://www.tensorflow.org/images/tsne.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 &lt;code&gt;Id537&lt;/code&gt;，“dog”可能表示为 &lt;code&gt;Id143&lt;/code&gt;。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="预处理" scheme="http://kodgv.xyz/tags/%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="分类变量" scheme="http://kodgv.xyz/tags/%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>特征选择</title>
    <link href="http://kodgv.xyz/2019/04/09/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    <id>http://kodgv.xyz/2019/04/09/数据竞赛/特征选择/</id>
    <published>2019-04-09T11:52:45.000Z</published>
    <updated>2019-04-12T08:29:21.212Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p>如何进行特征选择</p><a id="more"></a><h2 id="feature-importace"><a href="#feature-importace" class="headerlink" title="feature importace"></a>feature importace</h2><h3 id="特征重要度高"><a href="#特征重要度高" class="headerlink" title="特征重要度高"></a>特征重要度高</h3><p>如果发现特征重要度有明显很高的情况下，可以将这个特征去除再尝试，因为有可能模型过于依赖此特征而导致了过拟合的存在。比如下图中的addr_state</p><p><img src="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/_images/gbm_variable_importance1.png" alt></p><h3 id="特征重要度低"><a href="#特征重要度低" class="headerlink" title="特征重要度低"></a>特征重要度低</h3><p>来源：<a href="https://www.kaggle.com/ogrellier/feature-selection-with-null-importances" target="_blank" rel="noopener">https://www.kaggle.com/ogrellier/feature-selection-with-null-importances</a></p><p>来源：<a href="https://academic.oup.com/bioinformatics/article/26/10/1340/193348" target="_blank" rel="noopener">https://academic.oup.com/bioinformatics/article/26/10/1340/193348</a></p><p>​    传统做法就是根据feature importance然后就跑一个阈值，剔除低于阈值的feature。</p><p>剔除阈值的目的在于：</p><ul><li><p>消除高相关的feature</p></li><li><p>提高model的variance</p></li></ul><p>​    <strong>最好最后的分数剔除分类变量</strong>，因为它主要是平衡分类变量的bias</p><p>​    论文提出的做法是，对于树模型，它的特征重要度会对数量多的分类变量有bias，所以提出了一种基于null importance 的做法。即对target打乱s次，预测s次的特征重要度，然后再利用distribution进行分析，从而得到比较标注的特征重要度。主要的做法请看kaggle上的示例：</p><p>获取重要度，此处需要自定义一个训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shuffle决定是否打乱y值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_feature_importances</span><span class="params">(data, shuffle, seed=None)</span>:</span></span><br><span class="line">    <span class="comment"># Gather real features</span></span><br><span class="line">    train_features = [f <span class="keyword">for</span> f <span class="keyword">in</span> data <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'TARGET'</span>, <span class="string">'SK_ID_CURR'</span>]]</span><br><span class="line">    <span class="comment"># Go over fold and keep track of CV score (train and valid) and feature importances</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle target if required</span></span><br><span class="line">    y = data[<span class="string">'TARGET'</span>].copy()</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># Here you could as well use a binomial distribution</span></span><br><span class="line">        y = data[<span class="string">'TARGET'</span>].copy().sample(frac=<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest</span></span><br><span class="line">    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=<span class="literal">False</span>, silent=<span class="literal">True</span>)</span><br><span class="line">    lgb_params = &#123;</span><br><span class="line">        <span class="string">'objective'</span>: <span class="string">'binary'</span>,</span><br><span class="line">        <span class="string">'boosting_type'</span>: <span class="string">'rf'</span>,</span><br><span class="line">        <span class="string">'subsample'</span>: <span class="number">0.623</span>,</span><br><span class="line">        <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,</span><br><span class="line">        <span class="string">'num_leaves'</span>: <span class="number">127</span>,</span><br><span class="line">        <span class="string">'max_depth'</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">'seed'</span>: seed,</span><br><span class="line">        <span class="string">'bagging_freq'</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">'n_jobs'</span>: <span class="number">4</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fit the model</span></span><br><span class="line">    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=<span class="number">200</span>, categorical_feature=categorical_feats)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get feature importances</span></span><br><span class="line">    imp_df = pd.DataFrame()</span><br><span class="line">    imp_df[<span class="string">"feature"</span>] = list(train_features)</span><br><span class="line">    imp_df[<span class="string">"importance_gain"</span>] = clf.feature_importance(importance_type=<span class="string">'gain'</span>)</span><br><span class="line">    imp_df[<span class="string">"importance_split"</span>] = clf.feature_importance(importance_type=<span class="string">'split'</span>)</span><br><span class="line">    imp_df[<span class="string">'trn_score'</span>] = roc_auc_score(y, clf.predict(data[train_features]))</span><br><span class="line">    <span class="keyword">return</span> imp_df</span><br></pre></td></tr></table></figure><p>跑n轮得到Null importance </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">null_imp_df = pd.DataFrame()</span><br><span class="line">nb_runs = <span class="number">80</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start = time.time()</span><br><span class="line">dsp = <span class="string">''</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_runs):</span><br><span class="line">    <span class="comment"># Get current run importances</span></span><br><span class="line">    imp_df = get_feature_importances(data=data, shuffle=<span class="literal">True</span>)</span><br><span class="line">    imp_df[<span class="string">'run'</span>] = i + <span class="number">1</span> </span><br><span class="line">    <span class="comment"># Concat the latest importances with the old ones</span></span><br><span class="line">    null_imp_df = pd.concat([null_imp_df, imp_df], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>然后画分布图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_distributions</span><span class="params">(actual_imp_df_, null_imp_df_, feature_)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">13</span>, <span class="number">6</span>))</span><br><span class="line">    gs = gridspec.GridSpec(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># Plot Split importances</span></span><br><span class="line">    ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">    a = ax.hist(null_imp_df_.loc[null_imp_df_[<span class="string">'feature'</span>] == feature_, <span class="string">'importance_split'</span>].values, label=<span class="string">'Null importances'</span>)</span><br><span class="line">    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_[<span class="string">'feature'</span>] == feature_, <span class="string">'importance_split'</span>].mean(), </span><br><span class="line">               ymin=<span class="number">0</span>, ymax=np.max(a[<span class="number">0</span>]), color=<span class="string">'r'</span>,linewidth=<span class="number">10</span>, label=<span class="string">'Real Target'</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_title(<span class="string">'Split Importance of %s'</span> % feature_.upper(), fontweight=<span class="string">'bold'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Null Importance (split) Distribution for %s '</span> % feature_.upper())</span><br><span class="line">    <span class="comment"># Plot Gain importances</span></span><br><span class="line">    ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    a = ax.hist(null_imp_df_.loc[null_imp_df_[<span class="string">'feature'</span>] == feature_, <span class="string">'importance_gain'</span>].values, label=<span class="string">'Null importances'</span>)</span><br><span class="line">    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_[<span class="string">'feature'</span>] == feature_, <span class="string">'importance_gain'</span>].mean(), </span><br><span class="line">               ymin=<span class="number">0</span>, ymax=np.max(a[<span class="number">0</span>]), color=<span class="string">'r'</span>,linewidth=<span class="number">10</span>, label=<span class="string">'Real Target'</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_title(<span class="string">'Gain Importance of %s'</span> % feature_.upper(), fontweight=<span class="string">'bold'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Null Importance (gain) Distribution for %s '</span> % feature_.upper())</span><br><span class="line">display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_=<span class="string">'LIVINGAPARTMENTS_AVG'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2019040923253844.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NjYxMjMw,size_16,color_FFFFFF,t_70" alt></p><p><img src="https://img-blog.csdnimg.cn/20190409232626147.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NjYxMjMw,size_16,color_FFFFFF,t_70" alt></p><p>图主要展示了什么叫做好的分布:方差小的。同时远离真实的重要度（红线）</p><p>我们想要的是在与真实目标(即实际重要性)使用时具有很高重要性的特性，而在与噪声训练时得分不高(即null importance得分)。在零假设和正态分布下，如果一个特征的红色实际重要性在蓝色分布内，那么该特征与目标不相关的可能性就很大。如果它在蓝色分布的5%范围内或者在外面，那么它就是相关的。</p><p>然后下一步要做的就是（去掉与target不相关的，去掉自己高相关的）：</p><ul><li>Drop high variance features if they are not really related to the target</li><li>Remove the decaying factor on correlated features, showing their real importance (or unbiased importance)</li></ul><p>结合一些公式，然后进行挑选</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">correlation_scores = []</span><br><span class="line"><span class="keyword">for</span> _f <span class="keyword">in</span> actual_imp_df[<span class="string">'feature'</span>].unique():</span><br><span class="line">    f_null_imps = null_imp_df.loc[null_imp_df[<span class="string">'feature'</span>] == _f, <span class="string">'importance_gain'</span>].values</span><br><span class="line">    f_act_imps = actual_imp_df.loc[actual_imp_df[<span class="string">'feature'</span>] == _f, <span class="string">'importance_gain'</span>].values</span><br><span class="line">    gain_score = <span class="number">100</span> * (f_null_imps &lt; np.percentile(f_act_imps, <span class="number">25</span>)).sum() / f_null_imps.size</span><br><span class="line">    f_null_imps = null_imp_df.loc[null_imp_df[<span class="string">'feature'</span>] == _f, <span class="string">'importance_split'</span>].values</span><br><span class="line">    f_act_imps = actual_imp_df.loc[actual_imp_df[<span class="string">'feature'</span>] == _f, <span class="string">'importance_split'</span>].values</span><br><span class="line">    split_score = <span class="number">100</span> * (f_null_imps &lt; np.percentile(f_act_imps, <span class="number">25</span>)).sum() / f_null_imps.size</span><br><span class="line">    correlation_scores.append((_f, split_score, gain_score))</span><br><span class="line"></span><br><span class="line">corr_scores_df = pd.DataFrame(correlation_scores, columns=[<span class="string">'feature'</span>, <span class="string">'split_score'</span>, <span class="string">'gain_score'</span>])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">score_feature_selection</span><span class="params">(df=None, train_features=None, cat_feats=None, target=None)</span>:</span></span><br><span class="line">    <span class="comment"># Fit LightGBM </span></span><br><span class="line">    dtrain = lgb.Dataset(df[train_features], target, free_raw_data=<span class="literal">False</span>, silent=<span class="literal">True</span>)</span><br><span class="line">    lgb_params = &#123;</span><br><span class="line">        <span class="string">'objective'</span>: <span class="string">'binary'</span>,</span><br><span class="line">        <span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,</span><br><span class="line">        <span class="string">'learning_rate'</span>: <span class="number">.1</span>,</span><br><span class="line">        <span class="string">'subsample'</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">'num_leaves'</span>: <span class="number">31</span>,</span><br><span class="line">        <span class="string">'max_depth'</span>: <span class="number">-1</span>,</span><br><span class="line">        <span class="string">'seed'</span>: <span class="number">13</span>,</span><br><span class="line">        <span class="string">'n_jobs'</span>: <span class="number">4</span>,</span><br><span class="line">        <span class="string">'min_split_gain'</span>: <span class="number">.00001</span>,</span><br><span class="line">        <span class="string">'reg_alpha'</span>: <span class="number">.00001</span>,</span><br><span class="line">        <span class="string">'reg_lambda'</span>: <span class="number">.00001</span>,</span><br><span class="line">        <span class="string">'metric'</span>: <span class="string">'auc'</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fit the model</span></span><br><span class="line">    hist = lgb.cv(</span><br><span class="line">        params=lgb_params, </span><br><span class="line">        train_set=dtrain, </span><br><span class="line">        num_boost_round=<span class="number">2000</span>,</span><br><span class="line">        categorical_feature=cat_feats,</span><br><span class="line">        nfold=<span class="number">5</span>,</span><br><span class="line">        stratified=<span class="literal">True</span>,</span><br><span class="line">        shuffle=<span class="literal">True</span>,</span><br><span class="line">        early_stopping_rounds=<span class="number">50</span>,</span><br><span class="line">        verbose_eval=<span class="number">0</span>,</span><br><span class="line">        seed=<span class="number">17</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># Return the last mean / std values </span></span><br><span class="line">    <span class="keyword">return</span> hist[<span class="string">'auc-mean'</span>][<span class="number">-1</span>], hist[<span class="string">'auc-stdv'</span>][<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># features = [f for f in data.columns if f not in ['SK_ID_CURR', 'TARGET']]</span></span><br><span class="line"><span class="comment"># score_feature_selection(df=data[features], train_features=features, target=data['TARGET'])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> threshold <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span> , <span class="number">40</span>, <span class="number">50</span> ,<span class="number">60</span> , <span class="number">70</span>, <span class="number">80</span> , <span class="number">90</span>, <span class="number">95</span>, <span class="number">99</span>]:</span><br><span class="line">    split_feats = [_f <span class="keyword">for</span> _f, _score, _ <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> _score &gt;= threshold]</span><br><span class="line">    split_cat_feats = [_f <span class="keyword">for</span> _f, _score, _ <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> (_score &gt;= threshold) &amp; (_f <span class="keyword">in</span> categorical_feats)]</span><br><span class="line">    gain_feats = [_f <span class="keyword">for</span> _f, _, _score <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> _score &gt;= threshold]</span><br><span class="line">    gain_cat_feats = [_f <span class="keyword">for</span> _f, _, _score <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> (_score &gt;= threshold) &amp; (_f <span class="keyword">in</span> categorical_feats)]</span><br><span class="line">                                                                                             </span><br><span class="line">    print(<span class="string">'Results for threshold %3d'</span> % threshold)</span><br><span class="line">    split_results = score_feature_selection(df=data, train_features=split_feats, cat_feats=split_cat_feats, target=data[<span class="string">'TARGET'</span>])</span><br><span class="line">    print(<span class="string">'\t SPLIT : %.6f +/- %.6f'</span> % (split_results[<span class="number">0</span>], split_results[<span class="number">1</span>]))</span><br><span class="line">    gain_results = score_feature_selection(df=data, train_features=gain_feats, cat_feats=gain_cat_feats, target=data[<span class="string">'TARGET'</span>])</span><br><span class="line">    print(<span class="string">'\t GAIN  : %.6f +/- %.6f'</span> % (gain_results[<span class="number">0</span>], gain_results[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><h3 id="特征相关性低"><a href="#特征相关性低" class="headerlink" title="特征相关性低"></a>特征相关性低</h3><p>如果特征之间的相关性很低的情况下，可以进一步检测变量之间的独立性，如果变量之间是independent意味着可以单独将这些特征进行多个模型训练再融合</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;如何进行特征选择&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>分类变量：Target Encoding</title>
    <link href="http://kodgv.xyz/2019/04/08/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/targetencoding/"/>
    <id>http://kodgv.xyz/2019/04/08/竞赛经验/targetencoding/</id>
    <published>2019-04-08T14:57:06.000Z</published>
    <updated>2019-04-13T11:44:17.367Z</updated>
    
    <content type="html"><![CDATA[<p>​    当你在进行监督学习时，你经常要处理分类变量。也就是说，变量没有一个自然的数值表示。问题是大多数机器学习算法都要求输入数据是数值的。在某个时候，数据科学管道将需要将分类变量转换为数值变量。</p><a id="more"></a><p>来源：<a href="https://maxhalford.github.io/blog/target-encoding-done-the-right-way/" target="_blank" rel="noopener">https://maxhalford.github.io/blog/target-encoding-done-the-right-way/</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>​    当你在进行监督学习时，你经常要处理分类变量。也就是说，变量没有一个自然的数值表示。问题是大多数机器学习算法都要求输入数据是数值的。在某个时候，数据科学管道将需要将分类变量转换为数值变量。</p><p>有很多方法可以做到这一点:</p><ul><li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" target="_blank" rel="noopener">Label encoding</a> 为每个类别选择数字</li><li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank" rel="noopener">One-hot encoding</a>:为每个类别创建一个二进制列</li><li><a href="https://www.tensorflow.org/tutorials/representation/word2vec" target="_blank" rel="noopener">Vector representation</a> ：也就是word2vec，在这里您可以找到一个适合您的数据的低维子空间</li><li><a href="https://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst#categorical-feature-support" target="_blank" rel="noopener">Optimal binning</a> ：在依赖于LightGBM或CatBoost等树学习器</li><li><a href="http://www.saedsayad.com/encoding.htm" target="_blank" rel="noopener">Target encoding</a>: 按类别平均目标值</li></ul><p>​    每种方法都有其优缺点，通常取决于您的数据和需求。如果一个变量有很多类别，那么一个[<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank" rel="noopener">One-hot encoding</a>:方案将产生许多列，这可能导致内存问题。根据我的经验，依赖LightGBM/CatBoost是最好的分箱方法。<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" target="_blank" rel="noopener">Label encoding</a>是没有用的，最好不要使用它。然而，如果你的分类变量恰好是有序的，那么你可以而且应该用递增的数字来表示它(例如，“cold”变成0，“mild”变成1，“hot”变成2)。word2vec和其他类似的方法很酷也很好，但是它们需要进行一些微调，而且并不总是奏效。</p><h2 id="target-encoding"><a href="#target-encoding" class="headerlink" title="target encoding"></a>target encoding</h2><p>​    target encoding是很容易理解的一种思想，假设你有分类变量X和变量Y，然后对于X中每一个distinct的元素计算其对应Y值的平局之，然后用这个平均值替换 $ x_i $ .下面这个例子显而易见：</p><div class="table-container"><table><thead><tr><th>x0</th><th>x1</th><th>y</th></tr></thead><tbody><tr><td>aa</td><td>cc</td><td>1</td></tr><tr><td>aa</td><td>cc</td><td>1</td></tr><tr><td>aa</td><td>cc</td><td>1</td></tr><tr><td>aa</td><td>cc</td><td>1</td></tr><tr><td>aa</td><td>cc</td><td>0</td></tr><tr><td>bb</td><td>cc</td><td>1</td></tr><tr><td>bb</td><td>cc</td><td>0</td></tr><tr><td>bb</td><td>cc</td><td>0</td></tr><tr><td>bb</td><td>cc</td><td>0</td></tr><tr><td>bb</td><td>dd</td><td>0</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>x0</th><th>x1</th><th>y</th></tr></thead><tbody><tr><td>0.8</td><td>cc</td><td>1</td></tr><tr><td>0.8</td><td>cc</td><td>1</td></tr><tr><td>0.8</td><td>cc</td><td>1</td></tr><tr><td>0.8</td><td>cc</td><td>1</td></tr><tr><td>0.8</td><td>cc</td><td>0</td></tr><tr><td>0.2</td><td>cc</td><td>1</td></tr><tr><td>0.2</td><td>cc</td><td>0</td></tr><tr><td>0.2</td><td>cc</td><td>0</td></tr><tr><td>0.2</td><td>cc</td><td>0</td></tr><tr><td>0.2</td><td>dd</td><td>0</td></tr></tbody></table></div><p>​    Target encoding的好处在于它提取了那些可以解释Y值，比如这里得aa拥有了一个Y值的平均值0.8，这会很好的帮助下游机器学习分类算法。</p><p>​    目标编码的问题有一个名称:过拟合。事实上，当平均值的数值很低时，依赖平均值并不总是一个好主意。您必须记住，您正在训练的数据集是一个更大的数据集的样本。这意味着，当您将训练集中发现的任何规律应用到另一个数据集(即测试集)时，可能都不成立。比如有可能训练集内它对应的Y值都是0，但是在测试集中它对应的Y值1比较多。</p><p>​    得出结论，永远不要用基础的target encoding,要用也是用以下的进阶encoding。</p><h2 id="Target-encoding进阶"><a href="#Target-encoding进阶" class="headerlink" title="Target encoding进阶"></a>Target encoding进阶</h2><p>有很多方法可以处理这个问题。交叉验证和additive smoothing可以结合使用</p><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>一种流行的方法是使用交叉验证并计算每个切分数据集中的平均值。这就是H20和许多kaggler所做的。</p><ul><li><p>None: no holdout, mean是对训练集的所有数据行的计算应用于测试数据</p></li><li><p>leave eoneout: mean是对除了当前行本身之外的所有数据行进行计算。这可以用于训练数据。当前行本身的目标不包括在平均值中，以防止过度拟合</p></li><li><p>KFold:平均值只计算out-of-fold数据(需要K-fold)</p></li></ul><p>这可以用于训练数据。为了防止过拟合，目标均值是根据叠外数据计算的</p><h3 id="additive-smoothing"><a href="#additive-smoothing" class="headerlink" title="additive smoothing"></a>additive smoothing</h3><p> 使用<a href="https://www.wikiwand.com/en/Additive_smoothing" target="_blank" rel="noopener">additive smoothing</a>，因为数据集中存在数据的count较小，所以它的target值容易受到过拟合的影响。<br>它使用了全局的平均值来<strong>smooth</strong>较少数据带来过拟合的影响</p><p>数学上它等价于:</p><script type="math/tex; mode=display">u=\frac{n\times \hat x+m\times w}{n+m}</script><p>where</p><ul><li>μ is the mean we’re trying to compute (the one that’s going to replace our categorical values)</li><li>n is the number of values you have</li><li>¯x is your estimated mean</li><li>m is the “weight” you want to assign to the overall mean</li><li>w is the overall mean</li></ul><p>其中m就是用来调节全局的权重，根据他的经验，他发现m取300的时候适用于大多数场合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_smooth_mean</span><span class="params">(df, by, on, m)</span>:</span></span><br><span class="line">    <span class="comment"># Compute the global mean</span></span><br><span class="line">    mean = df[on].mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the number of values and the mean of each group</span></span><br><span class="line">    agg = df.groupby(by)[on].agg([<span class="string">'count'</span>, <span class="string">'mean'</span>])</span><br><span class="line">    counts = agg[<span class="string">'count'</span>]</span><br><span class="line">    means = agg[<span class="string">'mean'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the "smoothed" means</span></span><br><span class="line">    smooth = (counts * means + m * mean) / (counts + m)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Replace each value by the according smoothed mean</span></span><br><span class="line">    <span class="keyword">return</span> df[by].map(smooth)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    当你在进行监督学习时，你经常要处理分类变量。也就是说，变量没有一个自然的数值表示。问题是大多数机器学习算法都要求输入数据是数值的。在某个时候，数据科学管道将需要将分类变量转换为数值变量。&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="预处理" scheme="http://kodgv.xyz/tags/%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="分类变量" scheme="http://kodgv.xyz/tags/%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>比赛EDA</title>
    <link href="http://kodgv.xyz/2019/04/08/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E6%AF%94%E8%B5%9BEDA/"/>
    <id>http://kodgv.xyz/2019/04/08/数据竞赛/比赛EDA/</id>
    <published>2019-04-08T10:38:48.000Z</published>
    <updated>2019-04-13T11:44:23.671Z</updated>
    
    <content type="html"><![CDATA[<p>比赛常见的EDA总结</p><a id="more"></a><p>[TOC]</p><h2 id="检查缺失值"><a href="#检查缺失值" class="headerlink" title="检查缺失值"></a>检查缺失值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/gpreda/santander-eda-and-prediction</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">missing_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    total = data.isnull().sum()</span><br><span class="line">    percent = (data.isnull().sum()/data.isnull().count()*<span class="number">100</span>)</span><br><span class="line">    tt = pd.concat([total, percent], axis=<span class="number">1</span>, keys=[<span class="string">'Total'</span>, <span class="string">'Percent'</span>])</span><br><span class="line">    types = []</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> data.columns:</span><br><span class="line">        dtype = str(data[col].dtype)</span><br><span class="line">        types.append(dtype)</span><br><span class="line">    tt[<span class="string">'Types'</span>] = types</span><br><span class="line">    <span class="keyword">return</span>(np.transpose(tt))</span><br></pre></td></tr></table></figure><h2 id="观察分布值"><a href="#观察分布值" class="headerlink" title="观察分布值"></a>观察分布值</h2><h3 id="观察训练集的01分布-可以拓展到任意分布对比图"><a href="#观察训练集的01分布-可以拓展到任意分布对比图" class="headerlink" title="观察训练集的01分布(可以拓展到任意分布对比图)"></a>观察训练集的01分布(可以拓展到任意分布对比图)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/gpreda/santander-eda-and-prediction</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_feature_distribution</span><span class="params">(df1, df2, label1, label2, features)</span>:</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    sns.set_style(<span class="string">'whitegrid'</span>)</span><br><span class="line">    plt.figure()</span><br><span class="line">    fig, ax = plt.subplots(<span class="number">10</span>,<span class="number">10</span>,figsize=(<span class="number">18</span>,<span class="number">22</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        plt.subplot(<span class="number">10</span>,<span class="number">10</span>,i)</span><br><span class="line">        sns.distplot(df1[feature], hist=<span class="literal">False</span>,label=label1)</span><br><span class="line">        sns.distplot(df2[feature], hist=<span class="literal">False</span>,label=label2)</span><br><span class="line">        plt.xlabel(feature, fontsize=<span class="number">9</span>)</span><br><span class="line">        locs, labels = plt.xticks()</span><br><span class="line">        plt.tick_params(axis=<span class="string">'x'</span>, which=<span class="string">'major'</span>, labelsize=<span class="number">6</span>, pad=<span class="number">-6</span>)</span><br><span class="line">        plt.tick_params(axis=<span class="string">'y'</span>, which=<span class="string">'major'</span>, labelsize=<span class="number">6</span>)</span><br><span class="line">    plt.show();</span><br><span class="line">    </span><br><span class="line">t0 = train_df.loc[train_df[<span class="string">'target'</span>] == <span class="number">0</span>]</span><br><span class="line">t1 = train_df.loc[train_df[<span class="string">'target'</span>] == <span class="number">1</span>]</span><br><span class="line">features = train_df.columns.values[<span class="number">2</span>:<span class="number">102</span>]</span><br><span class="line">plot_feature_distribution(t0, t1, <span class="string">'0'</span>, <span class="string">'1'</span>, features)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/201904091741016.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NjYxMjMw,size_16,color_FFFFFF,t_70" alt></p><p>结论：分布图怎么看，就是看如果01分布完全一模一样那么可能说明这个特征是冗余的无关特征，如果01分布不同的情况下，也做不了太多东西，只能结合特征重要度反证这些特征是有用的。</p><h3 id="train-test的分布"><a href="#train-test的分布" class="headerlink" title="train_test的分布"></a>train_test的分布</h3><p>同理可以选择train和test的行和列的mean/std/min值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">https://www.kaggle.com/gpreda/santander-eda-<span class="keyword">and</span>-prediction</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">"Distribution of mean values per column in the train and test set"</span>)</span><br><span class="line">sns.distplot(train_df[features].mean(axis=<span class="number">0</span>),color=<span class="string">"magenta"</span>,kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'train'</span>)</span><br><span class="line">sns.distplot(test_df[features].mean(axis=<span class="number">0</span>),color=<span class="string">"darkblue"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'test'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">"Distribution of std values per row in the train and test set"</span>)</span><br><span class="line">sns.distplot(train_df[features].std(axis=<span class="number">1</span>),color=<span class="string">"black"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'train'</span>)</span><br><span class="line">sns.distplot(test_df[features].std(axis=<span class="number">1</span>),color=<span class="string">"red"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'test'</span>)</span><br><span class="line">plt.legend();plt.show()</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">"Distribution of std values per column in the train and test set"</span>)</span><br><span class="line">sns.distplot(train_df[features].std(axis=<span class="number">0</span>),color=<span class="string">"blue"</span>,kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'train'</span>)</span><br><span class="line">sns.distplot(test_df[features].std(axis=<span class="number">0</span>),color=<span class="string">"green"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'test'</span>)</span><br><span class="line">plt.legend(); plt.show()</span><br><span class="line">t0 = train_df.loc[train_df[<span class="string">'target'</span>] == <span class="number">0</span>]</span><br><span class="line">t1 = train_df.loc[train_df[<span class="string">'target'</span>] == <span class="number">1</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">"Distribution of mean values per row in the train set"</span>)</span><br><span class="line">sns.distplot(t0[features].mean(axis=<span class="number">1</span>),color=<span class="string">"red"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'target = 0'</span>)</span><br><span class="line">sns.distplot(t1[features].mean(axis=<span class="number">1</span>),color=<span class="string">"blue"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'target = 1'</span>)</span><br><span class="line">plt.legend(); plt.show()</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">"Distribution of mean values per column in the train set"</span>)</span><br><span class="line">sns.distplot(t0[features].mean(axis=<span class="number">0</span>),color=<span class="string">"green"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'target = 0'</span>)</span><br><span class="line">sns.distplot(t1[features].mean(axis=<span class="number">0</span>),color=<span class="string">"darkblue"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'target = 1'</span>)</span><br><span class="line">plt.legend(); plt.show()</span><br></pre></td></tr></table></figure><h2 id="检查duplicate-values"><a href="#检查duplicate-values" class="headerlink" title="检查duplicate values"></a>检查duplicate values</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">features = train_df.columns.values[<span class="number">2</span>:<span class="number">202</span>]</span><br><span class="line">unique_max_train = []</span><br><span class="line">unique_max_test = []</span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">    values = train_df[feature].value_counts()</span><br><span class="line">    unique_max_train.append([feature, values.max(), values.idxmax()])</span><br><span class="line">    values = test_df[feature].value_counts()</span><br><span class="line">    unique_max_test.append([feature, values.max(), values.idxmax()])</span><br><span class="line"><span class="comment"># === plot</span></span><br><span class="line">df = df.sort_values(by=<span class="string">'n_train_unique'</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df[[<span class="string">'n_train_unique'</span>, <span class="string">'n_test_unique'</span>, <span class="string">'n_overlap'</span>]].plot(kind=<span class="string">'barh'</span> ,figsize=(<span class="number">22</span>, <span class="number">100</span>), fontsize=<span class="number">20</span>, width=<span class="number">0.8</span>)</span><br><span class="line">plt.yticks(df.index, df[<span class="string">'feature'</span>].values)</span><br><span class="line">plt.xlabel(<span class="string">'n_unique'</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">'feature'</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'center right'</span>, fontsize=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p><a href="https://zhuanlan.zhihu.com/p/28909807" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28909807</a></p><p>主成分分析是统计方法里的一种降维方法，它的主要思想是将原有 <img src="https://www.zhihu.com/equation?tex=n" alt="n"> 个特征通过正交变换将一组可能存在相关性的特征缩减到 <img src="https://www.zhihu.com/equation?tex=k" alt="k"> 特征( <img src="https://www.zhihu.com/equation?tex=k%5Cleq+n" alt="k\leq n"> )。举例来说，在网站用户行为数据收集过程中，会话数(Visits)，浏览页数(PV)，网站总停留时间(Time Spend Total)，访问人数(Unique Visitor)，以上这几个指标，无论是从以往的数据统计还是业务经验来看，都存在一定正相关关系，如果将这些特征喂给模型，很容易造成过拟合。</p><p>通过转化，从而剔除噪声，而且可以看到转化后的值然后把一些无关的列再剔除</p><p><strong>PCA 使用要点</strong></p><ol><li><p>使用主成分分析，往往会丢失掉“少部分信息”（注意：这“少部分信息”仅仅指方差较小的数据，并非信息含量真的少的数据！）</p></li><li><p>因为1的特性，所以在机器学些中，不推荐使用 PCA 去优化特征达到避免过拟合的目的。</p></li><li><p>既然PCA不能避免过拟合，那为何还要使用，根据周志华老师的西瓜书中的描述：</p></li><li><ol><li>在舍弃特征值较小的特征之后，能够使样本采集密度增</li><li>当数据受到噪声影响时，最小的特征值对对应的特征向量往往与噪声有关将他们舍弃能够在一定程度上起到去噪效果</li></ol></li><li><p>在维度过多的情况下，变换后的坐标系代表的意义不明，不易于解释。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/roydatascience/eda-pca-lgbm-santander-transactions/notebook</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> KernelPCA</span><br><span class="line">lin_pca = KernelPCA(n_components = <span class="number">2</span>, kernel=<span class="string">"linear"</span>, fit_inverse_transform=<span class="literal">True</span>)</span><br><span class="line">rbf_pca = KernelPCA(n_components = <span class="number">2</span>, kernel=<span class="string">"rbf"</span>, gamma=<span class="number">0.0433</span>, fit_inverse_transform=<span class="literal">True</span>)</span><br><span class="line">sig_pca = KernelPCA(n_components = <span class="number">2</span>, kernel=<span class="string">"sigmoid"</span>, gamma=<span class="number">0.001</span>, coef0=<span class="number">1</span>, fit_inverse_transform=<span class="literal">True</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">11</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> subplot, pca, title <span class="keyword">in</span> ((<span class="number">131</span>, lin_pca, <span class="string">"Linear kernel"</span>), (<span class="number">132</span>, rbf_pca, <span class="string">"RBF kernel, $\gamma=0.04$"</span>), </span><br><span class="line">                            (<span class="number">133</span>, sig_pca, <span class="string">"Sigmoid kernel, $\gamma=10^&#123;-3&#125;, r=1$"</span>)):   </span><br><span class="line">    PCA_train_x = PCA(<span class="number">2</span>).fit_transform(train_scaled)</span><br><span class="line">    plt.subplot(subplot)</span><br><span class="line">    plt.title(title, fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.scatter(PCA_train_x[:, <span class="number">0</span>], PCA_train_x[:, <span class="number">1</span>], c=target, cmap=<span class="string">"nipy_spectral_r"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"$z_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">    <span class="keyword">if</span> subplot == <span class="number">131</span>:</span><br><span class="line">        plt.ylabel(<span class="string">"$z_2$"</span>, fontsize=<span class="number">18</span>, rotation=<span class="number">0</span>)</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="热度图"><a href="#热度图" class="headerlink" title="热度图"></a>热度图</h2><h3 id="查看prediction"><a href="#查看prediction" class="headerlink" title="查看prediction"></a>查看prediction</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920</span></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">sns.heatmap(x, cmap=<span class="string">'RdBu_r'</span>, center=<span class="number">0.0</span>) </span><br><span class="line">plt.title(<span class="string">'VAR_'</span>+str(j)+<span class="string">' Predictions without Magic'</span>,fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xticks(np.linspace(<span class="number">0</span>,<span class="number">49</span>,<span class="number">5</span>),np.round(np.linspace(mn,mx,<span class="number">5</span>),<span class="number">1</span>))</span><br><span class="line">plt.xlabel(<span class="string">'Var_'</span>+str(j))</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.ylabel(<span class="string">''</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190412150953124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NjYxMjMw,size_16,color_FFFFFF,t_70" alt></p><p>通过热度图可以看出模型是否明显利用了两个特征。虽然可能特征重要度也会说明，但是这样的热度图明显会更具说服力，比如说var_0，模型可以得出竖线15，横线4之类的分界线。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛常见的EDA总结&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="预处理" scheme="http://kodgv.xyz/tags/%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>numpy</title>
    <link href="http://kodgv.xyz/2019/04/08/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/numpy/"/>
    <id>http://kodgv.xyz/2019/04/08/数据竞赛/numpy/</id>
    <published>2019-04-08T08:31:21.000Z</published>
    <updated>2019-04-13T11:43:45.171Z</updated>
    
    <content type="html"><![CDATA[<p>numpy 操作小技巧<br><a id="more"></a></p><h2 id="增加"><a href="#增加" class="headerlink" title="增加"></a>增加</h2><p>concat/vstack/hstack</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:<span class="regexp">//</span>blog.csdn.net<span class="regexp">/xiaodongxiexie/</span>article<span class="regexp">/details/</span><span class="number">71774466</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;numpy 操作小技巧&lt;br&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Numpy" scheme="http://kodgv.xyz/tags/Numpy/"/>
    
      <category term="python" scheme="http://kodgv.xyz/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python代码</title>
    <link href="http://kodgv.xyz/2019/04/08/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/python%E4%BB%A3%E7%A0%81/"/>
    <id>http://kodgv.xyz/2019/04/08/数据竞赛/python代码/</id>
    <published>2019-04-08T08:12:31.000Z</published>
    <updated>2019-04-17T09:15:49.220Z</updated>
    
    <content type="html"><![CDATA[<p>python 代码小技巧</p><a id="more"></a><p>[TOC]</p><h2 id="通用"><a href="#通用" class="headerlink" title="通用"></a>通用</h2><h3 id="测试时间"><a href="#测试时间" class="headerlink" title="测试时间"></a>测试时间</h3><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">关注walltime</span><br><span class="line"><span class="meta">%</span><span class="meta">%</span>time</span><br></pre></td></tr></table></figure><h2 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h2><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index, item <span class="keyword">in</span> enumerate(list1):</span><br><span class="line">    <span class="keyword">print</span> index, item</span><br></pre></td></tr></table></figure><h3 id="并行遍历"><a href="#并行遍历" class="headerlink" title="并行遍历"></a>并行遍历</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (x,y) <span class="keyword">in</span> zip(a,b):</span><br><span class="line">   <span class="keyword">print</span> x,y</span><br></pre></td></tr></table></figure><h3 id="通过字典保存了所有中间过程，不仅命名容易，而且便于后续操作"><a href="#通过字典保存了所有中间过程，不仅命名容易，而且便于后续操作" class="headerlink" title="通过字典保存了所有中间过程，不仅命名容易，而且便于后续操作"></a>通过字典保存了所有中间过程，不仅命名容易，而且便于后续操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d=&#123;d_name:get_dvalue(d_name) <span class="keyword">for</span> d_name <span class="keyword">in</span> d_list&#125;</span><br><span class="line">best_dvalue_dname=min(d.items(),key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h3 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这样会比单纯用readlines()快</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> index, line <span class="keyword">in</span> enumerate(open(filepath,<span class="string">'r'</span>))： </span><br><span class="line">    count += <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections import defaultdict 通过这个声明dict&#123;里面的格式&#125;</span><br><span class="line">如 defaultdict(<span class="built_in">list</span>)</span><br></pre></td></tr></table></figure><h3 id="列表生成式"><a href="#列表生成式" class="headerlink" title="列表生成式"></a>列表生成式</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>. [<span class="selector-tag">i</span> <span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> range(k) <span class="keyword">if</span> condition]：此时<span class="keyword">if</span>起条件判断作用，满足条件的，将被返回成为最终生成的列表的一员。</span><br><span class="line"><span class="number">2</span>. [<span class="selector-tag">i</span> <span class="keyword">if</span> condition <span class="keyword">else</span> exp <span class="keyword">for</span> exp]：此时<span class="keyword">if</span>...<span class="keyword">else</span>被用来赋值，满足条件的i以及<span class="keyword">else</span>被用来生成最终的列表</span><br></pre></td></tr></table></figure><h2 id="提高处理速度"><a href="#提高处理速度" class="headerlink" title="提高处理速度"></a>提高处理速度</h2><h3 id="多进程提高数据预处理"><a href="#多进程提高数据预处理" class="headerlink" title="多进程提高数据预处理"></a>多进程提高数据预处理</h3><p>df和numpy都可以，注意windows下保证两个代码分开，否则函数无法识别主进程和子进程</p><p>jupyter上代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这样子会很好的传参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">np_parallelize_run</span><span class="params">(func,df,nlp)</span>:</span></span><br><span class="line">    df_split = np.array_split(df, num_partitions)</span><br><span class="line">    params=[]</span><br><span class="line">    <span class="keyword">for</span> sub_df <span class="keyword">in</span> df_split:</span><br><span class="line">        param = &#123;&#125;</span><br><span class="line">        param[<span class="string">'text'</span>]=sub_df</span><br><span class="line">        param[<span class="string">'nlp'</span>]=nlp</span><br><span class="line">        params.append(param)</span><br><span class="line">    pool = Pool(num_cores)</span><br><span class="line">    df = np.concatenate(pool.map(func, params))</span><br><span class="line">    <span class="comment">#df = sp.vstack(pool.map(func, df_split), format='csr') faster and mem efficient for</span></span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><p>py文件上的代码</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def spacy_split(<span class="built_in">param</span>):</span><br><span class="line"><span class="keyword">text</span>,nlp = <span class="built_in">param</span>[<span class="string">'text'</span>],<span class="built_in">param</span>[<span class="string">'nlp'</span>]</span><br><span class="line">docs = nlp.pipe(<span class="keyword">text</span>) </span><br><span class="line">print(<span class="string">'finish'</span>)</span><br><span class="line"><span class="literal">return</span> list(docs)</span><br></pre></td></tr></table></figure><h2 id="自带算法库"><a href="#自带算法库" class="headerlink" title="自带算法库"></a>自带算法库</h2><h3 id="数组中查找插入的位置"><a href="#数组中查找插入的位置" class="headerlink" title="数组中查找插入的位置"></a>数组中查找插入的位置</h3><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">返回排序插入的位置，并不是真的插入排序数组中</span><br><span class="line"><span class="symbol">from</span> <span class="keyword">bisect </span><span class="meta">import</span> <span class="keyword">bisect_left, </span><span class="keyword">bisect_right</span></span><br><span class="line"><span class="keyword">end </span>= <span class="keyword">bisect_left(keys, </span><span class="number">0</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python 代码小技巧&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://kodgv.xyz/tags/python/"/>
    
  </entry>
  
</feed>
