<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小黑屋</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://kodgv.xyz/"/>
  <updated>2019-05-27T11:19:16.885Z</updated>
  <id>http://kodgv.xyz/</id>
  
  <author>
    <name>KODGV</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>git知识点</title>
    <link href="http://kodgv.xyz/2019/05/26/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/git%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <id>http://kodgv.xyz/2019/05/26/项目管理/git知识点/</id>
    <published>2019-05-26T13:44:33.000Z</published>
    <updated>2019-05-27T11:19:16.885Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p>要有分支树的概念，各个分支都是在同一棵树上的</p><h2 id="命令解析"><a href="#命令解析" class="headerlink" title="命令解析"></a>命令解析</h2><p><a href="https://www.yiibai.com/git/git_commit.html" target="_blank" rel="noopener">中文API解释</a></p><h3 id="git-status-查看文件状态"><a href="#git-status-查看文件状态" class="headerlink" title="　git status 查看文件状态"></a>　git status 查看文件状态</h3><p>使用 <code>git status</code> 时，实际上可以使用更为方便的指令来达到更为紧凑的格式输出。比如使用 <code>git status -s</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ git status -s</span><br><span class="line"></span><br><span class="line">#  M 靠右的 M 表示修改过的文件并且还未被放入暂存区</span><br><span class="line"> M README</span><br><span class="line"> </span><br><span class="line"># MM 靠左的 M 表示该文件被修改后放入了暂存区，靠右的表示修改过的文件并且还未被放入暂存区，所以 Rakefile 文件被修改过后放入了暂存区，但是之后又进行了修改，还未将最后一次修改放入暂存区</span><br><span class="line">MM Rakefile</span><br><span class="line"></span><br><span class="line"># A 表示新添加到暂存区的文件</span><br><span class="line">A  lib/git.rb</span><br><span class="line"></span><br><span class="line"># M 靠左的 M 表示该文件被修改后放入了暂存区</span><br><span class="line">M  lib/simplegit.rb</span><br><span class="line"></span><br><span class="line"># ?? 表示还未被跟踪</span><br><span class="line">?? LICENSE.txt  </span><br><span class="line"></span><br><span class="line"># 所以此时暂存区中的文件有 Rakefile, lib/git.rb, lib/simplegit.rb</span><br></pre></td></tr></table></figure><h3 id="git-diff-查看文件前后对比"><a href="#git-diff-查看文件前后对比" class="headerlink" title="git diff 查看文件前后对比"></a>git diff 查看文件前后对比</h3><p><code>git diff</code> 可以说是 <code>git status</code> 的具体版本，<code>git status</code> 只能查看修改了哪些文件，而 <code>git diff</code> 能够具体到该文件的某一部分。通常有以下两个用法</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">diff </span>&lt;file&gt; <span class="comment"># 比较当前文件和暂存区文件差异 git diff</span></span><br><span class="line">git <span class="keyword">diff </span>&lt;id1&gt;&lt;id1&gt;&lt;id2&gt; <span class="comment"># 比较两次提交之间的差异</span></span><br><span class="line">git <span class="keyword">diff </span>&lt;<span class="keyword">branch1&gt; </span>&lt;<span class="keyword">branch2&gt; </span><span class="comment"># 在两个分支之间比较</span></span><br><span class="line">git <span class="keyword">diff </span>--staged <span class="comment"># 比较暂存区和版本库差异</span></span><br><span class="line">git <span class="keyword">diff </span>--<span class="keyword">cached </span><span class="comment"># 比较暂存区和版本库差异</span></span><br></pre></td></tr></table></figure><h3 id="git-commit-提交记录"><a href="#git-commit-提交记录" class="headerlink" title="git commit  提交记录"></a>git commit  提交记录</h3><p>​    Git 仓库中的提交记录保存的是你的目录下所有文件的快照，就像是把整个目录复制，然后再粘贴一样，但比复制粘贴优雅许多！Git 希望提交记录尽可能地轻量，因此在你每次进行提交时，它并不会盲目地复制整个目录，它会将当前版本与仓库中的上一个版本进行对比，并把所有的差异打包到一起作为一个提交记录。</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git add . </span><br><span class="line"><span class="variable">$ </span><span class="comment"># 或者~</span></span><br><span class="line"><span class="variable">$ </span>git add newfile.txt</span><br><span class="line"><span class="variable">$ </span>git commit -m <span class="string">"the commit message"</span> <span class="comment">#</span></span><br><span class="line"><span class="variable">$ </span>git commit -a <span class="comment"># 会先把所有已经track的文件的改动`git add`进来，然后提交(有点像svn的一次提交,不用先暂存)。对于没有track的文件,还是需要执行`git add &lt;file&gt;` 命令。</span></span><br><span class="line"><span class="variable">$ </span>git commit --amend <span class="comment"># 增补提交，会使用与当前提交节点相同的父节点进行一次新的提交，旧的提交将会被取消。</span></span><br></pre></td></tr></table></figure><p>​    git commit之前要保证修改文件已执行git add添加进入暂存区。</p><h3 id="git-rm-删除"><a href="#git-rm-删除" class="headerlink" title="git rm 删除"></a>git rm 删除</h3><p>要从 Git 中移除某个文件，就必须从已经跟踪的文件清单中删除，然后提交。</p><p>删除有两种方式</p><ul><li>第一种是简单的从暂存区中删除。但是文件还在被跟踪着。</li><li>第二种是直接在未暂存区域中移除文件，表示直接将文件移除版本控制中。不再跟踪</li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git rm [<span class="string">-f | --force</span>][<span class="symbol">-n</span>] [<span class="string">-r</span>][<span class="symbol">--cached</span>] [<span class="string">--ignore-unmatch</span>][<span class="symbol">--quiet</span>] [--] <span class="xml"><span class="tag">&lt;<span class="name">file</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>注意：</p><ul><li>如果删除文件之前文件修改过并且已经放入了暂存区域，则必须使用强制删除选项<code>-f</code> 才能将其删除。主要是为了防止误删。</li><li>当我们想要将文件从 Git 仓库中删除但是却想让他仍在我们的工作区域中时，（即保存在本地磁盘并且不被 Git 跟踪），为了达到这一目的，使用 <code>--cached</code> 选项</li><li>如果单纯执行rm,它虽然从磁盘删除，但是git依旧追踪，即远程仓库没有改变】</li></ul><h3 id="git-stash-备份"><a href="#git-stash-备份" class="headerlink" title="git stash 备份"></a>git stash 备份</h3><p>​    当我们已经在一个分支上修改文件后，如果必须要切换到其他分支展开其他的工作，而当前分支的工作还没有完成，此时我们需要使用 <code>$ git stash</code> 或 <code>$ git stash save</code> 命令将当前分支上的工作暂存到栈上，这时你的工作目录就干净了，就可以切换到其他分支工作，等工作完成后，再切换回原来的分支，可以使用 <code>$ git stash apply</code> 将你刚刚的储藏重新应用。</p><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">git stash: 备份当前的工作区的内容，从最近的一次提交中读取相关内容，让工作区保证和上次提交的内容一致。同时，将当前的工作区内容保存到Git栈中。</span><br><span class="line"></span><br><span class="line">git stash <span class="keyword">pop</span>: 从Git栈中读取最近一次保存的内容，恢复工作区的相关内容。由于可能存在多个Stash的内容，所以用栈来管理，<span class="keyword">pop</span>会从最近的一个stash中读取内容并恢复。</span><br><span class="line"></span><br><span class="line">git stash <span class="keyword">pop</span> –<span class="keyword">index</span> stash@&#123;0&#125;: 恢复编号为<span class="number">0</span>的进度的工作区和暂存区。</span><br><span class="line"></span><br><span class="line">git stash apply stash@&#123;1&#125; 以将你指定版本号为stash@&#123;1&#125;的工作取出来</span><br><span class="line"></span><br><span class="line">git stash drop[] 删除某一个进度，默认删除最新进度</span><br><span class="line"></span><br><span class="line">git stash list: 显示Git栈内的所有备份，可以利用这个列表来决定从那个地方恢复。</span><br><span class="line"></span><br><span class="line">git stash clear: 清空Git栈。此时使用gitg等图形化工具会发现，原来stash的哪些节点都消失了</span><br></pre></td></tr></table></figure><h3 id="git-log-查看提交历史"><a href="#git-log-查看提交历史" class="headerlink" title="git log 查看提交历史"></a>git log 查看提交历史</h3><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">选项                说明</span><br><span class="line">-p                按补丁格式显示每个更新之间的差异。</span><br><span class="line"><span class="comment">--stat            显示每次更新的文件修改统计信息。</span></span><br><span class="line"><span class="comment">--shortstat       只显示 --stat 中最后的行数修改添加移除统计。</span></span><br><span class="line"><span class="comment">--name-only       仅在提交信息后显示已修改的文件清单。</span></span><br><span class="line"><span class="comment">--name-status 显示新增、修改、删除的文件清单。</span></span><br><span class="line"><span class="comment">--abbrev-commit   仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。</span></span><br><span class="line"><span class="comment">--relative-date   使用较短的相对时间显示（比如，“2 weeks ago”）。</span></span><br><span class="line"><span class="comment">--graph           显示 ASCII 图形表示的分支合并历史。</span></span><br><span class="line"><span class="comment">--pretty      使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。</span></span><br></pre></td></tr></table></figure><p>常用选项 <code>-p</code> ,用来显示每次提交的内容差异，可以加上 -2 来仅仅显示最近两次提交。现在一般都是用工具或者在网上直接看记录</p><h3 id="Git-Branch-分支创建（合并分支和分离HEAD）"><a href="#Git-Branch-分支创建（合并分支和分离HEAD）" class="headerlink" title="Git Branch 分支创建（合并分支和分离HEAD）"></a>Git Branch 分支创建（合并分支和分离HEAD）</h3><p>​    Git 的分支也非常轻量。它们只是简单地指向某个提交纪录 —— 仅此而已。所以许多 Git 爱好者传颂：</p><blockquote><p>早建分支！多用分支！</p></blockquote><p>​    这是因为即使创建再多分的支也不会造成储存或内存上的开销，并且按逻辑分解工作到不同的分支要比维护那些特别臃肿的分支简单多了</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 查看本地分支</span></span><br><span class="line">git branch</span><br><span class="line"></span><br><span class="line"><span class="meta"># 查看远程分支</span></span><br><span class="line">git branch -r</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建本地分支(注意新分支创建后不会自动切换为当前分支)</span></span><br><span class="line">git branch [name]</span><br><span class="line"></span><br><span class="line"><span class="meta"># 切换分支</span></span><br><span class="line">git checkout [name]</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建新分支并立即切换到新分支</span></span><br><span class="line">git checkout -b [name]</span><br><span class="line"></span><br><span class="line"><span class="meta"># 强制删除一个分支</span></span><br><span class="line">git branch -D [name]</span><br><span class="line"></span><br><span class="line"><span class="meta"># 移动分支</span></span><br><span class="line">git branch -f master HEAD^</span><br></pre></td></tr></table></figure><p>​    Git 创建新分支的本质就是创建一个可以移动的新的指针。比如创建一个 testing 分支。<code>$ git branch testing</code> 这会在当前所提交的对象上创建一个指针，此时如图</p><p><img src="/2019/05/26/项目管理/git知识点/2.webp" alt></p><h3 id="git-merge-amp-git-rebase-合并分支"><a href="#git-merge-amp-git-rebase-合并分支" class="headerlink" title="git merge&amp;git rebase  合并分支"></a>git merge&amp;git rebase  合并分支</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git merge 分支<span class="number">1</span> 分支<span class="number">2</span></span><br><span class="line">git rebase   复制到目标分支 被复制分支</span><br></pre></td></tr></table></figure><p><img src="/2019/05/26/项目管理/git知识点/3.png" alt></p><p>git merge是将两个分支合并会生成一个新的节点，原来的分支没有了</p><p>git rebase 不会生成新的节点，是将两个分支融合成一个线性的提交。</p><ul><li>解决冲突时。merge操作遇到冲突的时候，当前merge不能继续进行下去。手动修改冲突内容后，add 修改，commit就可以了。而<code>rebase</code>操作的话，会中断rebase,同时会提示去解决冲突。解决冲突后,将修改add后执行<code>git rebase –continue</code>继续操作，或者<code>git rebase –skip</code>忽略冲突。</li></ul><h3 id="git-reset-amp-git-revert-撤销变更"><a href="#git-reset-amp-git-revert-撤销变更" class="headerlink" title="git reset&amp;git revert 撤销变更"></a>git reset&amp;git revert 撤销变更</h3><p>git revert<code>是用一次新的commit来回滚之前的commit，</code>git reset`是直接删除指定的commit。</p><p><strong>注意相对位置不同</strong></p><p>git reset HEAD^            撤销前一次commit</p><p>git reset HEAD            撤销前一次commit</p><p><strong>注意merge不同</strong></p><ul><li>reset相当于将HEAD指针往前移动，将HEAD指向的位置改变为之前存在的某个版本。（远程老commit还在,push的时候会merge）</li><li>revert相当于使用一次逆向的commit“中和”之前的提交，创建一个新的状态(push的时候不会在和远程老commit进行Merge)。</li></ul><p>在回滚这一操作上看，效果差不多。但是在日后继续merge以前的老版本时有区别。因为<code>git revert</code>是用一次逆向的commit“中和”之前的提交，因此日后合并老的branch时，导致这部分改变不会再次出现，但是<code>git reset</code>是之间把某些commit在某个branch上删除，因而和老的branch再次merge时，这些被回滚的commit应该还会被引入。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">reset</span> –mixed <span class="keyword">id</span>: 回退一个版本,且会将暂存区的内容和本地已提交的内容全部恢复到未暂存的状态,不影响原来本地文件(未提交的也 不受影响) </span><br><span class="line">git <span class="keyword">reset</span> –soft <span class="keyword">id</span>: 回退一个版本,不清空暂存区,将已提交的内容恢复到暂存区,不影响原来本地的文件(未提交的也不受影响) </span><br><span class="line">git <span class="keyword">reset</span> –herd <span class="keyword">id</span>: 回退一个版本,清空暂存区,将已提交的内容的版本恢复到本地,本地的文件也将被恢复的版本替换</span><br></pre></td></tr></table></figure><h3 id="git-cherry-pick-整理提交记录"><a href="#git-cherry-pick-整理提交记录" class="headerlink" title="git cherry-pick 整理提交记录"></a>git cherry-pick 整理提交记录</h3><p>​     cherry-pick的作用是，将某次或者某几次的提交,在另一个分支上进行重演。(并不想合并而是想拿一些修改状态过来)</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git cherry-pick commitid1 commitid2 commitid3 <span class="comment"># 多个哈希值空格分开</span></span><br><span class="line">git cherry-pick commitid1.<span class="string">..commitid3</span><span class="comment"># ...表示取两个哈希值区间所有状态（不包含第一个commitid）</span></span><br></pre></td></tr></table></figure><blockquote><p>看一个在开发中经常会遇到的情况：我正在解决某个特别棘手的 Bug，为了便于调试而在代码中添加了一些调试命令并向控制台打印了一些信息。这些调试和打印语句都在它们各自的提交记录里。最后我终于找到了造成这个 Bug 的根本原因，解决掉以后觉得沾沾自喜！最后就差把 <code>bugFix</code> 分支里的工作合并回 <code>master</code> 分支了。你可以选择通过 fast-forward 快速合并到 <code>master</code> 分支上，但这样的话 <code>master</code> 分支就会包含我这些调试语句了。实际我们只要让 Git 复制解决问题的那一个提交记录就可以了。跟之前我们在“整理提交记录”中学到的一样，我们可以使用git merge两个分支就可以了</p></blockquote><h3 id="git-fetch-远程下载"><a href="#git-fetch-远程下载" class="headerlink" title="git fetch 远程下载"></a>git fetch 远程下载</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git fetch origin <span class="tag">&lt;<span class="name">source</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>git fetch</code> 完成了仅有的但是很重要的两步:</p><ul><li>从远程仓库下载本地仓库中缺失的提交记录</li><li>更新远程分支指针(如 <code>o/master</code>)</li></ul><p><code>git fetch</code> 实际上将本地仓库中的远程分支更新成了远程仓库相应分支最新的状态,它不会改变你的本地仓库，也不会改变你的分支.如果不带参数，它将下载所有远程提交记录</p><p>既然我们已经知道了如何用 <code>git fetch</code> 获取远程的数据, 现在我们学习如何将这些变化更新到我们的工作当中。</p><p>其实有很多方法的 —— 当远程分支中有新的提交时，你可以像合并本地分支那样来合并远程分支。也就是说就是你可以执行以下命令:</p><ul><li><code>git cherry-pick o/master</code></li><li><code>git rebase o/master</code></li><li><code>git merge o/master</code></li><li>等等</li></ul><p>也就是说你可以选你喜欢的进行合并</p><h3 id="git-push-提交到远程"><a href="#git-push-提交到远程" class="headerlink" title="git push 提交到远程"></a>git push 提交到远程</h3><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin &lt;source&gt;<span class="symbol">:&lt;destination&gt;</span></span><br></pre></td></tr></table></figure><p>git push 提交成功之后本地对应的远程分支也会对应更新</p><h3 id="git-tag-版本标签"><a href="#git-tag-版本标签" class="headerlink" title="git tag 版本标签"></a>git tag 版本标签</h3><p>主要就是用于release的时候进行标记。</p><p>1.添加标签： git tag -a version -m “note”<br>注解：git tag 是打标签的命令，-a 是添加标签，其后要跟新标签号，-m 及后面的字符串是对该标签的注释。</p><p>2.提交标签到远程仓库 ：git push origin -tags<br>注解：就像git push origin master 把本地修改提交到远程仓库一样，-tags可以把本地的打的标签全部提交到远程仓库。</p><p>3.删除标签：git tag -d version<br>注解：-d 表示删除，后面跟要删除的tag名字</p><p>4.删除远程标签：git push origin :refs/tags/version<br>注解：就像git push origin :branch_1 可以删除远程仓库的分支branch_1一样， 冒号前为空表示删除远程仓库的tag。</p><p>5.查看标签：git tag或者git tag -l</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">tag</span> <span class="title">-a</span> v0.<span class="number">1.1</span> <span class="literal">master</span></span><br><span class="line">git push --tags</span><br></pre></td></tr></table></figure><h2 id="进阶知识"><a href="#进阶知识" class="headerlink" title="进阶知识"></a>进阶知识</h2><h3 id="生命周期"><a href="#生命周期" class="headerlink" title="生命周期"></a>生命周期</h3><p><img src="/2019/05/26/项目管理/git知识点/1.webp" alt></p><p>要查看哪些文件处于什么状态，可以用 git status 命令</p><p>Git 有四种状态，你的文件可能处于其中之一：</p><ul><li><strong>Untracked</strong>: 未跟踪, 此文件在文件夹中, 但并没有加入到git库, 不参与版本控制. 通过<code>git add</code> 状态变为<code>Staged</code>.</li><li><strong>Unmodify</strong>: 文件已经入库, 未修改, 即版本库中的文件快照内容与文件夹中完全一致. 这种类型的文件有两种去处, 如果它被修改, 而变为<code>Modified</code>. 如果使用<code>git rm</code>移出版本库, 则成为<code>Untracked</code>文件</li><li><strong>Modified</strong>: 文件已修改, 仅仅是修改, 并没有进行其他的操作. 这个文件也有两个去处, 通过<code>git add</code>可进入暂存<code>staged</code>状态, 使用<code>git checkout</code> 则丢弃修改过, 返回到<code>unmodify</code>状态, 这个<code>git checkout</code>即从库中取出文件, 覆盖当前修改</li><li><strong>Staged</strong>: 暂存状态. 执行<code>git commit</code>则将修改同步到库中, 这时库中的文件和本地文件又变为一致, 文件为<code>Unmodify</code>状态. 执行<code>git reset HEAD filename</code>取消暂存, 文件状态为<code>Modified</code></li></ul><p>Git 有三种工作区域，你的文件可能处于其中之一：</p><ul><li><p>工作目录 Working Directory ：对项目的某个版本独立提取出来的内容，这些从Git仓库的压缩数据库提取出来的文件，放在磁盘上供你使用或修改。</p></li><li><p>暂存区域 Staging Area ：是<strong>一个文件</strong>，<strong>保存了下次将提交的文件列表</strong>，是待提交文件的暂存区域。一般在Git仓库的目录中，有时也被称为索引。</p></li><li><p>Git仓库：用来保存项目的元数据和对象数据库的地方。是Git中最重要的部分，从其他计算机克隆仓库时拷贝的就是这里的数据</p></li></ul><h3 id="忽略内容"><a href="#忽略内容" class="headerlink" title="忽略内容"></a>忽略内容</h3><p>新建一个.gitignore 的文件，列出要忽略的文件模式。<br> 文件 .gitignore 的格式规范如下：</p><p>所有空行或者以 ＃ 开头的行都会被 Git 忽略。</p><p>可以使用标准的 glob 模式匹配。</p><p>匹配模式可以以（/）开头防止递归。</p><p>匹配模式可以以（/）结尾指定目录。</p><p>要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">\<span class="comment"># no .a files  忽略所有.a结尾的文件</span></span><br><span class="line">*.a</span><br><span class="line"></span><br><span class="line">\<span class="comment"># but do track lib.a, even though you're ignoring .a files above，被忽略的.a文件中不包括lib.a文件</span></span><br><span class="line">!<span class="class"><span class="keyword">lib</span>.<span class="title">a</span></span></span><br><span class="line"></span><br><span class="line">\<span class="comment"># only ignore the TODO file in the current directory, not subdir/TODO</span></span><br><span class="line">/TODO</span><br><span class="line"></span><br><span class="line">\<span class="comment"># ignore all files in the build/ directory</span></span><br><span class="line">build/</span><br><span class="line"></span><br><span class="line">\<span class="comment"># ignore doc/notes.txt, but not doc/server/arch.txt</span></span><br><span class="line">doc/*.txt</span><br><span class="line"></span><br><span class="line">\<span class="comment"># ignore all .pdf files in the doc/ directory</span></span><br><span class="line">doc/**<span class="regexp">/*.pdf</span></span><br></pre></td></tr></table></figure><h3 id="分离HEAD"><a href="#分离HEAD" class="headerlink" title="分离HEAD"></a>分离HEAD</h3><p>​    HEAD 是一个对当前检出记录的符号引用 —— 也就是指向你正在其基础上进行工作的提交记录。HEAD 总是指向当前分支上最近一次提交记录。大多数修改提交树的 Git 命令都是从改变 HEAD 的指向开始的。</p><p>如何判断 Git 当前在哪一个分支？此时就要依靠 HEAD 指针。该指针指向当前所在的本地分支。当HEAD指向一个分支的时候，会变成’分支*’，否则会以HEAD-&gt;形式存在。</p><p>如图</p><p><img src="/2019/05/26/项目管理/git知识点/3.webp" alt="img"></p><h3 id="相对引用"><a href="#相对引用" class="headerlink" title="相对引用"></a>相对引用</h3><p>指定某个状态可以用其哈希码，不过哈希码共40位。解决办法有两个：</p><ul><li><p>it 对哈希的处理很智能。你只需要提供能够唯一标识提交记录的前几个字符即可。因此我可以仅输入<code>fed2</code> 而不是一长串字符</p></li><li><p>相对引用（要有一个作为基准，如master或其他分支）：</p><ul><li><p>使用 <code>^&lt;num&gt;</code>向上一层水平移动多个提交记录</p></li><li><p>使用 <code>~&lt;num&gt;</code> 向上移动多个提交记录，如 <code>~3</code></p><h3 id="Git-Flow简介"><a href="#Git-Flow简介" class="headerlink" title="Git Flow简介"></a>Git Flow简介</h3></li></ul></li></ul><p>​    功能驱动式开发”（Feature-driven development，简称FDD）。它指的是，需求是开发的起点，先有需求再有功能分支（feature branch）或者补丁分支（hotfix branch）。完成开发后，该分支就合并到主分支，然后被删除。</p><p>​    它最主要的特点有两个。首先，项目存在两个长期分支，分别是：主分支master、开发分支develop。其次，项目存在三种短期分支，分别是：功能分支（feature branch）、补丁分支（hotfix branch）、预发分支（release branch），一旦完成开发，它们就会被合并进develop或master，然后被删除。</p><ul><li><code>Production</code>分支。也就是我们经常使用的Master分支，这个分支最近发布到生产环境的代码，最近发布的Release， 这个分支只能从其他分支合并，不能在这个分支直接修改。</li><li><code>Develop</code>分支。这个分支是我们是我们的主开发分支，包含所有要发布到下一个Release的代码，这个主要合并与其他分支，比如Feature分支。</li><li><code>Feature</code>分支。这个分支主要是用来开发一个新的功能，一旦开发完成，我们合并回Develop分支进入下一个Release。</li><li><code>Release</code>分支。当你需要一个发布一个新Release的时候，我们基于Develop分支创建一个Release分支，完成Release后，我们合并到Master和Develop分支。</li><li><code>Hotfix</code>分支。当我们在Production发现新的Bug时候，我们需要创建一个Hotfix, 完成Hotfix后，我们合并回Master和Develop分支，所以Hotfix的改动会进入下一个Release。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;要有分支树的概念，各个分支都是在同一棵树上的&lt;/p&gt;
&lt;h2 id=&quot;命令解析&quot;&gt;&lt;a href=&quot;#命令解析&quot; class=&quot;headerlink&quot; title=&quot;命令解析&quot;&gt;&lt;/a&gt;命令解析&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://ww
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>linux装软件</title>
    <link href="http://kodgv.xyz/2019/05/24/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/linux%E8%A3%85%E8%BD%AF%E4%BB%B6/"/>
    <id>http://kodgv.xyz/2019/05/24/项目管理/linux装软件/</id>
    <published>2019-05-24T13:29:03.000Z</published>
    <updated>2019-05-25T12:44:37.670Z</updated>
    
    <content type="html"><![CDATA[<p>Linux下的装软件历程</p><a id="more"></a><p>搜狗拼音：<a href="https://blog.csdn.net/lupengCSDN/article/details/80279177" target="_blank" rel="noopener">https://blog.csdn.net/lupengCSDN/article/details/80279177</a></p><p>java安装：<a href="http://www.manongjc.com/article/34273.html" target="_blank" rel="noopener">http://www.manongjc.com/article/34273.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux下的装软件历程&lt;/p&gt;
    
    </summary>
    
      <category term="项目管理" scheme="http://kodgv.xyz/categories/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="linux" scheme="http://kodgv.xyz/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux习题</title>
    <link href="http://kodgv.xyz/2019/05/21/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/Linux%E4%B9%A0%E9%A2%98/"/>
    <id>http://kodgv.xyz/2019/05/21/项目管理/Linux习题/</id>
    <published>2019-05-21T12:43:44.000Z</published>
    <updated>2019-05-25T12:44:57.559Z</updated>
    
    <content type="html"><![CDATA[<p>Linux收集习题</p><a id="more"></a><h2 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h2><p>1.请问如果我以文字模式登陆Linux主机时，我有几个终端机接口可以使用？如何切换各个不同的终端机接口？</p><p>​    共有六个， tty1 ~ tty6 ，切换的方式为 Crtl + Alt + [F1]~[F6]（六个终端无区别，只是为了避免某个终端程序卡死）</p><p>2.在Linux系统中，/VBird与/vbird是否为相同的文件？</p><p>​    两者为不同的文件，因为 Linux 系统中，大小写字母代表意义不一样！</p><p>3.Linux 提供相当多的线上查询，称为 man page，请问，我如何知道系统上有多少关于 passwd 的说明？</p><p>​    man -f passwd（查看man file的数量）</p><p>4.我使用dmtsai这个帐号登陆系统了，请问我能不能使用reboot来重新开机？ </p><p>​    理论上reboot仅能让root执行。不过，如果dmtsai是在主机前面以图形接口登陆时，则dmtsai还是可以通过图形接口功能来关机。</p><h2 id="第五章"><a href="#第五章" class="headerlink" title="　第五章"></a>　第五章</h2><p>1.请问testgroup这个群组的成员与其他人（others）是否可以进入本目录？</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drwxr-xr--  <span class="number"> 1 </span>test1    testgroup   <span class="number"> 5238 </span>Jun<span class="number"> 19 </span>10:25 groups/</span><br></pre></td></tr></table></figure><ul><li><p>文件拥有者test1[rwx]可以在本目录中进行任何工作；</p></li><li><p>而testgroup这个群组[r-x]的帐号，例如test2, test3亦可以进入本目录进行工作，但是不能在本目录下进行写入的动作；</p></li><li><p>至于other的权限中[r—]虽然有r ，但是由于没有x的权限，因此others的使用者，并不能进入此目录！</p></li></ul><p>打开该目录就是执行ｘ</p><p>2.当一个一般文件权限为 -rwxrwxrwx 则表示这个文件的意义为？</p><p>任何人皆可读取、修改或编辑、可以执行，但不一定能删除。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux收集习题&lt;/p&gt;
    
    </summary>
    
      <category term="项目管理" scheme="http://kodgv.xyz/categories/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="linux" scheme="http://kodgv.xyz/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>知识图谱综述</title>
    <link href="http://kodgv.xyz/2019/05/17/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%BB%BC%E8%BF%B0/"/>
    <id>http://kodgv.xyz/2019/05/17/知识图谱/知识图谱综述/</id>
    <published>2019-05-17T07:28:31.000Z</published>
    <updated>2019-05-25T12:46:03.990Z</updated>
    
    <content type="html"><![CDATA[<p>知识图谱学习整理的一些概念和小知识点</p><a id="more"></a><p>[TOC]</p><h2 id="知识图谱首要"><a href="#知识图谱首要" class="headerlink" title="知识图谱首要"></a>知识图谱首要</h2><ol><li>What are the <em>nodes</em>? In a knowledge graph, they will be related to semantic concepts such as persons, entities, events <em>etc.</em></li><li>What are the <em>edges</em>? They will be defined by <em>relationships</em> between nodes based on semantics.</li><li>Once you answered these two key questions, you can go to the next phase which is the <em>data acquisition strategy</em>.</li></ol><p>In general, there are two main ways of going about knowledge processing:</p><ol><li><strong>Simple heuristic approaches</strong> such as text processing based on regular expressions, simple parsing and NLP techniques. For images, this approach includes basic processing of metadata. The lack of depth in this approach can be compensated and mitigated by sheer numbers <em>i.e.</em> throwing lots and lots of data at it to cherry pick low hanging fruit.</li><li><strong>Deep learning</strong> techniques where you employ novel methods to experiment with your own kinds of knowledge. This will be great fun and you will be on the leading edge but you should also keep in mind that the edge is (b)leading too. Just do not let that deter you and keep moving through growing pains.</li></ol><p><a href="http://kns.cnki.net/KCMS/detail/11.5602.TP.20190517.1335.002.html?uid=WEEvREcwSlJHSldRa1FhdXNXaEd2Um5XT2VCQXJTY3JoN1JsS2FvQXpYYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&amp;v=MjEwNDNxRkNubFc3dkFKVjQ9TGpYZmZiRzRIOWpNcW81Q1pPc1BZdzlNem1SbjZqNTdUM2ZscVdNMENMTDdSN3FlYnVa" target="_blank" rel="noopener">民航突发事件领域本体关系提取方法的研究</a></p><ul><li><p>非结构化数据可以整理为半结构化数据：</p><p>将突发事件的文本信息整理为标题、时间、航班号、事<br>件描述的半结构化形式，将事件发生时间和航班号作为事件<br>的唯一标识；</p></li><li><p>关系标注：attribute-of/reason-of/result-of</p></li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://arxiv.org/pdf/1503.00759v1.pdf" target="_blank" rel="noopener">知识图谱开山基础</a></p><p><a href="https://kgtutorial.github.io/" target="_blank" rel="noopener">知识图谱教程</a></p><p><a href="http://blog.itpub.net/31562039/viewspace-2286939/" target="_blank" rel="noopener">事理图谱</a>以因果关系作为关系进行的推理</p><h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><ul><li>keypoint is how to define the relation</li><li>两侧实体的类型已经定义，关系好像已经没有定义的意义，问题好像不是知识图谱的问题，是实体的问题，现在是FILED-VALUE-REAL 我们尝试想用一个关系描述两个横线，甚至这两个横线的关系还不一定是一样的。</li></ul><ul><li><p>需要做的：</p><p>如何提取实体：</p></li></ul><p>  在民航领域，基于领域本体的突发事件应急管理已经取<br>  得了初步的成果[19-23]，民航突发事件领域本体的关系提取方<br>  法主要有基于 NNV 关联规则的方法[20]、基于改进的层次聚<br>  类 H_cluster 的方法[21]、基于 LDA 的方法[22]和基于 LSTM 的<br>  方法[23]。其中，基于 NNV 的方法将关联规则与自然语言处<br>  理方法相结合，完善了领域词典的构建方法、增添同义词表、<br>  丰富领域术语的过滤过程，利用关联规则法提取事务集，计<br>  算概念和非分类关系的支持度和置信度，解决了领域本体非<br>  分类关系获取中无法自动获取关系名称的问题，相较于模式<br>  匹配方法提取结果更好。但由于中文概念的多义性对非分类<br>  关系种类的影响等原因，该方法的准确率和召回率都很低；<br>  基于改进的层次聚类 H_cluster 的方法在概念获取的基础上，<br>  根据领域概念的上下文构建概念向量空间，计算概念相似度，<br>  解决了聚类结果的粒度过细问题，使其更加符合本体层次结<br>  构的需要，实现了概念间分类关系的提取，但该方法的自动<br>  化程度有限，且准确率和召回率均提升较小；基于 LDA 的<br>  方法以航空安全事件文本信息作为数据源，采用 NLPIR 自适<br>  应分词与过滤方法获取候选术语集，设计了领域本体的 LDA<br>  主题模型，通过吉布斯采样进行 LDA 模型训练与主题推断，<br>  实现了领域本体核心概念与关系的提取，可以有效解决大规<br>  模领域本体的自动更新问题。由于领域本体所有概念及其语<br>  义关系的复杂性，基于 LDA 概率分布的规则构建与本体实<br>  例自动获取的方法应进一步深入研究；基于 LSTM 的方法将<br>  深度学习模型 LSTM 应用于领域本体关系提取，首先将文本<br>  信息向量化，提取文本局部特征，然后将文本局部特征导入<br>  LSTM 模型中，获取文本整体特征，再将局部特征和整体特<br>  征进行特征融合，通过分类器进行分类。相较于传统方法，<br>  该方法能更加充分利用句子中的语义信息，更准确地表达深<br>  层语义，因此关系提取的 F 值有了较大提升，但还可以进一<br>  步优化。此外，当前互联网上包含越来越多的民航突发事件<br>  信息，涉及不同信息源(微博、微信、航空安全自愿报告系统<br>  等)对事件的不同评论，结构和内容越来越复杂，且民航突发<br>  事件领域本体的，因此迫切需要更加有效的关系抽取方法来<br>  支撑领域本体的自动构建</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;知识图谱学习整理的一些概念和小知识点&lt;/p&gt;
    
    </summary>
    
      <category term="知识图谱" scheme="http://kodgv.xyz/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
    
      <category term="知识图谱" scheme="http://kodgv.xyz/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>Linux</title>
    <link href="http://kodgv.xyz/2019/05/16/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/Linux/"/>
    <id>http://kodgv.xyz/2019/05/16/项目管理/Linux/</id>
    <published>2019-05-16T12:54:54.000Z</published>
    <updated>2019-05-25T12:43:08.058Z</updated>
    
    <content type="html"><![CDATA[<p>Linux学习</p><a id="more"></a><p>[TOC]<br><a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/49.html" target="_blank" rel="noopener">https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/49.html</a></p><h2 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h2><h3 id="基础命令"><a href="#基础命令" class="headerlink" title="基础命令"></a>基础命令</h3><ul><li>显示日期与时间的指令： date</li><li>显示日历的指令： cal [month] [year]</li><li>简单好用的计算器： bc</li></ul><h3 id="基础快捷键"><a href="#基础快捷键" class="headerlink" title="基础快捷键"></a>基础快捷键</h3><ul><li>tab补全： 命令补全；文件补全；选项补全(—)</li><li>ctrl+c:强制结束</li><li>ctrl+d:离开命令行</li><li>[shift]+{[PageUP]|[Page Down]}按键：上下翻页，仅限在命令行内</li></ul><h3 id="查询命令帮助"><a href="#查询命令帮助" class="headerlink" title="查询命令帮助"></a>查询命令帮助</h3><blockquote><p>man 命令<br>man page</p></blockquote><div class="table-container"><table><thead><tr><th>代号</th><th>内容说明</th></tr></thead><tbody><tr><td>NAME</td><td>简短的指令、数据名称说明</td></tr><tr><td>SYNOPSIS</td><td>简短的指令下达语法（syntax）简介</td></tr><tr><td>DESCRIPTION</td><td>较为完整的说明，这部分最好仔细看看！</td></tr><tr><td>OPTIONS</td><td>针对 SYNOPSIS 部分中，有列举的所有可用的选项说明</td></tr><tr><td>COMMANDS</td><td>当这个程序（软件）在执行的时候，可以在此程序（软件）中下达的指令</td></tr><tr><td>FILES</td><td>这个程序或数据所使用或参考或链接到的某些文件</td></tr><tr><td>SEE ALSO</td><td>可以参考的，跟这个指令或数据有相关的其他说明！</td></tr><tr><td>EXAMPLE</td><td>一些可以参考的范例</td></tr></tbody></table></div><blockquote><p>info 命令</p></blockquote><p>​    可读性会强很多，只是可能有些命令查不到，光标移到节点</p><ul><li>回车可进入</li><li>U返回上一节点</li><li>N进入下一节点</li><li>P进入上 一节点</li></ul><h3 id="正确的关机"><a href="#正确的关机" class="headerlink" title="正确的关机"></a>正确的关机</h3><p>关机前需要检查：</p><ul><li><p>who  看谁在线上</p></li><li><p>netstat -a 网络连线状态</p></li><li><p>ps -aux 查看背景执行程序</p></li><li><p>sync 将内存数据写入硬盘（很重要！）</p><p>因此在Linux系统中，为了加快数据的读取速度，所以在默认的情况中， 某些已经载入内存中的数据将不会直接被写回硬盘，而是先暂存在内存当中，如此一来， 如果一个数据被你重复的改写，那么由于他尚未被写入硬盘中，因此可以直接由内存当中读取出来， 在速度上一定是快上相当多的！</p></li></ul><p>需要以root执行，一般可以推迟一会关机，以免出错</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="symbol">@study</span> ~]<span class="meta"># shutdown -h now</span></span><br><span class="line">立刻关机，其中 now 相当于时间为 <span class="number">0</span> 的状态</span><br><span class="line">[root<span class="symbol">@study</span> ~]<span class="meta"># shutdown -h 20:25</span></span><br><span class="line">系统在今天的 <span class="number">20</span>:<span class="number">25</span> 分会关机，若在<span class="number">21</span>:<span class="number">25</span>才下达此指令，则隔天才关机</span><br><span class="line">[root<span class="symbol">@study</span> ~]<span class="meta"># shutdown -h +10</span></span><br><span class="line">系统再过十分钟后自动关机</span><br><span class="line">[root<span class="symbol">@study</span> ~]<span class="meta"># shutdown -r now</span></span><br><span class="line">系统立刻重新开机</span><br><span class="line">[root<span class="symbol">@study</span> ~]<span class="meta"># shutdown -r +30 <span class="string">'The system will reboot'</span> </span></span><br><span class="line">再过三十分钟系统会重新开机，并显示后面的讯息给所有在线上的使用者</span><br><span class="line">[root<span class="symbol">@study</span> ~]<span class="meta"># shutdown -c 取消关机</span></span><br></pre></td></tr></table></figure><h2 id="第五章"><a href="#第五章" class="headerlink" title="第五章"></a>第五章</h2><p>文件可存取身份三个类别：owner/group/others</p><p>可存取的权限：read/write/execute</p><h3 id="使用者与群组"><a href="#使用者与群组" class="headerlink" title="使用者与群组"></a>使用者与群组</h3><p>文件拥有者owner</p><p>Linux是个多用户多任务的系统，因此可能常常会有多人同时使用这部主机来进行工作的情况发生， 为了考虑每个人的隐私权以及每个人喜好的工作环境，因此，这个“文件拥有者”的角色就显的相当的重要。其他人无法查看你的文件</p><p>群组概念group</p><p>团队开发的时候，不同的团队在同一台主机上开发。</p><ul><li>使用者的意义：由于王家三人各自拥有自己的房间，所以， 王二毛虽然可以进入王三毛的房间，但是二毛不能翻三毛的抽屉喔！那样会被三毛K的！ 因为抽屉里面可能有三毛自己私人的东西，例如情书啦，日记啦等等的，这是“私人的空间”，所以当然不能让二毛拿啰！</li><li>群组的概念：由于共同拥有客厅，所以王家三兄弟可以在客厅打开电视机啦、 翻阅报纸啦、坐在沙发上面发呆啦等等的！ 反正，只要是在客厅的玩意儿，三兄弟都可以使用喔！ 因为大家都是一家人嘛！但是隔壁老王就不能进来。</li></ul><p>其他人的概念other</p><p>非同一群组的人，都叫其他人。因为它们相对别人来说都只是‘其他人’</p><p>天神root</p><p>可以有绝对权力，无视一切规则</p><ul><li>系统上的帐号与一般身份使用者储存在/etc/passwd</li><li>个人的密码/etc/shadow</li><li>群组名称/etc/group</li></ul><p>这三个配置文件的具体内容第十三章继续讲，很复杂的</p><h3 id="Linux-文件权限"><a href="#Linux-文件权限" class="headerlink" title="Linux 文件权限"></a>Linux 文件权限</h3><p>ls -al</p><p>这行信息要好好记住</p><p><img src="/2019/05/16/项目管理/Linux/1.gif" alt></p><p><img src="/2019/05/16/项目管理/Linux/E:/bolg\source\_posts\项目管理\Linux\2.gif" alt></p><ul><li><p>第一个字符代表这个文件是“目录、文件或链接文件等等”：</p><ul><li>当为[ d ]则是目录，例如<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html#table2.1.1" target="_blank" rel="noopener">上表</a>文件名为“.config”的那一行；</li><li>当为[ - ]则是文件，例如<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html#table2.1.1" target="_blank" rel="noopener">上表</a>文件名为“initial-setup-ks.cfg”那一行；</li><li>若是[ l ]则表示为链接文件（link file）；</li><li>若是[ b ]则表示为设备文件里面的可供储存的周边设备（可随机存取设备）；</li><li>若是[ c ]则表示为设备文件里面的序列埠设备，例如键盘、鼠标（一次性读取设备）。</li></ul></li><li><p>接下来的字符中，以三个为一组，且均为“rwx” 的三个参数的组合。</p><ul><li>r （read）：可读取此一文件的实际内容，如读取文本文件的文字内容等；</li><li>w （write）：可以编辑、新增或者是修改该文件的内容（<strong>但不含删除该文件</strong>）；</li><li>x （execute）：该文件具有可以被系统执行的权限。<br>要注意的是，这三个权限的位置不会改变，如果没有权限，就会出现减号[ - ]而已</li><li>第一组为“文件拥有者可具备的权限”，以“initial-setup-ks.cfg”那个文件为例， 该文件的拥有者可以读写，但不可执行；</li><li>第二组为“加入此群组之帐号的权限”；</li><li>第三组为“非本人且没有加入本群组之其他帐号的权限”。</li></ul></li></ul><h3 id="改变文件属性与权限"><a href="#改变文件属性与权限" class="headerlink" title="　改变文件属性与权限"></a>　改变文件属性与权限</h3><p>查看文件权限：ls -l 文件名</p><p>查看文件夹权限：ls -ld 文件夹名</p><ul><li><p>chgrp ：改变文件所属群组        chgrp [-R] 群族名 dirname/filename</p></li><li><p>chown ：改变文件拥有者            chown [-R] 帐号名称 文件或目录</p></li><li><p>chmod ：改变文件的权限, SUID, SGID, SBIT等等的特性   </p><p>chmod 644 .bashrc</p></li></ul><p>chgrp/chown应用场景，由于复制行为（cp）会复制执行者的属性与权限，所以当你想把文件给别人的时候，就会出现问题。</p><blockquote><p>r:4</p><p>w:2</p><p>x:1</p></blockquote><p>如果想rw-就是6，rwx就是7</p><h3 id="权限对目录的重要性"><a href="#权限对目录的重要性" class="headerlink" title="权限对目录的重要性"></a>权限对目录的重要性</h3><ul><li><p>r （read contents in directory）：</p><p>表示具有读取目录结构清单的权限，所以当你具有读取（r）一个目录的权限时，表示你可以查询该目录下的文件名数据。 所以你就可以利用 ls 这个指令将该目录的内容列表显示出来！</p></li><li><p>w （modify contents of directory）：</p><p>这个可写入的权限对目录来说，是很了不起的！ 因为他表示你具有异动该目录结构清单的权限，也就是下面这些权限：</p><ul><li>创建新的文件与目录；</li><li>删除已经存在的文件与目录（不论该文件的权限为何！）</li><li>将已存在的文件或目录进行更名；</li><li>搬移该目录内的文件、目录位置。 总之，目录的w权限就与该目录下面的文件名异动有关就对了啦！</li></ul></li><li><p>x （access directory）：</p><p>咦！目录的执行权限有啥用途啊？目录只是记录文件名而已，总不能拿来执行吧？没错！目录不可以被执行，目录的x代表的是使用者能否进入该目录，它相当于抽屉的钥匙，所以要想拿到抽屉里面的文件必须要先有钥匙。</p><p>| 元件 | 内容         | 叠代物件   | r            | w            | x                       |<br>| —— | —————— | ————— | —————— | —————— | ———————————- |<br>| 文件 | 详细数据data | 文件数据夹 | 读到文件内容 | 修改文件内容 | 执行文件内容            |<br>| 目录 | 文件名       | 可分类抽屉 | 读到文件名   | 修改文件名   | 进入该目录的权限（key） |</p></li></ul><h3 id="Linux目录配置的依据"><a href="#Linux目录配置的依据" class="headerlink" title="Linux目录配置的依据"></a>Linux目录配置的依据</h3><p>（需要筛选一遍重要的目录出来）</p><ul><li>/ （root, 根目录）：与开机系统有关；</li><li>/usr （unix software resource）：与软件安装/执行有关；</li><li>/var （variable）：与系统运行过程有关。</li></ul><p>根目录 （/） 的意义与内容</p><p>根目录是整个系统最重要的一个目录，它是系统的核心目录</p><div class="table-container"><table><thead><tr><th>目录</th><th>应放置文件内容</th></tr></thead><tbody><tr><td>第一部份：FHS 要求必须要存在的目录</td><td></td></tr><tr><td>/bin</td><td>系统有很多放置可执行文件的目录，但/bin比较特殊。因为/bin放置的是在单人维护模式下还能够被操作的指令。 在/bin下面的指令可以被root与一般帐号所使用，主要有：cat, chmod, chown, date, mv, mkdir, cp, bash等等常用的指令。</td></tr><tr><td>/boot</td><td>这个目录主要在放置开机会使用到的文件，包括Linux核心文件以及开机菜单与开机所需配置文件等等。 Linux kernel常用的文件名为：vmlinuz，如果使用的是grub2这个开机管理程序， 则还会存在/boot/grub2/这个目录喔！</td></tr><tr><td>/dev</td><td>在Linux系统上，任何设备与周边设备都是以文件的型态存在于这个目录当中的。 你只要通过存取这个目录下面的某个文件，就等于存取某个设备啰～ 比要重要的文件有/dev/null, /dev/zero, /dev/tty, /dev/loop<em>, /dev/sd</em>等等</td></tr><tr><td>/etc</td><td>系统主要的配置文件几乎都放置在这个目录内，例如人员的帐号密码档、 各种服务的启始档等等。一般来说，这个目录下的各文件属性是可以让一般使用者查阅的， 但是只有root有权力修改。FHS建议不要放置可可执行文件（binary）在这个目录中喔。比较重要的文件有： /etc/modprobe.d/, /etc/passwd, /etc/fstab, /etc/issue 等等。另外 FHS 还规范几个重要的目录最好要存在 /etc/ 目录下喔：/etc/opt（必要）：这个目录在放置第三方协力软件 /opt 的相关配置文件 /etc/X11/（建议）：与 X Window 有关的各种配置文件都在这里，尤其是 xorg.conf 这个 X Server 的配置文件。 /etc/sgml/（建议）：与 SGML 格式有关的各项配置文件 /etc/xml/（建议）：与 XML 格式有关的各项配置文件</td></tr><tr><td>/lib</td><td>系统的函数库非常的多，而/lib放置的则是在开机时会用到的函数库， 以及在/bin或/sbin下面的指令会调用的函数库而已。 什么是函数库呢？你可以将他想成是“外挂”，某些指令必须要有这些“外挂”才能够顺利完成程序的执行之意。 另外 FSH 还要求下面的目录必须要存在：/lib/modules/：这个目录主要放置可抽换式的核心相关模块（驱动程序）喔！</td></tr><tr><td>/media</td><td>media是“媒体”的英文，顾名思义，这个/media下面放置的就是可移除的设备啦！ 包括软盘、光盘、DVD等等设备都暂时挂载于此。常见的文件名有：/media/floppy, /media/cdrom等等。</td></tr><tr><td>/mnt</td><td>如果你想要暂时挂载某些额外的设备，一般建议你可以放置到这个目录中。 在古早时候，这个目录的用途与/media相同啦！只是有了/media之后，这个目录就用来暂时挂载用了。</td></tr><tr><td>/opt</td><td>这个是给第三方协力软件放置的目录。什么是第三方协力软件啊？ 举例来说，KDE这个桌面管理系统是一个独立的计划，不过他可以安装到Linux系统中，因此KDE的软件就建议放置到此目录下了。 另外，如果你想要自行安装额外的软件（非原本的distribution提供的），那么也能够将你的软件安装到这里来。 不过，以前的Linux系统中，我们还是习惯放置在/usr/local目录下呢！</td></tr><tr><td>/run</td><td>早期的 FHS 规定系统开机后所产生的各项信息应该要放置到 /var/run 目录下，新版的 FHS 则规范到 /run 下面。 由于 /run 可以使用内存来仿真，因此性能上会好很多！</td></tr><tr><td>/sbin</td><td>Linux有非常多指令是用来设置系统环境的，这些指令只有root才能够利用来“设置”系统，其他使用者最多只能用来“查询”而已。 放在/sbin下面的为开机过程中所需要的，里面包括了开机、修复、还原系统所需要的指令。 至于某些服务器软件程序，一般则放置到/usr/sbin/当中。至于本机自行安装的软件所产生的系统可执行文件（system binary）， 则放置到/usr/local/sbin/当中了。常见的指令包括：fdisk, fsck, ifconfig, mkfs等等。</td></tr><tr><td>/srv</td><td>srv可以视为“service”的缩写，是一些网络服务启动之后，这些服务所需要取用的数据目录。 常见的服务例如WWW, FTP等等。举例来说，WWW服务器需要的网页数据就可以放置在/srv/www/里面。 不过，系统的服务数据如果尚未要提供给网际网络任何人浏览的话，默认还是建议放置到 /var/lib 下面即可。</td></tr><tr><td>/tmp</td><td>这是让一般使用者或者是正在执行的程序暂时放置文件的地方。 这个目录是任何人都能够存取的，所以你需要定期的清理一下。当然，重要数据不可放置在此目录啊！ 因为FHS甚至建议在开机时，应该要将/tmp下的数据都删除唷！</td></tr><tr><td>/usr</td><td>第二层 FHS 设置，后续介绍</td></tr><tr><td>/var</td><td>第二曾 FHS 设置，主要为放置变动性的数据，后续介绍</td></tr><tr><td>第二部份：FHS 建议可以存在的目录</td><td></td></tr><tr><td>/home</td><td>这是系统默认的使用者主文件夹（home directory）。在你新增一个一般使用者帐号时， 默认的使用者主文件夹都会规范到这里来。比较重要的是，主文件夹有两种代号喔：~：代表目前这个使用者的主文件夹 ~dmtsai ：则代表 dmtsai 的主文件夹！</td></tr><tr><td>/lib<qual></qual></td><td>用来存放与 /lib 不同的格式的二进制函数库，例如支持 64 位的 /lib64 函数库等</td></tr><tr><td>/root</td><td>系统管理员（root）的主文件夹。之所以放在这里，是因为如果进入单人维护模式而仅挂载根目录时， 该目录就能够拥有root的主文件夹，所以我们会希望root的主文件夹与根目录放置在同一个分区中。</td></tr></tbody></table></div><p>/usr 的意义与内容：</p><p>usr叫做Unix Software Resource Unix操作系统软件资源”所放置的目录。所有系统默认的软件（distribution发布者提供的软件）都会放置到/usr下面，因此这个目录有点类似Windows 系统的“C:\Windows\ （当中的一部份） + C:\Program files\”这两个目录的综合体</p><div class="table-container"><table><thead><tr><th>目录</th><th>应放置文件内容</th></tr></thead><tbody><tr><td>第一部份：FHS 要求必须要存在的目录</td><td></td></tr><tr><td>/usr/bin/</td><td>所有一般用户能够使用的指令都放在这里！目前新的 CentOS 7 已经将全部的使用者指令放置于此，而使用链接文件的方式将 /bin 链接至此！ 也就是说， /usr/bin 与 /bin 是一模一样了！另外，FHS 要求在此目录下不应该有子目录！</td></tr><tr><td>/usr/lib/</td><td>基本上，与 /lib 功能相同，所以 /lib 就是链接到此目录中的！</td></tr><tr><td>/usr/local/</td><td>系统管理员在本机自行安装自己下载的软件（非distribution默认提供者），建议安装到此目录， 这样会比较便于管理。举例来说，你的distribution提供的软件较旧，你想安装较新的软件但又不想移除旧版， 此时你可以将新版软件安装于/usr/local/目录下，可与原先的旧版软件有分别啦！ 你可以自行到/usr/local去看看，该目录下也是具有bin, etc, include, lib…的次目录喔！</td></tr><tr><td>/usr/sbin/</td><td>非系统正常运行所需要的系统指令。最常见的就是某些网络服务器软件的服务指令（daemon）啰！不过基本功能与 /sbin 也差不多， 因此目前 /sbin 就是链接到此目录中的。</td></tr><tr><td>/usr/share/</td><td>主要放置只读架构的数据文件，当然也包括共享文件。在这个目录下放置的数据几乎是不分硬件架构均可读取的数据， 因为几乎都是文字文件嘛！在此目录下常见的还有这些次目录：/usr/share/man：线上说明文档 /usr/share/doc：软件杂项的文件说明 /usr/share/zoneinfo：与时区有关的时区文件</td></tr><tr><td>第二部份：FHS 建议可以存在的目录</td><td></td></tr><tr><td>/usr/games/</td><td>与游戏比较相关的数据放置处</td></tr><tr><td>/usr/include/</td><td>c/c++等程序语言的文件开始（header）与包含档（include）放置处，当我们以tarball方式 （*.tar.gz 的方式安装软件）安装某些数据时，会使用到里头的许多包含档喔！</td></tr><tr><td>/usr/libexec/</td><td>某些不被一般使用者惯用的可执行文件或脚本（script）等等，都会放置在此目录中。例如大部分的 X 窗口下面的操作指令， 很多都是放在此目录下的。</td></tr><tr><td>/usr/lib<qual>/</qual></td><td>与 /lib<qual>/功能相同，因此目前 /lib<qual> 就是链接到此目录中</qual></qual></td></tr><tr><td>/usr/src/</td><td>一般源代码建议放置到这里，src有source的意思。至于核心源代码则建议放置到/usr/src/linux/目录下。</td></tr></tbody></table></div><p>/var 的意义与内容：</p><p>如果/usr是安装时会占用较大硬盘容量的目录，那么/var就是在系统运行后才会渐渐占用硬盘容量的目录。 因为/var目录主要针对常态性变动的文件，包括高速缓存（cache）、登录文件（log file）以及某些软件运行所产生的文件， 包括程序文件（lock file, run file），或者例如MySQL数据库的文件等等。</p><div class="table-container"><table><thead><tr><th>目录</th><th>应放置文件内容</th></tr></thead><tbody><tr><td>第一部份：FHS 要求必须要存在的目录</td><td></td></tr><tr><td>/var/cache/</td><td>应用程序本身运行过程中会产生的一些暂存盘；</td></tr><tr><td>/var/lib/</td><td>程序本身执行的过程中，需要使用到的数据文件放置的目录。在此目录下各自的软件应该要有各自的目录。 举例来说，MySQL的数据库放置到/var/lib/mysql/而rpm的数据库则放到/var/lib/rpm去！</td></tr><tr><td>/var/lock/</td><td>某些设备或者是文件资源一次只能被一个应用程序所使用，如果同时有两个程序使用该设备时， 就可能产生一些错误的状况，因此就得要将该设备上锁（lock），以确保该设备只会给单一软件所使用。 举例来说，烧录机正在烧录一块光盘，你想一下，会不会有两个人同时在使用一个烧录机烧片？ 如果两个人同时烧录，那片子写入的是谁的数据？所以当第一个人在烧录时该烧录机就会被上锁， 第二个人就得要该设备被解除锁定（就是前一个人用完了）才能够继续使用啰。目前此目录也已经挪到 /run/lock 中！</td></tr><tr><td>/var/log/</td><td>重要到不行！这是登录文件放置的目录！里面比较重要的文件如/var/log/messages, /var/log/wtmp（记录登陆者的信息）等。</td></tr><tr><td>/var/mail/</td><td>放置个人电子邮件信箱的目录，不过这个目录也被放置到/var/spool/mail/目录中！ 通常这两个目录是互为链接文件啦！</td></tr><tr><td>/var/run/</td><td>某些程序或者是服务启动后，会将他们的PID放置在这个目录下喔！至于PID的意义我们会在后续章节提到的。 与 /run 相同，这个目录链接到 /run 去了！</td></tr><tr><td>/var/spool/</td><td>这个目录通常放置一些伫列数据，所谓的“伫列”就是排队等待其他程序使用的数据啦！ 这些数据被使用后通常都会被删除。举例来说，系统收到新信会放置到/var/spool/mail/中， 但使用者收下该信件后该封信原则上就会被删除。信件如果暂时寄不出去会被放到/var/spool/mqueue/中， 等到被送出后就被删除。如果是工作调度数据（crontab），就会被放置到/var/spool/cron/目录中！</td></tr></tbody></table></div><h2 id="第六章"><a href="#第六章" class="headerlink" title="第六章"></a>第六章</h2><h3 id="执行文件路径的变量：-PATH"><a href="#执行文件路径的变量：-PATH" class="headerlink" title="执行文件路径的变量： $PATH"></a>执行文件路径的变量： $PATH</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$PATH</span></span><br></pre></td></tr></table></figure><p>搜索过程</p><p>系统会依照PATH的设置去每个PATH定义的目录下搜寻文件名为ls的可可执行文件， 如果在PATH定义的目录中含有多个文件名为ls的可可执行文件，那么先搜寻到的同名指令先被执行！</p><p>新增PATH（仅仅在本次登录shell有效,也就是说重启后就会还原）</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">PATH</span>=<span class="string">"$&#123;PATH&#125;:/root"</span></span><br></pre></td></tr></table></figure><h2 id="Linux-快捷键"><a href="#Linux-快捷键" class="headerlink" title="Linux 快捷键"></a>Linux 快捷键</h2><p>编辑器快捷键</p><div class="table-container"><table><thead><tr><th>按键</th><th>进行工作</th></tr></thead><tbody><tr><td>空白键</td><td>向下翻一页</td></tr><tr><td>[Page Down]</td><td>向下翻一页</td></tr><tr><td>[Page Up]</td><td>向上翻一页</td></tr><tr><td>[Home]</td><td>去到第一页</td></tr><tr><td>[End]</td><td>去到最后一页</td></tr><tr><td>/string</td><td>向“下”搜寻 string 这个字串，如果要搜寻 vbird 的话，就输入 /vbird</td></tr><tr><td>?string</td><td>向“上”搜寻 string 这个字串</td></tr><tr><td>n, N</td><td>利用 / 或 ? 来搜寻字串时，可以用 n 来继续下一个搜寻 （不论是 / 或 ?） ，可以利用 N 来进行“反向”搜寻。举例来说，我以 /vbird 搜寻 vbird 字串， 那么可以 n 继续往下查询，用 N 往上查询。若以 ?vbird 向上查询 vbird 字串， 那我可以用 n 继续“向上”查询，用 N 反向查询。</td></tr><tr><td>q</td><td>结束这次的 man page</td></tr></tbody></table></div><h2 id="需要背的快捷键"><a href="#需要背的快捷键" class="headerlink" title="需要背的快捷键"></a>需要背的快捷键</h2><ul><li><p>tab补全： 命令补全；文件补全；选项补全(—)</p></li><li><p>ctrl+c:强制结束</p></li><li><p>ctrl+d:离开命令行</p></li><li><p>[shift]+{[PageUP]|[Page Down]}按键：上下翻页，仅限在命令行内</p></li></ul><p>编辑器快捷键</p><div class="table-container"><table><thead><tr><th>按键</th><th>进行工作</th></tr></thead><tbody><tr><td>空白键</td><td>向下翻一页</td></tr><tr><td>[Page Down]</td><td>向下翻一页</td></tr><tr><td>[Page Up]</td><td>向上翻一页</td></tr><tr><td>[Home]</td><td>去到第一页</td></tr><tr><td>[End]</td><td>去到最后一页</td></tr><tr><td>/string</td><td>向“下”搜寻 string 这个字串，如果要搜寻 vbird 的话，就输入 /vbird</td></tr><tr><td>?string</td><td>向“上”搜寻 string 这个字串</td></tr><tr><td>n, N</td><td>利用 / 或 ? 来搜寻字串时，可以用 n 来继续下一个搜寻 （不论是 / 或 ?） ，可以利用 N 来进行“反向”搜寻。举例来说，我以 /vbird 搜寻 vbird 字串， 那么可以 n 继续往下查询，用 N 往上查询。若以 ?vbird 向上查询 vbird 字串， 那我可以用 n 继续“向上”查询，用 N 反向查询。</td></tr><tr><td>q</td><td>结束这次的 man page</td></tr></tbody></table></div><p>cd 选项<br><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">.  </span>代表此层目录</span><br><span class="line"><span class="bullet">..        </span>代表上一层目录</span><br><span class="line"><span class="bullet">-         </span>代表前一个工作目录</span><br><span class="line">~         代表“目前使用者身份”所在的主文件夹</span><br><span class="line">~account  代表 account 这个使用者的主文件夹（account是个帐号名称）</span><br></pre></td></tr></table></figure></p><p>安装包</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dpkg -<span class="selector-tag">i</span> deb包</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux学习&lt;/p&gt;
    
    </summary>
    
      <category term="项目管理" scheme="http://kodgv.xyz/categories/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="linux" scheme="http://kodgv.xyz/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>代码整洁之道</title>
    <link href="http://kodgv.xyz/2019/05/16/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/%E4%BB%A3%E7%A0%81%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93/"/>
    <id>http://kodgv.xyz/2019/05/16/项目管理/代码整洁之道/</id>
    <published>2019-05-16T08:37:43.000Z</published>
    <updated>2019-05-27T11:47:03.996Z</updated>
    
    <content type="html"><![CDATA[<p>代码整洁规范<br>减少重复代码<br>提高表达力<br>提早构建简单抽象<br><a id="more"></a></p><p>[TOC]</p><h2 id="有意义的命名"><a href="#有意义的命名" class="headerlink" title="有意义的命名"></a>有意义的命名</h2><p><strong>名副其实</strong></p><ul><li>如果你需要注释来解释一个变量，那就说明它不是名副其实</li><li>一旦发现有更好的命名，就要换掉旧的，要记住换花的时间是值得的</li></ul><p><strong>避免误导</strong></p><ul><li>尽量少使用类型名，除非它真的是这个类型</li><li>不要使用过于相似的名字</li></ul><p><strong>命名规范</strong></p><ul><li>类名和对象是名词，方法是动词，注意get/set/is的前缀</li><li>即每个抽象概念选一个词，DeviceManager和Protocal-Controller是一样的</li><li>要保持前后术语规范。</li><li>用常量代替原始数字（魔术数）</li><li>不要在代码中重复写+1的边界条件，而是用变量进行封装</li><li>函数的长短与其作用范围息息相关，所以for循环可以用i,j因为它们作用域很小</li></ul><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><p>函数要做到自己理解，而不是通过测试用例，这需要改进和重构简洁。</p><p><strong>尽量短小</strong></p><ul><li>if/for/while中应该只有一行</li><li>函数只做一件事，抽象层上应该是同一级别！</li><li>函数的顺序，是自顶向下的规则，抽象到具体的顺序</li><li>函数要做什么事和函数要回答什么事区分。函数修改状态和返回状态区分</li></ul><p><strong>参数</strong></p><ul><li>参数最多不应该超过3个，如果参数多于3个的时候，就可以思考参数是否能构成一个类</li><li>参数不要传布尔值，应该将true/false分成两个函数，然后在布尔值的上一个抽象层直接做判断，而不是将其作为布尔值传递。（整数值，枚举元素等函数选择行为同理）</li></ul><p><strong>处理异常</strong></p><ul><li>try/catch的每个功能都应该拆出成一个函数</li><li>如果你用枚举类型来表示异常，那将意味着你会很不情愿修改它，使用异常代替错误码，新异常就可以从异常类中派生出来.</li></ul><p><strong>重复</strong></p><ul><li>永远无法容忍重复代码，这意味着抽象的遗漏3</li><li>如果你发现死代码（永不执行的代码块），果断删除</li><li>多个switch/ifelse，考虑多态</li></ul><p><strong>判断</strong></p><ul><li>if的条件如果过多，就要封一个函数，因为没有上下文是看不懂if的</li><li>避免否定条件，尽量保持一致，不要带！，可以直接把！函数封成函数</li></ul><p><strong>静态</strong></p><ul><li>如果一个函数不依赖其类的属性，都是依赖于参数，这时候该函数就应该声明为静态</li><li>静态导入（就是单纯导入），而不是用继承的方式，这样别人不知道函数的来源</li></ul><h2 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h2><p><strong>尽量少写注释</strong></p><ul><li>写注释意味着你的代码不够优雅，别人无法理解</li><li>能用函数和变量拆解的时候就拆解，而不是用注释去描述</li><li>你无法始终坚持维护你的注释，会出现各种问题</li><li>不要注释代码，我们有优秀的代码管理器，无用的代码就删掉就可以了，它会保留的。</li></ul><p><strong>好的注释</strong></p><ul><li>法律与版本信息</li><li>某个决定背后的意图解释，便于后续别人的修改</li><li>TODO</li></ul><p><strong>如何写注释</strong></p><ul><li>一旦写注释就要花时间写好注释，注释的角度尽量从别人的角度出来来想会有什么疑惑，而不是解决自己的疑惑</li><li>不要尝试描述代码本身</li></ul><h2 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h2><p>格式之所以重要是因为，你的功能可能会变，但是你代码的风格会一直保留</p><p><strong>正确格式</strong></p><ul><li>封包声明，导入声明，每个函数，每个有区别的地方都应该用一行空白行分割。</li><li>垂直关系：关系紧密的就应该靠在一起，函数调用了也应该放在一起</li><li>水平关系：赋值符前后空格，参数前后空格</li></ul><h2 id="数据，对象的反对称性"><a href="#数据，对象的反对称性" class="headerlink" title="数据，对象的反对称性"></a>数据，对象的反对称性</h2><p><strong>抽象</strong></p><ul><li>隐藏实现并非在变量之间放上一层函数层，也不是单纯使用取值器和赋值器往外推，而是暴露抽象接口，以便于用户无需了解数据形式就可以操作数据。</li><li><p>过程式代码便于添加函数（switch实现）；对象类代码便于添加数据类型（多态实现）</p></li><li><p>DTO(data transfer object)，豆结构，即只有赋值器和取值器操作的私有变量</p></li></ul><h2 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h2><p><strong>try-catch</strong></p><ul><li>try-catch: 一开始可以用exception捕捉，捕捉到之后要缩小到对应范围。</li><li>try的内容是一个原子事务，这一点要注意想清楚。</li></ul><p><strong>打包类</strong></p><p>如果需要多次捕捉异常，则可以将其封装在一个类中成第三方ＡＰＩ</p><p><img src="/2019/05/16/项目管理/代码整洁之道/1.jpg" alt="1558526742989"></p><p><img src="/2019/05/16/项目管理/代码整洁之道/3" alt="1558526925136"></p><p><strong>NULL值处理</strong></p><ul><li><p>绝对不要返回Null值，否则别人检查null值会很麻烦，而且检查一多就会乱(可以内部直接抛出错误，也可以定义为空值就好)</p></li><li><p>禁止参数传入Null值，否则你总不可能每个变量一个个检查null（如果实在没有办法就只能用断言assert检查一下吧，用if有时会太乱混淆）</p></li></ul><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ul><li>测试代码和生产代码一样重要。如果你随便对待，将来将花费更多时间塞入新的测试和检查旧的测试</li><li>包括三个环节：BUILD-OPERATE-CHECK 构造测试数据-操作测试数据-检验操作</li><li>F(Fast)I(Independent)R(repeatable)S(self-validating)T(timely)</li><li>TDD的思想很重要，否则你无法保证你的任何修改是正确的</li></ul><h2 id="类"><a href="#类" class="headerlink" title="类"></a>类</h2><p><strong>短小</strong></p><ul><li>类的名称要描述其权责而不能是一些模糊的词（Process,manger,super），而且描述该类职责的时候不应该出现if/and/or/but这些词语</li><li>类的权责应该是单一的，尽量拆成多个短小类吧</li><li>长的函数拆成小的函数的时候，参数是否要传，若是那为什么不把它们变成一个类，参数是它们的成员变量就好了。注意变的时候保持单一权责</li><li>注意职责的分隔。如果当你新增一个功能可能会影响其他时，说明耦合过强</li></ul><h2 id="好的代码示范"><a href="#好的代码示范" class="headerlink" title="好的代码示范"></a>好的代码示范</h2><p>书的附录记录了好的代码示范</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;代码整洁规范&lt;br&gt;减少重复代码&lt;br&gt;提高表达力&lt;br&gt;提早构建简单抽象&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="项目管理" scheme="http://kodgv.xyz/categories/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="代码规范" scheme="http://kodgv.xyz/tags/%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/"/>
    
  </entry>
  
  <entry>
    <title>java核心技术基础知识</title>
    <link href="http://kodgv.xyz/2019/04/30/JAVA%E5%AD%A6%E4%B9%A0/java%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/"/>
    <id>http://kodgv.xyz/2019/04/30/JAVA学习/java核心技术/</id>
    <published>2019-04-30T11:30:59.000Z</published>
    <updated>2019-05-23T14:58:24.825Z</updated>
    
    <content type="html"><![CDATA[<p>java基础知识<br><a id="more"></a></p><p>[TOC]</p><h2 id="java程序设计环境"><a href="#java程序设计环境" class="headerlink" title="java程序设计环境"></a>java程序设计环境</h2><p><img src="/2019/04/30/JAVA学习/java核心技术/1" alt="1557976698685"></p><ul><li><p>JDK 是 Java Development Kit 的缩写开发人员必须要安装，Java 运行时环境（JRE), 它包含虚拟机 但不包含编译器。</p></li><li><p>Java SE 会大量出现， 相对于 Java EE ( Enterprise Edition) 和 Java ME ( Micro Edition), 它是 Java 的标准版。 </p></li></ul><h2 id="java基本程序设计结构"><a href="#java基本程序设计结构" class="headerlink" title="java基本程序设计结构"></a>java基本程序设计结构</h2><ul><li>类是构建所有 Java 应用程序和 applet 的构建块。Java 应用程序中的全部内容都必须放置在类中。 </li></ul><p><strong>数据类型</strong></p><p>整型</p><p><img src="/2019/04/30/JAVA学习/java核心技术/E:/bolg\source\_posts\JAVA学习\java核心技术\2" alt="1557977861052"></p><p>浮点型</p><p><img src="/2019/04/30/JAVA学习/java核心技术/E:/bolg\source\_posts\JAVA学习\java核心技术\3" alt="1557977876477"></p><p>有三种类型，正无穷大，负无穷大，NAN：</p><ul><li>Double_POSITIVE_INFINITY</li><li>Double.NEGATIVEJNFINITY </li><li>Double.NaN </li></ul><blockquote><p>NAN与任何数字包括NAN都不相等，不能用==Double.NaN,而是要用isNaN()</p></blockquote><p>字符型</p><p><strong>byte</strong></p><p>byte 字节，数据存储容量1byte，byte作为基本数据类型表示的也是一个存储范围上的概念，有别于int、long等专门存数字的类型，这种类型的大小就是1byte,而int是4byte。<br>存数字的话就是1byte=8位，2^8=256 即-128-127。字符的话包括字母和汉字，一个字母是1byte，一个汉字2byte。也就是可以用byte变量去存储一个英文字符，但是却存不下一个中文汉字，因为一个汉字占2byte。</p><p>总结，byte是java中的一个基本数据类型，这个数据类型的长度是1byte，此byte就是彼byte,即是基本数据类型也是存储空间的基本计量单位。</p><p><strong>char</strong></p><p>char是Java中的保留字，与别的语言不同的是，char在Java中是16位的，因为Java用的是Unicode。不过8位的ASCII码包含在Unicode中，是从0~127的。</p><p>Java中使用Unicode的原因是，Java的Applet允许全世界范围内运行，那它就需要一种可以表述人类所有语言的字符编码。Unicode。</p><p>char本质上是一个固定占用两个字节的无符号正整数，这个正整数对应于Unicode编号，用于表示那个Unicode编号对应的字符。</p><p>由于固定占用两个字节，char只能表示Unicode编号在65536以内的字符，而不能表示超出范围的字符。</p><p><strong>Unicode</strong></p><p>需要注意的是，Unicode只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储。</p><p>比如，汉字”严”的unicode是十六进制数4E25，转换成二进制数足足有15位（100111000100101），也就是说这个符号的表示至少需要2个字节。表示其他更大的符号，可能需要3个字节或者4个字节，甚至更多。</p><p>这里就有两个严重的问题，第一个问题是，如何才能区别Unicode和ASCII？计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？第二个问题是，我们已经知道，英文字母只用一个字节表示就够了，如果Unicode统一规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。</p><p>它们造成的结果是：1）出现了Unicode的多种存储方式，也就是说有许多种不同的二进制格式，可以用来表示Unicode。2）Unicode在很长一段时间内无法推广，直到互联网的出现。</p><p><strong>UTF-8</strong></p><p>互联网的普及，强烈要求出现一种统一的编码方式。UTF-8就是在互联网上使用最广的一种Unicode的实现方式。其他实现方式还包括UTF-16（字符用两个字节或四个字节表示）和UTF-32（字符用四个字节表示），不过在互联网上基本不用。重复一遍，这里的关系是，UTF-8是Unicode的实现方式之一。</p><p>以utf8为例，utf8是一个变长编码标准，可以以1~4个字节表示一个字符，而中文占3个字节，ascII字符占1个字节。</p><p>为什么我们在java里面可以用一个char来表示一个中文呢？</p><p>因为java是以unicode作为编码方式的。unicode是一个定长的编码标准，每个字符都是2个字节，也就是1个char类型的空间。</p><p>在编译时会把utf8的中文字符转换成对应的unicode来进行传输运算。</p><p>(<strong>总结</strong>)[<a href="https://www.zhihu.com/question/23374078" target="_blank" rel="noopener">https://www.zhihu.com/question/23374078</a>]</p><ul><li><p>unicode是编码集，UTF8只是实现方式</p></li><li><p>一个char可以储存一个中文字符，因为它是两个byte</p></li><li>UTF8采用8位动长的方式，可以节省内存，区别中英文</li><li>unicode不够的时候，会采取增补代码点，即用2个2位字节来表示增补字符</li></ul><p><strong>常量</strong></p><p>final 关键字  一旦赋值不能够再更改</p><p><strong>String 字符串</strong></p><p>Java 字符串就是 Unicode 字符序列。Java 没有内置的字符串类型， 而是在标准 Java类库中提供了 一个预定义类，很自然地叫做 String。它是不可变的对象，每次赋值都会新开一个内存空间。可以把string理解成是char*的指针。</p><p>不要用==来比较字符串，而是要使用s.equal(s1)的方式</p><p>它有null和“”空串，两种不同形式，if (str != null &amp;&amp; str.length != 0) </p><p><strong>格式化输出</strong></p><p><img src="/2019/04/30/JAVA学习/java核心技术/4" alt="1558017586500"></p><p>String message = String.format(“Hello, %s. Next year, you’ll be %d”, name, age); </p><p><strong>大数值</strong></p><p>Biglnteger类实现了任意精度的整数运算； BigDecimal 实现了任意精度的浮点数运算。它们没有+和*必须用内置的函数</p><p>Biglnteger a = Biglnteger.valueOf(100);<br>Biglnteger c = a.add(b); // c = a + b<br>Biglnteger d = c.nultipiy(b.add(Biglnteger.valueOf(2))); // d = c * (b + 2)</p><p><strong>数组</strong><br>int[ ] a = new int[100];<br>使用以下直接访问数组内的元素，它将会遍历数组， 而不需要使用下标值<br>for (int element : a)<br>​    System.out.println(element): </p><p>数组拷贝</p><p>浅拷贝： 两个变量将引用同 一个数组</p><p>深拷贝：int[] copiedLuckyNumbers = Arrays.copyOf(luckyNumbers, luckyNumbers.length);（可以使用该方法增长数组长度）</p><p>数组排序：</p><p>Arrays.sort(a) </p><h2 id="对象与类"><a href="#对象与类" class="headerlink" title="对象与类"></a>对象与类</h2><p>要想使用 OOP, —定要清楚对象的三个主要特性： </p><ul><li><p>对象的行为（behavior)— —可以对对象施加哪些操作，或可以对对象施加哪些方法？ </p></li><li><p>对象的状态（state)— —当施加那些方法时，对象如何响应？ </p></li><li><p>对象标识（identity)— —如何辨别具有相同行为与状态的不同对象</p></li></ul><ul><li>对象状态的改变必须通过调用方法实现 </li><li>对象的状态并不能完全描述一个对象。每个对象都有一个唯一的身份（identity)。</li><li>对象的这些关键特性在彼此之间相互影响着。例如， 对象的状态影响它的行为（如果一 个订单 “ 已送货” 或“ 已付款”， 就应该拒绝调用具有增删订单中条目的方法。</li></ul><p>最常见类的关系：•依赖（ “ uses-a”） •聚合（ “ has-a”） •继承（is-a)</p><p>要用UML图画类的关系，设计的时候。</p><p><strong>对象变量</strong></p><p>一个对象变量并没有实际包含一个对象，而仅仅引用一个对象</p><blockquote><p>Date deadline<br>deadline = new Date()<br>deadline是对象变量，必须初始化它</p></blockquote><ul><li>只 访 问 对 象 而 不 修 改 对 象 的 方 法 有 时 称 为 访 问 器 方 法</li><li>修改对象的方法称更改器方法 ( mutator method ) </li><li>如果需要返回一个可变对象的引用， 应该首先对它进行克隆（clone)，否则破坏封装性。</li></ul><p><strong>构造器</strong></p><p>•构造器与类同名 </p><p>•每个类可以有一个以上的构造器 </p><p>•构造器可以有 0 个、1 个或多个参数 </p><p>•构造器没有返回值</p><p> •构造器总是伴随着 new 操作一起调用</p><p>关键字 this 表示隐式参数，就是说通过this.变量或方法来调用自身域内。</p><p><strong>final实例域</strong></p><p>实例域定义为 final。 构建对象时必须初始化这样的域。也就是说， 必须确保在每 一个构造器执行之后，这个域的值被设置， 并且在后面的操作中， 不能够再对它进行修改</p><p>private final String name;</p><p>final如果使用的是可变对象，那么只是说它不会指向其他位置，但是当前位置的内容仍然可以改变</p><p><strong>static 静态</strong></p><p>静态域：每一个对象对于所有的实例域 有自己的一份拷贝，对于静态域只有一个，它属于类，而不属于对象，所以可以直接类.静态域也可以调用。</p><p>静态方法：它只能访问静态域和显式参数（传参）无法访问实例域，一般直接用类.方法名使用</p><p><strong>方法参数</strong></p><p>call by value：表示方法接收的是调用者提供的值</p><p>call by reference：表示方法接收的是调用者提供的变量地址</p><p>java都是按值调用！！！只是当传入的时对象时，它拷贝的是对象引用~</p><blockquote><p>public static void tripieValue(double x) // doesn’t work<br>{ x = 3 * x; }<br>double percent = 10;<br>tripieValue(percent); </p></blockquote><p>1 ) x 被初始化为 percent 值的一个拷贝（也就 是 10 ) </p><p>2 ) x 被乘以 3后等于 30。 但是 percent 仍然 是 10</p><blockquote><p>public static void tripieSalary(Employee x)<br>{ x.raiseSa1ary(200); }<br>harry = new Employee(. . .);<br>tripieSalary(harry);</p></blockquote><p>1 ) X 被初始化为 harry 值的拷贝，这里是一个对象的引用。 </p><p>2 ) raiseSalary 方法应用于这个对象引用。x 和 harry 同时引用的那个 Employee 对象的薪 金提高了 200%。 </p><p>3 ) 方法结束后，参数变量 x 不再使用。当然，对象变量 harry 继续引用那个薪金增至 3 倍的雇员对象</p><p>比如说如果你用swap想交换两个对象是不行的，因为你交换的是两个拷贝后的对象，原对象无影响。</p><p><strong>对象构造</strong></p><p>重载：相同的名字、不同的参数，便产生了重载，重载的是签名，不能出现两个相同名字，相同参数，返回值不同的方法</p><p>默认域初始化 ：被自动初始化为默认值（0、false 或 null)</p><p>构造器：只有当你不提供任何构造器的时候，系统会提供一个默认的构造器，将所有域变量初始化为默认值。</p><p>this:当作是当前类的一个指针，而且可以在构造函数里面使用this(参数)，引用另一个构造函数。</p><p><strong>调用构造器的具体处理步骤：</strong></p><p>1 ) 所有数据域被初始化为默认值（0、false 或 null)。 </p><p>2 ) 按照在类声明中出现的次序， 依次执行所有域初始化语句和初始化块</p><p>3 ) 如果构造器第一行调用了第二个构造器，则执行第二个构造器主体</p><p> 4 ) 执行这个构造器的主体.</p><p><strong>包</strong></p><ul><li>import xx.*是不会有内存影响，有时会出现两个包下函数同名冲突</li><li>编译器不会检查目录结构，如果它没有在它声明的package下，不会出现编译错误，但是最终程序无法允许，虚拟机找不到类。</li></ul><p>可见性：</p><ul><li>public 的部分可以被任意的类使用</li><li>private 的部分只能被定义它们的类使用</li><li>果没有指定 public 或 private, 这个部分（类、方法或变量）可以被同一个包中的所有方法访问</li></ul><p><strong>文档注释</strong></p><p>javadoc, 它可以由源文件生成一个 HTML 文档</p><p>javadoc 实用程序（utility) 从下面几个特性中抽取信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> •包 </span><br><span class="line"></span><br><span class="line">•公有类与接口 </span><br><span class="line"></span><br><span class="line">•公有的和受保护的构造器及方法</span><br><span class="line"></span><br><span class="line"> •公有的和受保护的域</span><br></pre></td></tr></table></figure><p>类注释要放在import语句之后，类定义之前</p><p>方法注释放在方法之前。</p><p>@param 变量描述 这个标记将对当前方法的“ param” （参数）部分添加一个条目。这个描述可以占据多 行， 并可以使用 HTML 标记。一个方法的所有@param 标记必须放在一起。 </p><p>@return 描述 这个标记将对当前方法添加“ return” （返回）部分。这个描述可以跨越多行， 并可以 使用 HTML 标记。 </p><p>©throws 类描述 这个标记将添加一个注释， 用于表示这个方法有可能抛出异常。有关异常的详细内容。</p><p>@author 姓名 这个标记将产生一个 “author” (作者）条目。可以使用多个@author 标记，每个@ author 标记对应一个作者 </p><p>@version 这个标记将产生一个“ version”（版本）条目。这里的文本可以是对当前版本的任何描 述。 下面的标记可以用于所有的文档注释中。 </p><p>@since 文本 这个标记将产生一个“ since” （始于）条目。这里的 text 可以是对引人特性的版本描 述。例如，©since version 1.7.10</p><p>/<em>*<br>@param  car  ….</em>/</p><p><strong>类设计技巧</strong></p><ol><li>一定要保证数据私有</li><li>一定要对数据初始化 </li><li>不要在类中使用过多的基本类型（抽象）</li><li>不是所有的域都需要独立的域访问器和域更改器 （状态的相互影响）</li><li>类名和方法名要能够体现它们的职责</li></ol><h2 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h2><p><strong>简单介绍</strong></p><p>public class Manager extends Employee</p><ul><li>使用extends关键字</li><li>所有继承都是公有继承，没有C++中的私有继承</li><li>使用关键字super调用父类的方法</li><li>关键字 this 有两个用途： 一是引用隐式参数，二是调用该类其他的构 造器 ， 同样，super 关键字也有两个用途：一是调用超类的方法，二是调用超类的构造器。 </li></ul><p><strong>多态</strong></p><p>一个对象变量（例如， 变量 e) 可以指示多种实际类型的现象被称为多态（polymorphism)。 在运行时能够自动地选择调用哪个方法的现象称为动态绑定（dynamic binding)。</p><p>即函数写的是父类，但是调用时传入的是不同的子类，该父类可以自动调用多种实际类的覆盖方法</p><p><strong>方法调用顺序</strong></p><p>1.编译器査看对象的声明类型和方法名。编译器将会一一列举所有 C 类中名为 f 的方法和其超类中 访问属性为 public 且名为 f 的方法（超类的私有方法不可访问）</p><p>2.编译器将査看调用方法时提供的参数类型。如果在所有名为 f 的方法中存在 一个与提供的参数类型完全匹配，就选择这个方法。</p><p>至此， 编译器已获得需要调用的方法名字和参数类型。</p><p>3.静态绑定： 如果是 private 方法、 static 方法、final 方法，编译器将可以准确地知道应该调用哪个方法</p><p>4.动态绑定：调用的方法依赖于隐式参数的实际类型，在运行时虚拟机一定调用与 x 所引用对象的实 际类型最合适的那个类的方法</p><p>每次调用方法都要进行搜索，时间开销相当大。因此，虚拟机预先为每个类创建了一个 方法表（method table), 其中列出了所有方法的签名和实际调用的方法</p><p><strong>警告</strong>：在覆盖一个方法的时候，子类方法不能低于超类方法的可见性。</p><p><strong>final 类和方法</strong> </p><p>不允许扩展的类被称为 final 类</p><p>类中的特定方法也可以被声明为 final。如果这样做，子类就不能覆盖这个方法（final 类中的所有方法自动地成为 final 方法)。写上会比较好，这样可以避免一些不必要的覆盖。</p><p>本包是指包名相同的情况下</p><p>1 ) 仅对本类可见 private。<br>2 ) 对所有类可见 public<br>3 ) 对本包和所有子类可见 protected。<br>4 ) 对本包可见— —默认（很遗憾)， 不需要修饰符</p><p><strong>Object</strong></p><p>这是所有类的超类，其有一些方法需要注意：</p><ul><li>equal 判断对象是否相等</li><li>hashCode 每个对象的储存地址</li><li>toString 返回表示对象值的字符 串</li></ul><p><strong>泛型数组列表</strong></p><p>ArrayList<employee> staff = new ArrayList<eniployee>（）; </eniployee></employee></p><p>它可以动态更改数组大小</p><p>数组列表管理着对象引用的一个内部数组。最终， 数组的全部空间有可能被用尽。这就显现出数组列表的操作魅力： 如果调用 add且内部数组已经满了，数组列表就将自动地创建 一个更大的数组，并将所有的对象从较小的数组中拷贝到较大的数组中。</p><p>有两种声明长度的方法：</p><ol><li><p>ArrayList<employee> staff = new ArrayList&lt;&gt;(100);</employee></p><p>数组这样声明，系统会为数组分配100个元素的储存空间。而数组列表这样声明，只是表示拥有保存100个元素的潜力，在最初数组列表不会含有任何元素。</p></li><li><p>ensureCapacity(100）</p><p>如果已经清楚或能够估计出数组可能存储的元素数量， 就可以在填充数组之前调用 ensureCapacity方法，会分配一个包含100个对象的内部数组。</p></li></ol><p>API方法：</p><p>add：添加新元素</p><p>set：覆盖旧元素</p><p>get: 获取元素</p><p>remove:删除元素</p><p>注意对这类型的数组中间元素进行插入和删除，过程会比较费时</p><p>类型化与原始数组列表的兼容性<br>　　可以将一个类型化的数组列表传递给update方法，而不需要进行任何类型转换：即只要是ArrayList类对象，不管是什么具体类型化都可以编译通过。</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ArrayList&lt;Employee&gt; staff = ...<span class="comment">;</span></span><br><span class="line">employeeDB.update(staff)<span class="comment">;</span></span><br></pre></td></tr></table></figure><p>　　但是，如果将一个原始的没有类型化的ArrayList赋值给一个类型化ArrayList就会得到警告：因为得到的可能不是Employee类的类型的数组列表</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ArrayList&lt;Employee&gt; result = employeeDB.<span class="builtin-name">find</span>(<span class="built_in">..</span><span class="built_in">..</span>);</span><br></pre></td></tr></table></figure><p>　　这个时候可以使用强制类型转换：这个时候会得到另外一个警告，因为虚拟机中没有类型参数</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ArrayList&lt;Employee&gt; result = (ArrayList&lt;Employee&gt;)employeeDB.<span class="builtin-name">find</span>(<span class="built_in">..</span><span class="built_in">..</span>);</span><br></pre></td></tr></table></figure><p>　　这个时候，如果确保不会造成严重的后果，可以使用@SuppressWarnings(“unchecked”)来标记这个变量能够接受类型转换：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">@<span class="keyword">SuppressWarnings</span>("<span class="keyword">unchecked</span>")</span><br><span class="line"><span class="keyword">ArrayList</span>&lt;<span class="keyword">Employee</span>&gt; result = (ArrayList&lt;Employee&gt;)employeeDB.find(....);</span><br></pre></td></tr></table></figure><p>201</p><h2 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h2><ul><li>要永远记住浮点数的计算是会有误差，永远都无法精确，所以不要使用等于这个关系符</li><li>null的坑要填，判断的时候要永远记得空和长度为零都要同时写上。</li><li>非常小心可变对象的修改。’</li><li>类是类，对象是new，对象变量是引用</li></ul><p>疑惑</p><ul><li>声明类型未赋值后是Null，未声明类型是什么，内存的占用情况(未赋值时引用，占的时引用内存)</li><li>可变对象是什么</li><li>类初始化的执行顺序</li><li>类的搜索顺序</li><li>javadoc是否需要掌握使用</li><li>java.util和java.lang要新开一章</li><li>保护机制</li></ul><h2 id="奇怪的地方"><a href="#奇怪的地方" class="headerlink" title="奇怪的地方"></a>奇怪的地方</h2><p>class Employee {<br>public boolean equals(Employee other)<br> { return name.equals(other.name); }<br>}<br>if (harry,equals(boss))<br>方法可以访问所调用对象的私有数据。一个方法可以访问所属类的所有 对象的私有数据</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;java基础知识&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="java" scheme="http://kodgv.xyz/categories/java/"/>
    
    
      <category term="java" scheme="http://kodgv.xyz/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>后端学习历程</title>
    <link href="http://kodgv.xyz/2019/04/30/JAVA%E5%AD%A6%E4%B9%A0/java%E5%8F%82%E8%80%83%E8%B7%AF%E7%BA%BF/"/>
    <id>http://kodgv.xyz/2019/04/30/JAVA学习/java参考路线/</id>
    <published>2019-04-30T11:30:59.000Z</published>
    <updated>2019-05-25T12:32:49.938Z</updated>
    
    <content type="html"><![CDATA[<p>java学习路线与推荐<br><a id="more"></a></p><h2 id="6个月-Java-服务端入门和进阶指南"><a href="#6个月-Java-服务端入门和进阶指南" class="headerlink" title="6个月 Java 服务端入门和进阶指南"></a>6个月 Java 服务端入门和进阶指南</h2><p><a href="https://www.zhihu.com/question/29581524" target="_blank" rel="noopener">https://www.zhihu.com/question/29581524</a><br><img src="/2019/04/30/JAVA学习/java参考路线/1" alt="1557919436614"></p><p><img src="/2019/04/30/JAVA学习/java参考路线/2" alt="1557919469076"></p><p><img src="/2019/04/30/JAVA学习/java参考路线/3" alt="1557919495443"></p><p>资料参考，先学第一阶段，学完再来看第二阶段的任务吧</p><p><a href="https://www.zhihu.com/question/22340525" target="_blank" rel="noopener">https://www.zhihu.com/question/22340525</a></p><p><a href="https://www.zhihu.com/question/307096748/answers/updated" target="_blank" rel="noopener">https://www.zhihu.com/question/307096748/answers/updated</a></p><p><a href="https://mp.weixin.qq.com/s/GxIRqj8s1zrnnPSOJ0203Q" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/GxIRqj8s1zrnnPSOJ0203Q</a></p><p><a href="https://zhuanlan.zhihu.com/p/34880504" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34880504</a></p><p><a href="https://h2pl.github.io/" target="_blank" rel="noopener">后端校招以及大牛成长转折点</a></p><p><a href="https://www.nowcoder.com/discuss/16124" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/16124</a></p><p>知识点总结网站</p><p><a href="https://github.com/TransientWang/KnowledgeBase" target="_blank" rel="noopener">https://github.com/TransientWang/KnowledgeBase</a></p><p><a href="https://github.com/Snailclimb/JavaGuide" target="_blank" rel="noopener">https://github.com/Snailclimb/JavaGuide</a></p><h2 id="自己整理"><a href="#自己整理" class="headerlink" title="自己整理"></a>自己整理</h2><h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3><p><strong>JAVA基础的学习</strong></p><p>《Java 核心技术：卷1 基础知识》(或者《Java 编程思想》)必看</p><p>《Effective Java》</p><p>java.util包和java.lang包</p><p><a href="http://how2j.cn/" target="_blank" rel="noopener">做题网站</a></p><p><strong>代码规范</strong></p><p>《Git 权威指南》 <a href="https://link.zhihu.com/?target=https%3A//learngitbranching.js.org/" target="_blank" rel="noopener">Learn Git Branching通关网站</a></p><p>《Maven 实战》</p><p><a href="https://link.zhihu.com/?target=https%3A//book.douban.com/subject/4262627/" target="_blank" rel="noopener">《重构_改善既有代码的设计》</a></p><p>学习代码规范。我们大致上遵循 oracle 的 Java 语言编码规范，你可以先阅读并熟悉它。Code Formatting 文件在 git@xxx/coding-standard.git，在编写代码之前，请把它导入到 IDE 中。另外，确认 IDE 已经安装 Findbugs 和 CheckStyle 插件。</p><p><strong>开发工具</strong></p><ul><li>熟练使用一种 IDE。Intellij IDEA或者 Eclipse 都可以，推荐使用前者。至少熟悉常用的快捷键，会 debug(包括远程 debug)项目。</li><li>熟悉一种编辑器。比如 Vim/Emacs/Sublime Text，至少学会搜索/替换/代码补全。</li></ul><p><strong>开发环境</strong></p><p>Linux 的基本使用可以通过《鸟哥的Linux私房菜：基础学习篇（第三版）》学习</p><p> bash shell 脚本可以参考《Linux Shell脚本攻略》</p><h2 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h2><p>刷题：</p><p>先从简单的图解算法看起，然后做题的时候不要做太多，做别人整理好的经典题比较重要。TopCoder，codeforces。然后注意题要抄在本子上，便于作弊，注意它们说的板子。</p><p>图解HTTP协议和servlet。再JAVA进阶，然后框架和数据库并行。</p><p>设计模式要优先于框架，这样才能更好掌握</p><p>需要掌握三大框架SSM，需要掌握各类数据库，需要掌握各类协议</p><p>设计模式</p><p>学Unix</p><p>基本的单元测试</p><p><strong>JAVA进阶</strong></p><ul><li><p><strong>《Java并发编程的艺术》《深入理解Java虚拟机》</strong></p></li><li><p>并发编程网：<strong>并发编程网 - ifeve.com</strong> 重点掌握java内存模型，各种锁的原理及应用，JVM GC垃圾回收原理。</p></li></ul><h2 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h2><p>后端校招以及大牛成长转折点</p><p>源码</p><p>微服务</p><p>微架构</p><p>各种组件</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;java学习路线与推荐&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="java" scheme="http://kodgv.xyz/categories/java/"/>
    
    
      <category term="java" scheme="http://kodgv.xyz/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>elo比赛心得</title>
    <link href="http://kodgv.xyz/2019/04/30/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/elo%E6%AF%94%E8%B5%9B%E5%BF%83%E5%BE%97/"/>
    <id>http://kodgv.xyz/2019/04/30/竞赛经验/elo比赛心得/</id>
    <published>2019-04-30T11:30:59.000Z</published>
    <updated>2019-05-25T12:36:31.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="比赛简介"><a href="#比赛简介" class="headerlink" title="比赛简介"></a>比赛简介</h2><ul><li>回归问题</li><li><p>有异常点</p></li><li><p>商品信息，</p></li><li>每个用户的刷卡时间。</li><li>预测用户的忠诚度。</li></ul><a id="more"></a><p>[TOC]</p><h2 id="11th-place-solution"><a href="#11th-place-solution" class="headerlink" title="11th place solution"></a><a href="https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/82127#latest-502682" target="_blank" rel="noopener">11th place solution</a></h2><p><strong>FEATURE ENGINEERING</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> I refer the Kaggle Rank System Compute Formula（link:[https://www.kaggle.com/progression][<span class="number">4</span>])</span><br><span class="line">    df_data[<span class="string">'duration_sqrt_counts'</span>] = df_data[<span class="string">'durations'</span>]/sqrt(df_data[<span class="string">'card_id_counts'</span>])</span><br><span class="line">    df_data[<span class="string">'duration_log1p_counts'</span>] = df_data[<span class="string">'durations'</span>]/log1p(df_data[<span class="string">'card_id_counts'</span>])</span><br><span class="line">    df_data[<span class="string">'duration_counts'</span>] = df_data[<span class="string">'durations'</span>]/df_data[<span class="string">'card_id_counts'</span>]</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> Categorical features: frequence, Maxfrequence, MaxfrequenceRatio</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> card_id/merchant_id/mechant_category_id/city_id (visit sequence to sequence embedding)</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> purchase_amount:hist/new</span><br><span class="line"></span><br><span class="line"><span class="number">5.</span> features interactions between hist/new</span><br><span class="line">            df[<span class="string">'purchase_amount_ratio_v3'</span>] =                              df[<span class="string">'new_purchase_amount_max'</span>]/df[<span class="string">'hist_purchase_amount_sum'</span>]</span><br><span class="line">            df[<span class="string">'purchase_amount_diff_v1'</span>] = df[<span class="string">'new_purchase_amount_sum'</span>]-df[<span class="string">'hist_purchase_amount_sum'</span>]</span><br><span class="line">            df[<span class="string">'purchase_amount_diff_v2'</span>] = df[<span class="string">'new_purchase_amount_mean'</span>]-df[<span class="string">'hist_purchase_amount_mean'</span>]</span><br><span class="line">            df[<span class="string">'purchase_amount_diff_v3'</span>] = df[<span class="string">'new_purchase_amount_max'</span>]-df[<span class="string">'hist_purchase_amount_max'</span>]</span><br><span class="line">            df[<span class="string">'purchase_amount_diff_v4'</span>] = df[<span class="string">'new_purchase_amount_min'</span>]-df[<span class="string">'hist_purchase_amount_min'</span>]</span><br><span class="line">            df[<span class="string">'pa_mlag_ratio'</span>] = df[<span class="string">'new_purchase_amount_sum'</span>]/(df[<span class="string">'month_lag_mean'</span>] - <span class="number">1</span>)</span><br><span class="line">            df[<span class="string">'pa_new_hist_ratio'</span>] = df[<span class="string">'new_purchase_amount_sum'</span>]/(df[<span class="string">'hist_purchase_amount_sum'</span>])</span><br><span class="line">            df[<span class="string">'pa_new_hist_mean_ratio'</span>] = df[<span class="string">'new_purchase_amount_mean'</span>]/(df[<span class="string">'hist_purchase_amount_mean'</span>] )</span><br><span class="line">            df[<span class="string">'pa_new_hist_min_ratio'</span>] = df[<span class="string">'new_purchase_amount_min'</span>]/(df[<span class="string">'hist_purchase_amount_min'</span>] )</span><br><span class="line">            df[<span class="string">'pa_new_hist_max_ratio'</span>] = df[<span class="string">'new_purchase_amount_max'</span>]/(df[<span class="string">'hist_purchase_amount_max'</span>] )</span><br></pre></td></tr></table></figure><p>我们有两个单独的feature sets。一个具有+1000个feature ，另一个具有+200个feature </p><p>然后，我们取+200特征集的相关矩阵，<strong>将每个特征与其关联最小的特征配对。然后我们对每一对应用了大量的聚合</strong>，结果得到了非常强大的特征。</p><p>所以我们最终得到了两个功能集，每个功能集+1000个功能。”</p><p><strong>STACKING</strong></p><p>We stacked around 32 models using bayesian regression. Our models were well varied that it yielded a score of CV:3.630X LB :3.675</p><p><strong>STUFF THAT DID NOT WORK</strong></p><p>Of course these last two months were not all roses and rainbows. We pulled our hair trying a lot of things and we failed miserably.</p><p>Here are the bloopers of our participation :D :</p><ul><li>NN. We tried designing different architectures with the main focus on having a simple NN with heavy regularization (BatchNorm and Strong Dropout)</li><li>In the middle of the competition, we tried tackling the outliers detection as an anomaly detection problem using AutoEncoders trained only on the non outliers data</li><li>We tried PCA for more features. And it didn’t work</li><li>We tried TSNE. It didn’t work</li><li>We tried FM and FFM. It did not work</li><li>We tried isolation forest. Nope. Did not work.</li><li>We had a Ridge-based pairwise ranker that we intended to use for outliers detection but it didn’t match with the approach we had.</li><li>We tried a lot of weak models in the hope of adding diversity (simple tree-based, linear, svm, etc.). And guess what? It did not work.</li></ul><p>如何识别异常点？</p><p>两个不同的特征集怎么做？</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;比赛简介&quot;&gt;&lt;a href=&quot;#比赛简介&quot; class=&quot;headerlink&quot; title=&quot;比赛简介&quot;&gt;&lt;/a&gt;比赛简介&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;回归问题&lt;/li&gt;
&lt;li&gt;&lt;p&gt;有异常点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;商品信息，&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;每个用户的刷卡时间。&lt;/li&gt;
&lt;li&gt;预测用户的忠诚度。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>比赛常用图</title>
    <link href="http://kodgv.xyz/2019/04/29/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E6%AF%94%E8%B5%9B%E5%B8%B8%E7%94%A8%E5%9B%BE/"/>
    <id>http://kodgv.xyz/2019/04/29/数据竞赛/比赛常用图/</id>
    <published>2019-04-29T08:30:12.000Z</published>
    <updated>2019-05-27T11:42:17.063Z</updated>
    
    <content type="html"><![CDATA[<p>比赛常见的EDA总结</p><a id="more"></a><p><strong>要注意把以后整理函数的时候把图补上</strong></p><p>plt.show()会阻碍当前程序的执行，请再最后执行<br>如果想要不阻碍请执行plt.ion()，但是这样当程序结束时会自动关闭当前图像，所以需要再最后执行plt.ioff()以阻碍图像不被关闭</p><p>[TOC]</p><h2 id="曲线分布图"><a href="#曲线分布图" class="headerlink" title="曲线分布图"></a>曲线分布图</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/gpreda/santander-eda-and-prediction</span></span><br><span class="line">def plot_feature_distribution(df1, df2, label1, label2, features):</span><br><span class="line">    <span class="attr">i</span> = <span class="number">0</span></span><br><span class="line">    sns.set_style('whitegrid')</span><br><span class="line">    plt.figure()</span><br><span class="line">    fig, <span class="attr">ax</span> = plt.subplots(<span class="number">10</span>,<span class="number">10</span>,<span class="attr">figsize=(18,22))</span></span><br><span class="line"></span><br><span class="line">    for feature <span class="keyword">in</span> features:</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        plt.subplot(<span class="number">10</span>,<span class="number">10</span>,i)</span><br><span class="line">        sns.distplot(df1[feature], <span class="attr">hist=False,label=label1)</span></span><br><span class="line">        sns.distplot(df2[feature], <span class="attr">hist=False,label=label2)</span></span><br><span class="line">        plt.xlabel(feature, <span class="attr">fontsize=9)</span></span><br><span class="line">        locs, <span class="attr">labels</span> = plt.xticks()</span><br><span class="line">        plt.tick_params(<span class="attr">axis='x',</span> <span class="attr">which='major',</span> <span class="attr">labelsize=6,</span> <span class="attr">pad=-6)</span></span><br><span class="line">        plt.tick_params(<span class="attr">axis='y',</span> <span class="attr">which='major',</span> <span class="attr">labelsize=6)</span></span><br><span class="line">    plt.show();</span><br><span class="line">    </span><br><span class="line"><span class="attr">t0</span> = train_df.loc[train_df['target'] == <span class="number">0</span>]</span><br><span class="line"><span class="attr">t1</span> = train_df.loc[train_df['target'] == <span class="number">1</span>]</span><br><span class="line"><span class="attr">features</span> = train_df.columns.values[<span class="number">2</span>:<span class="number">102</span>]</span><br><span class="line">plot_feature_distribution(t0, t1, '<span class="number">0</span>', '<span class="number">1</span>', features)</span><br></pre></td></tr></table></figure><p><img src="/2019/04/29/数据竞赛/比赛常用图/2.png" alt></p><h2 id="条形分布图"><a href="#条形分布图" class="headerlink" title="条形分布图"></a>条形分布图</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.<span class="built_in">title</span>(<span class="string">"Distribution of mean values per column in the train and test set"</span>)</span><br><span class="line">sns.distplot(train_df[<span class="built_in">features</span>].<span class="built_in">mean</span>(axis=<span class="number">0</span>),<span class="built_in">color</span>=<span class="string">"magenta"</span>,kde=True,bins=<span class="number">120</span>, <span class="built_in">label</span>='train')</span><br><span class="line">sns.distplot(test_df[<span class="built_in">features</span>].<span class="built_in">mean</span>(axis=<span class="number">0</span>),<span class="built_in">color</span>=<span class="string">"darkblue"</span>, kde=True,bins=<span class="number">120</span>, <span class="built_in">label</span>='test')</span><br><span class="line">plt.<span class="built_in">legend</span>()</span><br><span class="line">plt.<span class="built_in">show</span>()</span><br></pre></td></tr></table></figure><p><img src="/2019/04/29/数据竞赛/比赛常用图/3.png" alt></p><h2 id="条形图"><a href="#条形图" class="headerlink" title="条形图"></a>条形图</h2><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#画条形图</span></span><br><span class="line">sns.barplot<span class="params">(<span class="attr">x</span>='Sex', <span class="attr">y</span>='Survived', <span class="attr">data</span>=train)</span>;<span class="comment"># seaborn 的 barplot() 利用矩阵条的高度反映数值变量的集中趋势，展示的是变量的平均值</span></span><br><span class="line">sns.barplot<span class="params">(<span class="attr">x</span>='Sex', <span class="attr">y</span>='Survived', <span class="attr">hue</span> = 'Pclass', <span class="attr">data</span>=train)</span>;<span class="comment">#加了图例的功能</span></span><br></pre></td></tr></table></figure><h2 id="相关性热度图"><a href="#相关性热度图" class="headerlink" title="　相关性热度图"></a>　相关性热度图</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">colormap</span> = plt.cm.RdBu</span><br><span class="line">plt.figure(<span class="attr">figsize=(14,12))</span></span><br><span class="line">plt.title('Pearson Correlation of Features', <span class="attr">y=1.05,</span> <span class="attr">size=15)</span></span><br><span class="line">sns.heatmap(train.astype(float).corr(),<span class="attr">linewidths=0.1,vmax=1.0,</span> </span><br><span class="line">            <span class="attr">square=True,</span> <span class="attr">cmap=colormap,</span> <span class="attr">linecolor='white',</span> <span class="attr">annot=True)</span></span><br></pre></td></tr></table></figure><h2 id="多变量相关性图"><a href="#多变量相关性图" class="headerlink" title="　多变量相关性图"></a>　多变量相关性图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多变量相关性图 要注意哦这里的变量所有的成对一一对应的</span></span><br><span class="line">g = sns.pairplot(train[[<span class="string">u'Survived'</span>, <span class="string">u'Pclass'</span>, <span class="string">u'Sex'</span>, <span class="string">u'Age'</span>, <span class="string">u'Parch'</span>, <span class="string">u'Fare'</span>, <span class="string">u'Embarked'</span>,</span><br><span class="line">       <span class="string">u'FamilySize'</span>, <span class="string">u'Title'</span>]], hue=<span class="string">'Survived'</span>, palette = <span class="string">'seismic'</span>,size=<span class="number">1.2</span>,diag_kind = <span class="string">'kde'</span>,diag_kws=dict(shade=<span class="literal">True</span>),plot_kws=dict(s=<span class="number">10</span>) )</span><br></pre></td></tr></table></figure><h2 id="箱图"><a href="#箱图" class="headerlink" title="箱图"></a>箱图</h2><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">''<span class="symbol">'Create</span> a <span class="keyword">function</span> <span class="keyword">to</span> count total outliers. <span class="keyword">And</span> plot variables <span class="keyword">with</span> <span class="keyword">and</span> without outliers.'''</span><br><span class="line">def outliers(<span class="keyword">variable</span>):</span><br><span class="line">    # Calculate <span class="number">1</span>st, <span class="number">3</span>rd quartiles <span class="keyword">and</span> iqr.</span><br><span class="line">    q1, q3 = <span class="keyword">variable</span>.quantile(<span class="number">0.25</span>), <span class="keyword">variable</span>.quantile(<span class="number">0.75</span>)</span><br><span class="line">    iqr = q3 - q1</span><br><span class="line">    </span><br><span class="line">    # Calculate lower fence <span class="keyword">and</span> upper fence <span class="keyword">for</span> outliers</span><br><span class="line">    l_fence, u_fence = q1 - <span class="number">1.5</span>*iqr , q3 + <span class="number">1.5</span>*iqr   # Any values less than l_fence <span class="keyword">and</span> greater than u_fence are outliers.</span><br><span class="line">    </span><br><span class="line">    # Observations that are outliers</span><br><span class="line">    outliers = <span class="keyword">variable</span>[(<span class="keyword">variable</span>&lt;l_fence) | (<span class="keyword">variable</span>&gt;u_fence)]</span><br><span class="line">    print(<span class="symbol">'Total</span> Outliers <span class="keyword">of</span>', <span class="keyword">variable</span>.name,':', outliers.count())</span><br><span class="line">    </span><br><span class="line">    # Drop obsevations that are outliers</span><br><span class="line">    filtered = <span class="keyword">variable</span>.drop(outliers.index, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    # Create subplots</span><br><span class="line">    fig, (ax1, ax2) = plt.subplots(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    # Gives space between two subplots</span><br><span class="line">    fig.subplots_adjust(hspace = <span class="number">1</span>) </span><br><span class="line">    </span><br><span class="line">    # Plot <span class="keyword">variable</span> <span class="keyword">with</span> outliers</span><br><span class="line">    <span class="keyword">variable</span>.plot.box(vert = <span class="literal">False</span>, color = <span class="symbol">'coral</span>', grid = <span class="literal">False</span>, ax = ax1, title = <span class="symbol">'Distribution</span> <span class="keyword">with</span> Outliers <span class="keyword">for</span> %s' %<span class="keyword">variable</span>.name)</span><br><span class="line"></span><br><span class="line">    # Plot <span class="keyword">variable</span> without outliers</span><br><span class="line">    filtered.plot.box(vert = <span class="literal">False</span>, color = <span class="symbol">'coral</span>', grid = <span class="literal">False</span>, ax = ax2, title = <span class="symbol">'Distribution</span> without Outliers <span class="keyword">for</span> %s' %<span class="keyword">variable</span>.name)</span><br></pre></td></tr></table></figure><h2 id="刻画变量的不平衡度"><a href="#刻画变量的不平衡度" class="headerlink" title="刻画变量的不平衡度"></a>刻画变量的不平衡度</h2><p>If skewness is less than −1 or greater than +1, the distribution can be considered as highly skewed.</p><p>If skewness is between −1 and −½ or between +½ and +1, the distribution can be considered as moderately skewed.</p><p>And finally if skewness is between −½ and +½, the distribution can be considered as approximately symmetric.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''#2.Density plot with skewness.'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">density_plot_and_skewness</span><span class="params">(variable)</span>:</span></span><br><span class="line">    variable.plot.hist(density = <span class="literal">True</span>)</span><br><span class="line">    variable.plot.kde(style = <span class="string">'k--'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'%s'</span>%variable.name)</span><br><span class="line">    plt.title(<span class="string">'Distribution of %s with Density Plot &amp; Histogram'</span> %variable.name)</span><br><span class="line">    print(<span class="string">'Skewness of '</span>, variable.name, <span class="string">':'</span>)</span><br><span class="line">    skewness = variable.skew()</span><br><span class="line">    <span class="keyword">return</span> display(skewness)</span><br></pre></td></tr></table></figure><h2 id="分类变量和分类变量的关系图"><a href="#分类变量和分类变量的关系图" class="headerlink" title="分类变量和分类变量的关系图"></a>分类变量和分类变量的关系图</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#############################<span class="number">2</span>X2列联表展示########################################</span><br><span class="line"><span class="string">''</span><span class="string">'#1.Create a function that calculates absolute and relative frequency of Survived variable by a categorical variable. And then plots the absolute and relative frequency of Survived by a categorical variable.'</span><span class="string">''</span></span><br><span class="line">def crosstab(cat, cat_target):</span><br><span class="line">    <span class="string">''</span><span class="string">'cat = categorical variable, cat_target = our target categorical variable.'</span><span class="string">''</span></span><br><span class="line">    global ax, ax1</span><br><span class="line">    cat_grouped_by_cat_target = pd.crosstab(index = cat, columns = cat_target)</span><br><span class="line">    cat_grouped_by_cat_target.rename(&#123;<span class="number">0</span>:<span class="string">'Victims'</span>, <span class="number">1</span>:<span class="string">'Survivors'</span>&#125;, axis = <span class="string">'columns'</span>, inplace = <span class="literal">True</span>)  # Renaming the columns</span><br><span class="line">    pct_cat_grouped_by_cat_target = round(pd.crosstab(index = cat, columns = cat_target, normalize = <span class="string">'index'</span>)*<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">    pct_cat_grouped_by_cat_target.rename(&#123;<span class="number">0</span>:<span class="string">'Victims(%)'</span>, <span class="number">1</span>:<span class="string">'Survivors(%)'</span>&#125;, axis = <span class="string">'columns'</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    print(<span class="string">'Survivals and Deaths by'</span>, cat.name,<span class="string">':'</span>, <span class="string">'\n'</span>,cat_grouped_by_cat_target )</span><br><span class="line">    print(<span class="string">'\nPercentage Survivals and Deaths by'</span>, cat.name, <span class="string">':'</span>,<span class="string">'\n'</span>, pct_cat_grouped_by_cat_target)</span><br><span class="line">    </span><br><span class="line">    # Plot absolute frequency <span class="keyword">of</span> Survived by a categorical variable</span><br><span class="line">    ax =  cat_grouped_by_cat_target.plot.bar(color = [<span class="string">'r'</span>, <span class="string">'g'</span>])</span><br><span class="line">    plt.title(<span class="string">'Survival vs Death Count by %s'</span> %cat.name)</span><br><span class="line">    abs_bar_labels()</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    # Plot relative frequrncy <span class="keyword">of</span> Survived by a categorical variable</span><br><span class="line">    ax1 = pct_cat_grouped_by_cat_target.plot.bar(color = [<span class="string">'r'</span>, <span class="string">'g'</span>])</span><br><span class="line">    plt.title(<span class="string">'Percentage Survival vs Death Count by %s'</span> %cat.name)</span><br><span class="line">    pct_bar_labels()</span><br><span class="line">    plt.show()</span><br><span class="line">###########################################卡方检验#######################################</span><br><span class="line"><span class="string">''</span><span class="string">'#2.Create a function to calculate chi_square test between a categorical and target categorical variable.'</span><span class="string">''</span></span><br><span class="line">def chi_square(cat, cat_target):</span><br><span class="line">    cat_grouped_by_cat_target = pd.crosstab(index = cat, columns = cat_target)</span><br><span class="line">    test_result = stats.chi2_contingency (cat_grouped_by_cat_target)</span><br><span class="line">    print(<span class="string">'Chi_square test result between Survived &amp; %s'</span> %cat.name)</span><br><span class="line">    return display(test_result)</span><br><span class="line"></span><br><span class="line">#############################################bonferroni adjusted检验###############################</span><br><span class="line"><span class="string">''</span><span class="string">'#3.Finally create another function to calculate Bonferroni-adjusted pvalue for a categorical and target categorical variable.'</span><span class="string">''</span></span><br><span class="line">def bonferroni_adjusted(cat, cat_target):</span><br><span class="line">    dummies = pd.get_dummies(cat)</span><br><span class="line">    for columns <span class="keyword">in</span> dummies:</span><br><span class="line">        crosstab = pd.crosstab(dummies[columns], cat_target)</span><br><span class="line">        print(stats.chi2_contingency(crosstab))</span><br><span class="line">    print(<span class="string">'\nColumns:'</span>, dummies.columns)</span><br></pre></td></tr></table></figure><h2 id="多个变量组合对因变量的影响图"><a href="#多个变量组合对因变量的影响图" class="headerlink" title="多个变量组合对因变量的影响图"></a>多个变量组合对因变量的影响图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''Create a function that plots the impact of 3 predictor variables at a time on a target variable.'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multivariate_analysis</span><span class="params">(cat1, cat2, cat3, cat_target)</span>:</span></span><br><span class="line">    grouped = round(pd.crosstab(index = [cat1, cat2, cat3], columns = cat_target, normalize = <span class="string">'index'</span>)*<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">    grouped.rename(&#123;<span class="number">0</span>:<span class="string">'Died%'</span>, <span class="number">1</span>:<span class="string">'Survived%'</span>&#125;, axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    ax = grouped.plot.bar(color = [<span class="string">'r'</span>, <span class="string">'g'</span>])</span><br><span class="line">    plt.ylabel(<span class="string">'Relative Frequency (%)'</span>)</span><br></pre></td></tr></table></figure><h2 id="热度图"><a href="#热度图" class="headerlink" title="热度图"></a>热度图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920</span></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">sns.heatmap(x, cmap=<span class="string">'RdBu_r'</span>, center=<span class="number">0.0</span>) </span><br><span class="line">plt.title(<span class="string">'VAR_'</span>+str(j)+<span class="string">' Predictions without Magic'</span>,fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xticks(np.linspace(<span class="number">0</span>,<span class="number">49</span>,<span class="number">5</span>),np.round(np.linspace(mn,mx,<span class="number">5</span>),<span class="number">1</span>))</span><br><span class="line">plt.xlabel(<span class="string">'Var_'</span>+str(j))</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.ylabel(<span class="string">''</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/04/29/数据竞赛/比赛常用图/5.png" alt></p><p>通过热度图可以看出模型是否明显利用了两个特征。虽然可能特征重要度也会说明，但是这样的热度图明显会更具说服力，比如说var_0，模型可以得出竖线15，横线4之类的分界线。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛常见的EDA总结&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="预处理" scheme="http://kodgv.xyz/tags/%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>迁移学习</title>
    <link href="http://kodgv.xyz/2019/04/29/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    <id>http://kodgv.xyz/2019/04/29/数据竞赛/迁移学习/</id>
    <published>2019-04-29T07:56:28.000Z</published>
    <updated>2019-05-25T12:40:41.962Z</updated>
    
    <content type="html"><![CDATA[<p>待完善</p><a id="more"></a><p>！！！说不一定可以切分原数据集为新旧数据集</p><ul><li>新旧特征：新数据集为1，旧数据集为0</li><li>合并新旧数据：合并新旧来做特征处理然后有两种操作<br>第一种是用旧训练模型，用模型预测新数据集的train和test概率，然后加在后面做特征，然后在用新数据重新做特征处理然后预测（这个比较好）。 第二种是直接上新旧合并数据训练的模型，然后直接加模型概率做新特征（这个可能会leak，但是在目前的模板上有cv应该会好一点）然后在用新数据重新做特征处理然后预测</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;待完善&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>去除重复值</title>
    <link href="http://kodgv.xyz/2019/04/29/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/%E5%8E%BB%E9%99%A4%E9%87%8D%E5%A4%8D%E5%80%BC/"/>
    <id>http://kodgv.xyz/2019/04/29/竞赛经验/去除重复值/</id>
    <published>2019-04-29T07:23:45.000Z</published>
    <updated>2019-05-27T11:44:21.037Z</updated>
    
    <content type="html"><![CDATA[<p>不去重的数据训练的模型 = 80%的非重复样本准确率 + 100%重复样本准确率<br>去重的数据训练的模型 = 85%的非重复样本准确率 + 85%重复样本准确率（直接规则覆盖后100%）<br>深度提取重复样本的真正目的是删除它们以提高模型预测非重复样本的准确率，非重复样本的准确率才是对业务有价值的</p><a id="more"></a><p><strong>1.Giba的home credit 里的重复样本</strong></p><p>如果八月份参加过kaggle的home credit比赛的一定知道，在最后一周才加入比赛的GM Giba通过丰富的比赛经验找到了重复样本trick帮助onodera队伍拿到了比赛的第二名，而另一位GM raddar在最后八个小时加入比赛就直接发现了重复样本问题，并solo到了45名。今天就先来分析一下他们的思路，<a href="https://link.zhihu.com/?target=https%3A//www.kaggle.com/raddar/a-competition-without-a-leak-or-is-it" target="_blank" rel="noopener">A competition without a leak. Or is it?</a></p><p>其实说起来简单，在home credit比赛里，每个客户都有多达两百列特征，但是人的基本特征是不会变的，性别、年龄、生日、银行开户时间等等。打个比方，尽管北京有三千万人，但是如果两样本同一天生日，同一天领驾照，办身份证，同一天结婚，同一天生娃，那么从统计上来说，这两样本肯定就是一个人，而不必再核对身高血型样貌了。radder 就是仅用[DAYS_BIRTH,DAYS_EMPLOYED,DAYS_REGISTRATION,DAYS_ID_PUBLISH,CODE_GENDER,REGION_POPULATION_RELATIVE]六个维度的特征，就把数据集里的重复样本给找出来了，其实其它两百列特征是否一样已经不重要了。</p><p><strong>小结</strong>：虽然我们在比赛里这个称之为trick，但背后是统计学意义的，也可以应用在工作之中。</p><p><strong>2.活学活用</strong></p><p>在我们最近参加的一个关于通讯用户套餐的比赛里，我们的GM piupiu也提到了这个trick。因为出题方把用户的流量消费精确到了byte，金钱消费精确到了分，通讯时间精确到了秒，所以如果用户A上月花了45.32元，用了1545MB 457kb 123byte的流量，打了78分12秒的电话，用户B上月也花了45.32元，用了1545MB 457kb 123byte的流量，打了78分12秒的电话，别管这家通讯公司用户量有多大，A和B肯定是同一用户。所以在其他队伍还在用给定的全部特征判定重复样本的时候，我们用八个特征就可以判定重复样本，把test里和train重复的样本给挑出来了。</p><p><strong>吐槽：</strong>因而我们得到了比其他队伍更多test里的重复样本，然后定义了白名单（piupiu的职业习惯）。在我们代码开源后有一小撮萌新看不懂我们的代码不说，还觉得是piupiu把竞赛网站黑了拿到test的label（滑稽）</p><p><strong>3.进阶</strong></p><p>有人会说了，拿到了test里的重复的样本的label，是不是可以上一波分了～～</p><p>答案是：<strong>并不能。</strong>因为xgb/lgb是具备非常强大的拟合能力，你不刻意找这些重复样本，xgb/lgb也能给你学出来，通过比较你会发现test里的重复样本已经全部给你预测对了。在home credit里是有重复用户的时序信息在里面可以利用，但是在我们这个比赛及大部分比赛就用不上了。</p><p>回到这个比赛，重复样本达到了全部数据的15%，test里的重复样本都预测对了，说明模型把train/test之间的重复样本的特性全学会了，有时候xgb/lgb学习能力太强也不是好事。这时候我们的GM piupiu提到了，test其实可以分为重复样本（15%）和非重复样本两部分（85%）来看待，真正对业务有意义的是非重复样本，可是所有参赛者却把重复样本过拟合的很好，我们真的目的应该是为出题方提供一个有效预测非重复样本的模型，因此提议我们应该把train里的重复样本去掉。打个比方，<strong>valid数据包含重复的和不重复两部分，不去重的数据训练的模型需要1000轮才能达到最优，去重的模型之后500轮最优，那说明有500轮是在过拟合重复样本，但这对于不重复样本来说是一个严重的过拟合，伤害了模型的泛化能力</strong>。而这个通讯比赛的重复样本达到了15%,对模型伤害很大。如果再根据包含重复样本的数据模型调参和特征工程，伤害就更大了。</p><p>简单的打个比方：</p><p>不去重的数据训练的模型 = 80%的非重复样本准确率 + 100%重复样本准确率</p><p>去重的数据训练的模型 = 85%的非重复样本准确率 + 85%重复样本准确率（直接规则覆盖后100%）</p><p>如果你要模型上线，你会上线哪个模型？</p><p>重复样本的准确率并不重要，无论是实际业务还是比赛，一个sql就搞定了。</p><p>根据piupiu的建议，于是我照着做了，果然模型变得特别稳定，也没有其他队伍所提到的抖动也消除了，提升的分数刚好是我们最后领先第二名的差距。</p><p>所以在这个比赛里，<strong>深度提取重复样本的真正目的是删除它们以提高模型预测非重复样本的准确率，非重复样本的准确率才是对业务有价值的。</strong></p><p>有些选手因为看到test里有重复样本就舍不得删的train里的重复样本，难道不用lgb预测的结果就不是结果了吗（流汗）</p><p><strong>小结：</strong>虽然第二节一直在说怎么更多的提取重复样本，但我们真正提升的是模型预测非重复样本的预测能力。</p><p><strong>吐槽：</strong>在我们代码开源后，有一小撮萌新看到我们在lgb输出的结果通过规则覆盖能上好多分，就觉得我们是在用重复样本的leak提升重复样本准确率，却不想想大家既然都能100%预测重复样本的情况下，差距在哪里…</p><p><strong>最后的吐槽：</strong>第一次代码开源就体会了当年plantgo开源携程代码的蛋疼。<a href="https://www.zhihu.com/question/64350623" target="_blank" rel="noopener">如何看待携程举办的大数据比赛？</a> 分享代码有一小撮萌新看不懂学不会清洗重复数据这种常规操作也就算了，还根据自己对代码自己的理解莫名其妙揣测，写本文章主要是帮助我们队友piupiu辟谣，顺便也给大家分享了点有价值的干货。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不去重的数据训练的模型 = 80%的非重复样本准确率 + 100%重复样本准确率&lt;br&gt;去重的数据训练的模型 = 85%的非重复样本准确率 + 85%重复样本准确率（直接规则覆盖后100%）&lt;br&gt;深度提取重复样本的真正目的是删除它们以提高模型预测非重复样本的准确率，非重复样本的准确率才是对业务有价值的&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>异常值检测</title>
    <link href="http://kodgv.xyz/2019/04/29/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B/"/>
    <id>http://kodgv.xyz/2019/04/29/竞赛经验/异常值检测/</id>
    <published>2019-04-29T07:12:19.000Z</published>
    <updated>2019-05-27T11:44:44.921Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/38066650" target="_blank" rel="noopener">https://www.zhihu.com/question/38066650</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/38066650&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.zhihu.com/question/38066650&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>缺失值处理</title>
    <link href="http://kodgv.xyz/2019/04/29/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/"/>
    <id>http://kodgv.xyz/2019/04/29/竞赛经验/缺失值处理/</id>
    <published>2019-04-29T02:52:21.000Z</published>
    <updated>2019-05-27T11:44:41.884Z</updated>
    
    <content type="html"><![CDATA[<p>数据缺失值处理</p><a id="more"></a><p>[TOC]</p><h2 id="数据丢失的原因。"><a href="#数据丢失的原因。" class="headerlink" title="数据丢失的原因。"></a>数据丢失的原因。</h2><ul><li><p>随机缺失(MAR):随机缺失意味着数据点缺失的倾向性与缺失的数据无关，而是与一些观察到的数据相关</p></li><li><p>完全随机缺失(MCAR):某个值缺失的事实与它的假设值以及其他变量的值无关。</p></li><li><p>非随机缺失(MNAR):两个可能的原因是,缺失值取决于假设的值(例如,工资高的人通常不愿透露他们的收入调查)或缺失值依赖于其他变量的值(例如假设女性一般不愿透露他们的年龄!此处年龄变量缺失值受性别变量影响)</p></li></ul><p>在前两种情况下，根据缺失值的出现情况删除缺失值的数据是安全的，而在第三种情况下，删除缺失值的观察值会在模型中产生偏差。所以在移除观测结果之前，我们必须非常小心。注意，归罪法不一定能给出更好的结果。</p><p><img src="/2019/04/29/竞赛经验/缺失值处理/1.png" alt></p><h4 id="删除"><a href="#删除" class="headerlink" title="　删除"></a>　删除</h4><p><strong>成列删除（listwise deletion）</strong></p><p>列表删除(完全案例分析)删除包含一个或多个缺失值的所有数据。特别是如果缺少的数据仅限于少量的观察，您可以选择从分析中删除这些情况。然而，在大多数情况下，使用列表删除通常是不利的。这是因为MCAR的假设(完全随机缺失)通常很少得到支持。因此，列表删除方法产生有偏差的参数和估计。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">newdata &lt;- na.omit(mydata)</span><br><span class="line"><span class="comment"># In python</span></span><br><span class="line">mydata.dropna(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>成对删除（pairwise deletion）</strong></p><p>一般的备选方案，在进行多变量的联立时，只删除掉需要执行的变量的缺失数据。例如在ABC三个变量间，需要计算A和C的协方差，那么只有同时具备A/C的数据会被使用。</p><p>文献指出，当变量间的相关性普遍较低时，成对删除会产生更有效的估计值。然而当变量间的相关性较高时，建议还是使用成列删除。</p><p>理论上成对删除不建议作为成列删除的备选方案。</p><p><strong>虚拟变量调整（哑变量，dummy variables）</strong></p><p>新建两个变量，其中一个变量D为“是否缺失”，缺失值设为0，存在值设为1。</p><p>另一个变量X’，将缺失值设为c（可以是任何常数），存在值设为本身。</p><p>随后，对X’，D和其他变量（因变量和其他预设模型中的自变量）进行回归。这种调整的好处是它利用了所有可用的缺失数据的信息（是否缺失）。为了便利，一个好的c的设置方式是现有非缺失数据X的均数。</p><p>这样做的好处是，D的系数可以被解释成“在控制了其他变量的情况下，X具缺失数据的个体其Y的预测值减去具X平均数的个体于Y的预测值”</p><h4 id="Time-Series-Specific-Methods"><a href="#Time-Series-Specific-Methods" class="headerlink" title="Time-Series Specific Methods"></a>Time-Series Specific Methods</h4><p><strong>Last Observation Carried Forward (LOCF) &amp; Next Observation Carried Backward (NOCB)</strong></p><p><strong>Linear Interpolation</strong></p><p><strong>Seasonal Adjustment + Linear Interpolation</strong></p><h4 id="Mean-Median-and-Mode"><a href="#Mean-Median-and-Mode" class="headerlink" title="Mean, Median and Mode"></a>Mean, Median and Mode</h4><p>计算总体均值、中值或模态是一种非常基本的推算方法，但它没有利用时间序列特征，也没有使用变量之间的关系。它非常快，但有明显的缺点。缺点之一是平均输入减少了数据集中的方差。</p><h4 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h4><p>首先，使用相关矩阵标识值缺失的变量的几个预测器。在回归方程中，选取最优预测因子作为自变量。缺少数据的变量作为因变量。采用预测变量数据完整的案例生成回归方程;然后，该方程用于预测不完全情况下的缺失值。在迭代过程中，插入缺失变量的值，然后用所有的情况来预测因变量。重复这些步骤，直到预测值之间的差值很小，即它们收敛。</p><p>它“理论上”为缺失的值提供了很好的估计。然而，这种模式有几个缺点，往往超过了优点。首先，由于替换后的值是由其他变量预测的，它们往往“太好”地匹配在一起，因此标准误差被缩小了。我们还必须假设回归方程中使用的变量之间存在线性关系，而回归方程中可能没有线性关系。</p><h4 id="Multiple-Imputation"><a href="#Multiple-Imputation" class="headerlink" title="Multiple Imputation"></a>Multiple Imputation</h4><ol><li><strong>Imputation</strong>: Impute the missing entries of the incomplete data sets <em>m</em>times (<em>m</em>=3 in the figure). Note that imputed values are drawn from a distribution. Simulating random draws doesn’t include uncertainty in model parameters. Better approach is to use Markov Chain Monte Carlo (MCMC) simulation. This step results in m complete data sets.</li><li><strong>Analysis</strong>: Analyze each of the <em>m</em> completed data sets.</li><li><strong>Pooling</strong>: Integrate the <em>m</em> analysis results into a final result</li></ol><p><img src="/2019/04/29/竞赛经验/缺失值处理/2.png" alt></p><p>This is by far the most preferred method for imputation for the following reasons:</p><ul><li>Easy to use</li><li>No biases (if imputation model is correct)</li></ul><p>实现的代码包:fancyimpute</p><h4 id="Imputation-of-Categorical-Variables"><a href="#Imputation-of-Categorical-Variables" class="headerlink" title="Imputation of Categorical Variables"></a>Imputation of Categorical Variables</h4><p>把缺失的值作为一个类别进行填补</p><h4 id="KNN-K-Nearest-Neighbors"><a href="#KNN-K-Nearest-Neighbors" class="headerlink" title="KNN (K Nearest Neighbors)"></a>KNN (K Nearest Neighbors)</h4><p> XGBoost and Random Forest 同样也有 data imputation</p><p>该方法基于距离测度选取k个邻域，并以邻域的平均值作为估计的归一化方法。该方法需要选择最近邻的数目和距离度量。KNN既可以预测离散属性(k个近邻中最频繁的值)，也可以预测连续属性(k个近邻中均值)</p><p>距离度量根据数据的类型而变化:</p><ol><li><p>连续数据:连续数据常用的距离度量是欧式、Manhattan和cos</p></li><li><p>分类数据:本例中一般使用汉明距离。它接受所有的分类属性，如果两个点之间的值不相同，则对每个属性进行计数。然后，汉明距离等于值不同的属性的数量。</p></li></ol><p>KNN算法最吸引人的特点之一是易于理解和实现。KNN的非参数特性使其在某些数据可能非常“不寻常”的情况下具有优势。</p><p>KNN算法的一个明显缺点是，在分析大型数据集时非常耗时，因为它在整个数据集中搜索类似的实例。此外，由于最近邻和最近邻之间的距离相差不大，高维数据会严重降低KNN的精度</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据缺失值处理&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>胶囊网络</title>
    <link href="http://kodgv.xyz/2019/04/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C/"/>
    <id>http://kodgv.xyz/2019/04/27/神经网络/胶囊网络/</id>
    <published>2019-04-27T08:30:18.000Z</published>
    <updated>2019-05-25T12:38:37.579Z</updated>
    
    <content type="html"><![CDATA[<p>胶囊网络</p><a id="more"></a><p>来源:<a href="https://spaces.ac.cn/archives/4819" target="_blank" rel="noopener">https://spaces.ac.cn/archives/4819</a></p><p>直接看上面的这个文章，描述的非常详细</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;胶囊网络&lt;/p&gt;
    
    </summary>
    
      <category term="神经网络" scheme="http://kodgv.xyz/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="网络结构" scheme="http://kodgv.xyz/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>FocalLoss针对不平衡数据</title>
    <link href="http://kodgv.xyz/2019/04/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/FocalLoss%E9%92%88%E5%AF%B9%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE/"/>
    <id>http://kodgv.xyz/2019/04/22/神经网络/FocalLoss针对不平衡数据/</id>
    <published>2019-04-22T10:32:49.000Z</published>
    <updated>2019-05-25T12:37:58.417Z</updated>
    
    <content type="html"><![CDATA[<p>Focal loss 一种特别的损失函数，其特点为专注于那些无法分辨的样本</p><a id="more"></a><p>[TOC]</p><p>参考来源:</p><p><a href="https://zhuanlan.zhihu.com/p/32423092" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32423092</a></p><p><a href="https://www.zhihu.com/question/63581984" target="_blank" rel="noopener">https://www.zhihu.com/question/63581984</a></p><p><a href="https://zhuanlan.zhihu.com/p/28527749" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28527749</a></p><h2 id="讲解"><a href="#讲解" class="headerlink" title="讲解"></a>讲解</h2><p>​    本质上讲，Focal Loss 就是一个解决<strong>分类问题中类别不平衡、分类难度差异</strong>的一个 loss，总之这个工作一片好评就是了。大家还可以看知乎的讨论：<a href="https://www.zhihu.com/question/63581984" target="_blank" rel="noopener">如何评价 Kaiming 的 Focal Loss for Dense Object Detection？</a></p><p><strong>核心思想</strong></p><p>这样的做法就是：<strong>正样本的预测值大于 0.5 的，或者负样本的预测值小于 0.5 的，我都不更新了，把注意力集中在预测不准的那些样本，当然这个阈值可以调整。</strong>这样做能部分地达到目的，但是所需要的迭代次数会大大增加。</p><p>原因是这样的：以正样本为例，<strong>我只告诉模型正样本的预测值大于 0.5 就不更新了，却没有告诉它要“保持”大于 0.5</strong>，所以下一阶段，它的预测值就很有可能变回小于 0.5 了。当然，如果是这样的话，下一回合它又被更新了，这样反复迭代，理论上也能达到目的，但是迭代次数会大大增加。</p><p>所以，要想改进的话，重点就是<strong>“不只是要告诉模型正样本的预测值大于0.5就不更新了，而是要告诉模型当其大于0.5后就只需要保持就好了”</strong>。好比老师看到一个学生及格了就不管了，这显然是不行的。如果学生已经及格，那么应该要想办法要他保持目前这个状态甚至变得更好，而不是不管。</p><p>所以除了单纯的区分外，必须使该loss可导，这样才可以告诉模型。</p><p><strong>目的是通过减少易分类样本的权重，从而使得模型在训练时更专注于难分类的样本</strong>。</p><p>Kaiming 大神的 Focal Loss 形式是：</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f1.jpg" alt></p><p>如果落实到 <em>ŷ =σ(x)</em> 这个预测，那么就有：</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f3.jpg" alt="img"></p><p>特别地，<strong>如果</strong> <strong>K</strong> <strong>和</strong> <strong>γ</strong> <strong>都取 1，那么</strong> <strong>L∗∗=Lfl</strong>。</p><p>事实上 <em>K</em> 和 <em>γ</em> 的作用都是一样的，都是调节权重曲线的陡度，只是调节的方式不一样。注意<em>L∗∗</em>或 <em>Lfl</em> 实际上都已经包含了对不均衡样本的解决方法，或者说，类别不均衡本质上就是分类难度差异的体现。</p><p>​    首先y’的范围是0到1，所以不管γ是多少，这个调制系数都是大于等于0的。易分类的样本再多，你的权重很小，那么对于total loss的共享也就不会太大。那么怎么控制样本权重呢？举个例子，假设一个二分类，样本x1属于类别1的y’=0.9，样本x2属于类别1的y’=0.6，显然前者更可能是类别1，假设γ=1，那么对于y’=0.9，调制系数则为0.1；对于y’=0.6，调制系数则为0.4，这个调制系数就是这个样本对loss的贡献程度，也就是权重，所以难分的样本（pt=0.6）的权重更大。</p><p>​    <strong>比如负样本远比正样本多的话，模型肯定会倾向于数目多的负类（可以想象全部样本都判为负类），这时候，负类的</strong> <strong>*ŷ γ*</strong> <strong>或</strong> <strong>σ(Kx) 都很小，而正类的</strong> <strong>(1−ŷ )γ</strong> <strong>或</strong> <strong>*σ(−Kx)*</strong> <strong>就很大，这时候模型就会开始集中精力关注正样本。</strong></p><p>当然，Kaiming 大神还发现对 <em>Lfl</em> 做个权重调整，结果会有微小提升。</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f2.jpg" alt="img"></p><p>通过一系列调参，得到 <em>α=0.25, γ=2</em>（在他的模型上）的效果最好。注意在他的任务中，正样本是属于少数样本，也就是说，本来正样本难以“匹敌”负样本，但经过 <em>(1−ŷ )γ</em> 和 <em>ŷγ</em> 的“操控”后，也许形势还逆转了，还要对正样本降权。</p><p>不过我认为这样调整只是经验结果，理论上很难有一个指导方案来决定 <em>α</em> 的值，如果没有大算力调参，倒不如直接让 <em>α=0.5</em>（均等）。</p><h2 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a><strong>多分类</strong></h2><p>Focal Loss 在多分类中的形式也很容易得到，其实就是：</p><p><img src="/2019/04/22/神经网络/FocalLoss针对不平衡数据/f4.jpg" alt="img"></p><p><em>ŷt</em> 是目标的预测值，一般就是经过 softmax 后的结果。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>多分类</p><p><a href="https://github.com/marvis/pytorch-yolo2/blob/master/FocalLoss.py" target="_blank" rel="noopener">https://github.com/marvis/pytorch-yolo2/blob/master/FocalLoss.py</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">r"""</span></span><br><span class="line"><span class="string">        This criterion is a implemenation of Focal Loss, which is proposed in </span></span><br><span class="line"><span class="string">        Focal Loss for Dense Object Detection.</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            Loss(x, class) = - \alpha (1-softmax(x)[class])^gamma \log(softmax(x)[class])</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        The losses are averaged across observations for each minibatch.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            alpha(1D Tensor, Variable) : the scalar factor for this criterion</span></span><br><span class="line"><span class="string">            gamma(float, double) : gamma &gt; 0; reduces the relative loss for well-classiﬁed examples (p &gt; .5), </span></span><br><span class="line"><span class="string">                                   putting more focus on hard, misclassiﬁed examples</span></span><br><span class="line"><span class="string">            size_average(bool): size_average(bool): By default, the losses are averaged over observations for each minibatch.</span></span><br><span class="line"><span class="string">                                However, if the field size_average is set to False, the losses are</span></span><br><span class="line"><span class="string">                                instead summed for each minibatch.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, class_num, alpha=None, gamma=<span class="number">2</span>, size_average=True)</span>:</span></span><br><span class="line">        super(FocalLoss, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> alpha <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.alpha = Variable(torch.ones(class_num, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> isinstance(alpha, Variable):</span><br><span class="line">                self.alpha = alpha</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.alpha = Variable(alpha)</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.class_num = class_num</span><br><span class="line">        self.size_average = size_average</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets)</span>:</span></span><br><span class="line">        N = inputs.size(<span class="number">0</span>)</span><br><span class="line">        print(N)</span><br><span class="line">        C = inputs.size(<span class="number">1</span>)</span><br><span class="line">        P = F.softmax(inputs)</span><br><span class="line"><span class="comment"># 这是为了获取onehot</span></span><br><span class="line">        class_mask = inputs.data.new(N, C).fill_(<span class="number">0</span>)</span><br><span class="line">        class_mask = Variable(class_mask)</span><br><span class="line">        ids = targets.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        class_mask.scatter_(<span class="number">1</span>, ids.data, <span class="number">1.</span>)</span><br><span class="line">        <span class="comment">#print(class_mask)</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs.is_cuda <span class="keyword">and</span> <span class="keyword">not</span> self.alpha.is_cuda:</span><br><span class="line">            self.alpha = self.alpha.cuda()</span><br><span class="line">        alpha = self.alpha[ids.data.view(<span class="number">-1</span>)]</span><br><span class="line">        </span><br><span class="line">        probs = (P*class_mask).sum(<span class="number">1</span>).view(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        log_p = probs.log()</span><br><span class="line">        <span class="comment">#print('probs size= &#123;&#125;'.format(probs.size()))</span></span><br><span class="line">        <span class="comment">#print(probs)</span></span><br><span class="line"></span><br><span class="line">        batch_loss = -alpha*(torch.pow((<span class="number">1</span>-probs), self.gamma))*log_p </span><br><span class="line">        <span class="comment">#print('-----bacth_loss------')</span></span><br><span class="line">        <span class="comment">#print(batch_loss)</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.size_average:</span><br><span class="line">            loss = batch_loss.mean()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = batch_loss.sum()</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    alpha = torch.rand(<span class="number">21</span>, <span class="number">1</span>)</span><br><span class="line">    print(alpha)</span><br><span class="line">    FL = FocalLoss(class_num=<span class="number">5</span>, gamma=<span class="number">0</span> )</span><br><span class="line">    CE = nn.CrossEntropyLoss()</span><br><span class="line">    N = <span class="number">4</span></span><br><span class="line">    C = <span class="number">5</span></span><br><span class="line">    inputs = torch.rand(N, C)</span><br><span class="line">    targets = torch.LongTensor(N).random_(C)</span><br><span class="line">    inputs_fl = Variable(inputs.clone(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    targets_fl = Variable(targets.clone())</span><br><span class="line"></span><br><span class="line">    inputs_ce = Variable(inputs.clone(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    targets_ce = Variable(targets.clone())</span><br><span class="line">    print(<span class="string">'----inputs----'</span>)</span><br><span class="line">    print(inputs)</span><br><span class="line">    print(<span class="string">'---target-----'</span>)</span><br><span class="line">    print(targets)</span><br><span class="line"></span><br><span class="line">    fl_loss = FL(inputs_fl, targets_fl)</span><br><span class="line">    ce_loss = CE(inputs_ce, targets_ce)</span><br><span class="line">    print(<span class="string">'ce = &#123;&#125;, fl =&#123;&#125;'</span>.format(ce_loss.data[<span class="number">0</span>], fl_loss.data[<span class="number">0</span>]))</span><br><span class="line">    fl_loss.backward()</span><br><span class="line">    ce_loss.backward()</span><br><span class="line">    <span class="comment">#print(inputs_fl.grad.data)</span></span><br><span class="line">    print(inputs_ce.grad.data)</span><br></pre></td></tr></table></figure><p>单分类</p><p><a href="https://www.kaggle.com/aakashnain/diving-deep-into-focal-loss" target="_blank" rel="noopener">https://www.kaggle.com/aakashnain/diving-deep-into-focal-loss</a></p><p><a href="https://www.kaggle.com/sfzero/focal-loss-feature-0-99994/comments" target="_blank" rel="noopener">https://www.kaggle.com/sfzero/focal-loss-feature-0-99994/comments</a></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, alpha=<span class="number">1</span>, gamma=<span class="number">2</span>, logits=True, reduction=<span class="string">'elementwise_mean'</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>(FocalLoss, <span class="keyword">self</span>).__init_<span class="number">_</span>()</span><br><span class="line">        <span class="keyword">self</span>.alpha = alpha</span><br><span class="line">        <span class="keyword">self</span>.gamma = gamma</span><br><span class="line">        <span class="keyword">self</span>.logits = logits</span><br><span class="line">        <span class="keyword">self</span>.reduction = reduction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, inputs, targets)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">logits:</span></span><br><span class="line">            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=<span class="string">'none'</span>)</span><br><span class="line">        <span class="symbol">else:</span></span><br><span class="line">            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction=<span class="string">'none'</span>)</span><br><span class="line">        pt = torch.exp(-BCE_loss)</span><br><span class="line">        F_loss = <span class="keyword">self</span>.alpha * (<span class="number">1</span>-pt)**<span class="keyword">self</span>.gamma * BCE_loss</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.reduction is <span class="symbol">None:</span></span><br><span class="line">            <span class="keyword">return</span> F_loss</span><br><span class="line">        <span class="symbol">else:</span></span><br><span class="line">            <span class="keyword">return</span> torch.mean(F_loss)</span><br></pre></td></tr></table></figure><p>简单版代码</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/83363#486607</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">staticWeightLoss</span><span class="params">(<span class="literal">true</span>,pred)</span></span><span class="symbol">:</span></span><br><span class="line">    loss = K.binary_crossentropy(<span class="literal">true</span>, pred)</span><br><span class="line">    positiveLoss = positiveWeights * loss</span><br><span class="line">    negativeLoss = negativeWeights * loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> K.switch(K.greater(<span class="literal">true</span>, <span class="number">0</span>.<span class="number">5</span>), positiveLoss, negativeLoss)</span><br></pre></td></tr></table></figure><p>!要注意softmax是要有两列以上，sigmod才是一列</p><h2 id="引申"><a href="#引申" class="headerlink" title="引申"></a>引申</h2><p>这就是为什么之前别人做数据增强的时候，把预测很高的数据当作1把预测很低的数据当作0放进去加强训练。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Focal loss 一种特别的损失函数，其特点为专注于那些无法分辨的样本&lt;/p&gt;
    
    </summary>
    
      <category term="神经网络" scheme="http://kodgv.xyz/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="损失函数" scheme="http://kodgv.xyz/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>比赛心得集合</title>
    <link href="http://kodgv.xyz/2019/04/21/%E6%AF%94%E8%B5%9B%E5%BF%83%E5%BE%97%E9%9B%86%E5%90%88/"/>
    <id>http://kodgv.xyz/2019/04/21/比赛心得集合/</id>
    <published>2019-04-21T06:45:42.000Z</published>
    <updated>2019-05-25T12:47:23.531Z</updated>
    
    <content type="html"><![CDATA[<p>收集好的竞赛网站和NLP博客，方便借鉴和模仿</p><a id="more"></a><p>[TOC]</p><h1 id="比赛心得"><a href="#比赛心得" class="headerlink" title="比赛心得"></a>比赛心得</h1><p>机器翻译注意力机制及其PyTorch实现</p><p><a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/" target="_blank" rel="noopener">https://plmsmile.github.io/2017/10/12/Attention-based-NMT/</a></p><p>各个NLP模型实现</p><p><a href="http://www.zhongruitech.com/921029206.html" target="_blank" rel="noopener">http://www.zhongruitech.com/921029206.html</a></p><p>苏剑林大神博客</p><p><a href="https://spaces.ac.cn/category/Resources" target="_blank" rel="noopener">https://spaces.ac.cn/category/Resources</a></p><p>QuroaNLP分类比赛心得</p><p><a href="https://www.kaggle.com/c/quora-insincere-questions-classification" target="_blank" rel="noopener">Quroa 识别不良句子</a></p><p><a href="https://www.getit01.com/p20190314357550039/" target="_blank" rel="noopener">https://www.getit01.com/p20190314357550039/</a></p><p>腾讯广告大赛比赛心得</p><p><a href="https://zhuanlan.zhihu.com/p/38341881" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38341881</a></p><p>摩拜杯目的地预测比赛心得</p><p><a href="https://zhuanlan.zhihu.com/p/32151090" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32151090</a></p><h1 id="比赛"><a href="#比赛" class="headerlink" title="比赛"></a>比赛</h1><p>如何不过拟合:<a href="https://www.kaggle.com/c/dont-overfit-ii" target="_blank" rel="noopener">https://www.kaggle.com/c/dont-overfit-ii</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;收集好的竞赛网站和NLP博客，方便借鉴和模仿&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="比赛收集" scheme="http://kodgv.xyz/tags/%E6%AF%94%E8%B5%9B%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>NLP代码汇总</title>
    <link href="http://kodgv.xyz/2019/04/20/NLP%E4%BB%A3%E7%A0%81%E6%B1%87%E6%80%BB/"/>
    <id>http://kodgv.xyz/2019/04/20/NLP代码汇总/</id>
    <published>2019-04-20T13:57:19.000Z</published>
    <updated>2019-04-27T07:13:55.279Z</updated>
    
    <content type="html"><![CDATA[<p>NLP汇总</p><a id="more"></a><p>[TOC]</p><h2 id="动态padding，节省时间"><a href="#动态padding，节省时间" class="headerlink" title="动态padding，节省时间"></a>动态padding，节省时间</h2><p>比起一开始固定化padding，动态padding，可以针对当前batch的长度来Padding，明显会增快速度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, text, lens, y=None)</span>:</span></span><br><span class="line">        self.text = text</span><br><span class="line">        self.y = y</span><br><span class="line">        self.lens = lens</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.lens)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self.text[index], self.lens[index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.text[index], self.lens[index], self.y[index]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(batch)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    batch = [dataset[i] for i in N]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    size = len(batch[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> size == <span class="number">3</span>:</span><br><span class="line">        texts, lens, y = zip(*batch)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        texts, lens = zip(*batch)</span><br><span class="line">    lens = np.array(lens)</span><br><span class="line">    sort_idx = np.argsort(<span class="number">-1</span> * lens)</span><br><span class="line">    reverse_idx = np.argsort(sort_idx)</span><br><span class="line">    max_len = min(int(np.percentile(lens, PERCENTILE)), MAX_LEN)</span><br><span class="line">    </span><br><span class="line">    lens = np.clip(lens, <span class="number">0</span>, max_len)[sort_idx]</span><br><span class="line">    texts = torch.tensor(sequence.pad_sequences(texts, maxlen=max_len)[sort_idx], dtype=torch.long).cuda()</span><br><span class="line">    <span class="keyword">if</span> size == <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">return</span> texts, lens, reverse_idx, torch.tensor(y, dtype=torch.float32).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> texts, lens, reverse_idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_data_loader</span><span class="params">(texts, lens, y=None, batch_size=BATCH_SIZE)</span>:</span></span><br><span class="line">    dset = MyDataset(texts, lens, y)</span><br><span class="line">    dloader = data.DataLoader(dset, batch_size=batch_size, shuffle=<span class="literal">False</span>, collate_fn=collate_fn)</span><br><span class="line">    <span class="keyword">return</span> dloader</span><br><span class="line">seqs = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>]]</span><br><span class="line">lens = [len(i) <span class="keyword">for</span> i <span class="keyword">in</span> seqs]</span><br><span class="line"></span><br><span class="line">data_loader = build_data_loader(seqs, lens)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data_loader:</span><br><span class="line">    seq_batch, lens_batch, reverse_idx_batch = batch</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">print(<span class="string">f'original seqs:'</span>)</span><br><span class="line">print(seqs)</span><br><span class="line">print(<span class="string">f'batch seqs, already sort by lens, and padding dynamic in batch:'</span>)</span><br><span class="line">print(seq_batch)</span><br><span class="line">print(<span class="string">f'reverse batch seqs:'</span>)</span><br><span class="line">print(seq_batch[reverse_idx_batch])</span><br><span class="line">h_embedding_pack = pack_padded_sequence(seq_batch,lens_batch,batch_first=<span class="literal">True</span>)</span><br><span class="line">print(h_embedding_pack)</span><br></pre></td></tr></table></figure><h2 id="mask-loss-避免无用结果的求导影响"><a href="#mask-loss-避免无用结果的求导影响" class="headerlink" title="mask loss 避免无用结果的求导影响"></a>mask loss 避免无用结果的求导影响</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> func</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CustomLoss,self).__init__()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, Y_hat, Y)</span>:</span></span><br><span class="line">       <span class="comment"># TRICK 3 ********************************</span></span><br><span class="line">       <span class="comment"># before we calculate the negative log likelihood, we need to mask out the activations</span></span><br><span class="line">       <span class="comment"># this means we don't want to take into account padded items in the output vector</span></span><br><span class="line">       <span class="comment"># simplest way to think about this is to flatten ALL sequences into a REALLY long sequence</span></span><br><span class="line">       <span class="comment"># and calculate the loss on that.</span></span><br><span class="line">       <span class="comment"># flatten all the labels</span></span><br><span class="line">        Y = Y.view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># flatten all predictions</span></span><br><span class="line">        Y_hat = Y_hat.view(<span class="number">-1</span>, self.nb_tags)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># create a mask by filtering out all tokens that ARE NOT the padding token</span></span><br><span class="line">        tag_pad_token = self.tags[<span class="string">'&lt;PAD&gt;'</span>]</span><br><span class="line">        mask = (Y &gt; tag_pad_token).float()</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># count how many tokens we have</span></span><br><span class="line">        nb_tokens = int(torch.sum(mask).data[<span class="number">0</span>])</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># pick the values for the label and zero out the rest with the mask</span></span><br><span class="line">        Y_hat = Y_hat[range(Y_hat.shape[<span class="number">0</span>]), Y] * mask</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># compute cross entropy loss which ignores all &lt;PAD&gt; tokens</span></span><br><span class="line">        ce_loss = -torch.sum(Y_hat) / nb_tokens</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ce_loss</span><br></pre></td></tr></table></figure><h2 id="问题解答专区"><a href="#问题解答专区" class="headerlink" title="问题解答专区"></a>问题解答专区</h2><h3 id="为什么LSTM不同batch的句子长度可以不一致"><a href="#为什么LSTM不同batch的句子长度可以不一致" class="headerlink" title="为什么LSTM不同batch的句子长度可以不一致"></a>为什么LSTM不同batch的句子长度可以不一致</h3><p>LSTM只要保证每个batch的句子长度一致即可，所以可以使用动态padding,但是为什么不同batch的长度可以不一样？</p><p>因为LSTM需要更新的参数是共享，就是每个状态的权重是共享的，所以就无所谓不同长度了。只要保证每个batch内能够更新权重即可。</p><h3 id="深度学习中-number-of-training-epochs-中的-epoch到底指什么？"><a href="#深度学习中-number-of-training-epochs-中的-epoch到底指什么？" class="headerlink" title="深度学习中 number of training epochs 中的 epoch到底指什么？"></a>深度学习中 number of training epochs 中的 epoch到底指什么？</h3><p>对于初学者来讲，有几个概念容易混淆：</p><p>（1）iteration：表示1次迭代（也叫training step），每次迭代更新1次网络结构的参数；</p><p>（2）batch-size：1次迭代所使用的样本量；</p><p>（3）epoch：1个epoch表示过了1遍训练集中的所有样本。</p><p>一次epoch=所有训练数据forward+backward后更新参数的过程。<br>一次iteration=[batch size]个训练数据forward+backward后更新参数过程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NLP汇总&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://kodgv.xyz/categories/NLP/"/>
    
    
      <category term="汇总" scheme="http://kodgv.xyz/tags/%E6%B1%87%E6%80%BB/"/>
    
  </entry>
  
  <entry>
    <title>python多进程</title>
    <link href="http://kodgv.xyz/2019/04/18/%E4%BB%A3%E7%A0%81%E5%9F%BA%E7%A1%80/python%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    <id>http://kodgv.xyz/2019/04/18/代码基础/python多进程/</id>
    <published>2019-04-18T01:46:38.000Z</published>
    <updated>2019-05-25T12:35:07.119Z</updated>
    
    <content type="html"><![CDATA[<p>python multiprocessing模块多进程详解</p><a id="more"></a><p>[TOC]</p><h2 id="multiprocessing模块API"><a href="#multiprocessing模块API" class="headerlink" title="multiprocessing模块API"></a>multiprocessing模块API</h2><p>Pool类用于需要执行的目标很多，而手动限制进程数量又太繁琐时，如果目标少且不用控制进程数量则可以用<a href="http://thief.one/2016/11/24/Multiprocessing-Process" target="_blank" rel="noopener">Process</a>类。</p><p>构造方法</p><ul><li>Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]])</li><li>processes ：使用的工作进程的数量，如果processes是None那么使用 os.cpu_count()返回的数量。</li><li>initializer： 如果initializer是None，那么每一个工作进程在开始的时候会调用initializer(*initargs)。</li><li>maxtasksperchild：工作进程退出之前可以完成的任务数，完成后用一个新的工作进程来替代原进程，来让闲置的资源被释放。maxtasksperchild默认是None，意味着只要Pool存在工作进程就会一直存活。</li><li>context: 用在制定工作进程启动时的上下文，一般使用 multiprocessing.Pool() 或者一个context对象的Pool()方法来创建一个池，两种方法都适当的设置了context。</li></ul><p>实例方法</p><ul><li>apply_async(func[, args[, kwds[, callback]]]) 它是非阻塞。</li><li>apply(func[, args[, kwds]])是阻塞的。</li><li>close() 关闭pool，使其不在接受新的任务。</li><li>terminate() 关闭pool，结束工作进程，不在处理未完成的任务。</li><li>join() 主进程阻塞，等待子进程的退出， join方法要在close或terminate之后使用。</li></ul><h3 id="Pool使用方法"><a href="#Pool使用方法" class="headerlink" title="Pool使用方法"></a>Pool使用方法</h3><p>Pool+map函数</p><p>说明：此写法缺点在于只能通过map向函数传递一个参数。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing import Pool</span><br><span class="line">def test(i):</span><br><span class="line">    <span class="builtin-name">print</span> i</span><br><span class="line"><span class="keyword">if</span> <span class="attribute">__name__</span>=="__main__":</span><br><span class="line">lists=[1,2,3]</span><br><span class="line"><span class="attribute">pool</span>=Pool(processes=2) #定义最大的进程数</span><br><span class="line">pool.map(test,lists)        #p必须是一个可迭代变量。</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure><p>异步进程池（非阻塞）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">pool = Pool(processes=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i  <span class="keyword">in</span> xrange(<span class="number">500</span>):</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">For循环中执行步骤：</span></span><br><span class="line"><span class="string">（1）循环遍历，将500个子进程添加到进程池（相对父进程会阻塞）</span></span><br><span class="line"><span class="string">（2）每次执行10个子进程，等一个子进程执行完后，立马启动新的子进程。（相对父进程不阻塞）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">apply_async为异步进程池写法。</span></span><br><span class="line"><span class="string">异步指的是启动子进程的过程，与父进程本身的执行（print）是异步的，而For循环中往进程池添加子进程的过程，与父进程本身的执行却是同步的。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">    pool.apply_async(test, args=(i,)) <span class="comment">#维持执行的进程总数为10，当一个进程执行完后启动一个新进程.       </span></span><br><span class="line"><span class="keyword">print</span> “test”</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure><p>执行顺序：For循环内执行了2个步骤，第一步：将500个对象放入进程池（阻塞）。第二步：同时执行10个子进程（非阻塞），有结束的就立即添加，维持10个子进程运行。（apply_async方法的会在执行完for循环的添加步骤后，直接执行后面的print语句，而apply方法会等所有进程池中的子进程运行完以后再执行后面的print语句）</p><p>注意：调用join之前，先调用close或者terminate方法，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束。</p><h2 id="多进程示例代码"><a href="#多进程示例代码" class="headerlink" title="多进程示例代码"></a>多进程示例代码</h2><p>纯建立Process<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#encoding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> os, time, random</span><br><span class="line"></span><br><span class="line"><span class="comment">#线程启动后实际执行的代码块</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">r1</span><span class="params">(process_name)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">print</span> process_name, os.getpid()     <span class="comment">#打印出当前进程的id</span></span><br><span class="line">        time.sleep(random.random())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">r2</span><span class="params">(process_name)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">print</span> process_name, os.getpid()     <span class="comment">#打印出当前进程的id</span></span><br><span class="line">        time.sleep(random.random())</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"main process run..."</span></span><br><span class="line">        p1 = Process(target=r1, args=(<span class="string">'process_name1'</span>, ))       <span class="comment">#target:指定进程执行的函数，args:该函数的参数，需要使用tuple,只需要不断往后加参数即可</span></span><br><span class="line">        p2 = Process(target=r2, args=(<span class="string">'process_name2'</span>, ))</span><br><span class="line">        </span><br><span class="line">        p1.start()    <span class="comment">#通过调用start方法启动进程，跟线程差不多。</span></span><br><span class="line">        p2.start()    <span class="comment">#但run方法在哪呢？待会说。。。</span></span><br><span class="line">        p1.join()     <span class="comment">#join方法也很有意思，寻思了一下午，终于理解了。待会演示。</span></span><br><span class="line">        p2.join()</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"main process runned all lines..."</span></span><br></pre></td></tr></table></figure></p><p>POOL池管理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(data, index, size)</span>:</span>  <span class="comment"># data 传入数据，index 数据分片索引，size进程数</span></span><br><span class="line">    size = math.ceil(len(data) / size)</span><br><span class="line">    start = size * index</span><br><span class="line">    end = (index + <span class="number">1</span>) * size <span class="keyword">if</span> (index + <span class="number">1</span>) * size &lt; len(data) <span class="keyword">else</span> len(data)</span><br><span class="line">    temp_data = data[start:end]</span><br><span class="line">    <span class="comment"># do something</span></span><br><span class="line">    <span class="keyword">return</span> data  <span class="comment"># 可以返回数据，在后面收集起来</span></span><br><span class="line"></span><br><span class="line">processor = <span class="number">40</span></span><br><span class="line">res = []</span><br><span class="line">p = Pool(processor)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(processor):</span><br><span class="line">    res.append(p.apply_async(run, args=(data, i, processor,)))</span><br><span class="line">    print(str(i) + <span class="string">' processor started !'</span>)</span><br><span class="line">p.close()</span><br><span class="line">p.join()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> res:</span><br><span class="line">    print(i.get())  <span class="comment"># 使用get获得多进程处理的结果</span></span><br></pre></td></tr></table></figure></p><h2 id="进程注意事项"><a href="#进程注意事项" class="headerlink" title="进程注意事项"></a>进程注意事项</h2><h3 id="进程之间内存独立"><a href="#进程之间内存独立" class="headerlink" title="进程之间内存独立"></a>进程之间内存独立</h3><p>多进程与多线程最大的不同在于，多进程的每一个进程都有一份变量的拷贝，进程之间的操作互不影响，Process进程创建时，子进程会将主进程的Process对象完全复制一份，这样在主进程和子进程各有一个 Process对象，但是p.start()启动的是子进程，主进程中的Process对象作为一个静态对象存在，不执行。</p><p>就是这个地方，要注意的是，对于多进程来说，它是不会复制内存的，所以不用担心main中的变量被复制，但是它会复制main函数以外的变量</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import multiprocessing</span><br><span class="line">import <span class="built_in">time</span></span><br><span class="line"></span><br><span class="line"><span class="literal">zero</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">def change_zero():</span><br><span class="line">    <span class="built_in">global</span> <span class="literal">zero</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        <span class="literal">zero</span> = <span class="literal">zero</span> + <span class="number">1</span></span><br><span class="line">        print(multiprocessing.current_process().name, <span class="literal">zero</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    p1 = multiprocessing.Process(target = change_zero)</span><br><span class="line">    p2 = multiprocessing.Process(target = change_zero)</span><br><span class="line">    p1.<span class="built_in">start</span>()</span><br><span class="line">    p2.<span class="built_in">start</span>()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.join()</span><br><span class="line">    print(<span class="literal">zero</span>)</span><br></pre></td></tr></table></figure><p>运行结果如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Process-1 1</span><br><span class="line">Process-1 2</span><br><span class="line">Process-1 3</span><br><span class="line">Process-2 1</span><br><span class="line">Process-2 2</span><br><span class="line">Process-2 3</span><br><span class="line">0</span><br></pre></td></tr></table></figure><h3 id="共享变量Queue"><a href="#共享变量Queue" class="headerlink" title="共享变量Queue"></a>共享变量Queue</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">下面我们要讨论第一种情况，如果真的要在两个进程之间共享变量需要怎么办</span><br><span class="line"></span><br><span class="line">队列</span><br><span class="line">这里介绍进程之间的第一种交流方式——队列。multiprocessing模块中提供了multiprocessing.Queue，它和Queue.Queue的区别在于，它里面封装了进程之间的数据交流，不同进程可以操作同一个multiprocessing.Queue。</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Queue</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addone</span><span class="params">(q)</span>:</span></span><br><span class="line">    q.put(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addtwo</span><span class="params">(q)</span>:</span></span><br><span class="line">    q.put(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    q = Queue()</span><br><span class="line">    p1 = Process(target=addone, args = (q, ))</span><br><span class="line">    p2 = Process(target=addtwo, args = (q, ))</span><br><span class="line">    p1.start()</span><br><span class="line">    p2.start()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.join()</span><br><span class="line">    print(q.get())</span><br><span class="line">    print(q.get())</span><br><span class="line">运行结果如下</span><br><span class="line"></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">这个队列是线程、进程安全的，即对队列的每一次修改中间不会被中断从而造成结果错误。</span><br></pre></td></tr></table></figure><h3 id="进程锁"><a href="#进程锁" class="headerlink" title="进程锁"></a>进程锁</h3><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">进程锁</span><br><span class="line">既然变量在进程之间可以共享了，那么同时操作一个变量导致的不安全也随之出现。同多线程一样，进程也是通过锁来解决，而且使用方法都和多线程里相同。</span><br><span class="line"><span class="built_in">lock</span> = multiprocessing.<span class="built_in">Lock</span>()</span><br><span class="line"><span class="built_in">lock</span>.acquire()</span><br><span class="line"><span class="built_in">lock</span>.release()</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">lock</span>:</span><br><span class="line">这些用法和功能都和多线程是一样的</span><br><span class="line">另外，multiprocessing.Semaphore Condition Event RLock也和多线程相同</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python multiprocessing模块多进程详解&lt;/p&gt;
    
    </summary>
    
    
      <category term="多进程" scheme="http://kodgv.xyz/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>CNN</title>
    <link href="http://kodgv.xyz/2019/04/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN/"/>
    <id>http://kodgv.xyz/2019/04/17/神经网络/CNN/</id>
    <published>2019-04-17T02:32:46.000Z</published>
    <updated>2019-04-27T14:57:16.609Z</updated>
    
    <content type="html"><![CDATA[<p>CNN学习</p><a id="more"></a><p>[TOC]</p><h1 id="CNN结构基础"><a href="#CNN结构基础" class="headerlink" title="CNN结构基础"></a>CNN结构基础</h1><p>首先，提出这样一个问题：识别一幅图片是包含有字母”X”还是字母”O”？</p><p>为了帮助指导你理解卷积神经网络，我们讲采用一个非常简化的例子：确定一幅图像是包含有”X”还是”O”？<br><img src="/2019/04/17/神经网络/CNN/p1" alt="img"><br>这个例子足够说明CNN背后的原理，同时它足够简单，能够避免陷入不必要的细节。</p><p>在CNN中有这样一个问题，就是每次给你一张图，你需要判断它是否含有”X”或者”O”。并且假设必须两者选其一，不是”X”就是”O”。理想的情况就像下面这个样子：<br><img src="/2019/04/17/神经网络/CNN/p2" alt="img"><br>标准的”X”和”O”，字母位于图像的正中央，并且比例合适，无变形</p><p>对于计算机来说，只要图像稍稍有一点变化，不是标准的，那么要解决这个问题还是不是那么容易的：<br><img src="/2019/04/17/神经网络/CNN/p3" alt="img"></p><p>计算机要解决上面这个问题，一个比较天真的做法就是先保存一张”X”和”O”的标准图像（就像前面给出的例子），然后将其他的新给出的图像来和这两张标准图像进行对比，看看到底和哪一张图更匹配，就判断为哪个字母。</p><p>但是这么做的话，其实是非常不可靠的，因为计算机还是比较死板的。在计算机的“视觉”中，一幅图看起来就像是一个二维的像素数组（可以想象成一个棋盘），每一个位置对应一个数字。在我们这个例子当中，像素值”1”代表白色，像素值”-1”代表黑色。<br><img src="/2019/04/17/神经网络/CNN/p4" alt="img"></p><p>当比较两幅图的时候，如果有任何一个像素值不匹配，那么这两幅图就不匹配，至少对于计算机来说是这样的。</p><p>对于这个例子，计算机认为上述两幅图中的白色像素除了中间的3*3的小方格里面是相同的，其他四个角上都不同：<br><img src="/2019/04/17/神经网络/CNN/p5" alt="img"></p><p>因此，从表面上看，计算机判别右边那幅图不是”X”，两幅图不同，得出结论：<br><img src="/2019/04/17/神经网络/CNN/p6" alt="img"></p><p>但是这么做，显得太不合理了。理想的情况下，我们希望，对于那些仅仅只是做了一些像平移，缩放，旋转，微变形等简单变换的图像，计算机仍然能够识别出图中的”X”和”O”。就像下面这些情况，我们希望计算机依然能够很快并且很准的识别出来：<br><img src="/2019/04/17/神经网络/CNN/p7" alt="img"></p><p>这也就是CNN出现所要解决的问题。</p><p>Features<br><img src="/2019/04/17/神经网络/CNN/p8" alt="img"></p><p>对于CNN来说，它是一块一块地来进行比对。它拿来比对的这个“小块”我们称之为Features（特征）。在两幅图中大致相同的位置找到一些粗糙的特征进行匹配，CNN能够更好的看到两幅图的相似性，相比起传统的整幅图逐一比对的方法。</p><p>每一个feature就像是一个小图（就是一个比较小的有值的二维数组）。不同的Feature匹配图像中不同的特征。在字母”X”的例子中，那些由对角线和交叉线组成的features基本上能够识别出大多数”X”所具有的重要特征。<br><img src="/2019/04/17/神经网络/CNN/p" alt="img"></p><p>这些features很有可能就是匹配任何含有字母”X”的图中字母X的四个角和它的中心。那么具体到底是怎么匹配的呢？如下：<br><img src="/2019/04/17/神经网络/CNN/p9" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p10" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p11" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p12" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p13" alt="img"></p><p>看到这里是不是有了一点头目呢。但其实这只是第一步，你知道了这些Features是怎么在原图上面进行匹配的。但是你还不知道在这里面究竟进行的是怎样的数学计算，比如这个下面3*3的小块到底干了什么？<br><img src="/2019/04/17/神经网络/CNN/p14" alt="img"><br>接下来就跟进介绍里面的数学操作，也就是我们常说的“卷积”操作。</p><p>卷积(Convolution)<br><img src="/2019/04/17/神经网络/CNN/p15" alt="img"><br>Convolution</p><p><img src="/2019/04/17/神经网络/CNN/p16" alt="img"><br>当给你一张新的图时，CNN并不能准确地知道这些features到底要匹配原图的哪些部分，所以它会在原图中每一个可能的位置进行尝试。这样在原始整幅图上每一个位置进行匹配计算，我们相当于<strong>把这个feature变成了一个过滤器</strong>。这个我们用来匹配的过程就被称为卷积操作，这也就是卷积神经网络名字的由来。</p><p>这个卷积操作背后的数学知识其实非常的简单。要计算一个feature和其在原图上对应的某一小块的结果，只需要简单地将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可。</p><p>如果两个像素点都是白色（也就是值均为1），那么1<em>1 = 1，如果均为黑色，那么(-1)</em>(-1) = 1。不管哪种情况，每一对能够匹配上的像素，其相乘结果为1。类似地，任何不匹配的像素相乘结果为-1。如果一个feature（比如n<em>n）内部所有的像素都和原图中对应一小块（n</em>n）匹配上了，那么它们对应像素值相乘再累加就等于n2，然后除以像素点总个数n2，结果就是1。同理，如果每一个像素都不匹配，那么结果就是-1。</p><p>具体过程如下：</p><p><img src="/2019/04/17/神经网络/CNN/p17" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p18" alt="img"><img src="https://img-blog.csdn.net/2018030618132177" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p19" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p20" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p21" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p22" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p23" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p24" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p25" alt="img"></p><p>对于中间部分，也是一样的操作：</p><p><img src="/2019/04/17/神经网络/CNN/p26" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p27" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p28" alt="img"><br><img src="https://img-blog.csdn.net/20180306181612616" alt="img"><br>最后整张图算完，大概就像下面这个样子：<br><img src="/2019/04/17/神经网络/CNN/p29" alt="img"><br>然后换用其他feature进行同样的操作，最后得到的结果就是这样了：<br><img src="/2019/04/17/神经网络/CNN/p30" alt="img"><br>为了完成我们的卷积，我们不断地重复着上述过程，将feature和图中每一块进行卷积操作。最后通过每一个feature的卷积操作，我们会得到一个新的二维数组。</p><p>这也可以理解为对原始图像进行过滤的结果，我们称之为feature map，它是每一个feature从原始图像中提取出来的“特征”。其中的值，越接近为1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。<br><img src="/2019/04/17/神经网络/CNN/p31" alt="img"><br>这样我们的原始图，经过不同feature的卷积操作就变成了一系列的feature map。我们可以很方便，直观地将这整个操作视为一个单独的处理过程。在CNN中，我们称之为卷积层(convolution layer)，这样你可能很快就会想到后面肯定还有其他的layer。没错，后面会提到。</p><p>我们可以将卷积层看成下面这个样子：<br><img src="/2019/04/17/神经网络/CNN/p32" alt="img"><br>因此可想而知，CNN其实做的操作也没什么复杂的。但是尽管我们能够以这一点篇幅就描述了CNN的工作，其内部的加法，乘法和除法操作的次数其实会增加地很快。从数学的角度来说，它们会随着图像的大小，每一个filter的大小和filter的数目呈线性增长。由于有这么多因素的影响，很容易使得这个问题的计算量变得相当的庞大，这也难怪很多微处理器制造商现在都在生产制造专业的芯片来跟上CNN计算的需求。</p><p>池化(Pooling)<br><img src="/2019/04/17/神经网络/CNN/p33" alt="img"><br>Pooling</p><p>CNN中使用的另一个有效的工具被称为“池化(Pooling)”。池化可以将一幅大的图像缩小，同时又保留其中的重要信息。池化背后的数学顶多也就是小学二年级水平。它就是将输入图像进行缩小，减少像素信息，只保留重要信息。通常情况下，池化都是2<em>2大小，比如对于max-pooling来说，就是取输入图像中2</em>2大小的块中的最大值，作为结果的像素值，相当于将原始图像缩小了4倍(注：同理，对于average-pooling来说，就是取2*2大小块的平均值作为结果的像素值)。</p><p>对于本文的这个例子，池化操作具体如下：</p><p><img src="/2019/04/17/神经网络/CNN/p34" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p35" alt="img"><br><img src="/2019/04/17/神经网络/CNN/p37" alt="img"></p><p>不足的外面补”0”：</p><p><img src="/2019/04/17/神经网络/CNN/p36" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p38" alt="img"><br>经过最大池化操作（比如2*2大小）之后，一幅图就缩小为原来的四分之一了：<br><img src="/2019/04/17/神经网络/CNN/p39" alt="img"><br>然后对所有的feature map执行同样的操作，得到如下结果：<br><img src="/2019/04/17/神经网络/CNN/p40" alt="img"><br>因为最大池化（max-pooling）保留了每一个小块内的最大值，所以它相当于保留了这一块最佳的匹配结果（因为值越接近1表示匹配越好）。这也就意味着它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。这也就能够看出，CNN能够发现图像中是否具有某种特征，而不用在意到底在哪里具有这种特征。这也就能够帮助解决之前提到的计算机逐一像素匹配的死板做法。</p><p>当对所有的feature map执行池化操作之后，相当于一系列输入的大图变成了一系列小图。同样地，我们可以将这整个操作看作是一个操作，这也就是CNN中的池化层(pooling layer)，如下：<br><img src="/2019/04/17/神经网络/CNN/p41" alt="img"><br>通过加入池化层，可以很大程度上减少计算量，降低机器负载。</p><p>Normalization<br>激活函数Relu (Rectified Linear Units)<br>这是一个很小但是很重要的操作，叫做Relu(Rectified Linear Units)，或者修正线性单元。它的数学公式也很简单:<br>f(x) = max(0, x)</p><p>对于输入的负值，输出全为0，对于正值，原样输出。关于其功能，更多详见这里。</p><p>下面我们看一下本文的离例子中relu激活函数具体操作：<br><img src="/2019/04/17/神经网络/CNN/p42" alt="img"><br><img src="/2019/04/17/神经网络/CNN/p43" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p44" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p45" alt="img"></p><p>最后，对整幅图操作之后，结果如下：<br><img src="/2019/04/17/神经网络/CNN/p46" alt="img"><br>同样地，在CNN中，我们这一系列操作视为一个操作，那么就得到Relu Layer，如下：<br><img src="/2019/04/17/神经网络/CNN/p47" alt="img"><br>Deep Learning<br>最后，我们将上面所提到的卷积，池化，激活放在一起，就是下面这个样子：<br><img src="/2019/04/17/神经网络/CNN/p48" alt="img"><br>然后，我们加大网络的深度，增加更多的层，就得到深度神经网络了：<br><img src="/2019/04/17/神经网络/CNN/p49" alt="img"><br>然后在不同的层，我们进行可视化，就可以看到本文开头提到的先验知识里面的结果了：<br><img src="/2019/04/17/神经网络/CNN/p50" alt="img"><br>全连接层(Fully connected layers)<br><img src="/2019/04/17/神经网络/CNN/p51" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p52" alt="img"><img src="/2019/04/17/神经网络/CNN/p53" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p54" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p55" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p56" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p57" alt="img"></p><p><img src="/2019/04/17/神经网络/CNN/p58" alt="img"></p><p>根据结果判定为”X”：<br><img src="/2019/04/17/神经网络/CNN/p59" alt="img"></p><p>在这个过程中，我们定义这一系列操作为”全连接层“(Fully connected layers)：<br><img src="/2019/04/17/神经网络/CNN/p60" alt="img"><br>全连接层也能够有很多个，如下：<br><img src="/2019/04/17/神经网络/CNN/p61" alt="img"><br>【综合上述所有结构】<br><img src="/2019/04/17/神经网络/CNN/p62" alt="img"></p><h1 id="CNN三大核心思想"><a href="#CNN三大核心思想" class="headerlink" title="CNN三大核心思想"></a><strong>CNN三大核心思想</strong></h1><p>卷积神经网络CNN的出现是为了解决MLP多层感知器全连接和梯度发散的问题。其引入三个核心思想：1.局部感知(local field)，2.权值共享(Shared Weights)，3.下采样(subsampling)。极大地提升了计算速度，减少了连接数量。</p><p><strong>2.1 局部感知</strong></p><p>形象地说，就是模仿你的眼睛，想想看，你在看东西的时候，目光是聚焦在一个相对很小的局部的吧？严格一些说，普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上，而在卷积神经网络中，每个隐层节点只连接到图像某个足够小局部的像素点上，从而大大减少需要训练的权值参数。</p><p>对于一个 1000∗1000 的输入图像而言，如果下一个隐藏层的神经元数目为 106 个，采用全连接则有 1000∗1000∗106=1012 个权值参数，如此数目巨大的参数几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中 10∗10的局部图像相连接，那么此时的权值参数数量为 10∗10∗106=108，将直接减少4个数量级。</p><p>如下图所示，左边是每个像素的全连接，右边是每行隔两个像素作为局部连接，因此在数量上，少了很多权值参数数量（每一条连接每一条线需要有一个权值参数，具体忘记了的可以回顾单个[神经元模型]。因此 局部感知就是：<br>通过卷积操作，把 全连接变成局部连接 ，因为多层网络能够抽取高阶统计特性， 即使网络为局部连接，由于格外的突触连接和额外的神经交互作用，也可以使网络在不十分严格的意义下获得一个全局关系。</p><p><img src="/2019/04/17/神经网络/CNN/p63.jpg" alt="img"></p><p>下面我们来详细看一下到底局部感知如何使 全连接变成局部连接，按照人工神经网络的方法，把输入图像的像素一字排开之后，每一个像素值就是一个神经元输入，需要对隐层或者输出层做全连接，如上图左侧所示。卷积神经网络引入卷积概念后，卷积核通过原图像，然后卷积核对原图像上符合卷积核大小的像素进行加权求和，每一次只是对符合卷积核的图像像素做卷积，这就是局部感知的概念，使 全连接变成局部连接。</p><p><img src="/2019/04/17/神经网络/CNN/p64.png" alt="img"></p><p><strong>2.2 权值共享</strong></p><p>尽管局部感知使计算量减少了几个数量级，但权重参数数量依然很多。能不能再进一步减少呢？方法就是权值共享。</p><p>权值共享：不同的图像或者同一张图像共用一个卷积核，减少重复的卷积核。同一张图像当中可能会出现相同的特征，共享卷积核能够进一步减少权值参数。</p><p>如下图所示，为了找到鸟嘴，一个激活函数A需要检测图像左侧有没有鸟嘴，另外一个激活函数B需要检测另外一张图像中间有没有类似的鸟嘴。其实，鸟嘴都可能具有同样的特征，只需要一个激活函数C就可以了，这个时候，就可以共享同样的权值参数（也就是卷积核）。</p><p><img src="/2019/04/17/神经网络/CNN/p65.png" alt="img"></p><p>如果使用了权值共享（共同使用一个卷积核），那么将可以大大减少卷积核的数量，加快运算速度。天下武功，唯快不破。</p><p><img src="/2019/04/17/神经网络/CNN/p66.png" alt="img"></p><p>举个栗子，在局部连接中隐藏层的每一个神经元连接的是一个 10∗10 的局部图像，因此有 10∗10 个权值参数，将这 10∗10 个权值参数共享给剩下的神经元，也就是说隐藏层中 106 个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10∗10 个权值参数（也就是卷积核(也称滤波器)的大小。</p><p>尽管只有这么少的参数，依旧有出色的性能。但是，这样仅提取了图像的一种特征，如果要多提取出一些特征，需要增加多个卷积核，不同的卷积核能够得到图像的不同映射下的特征，称之为 Feature Map。如果有100个卷积核，最终的权值参数也仅为 100∗100=104 个而已。另外，偏置参数b也是共享的，同一种滤波器共享一个。</p><p><strong>2.3 池化</strong></p><p>在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。</p><p>pooling的好处有什么？<br>\1. 这些统计特征能够有更低的维度，减少计算量。<br>\2. 不容易过拟合，当参数过多的时候很容易造成过度拟合。<br>\3. 缩小图像的规模，提升计算速度。</p><p>如下图所示，原图是一张500∗500 的图像，经过subsampling之后哦，变成了一张 250∗250 的图像。这样操作的好处非常明显，虽然经过权值共享和局部连接后的图像权值参数已经大大减少，但是对于计算量来说，还是非常巨大，需要消费很大的计算时间，于是为了进一步减少计算量，于是加入了subsampling这个概念，不仅仅使图像像素减少了， 同时也减少计算时间。</p><p><img src="/2019/04/17/神经网络/CNN/p67.png" alt="img"></p><p>举个栗子：以最大池化（Max Pooling）为例，1000×1000的图像经过10×10的卷积核卷积后，得到的是991×991的特征图，然后使用2×2的池化规模，即每4个点组成的小方块中，取最大的一个作为输出，最终得到的是496×496大小的特征图。</p><p>下采样，即池化，目的是减小特征图，池化规模一般为2×2。常用的池化方法有：</p><p>Pooling算法</p><p>最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。<br>均值池化（Mean Pooling）。取4个点的均值。<br>可训练池化。训练函数 f ，接受4个点为输入，出入1个点。</p><p>由于特征图的变长不一定是2的倍数，所以在边缘处理上也有两种方案：</p><p>保留边缘。将特征图的变长用0填充为2的倍数，然后再池化。<br>忽略边缘。将多出来的边缘直接省去。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CNN学习&lt;/p&gt;
    
    </summary>
    
      <category term="神经网络" scheme="http://kodgv.xyz/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="网络结构" scheme="http://kodgv.xyz/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
</feed>
