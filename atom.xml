<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小黑屋</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://kodgv.xyz/"/>
  <updated>2019-04-12T09:16:01.824Z</updated>
  <id>http://kodgv.xyz/</id>
  
  <author>
    <name>KODGV</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>比赛规范经验</title>
    <link href="http://kodgv.xyz/2019/04/12/%E6%AF%94%E8%B5%9B%E8%A7%84%E8%8C%83%E7%BB%8F%E9%AA%8C/"/>
    <id>http://kodgv.xyz/2019/04/12/比赛规范经验/</id>
    <published>2019-04-12T09:12:58.000Z</published>
    <updated>2019-04-12T09:16:01.824Z</updated>
    
    <content type="html"><![CDATA[<p>比赛中的代码规范,代码不规范，亲人两行泪</p><a id="more"></a><p>先杂七杂八写，有一定量了再整理</p><ul><li>思路要记在云笔记里，纸上会丢，而且换环境忘记带</li><li>临近比赛前一个月注意多保存看过的kernel，因为它们做出来之后就会删除。也就是说快结束的时候，那些都不是重要的magic</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛中的代码规范,代码不规范，亲人两行泪&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="代码规范" scheme="http://kodgv.xyz/tags/%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/"/>
    
  </entry>
  
  <entry>
    <title>特征构造</title>
    <link href="http://kodgv.xyz/2019/04/11/%E7%89%B9%E5%BE%81%E6%9E%84%E9%80%A0/"/>
    <id>http://kodgv.xyz/2019/04/11/特征构造/</id>
    <published>2019-04-11T02:47:02.000Z</published>
    <updated>2019-04-12T07:55:16.513Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p>函数统一返回新的df，同时删除原有df</p><h2 id="特征频数"><a href="#特征频数" class="headerlink" title="特征频数"></a>特征频数</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 构造特征时测试集要和训练集一样</span><br><span class="line">def encode_FE(df,<span class="built_in">col</span>,test)：</span><br><span class="line">    <span class="built_in">cv</span> = df[<span class="built_in">col</span>].value_counts()</span><br><span class="line">    <span class="built_in">nm</span> = <span class="built_in">col</span>+'_FE'</span><br><span class="line">    df[<span class="built_in">nm</span>] = df[<span class="built_in">col</span>].<span class="built_in">map</span>(<span class="built_in">cv</span>)</span><br><span class="line">    test[<span class="built_in">nm</span>] = test[<span class="built_in">col</span>].<span class="built_in">map</span>(<span class="built_in">cv</span>)</span><br><span class="line">    <span class="built_in">return</span> df,test</span><br></pre></td></tr></table></figure><h2 id="data-augment"><a href="#data-augment" class="headerlink" title="data augment"></a>data augment</h2><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def augment(x,y,t=<span class="number">2</span>):</span><br><span class="line">    xs,xn = [],[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(t):</span><br><span class="line">        mask = y&gt;<span class="number">0</span></span><br><span class="line">        x1 = x[mask].<span class="built_in">copy</span>()</span><br><span class="line">        ids = <span class="built_in">np</span>.arange(x1.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(x1.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="built_in">np</span>.<span class="built_in">random</span>.shuffle(ids)</span><br><span class="line">            x1[:,c] = x1[ids][:,c]</span><br><span class="line">        xs.<span class="built_in">append</span>(x1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(t//<span class="number">2</span>):</span><br><span class="line">        mask = y==<span class="number">0</span></span><br><span class="line">        x1 = x[mask].<span class="built_in">copy</span>()</span><br><span class="line">        ids = <span class="built_in">np</span>.arange(x1.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(x1.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="built_in">np</span>.<span class="built_in">random</span>.shuffle(ids)</span><br><span class="line">            x1[:,c] = x1[ids][:,c]</span><br><span class="line">        xn.<span class="built_in">append</span>(x1)</span><br><span class="line"></span><br><span class="line">    xs = <span class="built_in">np</span>.vstack(xs)</span><br><span class="line">    xn = <span class="built_in">np</span>.vstack(xn)</span><br><span class="line">    ys = <span class="built_in">np</span>.ones(xs.shape[<span class="number">0</span>])</span><br><span class="line">    yn = <span class="built_in">np</span>.zeros(xn.shape[<span class="number">0</span>])</span><br><span class="line">    x = <span class="built_in">np</span>.vstack([x,xs,xn])</span><br><span class="line">    y = <span class="built_in">np</span>.concatenate([y,ys,yn])</span><br><span class="line">    <span class="built_in">return</span> x,y</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;函数统一返回新的df，同时删除原有df&lt;/p&gt;
&lt;h2 id=&quot;特征频数&quot;&gt;&lt;a href=&quot;#特征频数&quot; class=&quot;headerlink&quot; title=&quot;特征频数&quot;&gt;&lt;/a&gt;特征频数&lt;/h2&gt;&lt;figure class=&quot;highlight 
      
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Santander Customer Transaction Prediction 比赛经验</title>
    <link href="http://kodgv.xyz/2019/04/11/Santander%20Customer%20Transaction%20Prediction%20%E6%AF%94%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    <id>http://kodgv.xyz/2019/04/11/Santander Customer Transaction Prediction 比赛经验/</id>
    <published>2019-04-11T02:46:10.000Z</published>
    <updated>2019-04-12T08:33:02.380Z</updated>
    
    <content type="html"><![CDATA[<p>Santander Customer Transaction Prediction 比赛，01分类问题，200维连续型匿名变量。</p><p>讲述了magic操作，count的新特征构造，特征相关性。。。</p><a id="more"></a><p>[TOC]</p><p>来源：<a href="https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920" target="_blank" rel="noopener">https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920</a></p><h1 id="magic-kernel"><a href="#magic-kernel" class="headerlink" title="magic kernel"></a>magic kernel</h1><h2 id="树模型的缺点"><a href="#树模型的缺点" class="headerlink" title="树模型的缺点"></a>树模型的缺点</h2><h3 id="无法学习到的东西"><a href="#无法学习到的东西" class="headerlink" title="无法学习到的东西"></a>无法学习到的东西</h3><p><img src="http://playagricola.com/Kaggle/198without.png" alt></p><p>LGBM用竖线划分直方图，因为LGBM看不到水平差异。一个直方图会将多个值放置在一个Bin中并且产生一个较为平滑的图。如果你把多个值放在一个bin中，你会得到一个锯齿的图，其中每个bin中有些值是惟一的，有些值出现了几十次，LGBM无法学习到这些事情。</p><p><img src="http://playagricola.com/Kaggle/198zoom3.png" alt></p><p>如上，构造的count图可以看出在相同值的附近存在不同频数的区别。</p><h3 id="对特征敏感的参数设置"><a href="#对特征敏感的参数设置" class="headerlink" title="对特征敏感的参数设置"></a>对特征敏感的参数设置</h3><p>​    仅仅做到上述构造特征还是没有用的，因为你添加新特征到LGBM的时候设置参数feature_fraction=0.05，这会导致特征被随机的采样，破坏了var_1和var_1count之间的依赖关系(目的就是让模型需要同时学习横向和纵向)。所以设置feature_fraction=1，能从0.901到0.910，但是如果要到0.920则需要剔除原始变量之间的<a href="https://en.wikipedia.org/wiki/Spurious_relationship" target="_blank" rel="noopener">spurious effects</a>,因为原始特征对模型敏感，同时相关系数普遍很低，则说明它们虽然有相关但是没有因果关联。</p><ul><li>Use Data Augmentation (as shown in Jiwei’s awesome kernel <a href="https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment" target="_blank" rel="noopener">here</a>). You must keep original and new feature in same row.</li><li>Use 200 separate models as shown in this kernel below.</li><li>Merge new feature and original feature into one feature. In original data, simply add 200 to each unique value. (And don’t add new columns)</li><li>使用数据增强，就是随机打乱特征的时候，保持count和var是一致的，保证count和var的相关性，又去除了var之间相关性</li><li>单独使用count和var预测，然后200个模型再融合(一共200个var），这与第三点的merge我觉得是一致的，将两列合成一列（单纯加减应该是不够的)</li></ul><p>​    <strong>注意计算频数的时候，原则是同分布下越多数据越好。</strong>所以train和test在不在一起取决于它们的分布是否一致，或者如何让它们分布一致。在该比赛中就对test集进行了划分，剔除了和train不一致的数据，再合在一起做频数</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>主要参考链接</p><h1 id="first-kernel"><a href="#first-kernel" class="headerlink" title="first kernel"></a>first kernel</h1><p>来源：<a href="https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920" target="_blank" rel="noopener">https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920</a></p><h2 id="magic-feature"><a href="#magic-feature" class="headerlink" title="magic feature"></a>magic feature</h2><p>不单单是用count ,而是更深入的使用count，告诉模型它所不知道的事情。比如说Unique in train 和test</p><ul><li>This value appears at least another time in data with target==1 and no 0;</li><li>This value appears at least another time in data with target==0 and no 1;</li><li>This value appears at least two more time in data with target==0 &amp; 1;</li><li>This value is unique in data;</li><li>This value is unique in data + test (only including real test samples);</li></ul><p>The other 200 (one per raw feature) features are numerical, let’s call them “not unique feat”, and correspond to the raw feature replacing values that are unique in data + test with the mean of the feature.</p><p>用该列的均值去替换该列的Unique值，使其成为非unique的列，这个操作是尝试出来的，它们也用了nan，median</p><h2 id="magic-insight"><a href="#magic-insight" class="headerlink" title="magic insight"></a>magic insight</h2><p>两个重要的节点：</p><ul><li>I looked at my LGBM trees (with only 3 leafs that’s easy to do) and noticed the trees were using the uniqueness information.通过树画图，看出树当前不能学习的东西</li><li>number of different values in train and test was not the same。 虽然数值上分布是一致的，但是在值的个数上不一致</li></ul><h2 id="technial-part"><a href="#technial-part" class="headerlink" title="technial part:"></a>technial part:</h2><p>匿名变量用NN会更好</p><p>NN的concat可以更好的处理变量之间的关系</p><h2 id="rest-magic"><a href="#rest-magic" class="headerlink" title="rest magic"></a>rest magic</h2><p> using the shuffle augmentation (duplicate and shuffle 16 times samples with target == 1, 4 for target ==0) and added pseudo label (2700 highest predicted test points as 1 and 2000 lowest as 0)</p><p>pseudo label指从测试集中取最高的为1最低的为0，加进去</p><p>shuffle augmentation 指数据增强(eda代码中有)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Santander Customer Transaction Prediction 比赛，01分类问题，200维连续型匿名变量。&lt;/p&gt;
&lt;p&gt;讲述了magic操作，count的新特征构造，特征相关性。。。&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>比赛模型</title>
    <link href="http://kodgv.xyz/2019/04/10/%E6%AF%94%E8%B5%9B%E6%A8%A1%E5%9E%8B/"/>
    <id>http://kodgv.xyz/2019/04/10/比赛模型/</id>
    <published>2019-04-10T02:20:03.000Z</published>
    <updated>2019-04-12T09:04:51.957Z</updated>
    
    <content type="html"><![CDATA[<p>比赛通用模型代码</p><a id="more"></a><p>[TOC]</p><h2 id="载入模型前操作"><a href="#载入模型前操作" class="headerlink" title="载入模型前操作"></a>载入模型前操作</h2><h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler,StandardScaler</span><br><span class="line">sc = StandardScaler()</span><br><span class="line">train_features = sc.fit_transform(train_features)</span><br><span class="line">test_features = sc.transform(test_features)</span><br></pre></td></tr></table></figure><h3 id="切分训练集和验证集"><a href="#切分训练集和验证集" class="headerlink" title="切分训练集和验证集"></a>切分训练集和验证集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_split</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment"># K-fold</span></span><br><span class="line"><span class="comment"># scikit-learn k-fold cross-validation</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="comment"># data sample</span></span><br><span class="line"><span class="comment"># prepare cross validation</span></span><br><span class="line">kfold = KFold(n_splits=<span class="number">3</span>, shuffle = <span class="literal">True</span>, random_state= <span class="number">1</span>)</span><br><span class="line"><span class="comment"># enumerate splits</span></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kfold.split(data):</span><br><span class="line">    print(<span class="string">'train: %s, test: %s'</span> % (data[train], data[test]))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="comment"># StratifiedKFold</span></span><br><span class="line">folds = StratifiedKFold(n_splits=num_folds, shuffle=<span class="literal">False</span>, random_state=<span class="number">2319</span>)</span><br><span class="line"><span class="keyword">for</span> fold_, (trn_idx, val_idx) <span class="keyword">in</span> enumerate(folds.split(train.values, target.values)):</span><br><span class="line">    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]</span><br><span class="line">   X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]</span><br></pre></td></tr></table></figure><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="xgb回归"><a href="#xgb回归" class="headerlink" title="xgb回归"></a>xgb回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn</span></span><br><span class="line"><span class="comment">########################################################################### 回归</span></span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, StratifiedKFold,GroupKFold</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 别人的自定义损失函数,在parameter里面：object里面赋值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">custom_loss</span><span class="params">(y_true,y_pred)</span>:</span></span><br><span class="line">    penalty=<span class="number">2.0</span></span><br><span class="line">    grad=-y_true/y_pred+penalty*(<span class="number">1</span>-y_true)/(<span class="number">1</span>-y_pred) <span class="comment">#梯度</span></span><br><span class="line">    hess=y_true/(y_pred**<span class="number">2</span>)+penalty*(<span class="number">1</span>-y_true)/(<span class="number">1</span>-y_pred)**<span class="number">2</span> <span class="comment">#2阶导</span></span><br><span class="line">    <span class="keyword">return</span> grad,hess</span><br><span class="line"><span class="comment"># 自定义评价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse</span><span class="params">(y_pred,dtrain)</span>:</span> <span class="comment">#preds是结果（概率值），dtrain是个带label的DMatrix</span></span><br><span class="line">    labels=dtrain.get_label() <span class="comment">#提取label</span></span><br><span class="line">    t=mean_absolute_error(labels, y_pred)</span><br><span class="line">    print(t)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'mse'</span>,t</span><br><span class="line">parameters = &#123;<span class="string">'nthread'</span>:<span class="number">-1</span>, <span class="comment"># cpu 线程数 默认最大</span></span><br><span class="line">              <span class="string">'objective'</span>:<span class="string">'reg:linear'</span>,<span class="comment">#多分类or 回归的问题    若要自定义就替换为custom_loss（不带引号）</span></span><br><span class="line">              <span class="string">'learning_rate'</span>: <span class="number">.01</span>, <span class="comment">#so called `eta` value 如同学习率</span></span><br><span class="line">              <span class="string">'max_depth'</span>: <span class="number">6</span>,<span class="comment"># 构建树的深度，越大越容易过拟合</span></span><br><span class="line">              <span class="string">'min_child_weight'</span>: <span class="number">4</span>,</span><br><span class="line"><span class="comment"># 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言</span></span><br><span class="line"><span class="comment">#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。</span></span><br><span class="line"><span class="comment">#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。</span></span><br><span class="line">              <span class="string">'silent'</span>: <span class="number">1</span>,<span class="comment">#设置成1则没有运行信息输出，最好是设置为0.</span></span><br><span class="line">              <span class="string">'subsample'</span>: <span class="number">0.7</span>, <span class="comment"># 随机采样训练样本</span></span><br><span class="line">              <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,<span class="comment"># 生成树时进行的列采样</span></span><br><span class="line">              <span class="string">'n_estimators'</span>: <span class="number">100</span>,<span class="comment"># 树的个数跟num_boost_round是一样的，所以可以设置无限大，靠early_stop</span></span><br><span class="line">              <span class="string">'gamma'</span>:<span class="number">0.1</span>,<span class="comment"># 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。</span></span><br><span class="line">              <span class="string">'seed'</span>:<span class="number">1000</span> <span class="comment">#随机种子</span></span><br><span class="line">              <span class="comment">#'alpha':0, # L1 正则项参数</span></span><br><span class="line">              <span class="comment">#'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。</span></span><br><span class="line">              <span class="comment">#'num_class':10, # 类别数，多分类与 multisoftmax 并用</span></span><br><span class="line">              &#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergeToOne</span><span class="params">( X, X2)</span>:</span></span><br><span class="line">    X3 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">        tmp = np.array([list(X[i]), list(X2[i])])</span><br><span class="line">        X3.append(list(np.hstack(tmp)))</span><br><span class="line">    X3 = np.array(X3)</span><br><span class="line">    <span class="keyword">return</span> X3</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_XgbRegressor</span><span class="params">(train_data,train_target,test_data,feature_names,parameters,early_stopping_rounds,num_folds,eval_metric,model_name=<span class="string">'model'</span>,stratified=False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    :param train_data: 一定是numpy</span></span><br><span class="line"><span class="string">    :param train_target:</span></span><br><span class="line"><span class="string">    :param parameters:</span></span><br><span class="line"><span class="string">    :param round:</span></span><br><span class="line"><span class="string">    :param k:</span></span><br><span class="line"><span class="string">    :param eval_metrics:自定义 or 内置字符串</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    reg=XGBRegressor()</span><br><span class="line">    reg.set_params(**parameters)</span><br><span class="line">    <span class="comment"># 定义一些变量</span></span><br><span class="line">    oof_preds = np.zeros((train_data.shape[<span class="number">0</span>],))</span><br><span class="line">    sub_preds = np.zeros((test_data.shape[<span class="number">0</span>],))</span><br><span class="line">    feature_importance_df = pd.DataFrame()</span><br><span class="line">    cv_result = []</span><br><span class="line">    <span class="comment"># K-flod</span></span><br><span class="line">    <span class="keyword">if</span> stratified:</span><br><span class="line">        folds = StratifiedKFold(n_splits= num_folds, shuffle=<span class="literal">True</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        folds = KFold(n_splits= num_folds, shuffle=<span class="literal">True</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">    X_train_newfeature=np.zeros((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> n_flod, (train_index, val_index) <span class="keyword">in</span> enumerate(folds.split(train_data, train_target)):</span><br><span class="line">        train_X=train_data[train_index]</span><br><span class="line">        val_X=train_data[val_index]</span><br><span class="line">        train_Y=train_target[train_index]</span><br><span class="line">        val_Y=train_target[val_index]</span><br><span class="line">        <span class="comment"># 参数初步定之后划分20%为验证集，准备一个watchlist 给train和validation set ,设置num_round 足够大（比如100000），以至于你能发现每一个round 的验证集预测结果，</span></span><br><span class="line">        <span class="comment"># 如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了。</span></span><br><span class="line">        watchlist= [(train_X, train_Y), (val_X, val_Y)]</span><br><span class="line">        <span class="comment"># early_stop 看validate的eval是否下降，这时候必须传eval_set,并取eval_set的最后一个作为validate</span></span><br><span class="line">        reg.fit(train_X,train_Y,early_stopping_rounds=early_stopping_rounds, eval_set=watchlist,eval_metric=eval_metric)</span><br><span class="line">        <span class="comment">## 生成gbdt新特征</span></span><br><span class="line">        new_feature = reg.apply(val_X)</span><br><span class="line">        <span class="keyword">if</span> X_train_newfeature.shape[<span class="number">0</span>]==<span class="number">1</span>:</span><br><span class="line">            X_train_newfeature=mergeToOne(val_X,new_feature)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X_train_newfeature = mergeToOne(val_X,new_feature)</span><br><span class="line">            X_train_newfeature=np.concatenate((X_train_newfeature,mergeToOne(new_feature, val_X)),axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">print</span> (X_train_newfeature)</span><br><span class="line">       <span class="comment"># 获得每次的预测值补充</span></span><br><span class="line">        oof_preds[val_index]=reg.predict(val_X)</span><br><span class="line">        <span class="comment"># 获得预测的平均值，这里直接加完再除m</span></span><br><span class="line">        sub_preds+= reg.predict(test_data)</span><br><span class="line">        result = mean_absolute_error(val_Y, reg.predict(val_X))</span><br><span class="line">        print(<span class="string">'Fold %2d macro-f1 : %.6f'</span> % (n_flod + <span class="number">1</span>, result))</span><br><span class="line">        cv_result.append(round(result,<span class="number">5</span>))</span><br><span class="line">        gc.collect()</span><br><span class="line">        <span class="comment"># 默认就是gain 如果要修改要再参数定义中修改importance_type</span></span><br><span class="line">        <span class="comment"># 保存特征重要度</span></span><br><span class="line">        gain = reg.feature_importances_</span><br><span class="line">        fold_importance_df = pd.DataFrame(&#123;<span class="string">'feature'</span>: feature_names,</span><br><span class="line">                                           <span class="string">'gain'</span>: <span class="number">100</span> * gain / gain.sum(),</span><br><span class="line">                                           <span class="string">'fold'</span>: n_flod,</span><br><span class="line">                                           &#125;).sort_values(<span class="string">'gain'</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 进行保存</span></span><br><span class="line">    sub_preds=sub_preds/folds.n_splits</span><br><span class="line">    new_feature=reg.apply(test_data)</span><br><span class="line">    X_test_newfeature = mergeToOne(test_data, new_feature)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./sub'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./sub'</span>)</span><br><span class="line">    pd.DataFrame(oof_preds,columns=[<span class="string">'class'</span>]).to_csv(<span class="string">'./sub/val_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>)</span><br><span class="line">    pd.DataFrame(sub_preds, columns=[<span class="string">'class'</span>]).to_csv(<span class="string">'./sub/test_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>)</span><br><span class="line">    print(<span class="string">'cv_result'</span>, cv_result)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./gbdt_newfeature'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./gbdt_newfeature'</span>)</span><br><span class="line"></span><br><span class="line">    np.save(<span class="string">"./gbdt_newfeature/train_newfeature.npy"</span>, X_train_newfeature)</span><br><span class="line">    np.save(<span class="string">"./gbdt_newfeature/test_newfeature.npy"</span>, X_test_newfeature)</span><br><span class="line">    save_importances(feature_importance_df, model_name)</span><br><span class="line">    <span class="keyword">return</span> reg,sub_preds</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_importances</span><span class="params">(feature_importance_df,model_name)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./feature_importance'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./feature_importance'</span>)</span><br><span class="line">    ft = feature_importance_df[[<span class="string">"feature"</span>, <span class="string">"gain"</span>]].groupby(<span class="string">"feature"</span>).mean().sort_values(by=<span class="string">"gain"</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">    ft.to_csv(<span class="string">'./feature_importance/importance_lightgbm_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="xgb回归-1"><a href="#xgb回归-1" class="headerlink" title="xgb回归"></a>xgb回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">########################################################################################## 分类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span>  sklearn.datasets  <span class="keyword">import</span>  make_hastie_10_2</span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line">clf_parameters = &#123;<span class="string">'nthread'</span>:<span class="number">-1</span>, <span class="comment"># cpu 线程数 默认最大</span></span><br><span class="line">              <span class="string">'objective'</span>:<span class="string">'multi:softmax'</span>,<span class="comment">#多分类or 回归的问题    若要自定义就替换为custom_loss（不带引号）</span></span><br><span class="line">              <span class="string">'learning_rate'</span>: <span class="number">.01</span>, <span class="comment">#so called `eta` value 如同学习率</span></span><br><span class="line">              <span class="string">'max_depth'</span>: <span class="number">6</span>,<span class="comment"># 构建树的深度，越大越容易过拟合</span></span><br><span class="line">              <span class="string">'min_child_weight'</span>: <span class="number">4</span>,</span><br><span class="line"><span class="comment"># 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言</span></span><br><span class="line"><span class="comment">#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。</span></span><br><span class="line"><span class="comment">#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。</span></span><br><span class="line">              <span class="string">'silent'</span>: <span class="number">1</span>,<span class="comment">#设置成1则没有运行信息输出，最好是设置为0.</span></span><br><span class="line">              <span class="string">'subsample'</span>: <span class="number">0.7</span>, <span class="comment"># 随机采样训练样本</span></span><br><span class="line">              <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,<span class="comment"># 生成树时进行的列采样</span></span><br><span class="line">              <span class="string">'n_estimators'</span>: <span class="number">500</span>,<span class="comment"># 树的个数跟num_boost_round是一样的，所以可以设置无限大，靠early_stop</span></span><br><span class="line">              <span class="string">'gamma'</span>:<span class="number">0.1</span>,<span class="comment"># 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。</span></span><br><span class="line">              <span class="string">'seed'</span>:<span class="number">1000</span> <span class="comment">#随机种子</span></span><br><span class="line">              <span class="comment">#'alpha':0, # L1 正则项参数</span></span><br><span class="line">              <span class="comment">#'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。</span></span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">n_class=<span class="number">3</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clf_custom_loss</span><span class="params">(y_true,y_pred)</span>:</span></span><br><span class="line">    penalty=<span class="number">2.0</span></span><br><span class="line">    grad=-y_true/y_pred+penalty*(<span class="number">1</span>-y_true)/(<span class="number">1</span>-y_pred) <span class="comment">#梯度</span></span><br><span class="line">    hess=y_true/(y_pred**<span class="number">2</span>)+penalty*(<span class="number">1</span>-y_true)/(<span class="number">1</span>-y_pred)**<span class="number">2</span> <span class="comment">#2阶导</span></span><br><span class="line">    <span class="keyword">return</span> grad,hess</span><br><span class="line"><span class="comment"># 自定义评价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clf_mse</span><span class="params">(y_pred,dtrain)</span>:</span> <span class="comment">#preds是结果（概率值），dtrain是个带label的DMatrix</span></span><br><span class="line">    labels=dtrain.get_label() <span class="comment">#提取label</span></span><br><span class="line">    <span class="comment">######### 分类预测的都是概率哦，所以这里要取一个max类别</span></span><br><span class="line">    y_pred = np.argmax(y_pred.reshape(n_class, <span class="number">-1</span>), axis=<span class="number">0</span>)</span><br><span class="line">    score=mean_absolute_error(labels, y_pred)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'mse'</span>,score</span><br><span class="line"><span class="comment"># 分类的时候要注意！！！！！！！！</span></span><br><span class="line"><span class="comment"># k-flod的时候要按层次拿出来，有一个shuffler我这里就没实现了，否则预测的类别会出现变小甚至报错</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_XgbClassifer</span><span class="params">(train_data,train_target,test_data,feature_names,parameters,early_stopping_rounds,num_folds,eval_metric,model_name=<span class="string">'model'</span>,stratified=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    :param train_data: 一定是numpy</span></span><br><span class="line"><span class="string">    :param train_target:</span></span><br><span class="line"><span class="string">    :param parameters:</span></span><br><span class="line"><span class="string">    :param round:</span></span><br><span class="line"><span class="string">    :param k:</span></span><br><span class="line"><span class="string">    :param eval_metrics:自定义 or 内置字符串</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 如果在param中设置，会莫名报参数不存在的错误</span></span><br><span class="line">    clf=XGBClassifier(num_class=n_class)</span><br><span class="line">    clf.set_params(**parameters)</span><br><span class="line">    <span class="comment"># 定义一些变量</span></span><br><span class="line">    oof_preds = np.zeros((train_data.shape[<span class="number">0</span>],n_class))</span><br><span class="line">    sub_preds = np.zeros((test_data.shape[<span class="number">0</span>],n_class))</span><br><span class="line">    feature_importance_df = pd.DataFrame()</span><br><span class="line">    cv_result = []</span><br><span class="line">    <span class="comment"># K-flod</span></span><br><span class="line">    <span class="keyword">if</span> stratified:</span><br><span class="line">        folds = StratifiedKFold(n_splits= num_folds, shuffle=<span class="literal">True</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        folds = KFold(n_splits= num_folds, shuffle=<span class="literal">True</span>, random_state=<span class="number">1234</span>)</span><br><span class="line">    <span class="keyword">for</span> n_flod,(train_index, val_index) <span class="keyword">in</span> enumerate(folds.split(train_data,train_target)):</span><br><span class="line">        train_X=train_data[train_index]</span><br><span class="line">        val_X=train_data[val_index]</span><br><span class="line">        train_Y=train_target[train_index]</span><br><span class="line">        val_Y=train_target[val_index]</span><br><span class="line">        <span class="comment"># 参数初步定之后划分20%为验证集，准备一个watchlist 给train和validation set ,设置num_round 足够大（比如100000），以至于你能发现每一个round 的验证集预测结果，</span></span><br><span class="line">        <span class="comment"># 如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了。</span></span><br><span class="line">        watchlist= [(train_X, train_Y)]</span><br><span class="line">        <span class="comment"># early_stop 看validate的eval是否下降，这时候必须传eval_set,并取eval_set的最后一个作为validate</span></span><br><span class="line">        clf.fit(train_X,train_Y,early_stopping_rounds=early_stopping_rounds, eval_set=watchlist,eval_metric=eval_metric)</span><br><span class="line">        <span class="comment"># 获得每次的预测值补充</span></span><br><span class="line">        oof_preds[val_index]=clf.predict_proba(val_X)</span><br><span class="line">        <span class="comment"># 获得预测的平均值，这里直接加完再除m</span></span><br><span class="line">        sub_preds+= clf.predict_proba(test_data)</span><br><span class="line">        <span class="comment"># 计算当前准确率</span></span><br><span class="line">        result=mean_absolute_error(val_Y,clf.predict(val_X))</span><br><span class="line">        print(<span class="string">'Fold %2d macro-f1 : %.6f'</span> % (n_flod + <span class="number">1</span>, result))</span><br><span class="line">        print(type(result))</span><br><span class="line">        cv_result.append(round(result,<span class="number">5</span>))</span><br><span class="line">        gc.collect()</span><br><span class="line">        <span class="comment"># 默认就是gain 如果要修改要再参数定义中修改importance_type</span></span><br><span class="line">        <span class="comment"># 保存特征重要度</span></span><br><span class="line">        gain = clf.feature_importances_</span><br><span class="line">        fold_importance_df = pd.DataFrame(&#123;<span class="string">'feature'</span>:feature_names,</span><br><span class="line">                                           <span class="string">'gain'</span>:<span class="number">100</span>*gain/gain.sum(),</span><br><span class="line">                                           <span class="string">'fold'</span>:n_flod,</span><br><span class="line">                                           &#125;).sort_values(<span class="string">'gain'</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 进行保存</span></span><br><span class="line">    sub_preds=sub_preds/folds.n_splits</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./cv'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./cv'</span>)</span><br><span class="line">    pd.DataFrame(oof_preds,columns=[<span class="string">'class_'</span>+ str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_class)]).to_csv(<span class="string">'./cv/val_prob_&#123;&#125;.csv'</span>.format(model_name), index= <span class="literal">False</span>, float_format = <span class="string">'%.4f'</span>)</span><br><span class="line">    pd.DataFrame(sub_preds, columns=[<span class="string">'class_'</span> + str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_class)]).to_csv(<span class="string">'./cv/test_prob_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>, float_format=<span class="string">'%.4f'</span>)</span><br><span class="line">    oof_preds = [np.argmax(x) <span class="keyword">for</span> x <span class="keyword">in</span> oof_preds]</span><br><span class="line">    sub_preds = [np.argmax(x) <span class="keyword">for</span> x <span class="keyword">in</span> sub_preds]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./sub'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./sub'</span>)</span><br><span class="line">    pd.DataFrame(oof_preds,columns=[<span class="string">'class'</span>]).to_csv(<span class="string">'./sub/val_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>)</span><br><span class="line">    pd.DataFrame(sub_preds, columns=[<span class="string">'class'</span>]).to_csv(<span class="string">'./sub/test_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    save_importances(feature_importance_df, model_name)</span><br><span class="line">    <span class="keyword">return</span> clf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_importances</span><span class="params">(feature_importance_df,model_name)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">'./feature_importance'</span>):</span><br><span class="line">        os.makedirs(<span class="string">'./feature_importance'</span>)</span><br><span class="line">    ft = feature_importance_df[[<span class="string">"feature"</span>, <span class="string">"gain"</span>]].groupby(<span class="string">"feature"</span>).mean().sort_values(by=<span class="string">"gain"</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">    ft.to_csv(<span class="string">'./feature_importance/importance_lightgbm_&#123;&#125;.csv'</span>.format(model_name), index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="lgb回归"><a href="#lgb回归" class="headerlink" title="lgb回归"></a>lgb回归</h3><p>注意Lgb对分类变量会有特殊的支持</p><p>只用看参数和pythonAPI</p><p><a href="http://lightgbm.apachecn.org/" target="_blank" rel="noopener">http://lightgbm.apachecn.org/</a></p><p><a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters" target="_blank" rel="noopener">https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">param = &#123;</span><br><span class="line">    <span class="string">'bagging_freq'</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">'bagging_fraction'</span>: <span class="number">0.335</span>,</span><br><span class="line">    <span class="string">'boost_from_average'</span>:<span class="string">'false'</span>,</span><br><span class="line">    <span class="string">'boost'</span>: <span class="string">'gbdt'</span>,</span><br><span class="line">    <span class="string">'feature_fraction'</span>: <span class="number">0.041</span>,</span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">0.0083</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">-1</span>,</span><br><span class="line">    <span class="string">'metric'</span>:<span class="string">'auc'</span>,</span><br><span class="line">    <span class="string">'min_data_in_leaf'</span>: <span class="number">80</span>,</span><br><span class="line">    <span class="string">'min_sum_hessian_in_leaf'</span>: <span class="number">10.0</span>,</span><br><span class="line">    <span class="string">'num_leaves'</span>: <span class="number">13</span>,</span><br><span class="line">    <span class="string">'num_threads'</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">'tree_learner'</span>: <span class="string">'serial'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'binary'</span>, </span><br><span class="line">    <span class="string">'verbosity'</span>: <span class="number">-1</span></span><br><span class="line">&#125;</span><br><span class="line">num_folds = <span class="number">11</span></span><br><span class="line">features = [c <span class="keyword">for</span> c <span class="keyword">in</span> train.columns <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'ID_code'</span>, <span class="string">'target'</span>]]</span><br><span class="line">folds = KFold(n_splits=num_folds, random_state=<span class="number">2319</span>)</span><br><span class="line">oof = np.zeros(len(train))</span><br><span class="line">getVal = np.zeros(len(train))</span><br><span class="line">predictions = np.zeros(len(target))</span><br><span class="line">feature_importance_df = pd.DataFrame()</span><br><span class="line">print(<span class="string">'Light GBM Model'</span>)</span><br><span class="line"><span class="keyword">for</span> fold_, (trn_idx, val_idx) <span class="keyword">in</span> enumerate(folds.split(train.values, target.values)): </span><br><span class="line">    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]</span><br><span class="line">    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]</span><br><span class="line">    print(<span class="string">"Fold idx:&#123;&#125;"</span>.format(fold_ + <span class="number">1</span>))</span><br><span class="line">    trn_data = lgb.Dataset(X_train, label=y_train)</span><br><span class="line">    val_data = lgb.Dataset(X_valid, label=y_valid)</span><br><span class="line">    clf = lgb.train(param, trn_data, <span class="number">1000000</span>, valid_sets = [trn_data, val_data], verbose_eval=<span class="number">5000</span>, early_stopping_rounds = <span class="number">4000</span>)</span><br><span class="line">    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)</span><br><span class="line">    fold_importance_df = pd.DataFrame()</span><br><span class="line">    fold_importance_df[<span class="string">"feature"</span>] = features</span><br><span class="line">    fold_importance_df[<span class="string">"importance"</span>] = clf.feature_importance()</span><br><span class="line">    fold_importance_df[<span class="string">"fold"</span>] = fold_ + <span class="number">1</span></span><br><span class="line">    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=<span class="number">0</span>)</span><br><span class="line">    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits</span><br><span class="line">print(<span class="string">"CV score: &#123;:&lt;8.5f&#125;"</span>.format(roc_auc_score(target, oof)))</span><br></pre></td></tr></table></figure><h3 id="lgb分类"><a href="#lgb分类" class="headerlink" title="lgb分类"></a>lgb分类</h3><p>回归和分类一致，只是参数不一样而已</p><p>注意Lgb对分类变量会有特殊的支持</p><p>只用看参数和pythonAPI</p><p><a href="http://lightgbm.apachecn.org/" target="_blank" rel="noopener">http://lightgbm.apachecn.org/</a></p><p><a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters" target="_blank" rel="noopener">https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters</a></p><h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><p>比较常用的手工概率blending</p><p><a href="https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899" target="_blank" rel="noopener">https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899</a></p><h2 id="调整参数"><a href="#调整参数" class="headerlink" title="调整参数"></a>调整参数</h2><h3 id="贝叶斯优化"><a href="#贝叶斯优化" class="headerlink" title="贝叶斯优化"></a>贝叶斯优化</h3><p>来源:<a href="https://www.cnblogs.com/yangruiGB2312/p/9374377.html" target="_blank" rel="noopener">https://www.cnblogs.com/yangruiGB2312/p/9374377.html</a></p><p>可以说是目前最好的调参的方法</p><ul><li><p>贝叶斯调参采用高斯过程，<strong>考虑之前的参数信息</strong>，不断地更新先验；网格搜索未考虑之前的参数信息</p></li><li><p>贝叶斯调参<strong>迭代次数少，速度快</strong>；网格搜索速度慢,参数多时易导致维度爆炸</p></li><li><p>贝叶斯调参针对非凸问题依然<strong>稳健</strong>；网格搜索针对非凸问题易得到局部优最</p></li></ul><p>​    公式推导就不在这里展开，主要阐述主要思想：给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程<strong>,直到后验分布基本贴合于真实分布。简单的说，就是</strong>考虑了上一次参数的信息，从而更好的调整当前的参数。</p><p>​    假设这个寻找最优化参数的过程是一个高斯过程。高斯过程有个特点，就是当随机遍历一定的数据点并拿到结果之后，可以大致绘制出整个数据的分布曲线，当然这是也会有一定误差。 </p><p><img src="https://img-blog.csdn.net/20180821135039882?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dzaGVuZ29k/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt></p><p>python代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rf_cv</span><span class="params">(n_estimators, min_samples_split, max_features, max_depth)</span>:</span></span><br><span class="line">    val = cross_val_score(</span><br><span class="line">        RandomForestClassifier(n_estimators=int(n_estimators),</span><br><span class="line">            min_samples_split=int(min_samples_split),</span><br><span class="line">            max_features=min(max_features, <span class="number">0.999</span>), <span class="comment"># float</span></span><br><span class="line">            max_depth=int(max_depth),</span><br><span class="line">            random_state=<span class="number">2</span></span><br><span class="line">        ),</span><br><span class="line">        x, y, scoring=<span class="string">'roc_auc'</span>, cv=<span class="number">5</span></span><br><span class="line">    ).mean()</span><br><span class="line">    <span class="keyword">return</span> val</span><br><span class="line"><span class="comment"># 注意参数名字要对应</span></span><br><span class="line">rf_bo = BayesianOptimization(</span><br><span class="line">        rf_cv,</span><br><span class="line">        &#123;<span class="string">'n_estimators'</span>: (<span class="number">10</span>, <span class="number">250</span>),</span><br><span class="line">        <span class="string">'min_samples_split'</span>: (<span class="number">2</span>, <span class="number">25</span>),</span><br><span class="line">        <span class="string">'max_features'</span>: (<span class="number">0.1</span>, <span class="number">0.999</span>),</span><br><span class="line">        <span class="string">'max_depth'</span>: (<span class="number">5</span>, <span class="number">15</span>)&#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>可以执行的操作</p><ul><li><p>以查看当前最优的参数和结果(同时，我们还可以修改高斯过程的参数，高斯过程主要参数是核函数(<code>kernel</code>)，还有其他参数可以参考<a href="http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html" target="_blank" rel="noopener">sklearn.gaussianprocess</a>)：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gp_param=&#123;<span class="string">'kernel'</span>:None&#125;</span><br><span class="line">rf_bo.maximize(**gp_param)</span><br><span class="line">rf_bo<span class="selector-class">.res</span>[<span class="string">'max'</span>]</span><br></pre></td></tr></table></figure></li><li><p>上面bayes算法得到的参数并不一定最优，当然我们会遇到一种情况，就是我们已经知道有一组或是几组参数是非常好的了，我们想知道其附近有没有更好的。</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rf_bo.explore(</span><br><span class="line">    &#123;<span class="string">'n_estimators'</span>: [<span class="number">10</span>, <span class="number">100</span>, <span class="number">200</span>],</span><br><span class="line">        <span class="string">'min_samples_split'</span>: [<span class="number">2</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">        <span class="string">'max_features'</span>: [<span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.9</span>],</span><br><span class="line">        <span class="string">'max_depth'</span>: [<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li></ul><h2 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛通用模型代码&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型" scheme="http://kodgv.xyz/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Vector Representation</title>
    <link href="http://kodgv.xyz/2019/04/09/representation/"/>
    <id>http://kodgv.xyz/2019/04/09/representation/</id>
    <published>2019-04-09T12:10:18.000Z</published>
    <updated>2019-04-11T02:48:44.628Z</updated>
    
    <content type="html"><![CDATA[<p>​    自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 <code>Id537</code>，“dog”可能表示为 <code>Id143</code>。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。</p><a id="more"></a><p>来源：<a href="https://www.tensorflow.org/tutorials/representation/word2vec" target="_blank" rel="noopener">https://www.tensorflow.org/tutorials/representation/word2vec</a></p><p>​    自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 <code>Id537</code>，“dog”可能表示为 <code>Id143</code>。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。</p><p>​    <a href="https://en.wikipedia.org/wiki/Vector_space_model" target="_blank" rel="noopener">向量空间模型</a> (VSM) 在连续向量空间中表示（嵌入）字词，其中语义相似的字词会映射到附近的点（“在彼此附近嵌入”）。VSM 在 NLP 方面有着悠久而丰富的历史，但所有方法均以某种方式依赖于<a href="https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis" target="_blank" rel="noopener">分布假设</a>，这种假设指明在相同上下文中显示的字词语义相同。利用该原则的不同方法可分为两类：基于计数的方法（例如<a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" target="_blank" rel="noopener">潜在语义分析</a>）以及预测方法（例如<a href="http://www.scholarpedia.org/article/Neural_net_language_models" target="_blank" rel="noopener">神经概率语言模型</a>）。</p><p>​    Word2vec 是一种计算效率特别高的预测模型，用于学习原始文本中的字词嵌入。它分为两种类型：连续词袋模型 (CBOW) 和 Skip-Gram 模型。数据量少的时候使用CBOW，数据量大的时候Skip-gram，一般都用Skip-gram。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">https:</span>//www.tensorflow<span class="meta">.org</span>/tutorials/representation/word2vec</span><br></pre></td></tr></table></figure><h3 id="python"><a href="#python" class="headerlink" title="python"></a>python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.corpora <span class="keyword">import</span> WikiCorpus</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> LineSentence</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">L = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">path = <span class="string">'./'</span></span><br><span class="line">save_path = path + <span class="string">'/w2v'</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(save_path):</span><br><span class="line">    print(save_path)</span><br><span class="line">    os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">train1 = pd.read_csv(path + <span class="string">'/train.csv'</span>)</span><br><span class="line">train = pd.read_csv(path + <span class="string">'/train_old.csv'</span>)</span><br><span class="line">test = pd.read_csv(path + <span class="string">'/test.csv'</span>)</span><br><span class="line"></span><br><span class="line">data = pd.concat([train, test, train1]).reset_index(drop=<span class="literal">True</span>).sample(frac=<span class="number">1</span>, random_state=<span class="number">2018</span>).fillna(<span class="number">0</span>)</span><br><span class="line">data = data.replace(<span class="string">'\\N'</span>, <span class="number">999</span>)</span><br><span class="line">sentence = []</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> list(data[[<span class="string">'1_total_fee'</span>, <span class="string">'2_total_fee'</span>, <span class="string">'3_total_fee'</span>, <span class="string">'4_total_fee'</span>]].values):</span><br><span class="line">    sentence.append([str(float(l)) <span class="keyword">for</span> idx, l <span class="keyword">in</span> enumerate(line)])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'training...'</span>)</span><br><span class="line">model = Word2Vec(sentence, size=L, window=<span class="number">2</span>, min_count=<span class="number">1</span>, workers=multiprocessing.cpu_count(),</span><br><span class="line">                 iter=<span class="number">10</span>)</span><br><span class="line">print(<span class="string">'outputing...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> fea <span class="keyword">in</span> [<span class="string">'1_total_fee'</span>, <span class="string">'2_total_fee'</span>, <span class="string">'3_total_fee'</span>, <span class="string">'4_total_fee'</span>]:</span><br><span class="line">    values = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> list(data[fea].values):</span><br><span class="line">        values.append(line)</span><br><span class="line">    values = set(values)</span><br><span class="line">    print(len(values))</span><br><span class="line">    w2v = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> values:</span><br><span class="line">        a = [i]</span><br><span class="line">        a.extend(model[str(float(i))])</span><br><span class="line">        w2v.append(a)</span><br><span class="line">    out_df = pd.DataFrame(w2v)</span><br><span class="line"></span><br><span class="line">    name = [fea]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">        name.append(name[<span class="number">0</span>] + <span class="string">'W'</span> + str(i))</span><br><span class="line">    out_df.columns = name</span><br><span class="line">    out_df.to_csv(save_path + <span class="string">'/'</span> + fea + <span class="string">'.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="TSNE降维可视化"><a href="#TSNE降维可视化" class="headerlink" title="TSNE降维可视化"></a>TSNE降维可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_with_labels</span><span class="params">(low_dim_embs, labels, filename = <span class="string">'tsne.png'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>] &gt;= len(labels), <span class="string">"More labels than embeddings"</span></span><br><span class="line">    plt.figure(figsize= (<span class="number">10</span>, <span class="number">18</span>))</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">        x, y = low_dim_embs[i, :]</span><br><span class="line">        plt.scatter(x, y)</span><br><span class="line">        plt.annotate(label, xy = (x, y), textcoords = <span class="string">'offset points'</span>, ha = <span class="string">'right'</span>, va = <span class="string">'bottom'</span>)</span><br><span class="line">    plt.savefig(filename)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polt_tnse</span><span class="params">(df,df_target,plot_only=<span class="number">300</span>)</span>:</span></span><br><span class="line">    df_target=list(df_target.astype(<span class="string">'str'</span>))</span><br><span class="line">    tsne = TSNE(perplexity = <span class="number">30</span>, n_components = <span class="number">2</span>, init = <span class="string">'pca'</span>, n_iter = <span class="number">5000</span>)</span><br><span class="line">    low_dim_embs = tsne.fit_transform(df.iloc[:plot_only][:])</span><br><span class="line">    labels = [df_target[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(plot_only)]</span><br><span class="line">    plot_with_labels(low_dim_embs, labels)</span><br></pre></td></tr></table></figure><p><img src="https://www.tensorflow.org/images/tsne.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 &lt;code&gt;Id537&lt;/code&gt;，“dog”可能表示为 &lt;code&gt;Id143&lt;/code&gt;。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="预处理" scheme="http://kodgv.xyz/tags/%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="分类变量" scheme="http://kodgv.xyz/tags/%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>特征选择</title>
    <link href="http://kodgv.xyz/2019/04/09/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    <id>http://kodgv.xyz/2019/04/09/特征选择/</id>
    <published>2019-04-09T11:52:45.000Z</published>
    <updated>2019-04-12T08:29:21.212Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p>如何进行特征选择</p><a id="more"></a><h2 id="feature-importace"><a href="#feature-importace" class="headerlink" title="feature importace"></a>feature importace</h2><h3 id="特征重要度高"><a href="#特征重要度高" class="headerlink" title="特征重要度高"></a>特征重要度高</h3><p>如果发现特征重要度有明显很高的情况下，可以将这个特征去除再尝试，因为有可能模型过于依赖此特征而导致了过拟合的存在。比如下图中的addr_state</p><p><img src="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/_images/gbm_variable_importance1.png" alt></p><h3 id="特征重要度低"><a href="#特征重要度低" class="headerlink" title="特征重要度低"></a>特征重要度低</h3><p>来源：<a href="https://www.kaggle.com/ogrellier/feature-selection-with-null-importances" target="_blank" rel="noopener">https://www.kaggle.com/ogrellier/feature-selection-with-null-importances</a></p><p>来源：<a href="https://academic.oup.com/bioinformatics/article/26/10/1340/193348" target="_blank" rel="noopener">https://academic.oup.com/bioinformatics/article/26/10/1340/193348</a></p><p>​    传统做法就是根据feature importance然后就跑一个阈值，剔除低于阈值的feature。</p><p>剔除阈值的目的在于：</p><ul><li><p>消除高相关的feature</p></li><li><p>提高model的variance</p></li></ul><p>​    <strong>最好最后的分数剔除分类变量</strong>，因为它主要是平衡分类变量的bias</p><p>​    论文提出的做法是，对于树模型，它的特征重要度会对数量多的分类变量有bias，所以提出了一种基于null importance 的做法。即对target打乱s次，预测s次的特征重要度，然后再利用distribution进行分析，从而得到比较标注的特征重要度。主要的做法请看kaggle上的示例：</p><p>获取重要度，此处需要自定义一个训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shuffle决定是否打乱y值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_feature_importances</span><span class="params">(data, shuffle, seed=None)</span>:</span></span><br><span class="line">    <span class="comment"># Gather real features</span></span><br><span class="line">    train_features = [f <span class="keyword">for</span> f <span class="keyword">in</span> data <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'TARGET'</span>, <span class="string">'SK_ID_CURR'</span>]]</span><br><span class="line">    <span class="comment"># Go over fold and keep track of CV score (train and valid) and feature importances</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle target if required</span></span><br><span class="line">    y = data[<span class="string">'TARGET'</span>].copy()</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># Here you could as well use a binomial distribution</span></span><br><span class="line">        y = data[<span class="string">'TARGET'</span>].copy().sample(frac=<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest</span></span><br><span class="line">    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=<span class="literal">False</span>, silent=<span class="literal">True</span>)</span><br><span class="line">    lgb_params = &#123;</span><br><span class="line">        <span class="string">'objective'</span>: <span class="string">'binary'</span>,</span><br><span class="line">        <span class="string">'boosting_type'</span>: <span class="string">'rf'</span>,</span><br><span class="line">        <span class="string">'subsample'</span>: <span class="number">0.623</span>,</span><br><span class="line">        <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,</span><br><span class="line">        <span class="string">'num_leaves'</span>: <span class="number">127</span>,</span><br><span class="line">        <span class="string">'max_depth'</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">'seed'</span>: seed,</span><br><span class="line">        <span class="string">'bagging_freq'</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">'n_jobs'</span>: <span class="number">4</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fit the model</span></span><br><span class="line">    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=<span class="number">200</span>, categorical_feature=categorical_feats)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get feature importances</span></span><br><span class="line">    imp_df = pd.DataFrame()</span><br><span class="line">    imp_df[<span class="string">"feature"</span>] = list(train_features)</span><br><span class="line">    imp_df[<span class="string">"importance_gain"</span>] = clf.feature_importance(importance_type=<span class="string">'gain'</span>)</span><br><span class="line">    imp_df[<span class="string">"importance_split"</span>] = clf.feature_importance(importance_type=<span class="string">'split'</span>)</span><br><span class="line">    imp_df[<span class="string">'trn_score'</span>] = roc_auc_score(y, clf.predict(data[train_features]))</span><br><span class="line">    <span class="keyword">return</span> imp_df</span><br></pre></td></tr></table></figure><p>跑n轮得到Null importance </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">null_imp_df = pd.DataFrame()</span><br><span class="line">nb_runs = <span class="number">80</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start = time.time()</span><br><span class="line">dsp = <span class="string">''</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_runs):</span><br><span class="line">    <span class="comment"># Get current run importances</span></span><br><span class="line">    imp_df = get_feature_importances(data=data, shuffle=<span class="literal">True</span>)</span><br><span class="line">    imp_df[<span class="string">'run'</span>] = i + <span class="number">1</span> </span><br><span class="line">    <span class="comment"># Concat the latest importances with the old ones</span></span><br><span class="line">    null_imp_df = pd.concat([null_imp_df, imp_df], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>然后画分布图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_distributions</span><span class="params">(actual_imp_df_, null_imp_df_, feature_)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">13</span>, <span class="number">6</span>))</span><br><span class="line">    gs = gridspec.GridSpec(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># Plot Split importances</span></span><br><span class="line">    ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">    a = ax.hist(null_imp_df_.loc[null_imp_df_[<span class="string">'feature'</span>] == feature_, <span class="string">'importance_split'</span>].values, label=<span class="string">'Null importances'</span>)</span><br><span class="line">    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_[<span class="string">'feature'</span>] == feature_, <span class="string">'importance_split'</span>].mean(), </span><br><span class="line">               ymin=<span class="number">0</span>, ymax=np.max(a[<span class="number">0</span>]), color=<span class="string">'r'</span>,linewidth=<span class="number">10</span>, label=<span class="string">'Real Target'</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_title(<span class="string">'Split Importance of %s'</span> % feature_.upper(), fontweight=<span class="string">'bold'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Null Importance (split) Distribution for %s '</span> % feature_.upper())</span><br><span class="line">    <span class="comment"># Plot Gain importances</span></span><br><span class="line">    ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    a = ax.hist(null_imp_df_.loc[null_imp_df_[<span class="string">'feature'</span>] == feature_, <span class="string">'importance_gain'</span>].values, label=<span class="string">'Null importances'</span>)</span><br><span class="line">    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_[<span class="string">'feature'</span>] == feature_, <span class="string">'importance_gain'</span>].mean(), </span><br><span class="line">               ymin=<span class="number">0</span>, ymax=np.max(a[<span class="number">0</span>]), color=<span class="string">'r'</span>,linewidth=<span class="number">10</span>, label=<span class="string">'Real Target'</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_title(<span class="string">'Gain Importance of %s'</span> % feature_.upper(), fontweight=<span class="string">'bold'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Null Importance (gain) Distribution for %s '</span> % feature_.upper())</span><br><span class="line">display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_=<span class="string">'LIVINGAPARTMENTS_AVG'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2019040923253844.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NjYxMjMw,size_16,color_FFFFFF,t_70" alt></p><p><img src="https://img-blog.csdnimg.cn/20190409232626147.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NjYxMjMw,size_16,color_FFFFFF,t_70" alt></p><p>图主要展示了什么叫做好的分布:方差小的。同时远离真实的重要度（红线）</p><p>我们想要的是在与真实目标(即实际重要性)使用时具有很高重要性的特性，而在与噪声训练时得分不高(即null importance得分)。在零假设和正态分布下，如果一个特征的红色实际重要性在蓝色分布内，那么该特征与目标不相关的可能性就很大。如果它在蓝色分布的5%范围内或者在外面，那么它就是相关的。</p><p>然后下一步要做的就是（去掉与target不相关的，去掉自己高相关的）：</p><ul><li>Drop high variance features if they are not really related to the target</li><li>Remove the decaying factor on correlated features, showing their real importance (or unbiased importance)</li></ul><p>结合一些公式，然后进行挑选</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">correlation_scores = []</span><br><span class="line"><span class="keyword">for</span> _f <span class="keyword">in</span> actual_imp_df[<span class="string">'feature'</span>].unique():</span><br><span class="line">    f_null_imps = null_imp_df.loc[null_imp_df[<span class="string">'feature'</span>] == _f, <span class="string">'importance_gain'</span>].values</span><br><span class="line">    f_act_imps = actual_imp_df.loc[actual_imp_df[<span class="string">'feature'</span>] == _f, <span class="string">'importance_gain'</span>].values</span><br><span class="line">    gain_score = <span class="number">100</span> * (f_null_imps &lt; np.percentile(f_act_imps, <span class="number">25</span>)).sum() / f_null_imps.size</span><br><span class="line">    f_null_imps = null_imp_df.loc[null_imp_df[<span class="string">'feature'</span>] == _f, <span class="string">'importance_split'</span>].values</span><br><span class="line">    f_act_imps = actual_imp_df.loc[actual_imp_df[<span class="string">'feature'</span>] == _f, <span class="string">'importance_split'</span>].values</span><br><span class="line">    split_score = <span class="number">100</span> * (f_null_imps &lt; np.percentile(f_act_imps, <span class="number">25</span>)).sum() / f_null_imps.size</span><br><span class="line">    correlation_scores.append((_f, split_score, gain_score))</span><br><span class="line"></span><br><span class="line">corr_scores_df = pd.DataFrame(correlation_scores, columns=[<span class="string">'feature'</span>, <span class="string">'split_score'</span>, <span class="string">'gain_score'</span>])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">score_feature_selection</span><span class="params">(df=None, train_features=None, cat_feats=None, target=None)</span>:</span></span><br><span class="line">    <span class="comment"># Fit LightGBM </span></span><br><span class="line">    dtrain = lgb.Dataset(df[train_features], target, free_raw_data=<span class="literal">False</span>, silent=<span class="literal">True</span>)</span><br><span class="line">    lgb_params = &#123;</span><br><span class="line">        <span class="string">'objective'</span>: <span class="string">'binary'</span>,</span><br><span class="line">        <span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,</span><br><span class="line">        <span class="string">'learning_rate'</span>: <span class="number">.1</span>,</span><br><span class="line">        <span class="string">'subsample'</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">'num_leaves'</span>: <span class="number">31</span>,</span><br><span class="line">        <span class="string">'max_depth'</span>: <span class="number">-1</span>,</span><br><span class="line">        <span class="string">'seed'</span>: <span class="number">13</span>,</span><br><span class="line">        <span class="string">'n_jobs'</span>: <span class="number">4</span>,</span><br><span class="line">        <span class="string">'min_split_gain'</span>: <span class="number">.00001</span>,</span><br><span class="line">        <span class="string">'reg_alpha'</span>: <span class="number">.00001</span>,</span><br><span class="line">        <span class="string">'reg_lambda'</span>: <span class="number">.00001</span>,</span><br><span class="line">        <span class="string">'metric'</span>: <span class="string">'auc'</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fit the model</span></span><br><span class="line">    hist = lgb.cv(</span><br><span class="line">        params=lgb_params, </span><br><span class="line">        train_set=dtrain, </span><br><span class="line">        num_boost_round=<span class="number">2000</span>,</span><br><span class="line">        categorical_feature=cat_feats,</span><br><span class="line">        nfold=<span class="number">5</span>,</span><br><span class="line">        stratified=<span class="literal">True</span>,</span><br><span class="line">        shuffle=<span class="literal">True</span>,</span><br><span class="line">        early_stopping_rounds=<span class="number">50</span>,</span><br><span class="line">        verbose_eval=<span class="number">0</span>,</span><br><span class="line">        seed=<span class="number">17</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># Return the last mean / std values </span></span><br><span class="line">    <span class="keyword">return</span> hist[<span class="string">'auc-mean'</span>][<span class="number">-1</span>], hist[<span class="string">'auc-stdv'</span>][<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># features = [f for f in data.columns if f not in ['SK_ID_CURR', 'TARGET']]</span></span><br><span class="line"><span class="comment"># score_feature_selection(df=data[features], train_features=features, target=data['TARGET'])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> threshold <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span> , <span class="number">40</span>, <span class="number">50</span> ,<span class="number">60</span> , <span class="number">70</span>, <span class="number">80</span> , <span class="number">90</span>, <span class="number">95</span>, <span class="number">99</span>]:</span><br><span class="line">    split_feats = [_f <span class="keyword">for</span> _f, _score, _ <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> _score &gt;= threshold]</span><br><span class="line">    split_cat_feats = [_f <span class="keyword">for</span> _f, _score, _ <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> (_score &gt;= threshold) &amp; (_f <span class="keyword">in</span> categorical_feats)]</span><br><span class="line">    gain_feats = [_f <span class="keyword">for</span> _f, _, _score <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> _score &gt;= threshold]</span><br><span class="line">    gain_cat_feats = [_f <span class="keyword">for</span> _f, _, _score <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> (_score &gt;= threshold) &amp; (_f <span class="keyword">in</span> categorical_feats)]</span><br><span class="line">                                                                                             </span><br><span class="line">    print(<span class="string">'Results for threshold %3d'</span> % threshold)</span><br><span class="line">    split_results = score_feature_selection(df=data, train_features=split_feats, cat_feats=split_cat_feats, target=data[<span class="string">'TARGET'</span>])</span><br><span class="line">    print(<span class="string">'\t SPLIT : %.6f +/- %.6f'</span> % (split_results[<span class="number">0</span>], split_results[<span class="number">1</span>]))</span><br><span class="line">    gain_results = score_feature_selection(df=data, train_features=gain_feats, cat_feats=gain_cat_feats, target=data[<span class="string">'TARGET'</span>])</span><br><span class="line">    print(<span class="string">'\t GAIN  : %.6f +/- %.6f'</span> % (gain_results[<span class="number">0</span>], gain_results[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><h3 id="特征相关性低"><a href="#特征相关性低" class="headerlink" title="特征相关性低"></a>特征相关性低</h3><p>如果特征之间的相关性很低的情况下，可以进一步检测变量之间的独立性，如果变量之间是independent意味着可以单独将这些特征进行多个模型训练再融合</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;如何进行特征选择&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>分类变量：Target Encoding</title>
    <link href="http://kodgv.xyz/2019/04/08/targetencoding/"/>
    <id>http://kodgv.xyz/2019/04/08/targetencoding/</id>
    <published>2019-04-08T14:57:06.000Z</published>
    <updated>2019-04-11T02:48:35.494Z</updated>
    
    <content type="html"><![CDATA[<p>​    当你在进行监督学习时，你经常要处理分类变量。也就是说，变量没有一个自然的数值表示。问题是大多数机器学习算法都要求输入数据是数值的。在某个时候，数据科学管道将需要将分类变量转换为数值变量。</p><a id="more"></a><p>来源：<a href="https://maxhalford.github.io/blog/target-encoding-done-the-right-way/" target="_blank" rel="noopener">https://maxhalford.github.io/blog/target-encoding-done-the-right-way/</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>​    当你在进行监督学习时，你经常要处理分类变量。也就是说，变量没有一个自然的数值表示。问题是大多数机器学习算法都要求输入数据是数值的。在某个时候，数据科学管道将需要将分类变量转换为数值变量。</p><p>有很多方法可以做到这一点:</p><ul><li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" target="_blank" rel="noopener">Label encoding</a> 为每个类别选择数字</li><li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank" rel="noopener">One-hot encoding</a>:为每个类别创建一个二进制列</li><li><a href="https://www.tensorflow.org/tutorials/representation/word2vec" target="_blank" rel="noopener">Vector representation</a> ：也就是word2vec，在这里您可以找到一个适合您的数据的低维子空间</li><li><a href="https://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst#categorical-feature-support" target="_blank" rel="noopener">Optimal binning</a> ：在依赖于LightGBM或CatBoost等树学习器</li><li><a href="http://www.saedsayad.com/encoding.htm" target="_blank" rel="noopener">Target encoding</a>: 按类别平均目标值</li></ul><p>​    每种方法都有其优缺点，通常取决于您的数据和需求。如果一个变量有很多类别，那么一个[<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank" rel="noopener">One-hot encoding</a>:方案将产生许多列，这可能导致内存问题。根据我的经验，依赖LightGBM/CatBoost是最好的分箱方法。<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" target="_blank" rel="noopener">Label encoding</a>是没有用的，最好不要使用它。然而，如果你的分类变量恰好是有序的，那么你可以而且应该用递增的数字来表示它(例如，“cold”变成0，“mild”变成1，“hot”变成2)。word2vec和其他类似的方法很酷也很好，但是它们需要进行一些微调，而且并不总是奏效。</p><h2 id="target-encoding"><a href="#target-encoding" class="headerlink" title="target encoding"></a>target encoding</h2><p>​    target encoding是很容易理解的一种思想，假设你有分类变量X和变量Y，然后对于X中每一个distinct的元素计算其对应Y值的平局之，然后用这个平均值替换 $ x_i $ .下面这个例子显而易见：</p><div class="table-container"><table><thead><tr><th>x0</th><th>x1</th><th>y</th></tr></thead><tbody><tr><td>aa</td><td>cc</td><td>1</td></tr><tr><td>aa</td><td>cc</td><td>1</td></tr><tr><td>aa</td><td>cc</td><td>1</td></tr><tr><td>aa</td><td>cc</td><td>1</td></tr><tr><td>aa</td><td>cc</td><td>0</td></tr><tr><td>bb</td><td>cc</td><td>1</td></tr><tr><td>bb</td><td>cc</td><td>0</td></tr><tr><td>bb</td><td>cc</td><td>0</td></tr><tr><td>bb</td><td>cc</td><td>0</td></tr><tr><td>bb</td><td>dd</td><td>0</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>x0</th><th>x1</th><th>y</th></tr></thead><tbody><tr><td>0.8</td><td>cc</td><td>1</td></tr><tr><td>0.8</td><td>cc</td><td>1</td></tr><tr><td>0.8</td><td>cc</td><td>1</td></tr><tr><td>0.8</td><td>cc</td><td>1</td></tr><tr><td>0.8</td><td>cc</td><td>0</td></tr><tr><td>0.2</td><td>cc</td><td>1</td></tr><tr><td>0.2</td><td>cc</td><td>0</td></tr><tr><td>0.2</td><td>cc</td><td>0</td></tr><tr><td>0.2</td><td>cc</td><td>0</td></tr><tr><td>0.2</td><td>dd</td><td>0</td></tr></tbody></table></div><p>​    Target encoding的好处在于它提取了那些可以解释Y值，比如这里得aa拥有了一个Y值的平均值0.8，这会很好的帮助下游机器学习分类算法。</p><p>​    目标编码的问题有一个名称:过拟合。事实上，当平均值的数值很低时，依赖平均值并不总是一个好主意。您必须记住，您正在训练的数据集是一个更大的数据集的样本。这意味着，当您将训练集中发现的任何规律应用到另一个数据集(即测试集)时，可能都不成立。比如有可能训练集内它对应的Y值都是0，但是在测试集中它对应的Y值1比较多。</p><p>​    得出结论，永远不要用基础的target encoding,要用也是用以下的进阶encoding。</p><h2 id="Target-encoding进阶"><a href="#Target-encoding进阶" class="headerlink" title="Target encoding进阶"></a>Target encoding进阶</h2><p>有很多方法可以处理这个问题。交叉验证和additive smoothing可以结合使用</p><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>一种流行的方法是使用交叉验证并计算每个切分数据集中的平均值。这就是H20和许多kaggler所做的。</p><ul><li><p>None: no holdout, mean是对训练集的所有数据行的计算应用于测试数据</p></li><li><p>leave eoneout: mean是对除了当前行本身之外的所有数据行进行计算。这可以用于训练数据。当前行本身的目标不包括在平均值中，以防止过度拟合</p></li><li><p>KFold:平均值只计算out-of-fold数据(需要K-fold)</p></li></ul><p>这可以用于训练数据。为了防止过拟合，目标均值是根据叠外数据计算的</p><h3 id="additive-smoothing"><a href="#additive-smoothing" class="headerlink" title="additive smoothing"></a>additive smoothing</h3><p> 使用<a href="https://www.wikiwand.com/en/Additive_smoothing" target="_blank" rel="noopener">additive smoothing</a>，因为数据集中存在数据的count较小，所以它的target值容易受到过拟合的影响。<br>它使用了全局的平均值来<strong>smooth</strong>较少数据带来过拟合的影响</p><p>数学上它等价于:</p><script type="math/tex; mode=display">u=\frac{n\times \hat x+m\times w}{n+m}</script><p>where</p><ul><li>μ is the mean we’re trying to compute (the one that’s going to replace our categorical values)</li><li>n is the number of values you have</li><li>¯x is your estimated mean</li><li>m is the “weight” you want to assign to the overall mean</li><li>w is the overall mean</li></ul><p>其中m就是用来调节全局的权重，根据他的经验，他发现m取300的时候适用于大多数场合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_smooth_mean</span><span class="params">(df, by, on, m)</span>:</span></span><br><span class="line">    <span class="comment"># Compute the global mean</span></span><br><span class="line">    mean = df[on].mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the number of values and the mean of each group</span></span><br><span class="line">    agg = df.groupby(by)[on].agg([<span class="string">'count'</span>, <span class="string">'mean'</span>])</span><br><span class="line">    counts = agg[<span class="string">'count'</span>]</span><br><span class="line">    means = agg[<span class="string">'mean'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the "smoothed" means</span></span><br><span class="line">    smooth = (counts * means + m * mean) / (counts + m)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Replace each value by the according smoothed mean</span></span><br><span class="line">    <span class="keyword">return</span> df[by].map(smooth)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    当你在进行监督学习时，你经常要处理分类变量。也就是说，变量没有一个自然的数值表示。问题是大多数机器学习算法都要求输入数据是数值的。在某个时候，数据科学管道将需要将分类变量转换为数值变量。&lt;/p&gt;
    
    </summary>
    
      <category term="竞赛经验" scheme="http://kodgv.xyz/categories/%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="预处理" scheme="http://kodgv.xyz/tags/%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="分类变量" scheme="http://kodgv.xyz/tags/%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>比赛EDA</title>
    <link href="http://kodgv.xyz/2019/04/08/%E6%AF%94%E8%B5%9BEDA/"/>
    <id>http://kodgv.xyz/2019/04/08/比赛EDA/</id>
    <published>2019-04-08T10:38:48.000Z</published>
    <updated>2019-04-12T07:10:02.718Z</updated>
    
    <content type="html"><![CDATA[<p>比赛常见的EDA总结</p><a id="more"></a><p>[TOC]</p><h2 id="检查缺失值"><a href="#检查缺失值" class="headerlink" title="检查缺失值"></a>检查缺失值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/gpreda/santander-eda-and-prediction</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">missing_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    total = data.isnull().sum()</span><br><span class="line">    percent = (data.isnull().sum()/data.isnull().count()*<span class="number">100</span>)</span><br><span class="line">    tt = pd.concat([total, percent], axis=<span class="number">1</span>, keys=[<span class="string">'Total'</span>, <span class="string">'Percent'</span>])</span><br><span class="line">    types = []</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> data.columns:</span><br><span class="line">        dtype = str(data[col].dtype)</span><br><span class="line">        types.append(dtype)</span><br><span class="line">    tt[<span class="string">'Types'</span>] = types</span><br><span class="line">    <span class="keyword">return</span>(np.transpose(tt))</span><br></pre></td></tr></table></figure><h2 id="观察分布值"><a href="#观察分布值" class="headerlink" title="观察分布值"></a>观察分布值</h2><h3 id="观察训练集的01分布-可以拓展到任意分布对比图"><a href="#观察训练集的01分布-可以拓展到任意分布对比图" class="headerlink" title="观察训练集的01分布(可以拓展到任意分布对比图)"></a>观察训练集的01分布(可以拓展到任意分布对比图)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/gpreda/santander-eda-and-prediction</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_feature_distribution</span><span class="params">(df1, df2, label1, label2, features)</span>:</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    sns.set_style(<span class="string">'whitegrid'</span>)</span><br><span class="line">    plt.figure()</span><br><span class="line">    fig, ax = plt.subplots(<span class="number">10</span>,<span class="number">10</span>,figsize=(<span class="number">18</span>,<span class="number">22</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        plt.subplot(<span class="number">10</span>,<span class="number">10</span>,i)</span><br><span class="line">        sns.distplot(df1[feature], hist=<span class="literal">False</span>,label=label1)</span><br><span class="line">        sns.distplot(df2[feature], hist=<span class="literal">False</span>,label=label2)</span><br><span class="line">        plt.xlabel(feature, fontsize=<span class="number">9</span>)</span><br><span class="line">        locs, labels = plt.xticks()</span><br><span class="line">        plt.tick_params(axis=<span class="string">'x'</span>, which=<span class="string">'major'</span>, labelsize=<span class="number">6</span>, pad=<span class="number">-6</span>)</span><br><span class="line">        plt.tick_params(axis=<span class="string">'y'</span>, which=<span class="string">'major'</span>, labelsize=<span class="number">6</span>)</span><br><span class="line">    plt.show();</span><br><span class="line">    </span><br><span class="line">t0 = train_df.loc[train_df[<span class="string">'target'</span>] == <span class="number">0</span>]</span><br><span class="line">t1 = train_df.loc[train_df[<span class="string">'target'</span>] == <span class="number">1</span>]</span><br><span class="line">features = train_df.columns.values[<span class="number">2</span>:<span class="number">102</span>]</span><br><span class="line">plot_feature_distribution(t0, t1, <span class="string">'0'</span>, <span class="string">'1'</span>, features)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/201904091741016.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NjYxMjMw,size_16,color_FFFFFF,t_70" alt></p><p>结论：分布图怎么看，就是看如果01分布完全一模一样那么可能说明这个特征是冗余的无关特征，如果01分布不同的情况下，也做不了太多东西，只能结合特征重要度反证这些特征是有用的。</p><h3 id="train-test的分布"><a href="#train-test的分布" class="headerlink" title="train_test的分布"></a>train_test的分布</h3><p>同理可以选择train和test的行和列的mean/std/min值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">https://www.kaggle.com/gpreda/santander-eda-<span class="keyword">and</span>-prediction</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">"Distribution of mean values per column in the train and test set"</span>)</span><br><span class="line">sns.distplot(train_df[features].mean(axis=<span class="number">0</span>),color=<span class="string">"magenta"</span>,kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'train'</span>)</span><br><span class="line">sns.distplot(test_df[features].mean(axis=<span class="number">0</span>),color=<span class="string">"darkblue"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'test'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">"Distribution of std values per row in the train and test set"</span>)</span><br><span class="line">sns.distplot(train_df[features].std(axis=<span class="number">1</span>),color=<span class="string">"black"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'train'</span>)</span><br><span class="line">sns.distplot(test_df[features].std(axis=<span class="number">1</span>),color=<span class="string">"red"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'test'</span>)</span><br><span class="line">plt.legend();plt.show()</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">"Distribution of std values per column in the train and test set"</span>)</span><br><span class="line">sns.distplot(train_df[features].std(axis=<span class="number">0</span>),color=<span class="string">"blue"</span>,kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'train'</span>)</span><br><span class="line">sns.distplot(test_df[features].std(axis=<span class="number">0</span>),color=<span class="string">"green"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'test'</span>)</span><br><span class="line">plt.legend(); plt.show()</span><br><span class="line">t0 = train_df.loc[train_df[<span class="string">'target'</span>] == <span class="number">0</span>]</span><br><span class="line">t1 = train_df.loc[train_df[<span class="string">'target'</span>] == <span class="number">1</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">"Distribution of mean values per row in the train set"</span>)</span><br><span class="line">sns.distplot(t0[features].mean(axis=<span class="number">1</span>),color=<span class="string">"red"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'target = 0'</span>)</span><br><span class="line">sns.distplot(t1[features].mean(axis=<span class="number">1</span>),color=<span class="string">"blue"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'target = 1'</span>)</span><br><span class="line">plt.legend(); plt.show()</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">"Distribution of mean values per column in the train set"</span>)</span><br><span class="line">sns.distplot(t0[features].mean(axis=<span class="number">0</span>),color=<span class="string">"green"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'target = 0'</span>)</span><br><span class="line">sns.distplot(t1[features].mean(axis=<span class="number">0</span>),color=<span class="string">"darkblue"</span>, kde=<span class="literal">True</span>,bins=<span class="number">120</span>, label=<span class="string">'target = 1'</span>)</span><br><span class="line">plt.legend(); plt.show()</span><br></pre></td></tr></table></figure><h2 id="检查duplicate-values"><a href="#检查duplicate-values" class="headerlink" title="检查duplicate values"></a>检查duplicate values</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">features = train_df.columns.values[<span class="number">2</span>:<span class="number">202</span>]</span><br><span class="line">unique_max_train = []</span><br><span class="line">unique_max_test = []</span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">    values = train_df[feature].value_counts()</span><br><span class="line">    unique_max_train.append([feature, values.max(), values.idxmax()])</span><br><span class="line">    values = test_df[feature].value_counts()</span><br><span class="line">    unique_max_test.append([feature, values.max(), values.idxmax()])</span><br></pre></td></tr></table></figure><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p><a href="https://zhuanlan.zhihu.com/p/28909807" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28909807</a></p><p>主成分分析是统计方法里的一种降维方法，它的主要思想是将原有 <img src="https://www.zhihu.com/equation?tex=n" alt="n"> 个特征通过正交变换将一组可能存在相关性的特征缩减到 <img src="https://www.zhihu.com/equation?tex=k" alt="k"> 特征( <img src="https://www.zhihu.com/equation?tex=k%5Cleq+n" alt="k\leq n"> )。举例来说，在网站用户行为数据收集过程中，会话数(Visits)，浏览页数(PV)，网站总停留时间(Time Spend Total)，访问人数(Unique Visitor)，以上这几个指标，无论是从以往的数据统计还是业务经验来看，都存在一定正相关关系，如果将这些特征喂给模型，很容易造成过拟合。</p><p>通过转化，从而剔除噪声，而且可以看到转化后的值然后把一些无关的列再剔除</p><p><strong>PCA 使用要点</strong></p><ol><li><p>使用主成分分析，往往会丢失掉“少部分信息”（注意：这“少部分信息”仅仅指方差较小的数据，并非信息含量真的少的数据！）</p></li><li><p>因为1的特性，所以在机器学些中，不推荐使用 PCA 去优化特征达到避免过拟合的目的。</p></li><li><p>既然PCA不能避免过拟合，那为何还要使用，根据周志华老师的西瓜书中的描述：</p></li><li><ol><li>在舍弃特征值较小的特征之后，能够使样本采集密度增</li><li>当数据受到噪声影响时，最小的特征值对对应的特征向量往往与噪声有关将他们舍弃能够在一定程度上起到去噪效果</li></ol></li><li><p>在维度过多的情况下，变换后的坐标系代表的意义不明，不易于解释。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/roydatascience/eda-pca-lgbm-santander-transactions/notebook</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> KernelPCA</span><br><span class="line">lin_pca = KernelPCA(n_components = <span class="number">2</span>, kernel=<span class="string">"linear"</span>, fit_inverse_transform=<span class="literal">True</span>)</span><br><span class="line">rbf_pca = KernelPCA(n_components = <span class="number">2</span>, kernel=<span class="string">"rbf"</span>, gamma=<span class="number">0.0433</span>, fit_inverse_transform=<span class="literal">True</span>)</span><br><span class="line">sig_pca = KernelPCA(n_components = <span class="number">2</span>, kernel=<span class="string">"sigmoid"</span>, gamma=<span class="number">0.001</span>, coef0=<span class="number">1</span>, fit_inverse_transform=<span class="literal">True</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">11</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> subplot, pca, title <span class="keyword">in</span> ((<span class="number">131</span>, lin_pca, <span class="string">"Linear kernel"</span>), (<span class="number">132</span>, rbf_pca, <span class="string">"RBF kernel, $\gamma=0.04$"</span>), </span><br><span class="line">                            (<span class="number">133</span>, sig_pca, <span class="string">"Sigmoid kernel, $\gamma=10^&#123;-3&#125;, r=1$"</span>)):   </span><br><span class="line">    PCA_train_x = PCA(<span class="number">2</span>).fit_transform(train_scaled)</span><br><span class="line">    plt.subplot(subplot)</span><br><span class="line">    plt.title(title, fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.scatter(PCA_train_x[:, <span class="number">0</span>], PCA_train_x[:, <span class="number">1</span>], c=target, cmap=<span class="string">"nipy_spectral_r"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"$z_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">    <span class="keyword">if</span> subplot == <span class="number">131</span>:</span><br><span class="line">        plt.ylabel(<span class="string">"$z_2$"</span>, fontsize=<span class="number">18</span>, rotation=<span class="number">0</span>)</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="热度图"><a href="#热度图" class="headerlink" title="热度图"></a>热度图</h2><h3 id="查看prediction"><a href="#查看prediction" class="headerlink" title="查看prediction"></a>查看prediction</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920</span></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">sns.heatmap(x, cmap=<span class="string">'RdBu_r'</span>, center=<span class="number">0.0</span>) </span><br><span class="line">plt.title(<span class="string">'VAR_'</span>+str(j)+<span class="string">' Predictions without Magic'</span>,fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xticks(np.linspace(<span class="number">0</span>,<span class="number">49</span>,<span class="number">5</span>),np.round(np.linspace(mn,mx,<span class="number">5</span>),<span class="number">1</span>))</span><br><span class="line">plt.xlabel(<span class="string">'Var_'</span>+str(j))</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.ylabel(<span class="string">''</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190412150953124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NjYxMjMw,size_16,color_FFFFFF,t_70" alt></p><p>通过热度图可以看出模型是否明显利用了两个特征。虽然可能特征重要度也会说明，但是这样的热度图明显会更具说服力，比如说var_0，模型可以得出竖线15，横线4之类的分界线。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛常见的EDA总结&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="预处理" scheme="http://kodgv.xyz/tags/%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="特征工程" scheme="http://kodgv.xyz/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>numpy</title>
    <link href="http://kodgv.xyz/2019/04/08/numpy/"/>
    <id>http://kodgv.xyz/2019/04/08/numpy/</id>
    <published>2019-04-08T08:31:21.000Z</published>
    <updated>2019-04-08T10:18:18.966Z</updated>
    
    <content type="html"><![CDATA[<p>numpy 操作小技巧<br><a id="more"></a></p><h2 id="增加"><a href="#增加" class="headerlink" title="增加"></a>增加</h2><p>concat/vstack/hstack</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:<span class="regexp">//</span>blog.csdn.net<span class="regexp">/xiaodongxiexie/</span>article<span class="regexp">/details/</span><span class="number">71774466</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;numpy 操作小技巧&lt;br&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="Numpy" scheme="http://kodgv.xyz/tags/Numpy/"/>
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://kodgv.xyz/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python代码</title>
    <link href="http://kodgv.xyz/2019/04/08/python%E4%BB%A3%E7%A0%81/"/>
    <id>http://kodgv.xyz/2019/04/08/python代码/</id>
    <published>2019-04-08T08:12:31.000Z</published>
    <updated>2019-04-09T09:39:10.715Z</updated>
    
    <content type="html"><![CDATA[<p>python 代码小技巧</p><a id="more"></a><p>[TOC]</p><h2 id="通用"><a href="#通用" class="headerlink" title="通用"></a>通用</h2><h3 id="测试时间"><a href="#测试时间" class="headerlink" title="测试时间"></a>测试时间</h3><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">关注walltime</span><br><span class="line"><span class="meta">%</span><span class="meta">%</span>time</span><br></pre></td></tr></table></figure><h2 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h2><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index, item <span class="keyword">in</span> enumerate(list1):</span><br><span class="line">    <span class="keyword">print</span> index, item</span><br></pre></td></tr></table></figure><h3 id="并行遍历"><a href="#并行遍历" class="headerlink" title="并行遍历"></a>并行遍历</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (x,y) <span class="keyword">in</span> zip(a,b):</span><br><span class="line">   <span class="keyword">print</span> x,y</span><br></pre></td></tr></table></figure><h3 id="通过字典保存了所有中间过程，不仅命名容易，而且便于后续操作"><a href="#通过字典保存了所有中间过程，不仅命名容易，而且便于后续操作" class="headerlink" title="通过字典保存了所有中间过程，不仅命名容易，而且便于后续操作"></a>通过字典保存了所有中间过程，不仅命名容易，而且便于后续操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d=&#123;d_name:get_dvalue(d_name) <span class="keyword">for</span> d_name <span class="keyword">in</span> d_list&#125;</span><br><span class="line">best_dvalue_dname=min(d.items(),key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h3 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这样会比单纯用readlines()快</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> index, line <span class="keyword">in</span> enumerate(open(filepath,<span class="string">'r'</span>))： </span><br><span class="line">    count += <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections import defaultdict 通过这个声明dict&#123;里面的格式&#125;</span><br><span class="line">如 defaultdict(<span class="built_in">list</span>)</span><br></pre></td></tr></table></figure><h3 id="列表生成式"><a href="#列表生成式" class="headerlink" title="列表生成式"></a>列表生成式</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>. [<span class="selector-tag">i</span> <span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> range(k) <span class="keyword">if</span> condition]：此时<span class="keyword">if</span>起条件判断作用，满足条件的，将被返回成为最终生成的列表的一员。</span><br><span class="line"><span class="number">2</span>. [<span class="selector-tag">i</span> <span class="keyword">if</span> condition <span class="keyword">else</span> exp <span class="keyword">for</span> exp]：此时<span class="keyword">if</span>...<span class="keyword">else</span>被用来赋值，满足条件的i以及<span class="keyword">else</span>被用来生成最终的列表</span><br></pre></td></tr></table></figure><h2 id="自带算法库"><a href="#自带算法库" class="headerlink" title="自带算法库"></a>自带算法库</h2><h3 id="数组中查找插入的位置"><a href="#数组中查找插入的位置" class="headerlink" title="数组中查找插入的位置"></a>数组中查找插入的位置</h3><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">返回排序插入的位置，并不是真的插入排序数组中</span><br><span class="line"><span class="symbol">from</span> <span class="keyword">bisect </span><span class="meta">import</span> <span class="keyword">bisect_left, </span><span class="keyword">bisect_right</span></span><br><span class="line"><span class="keyword">end </span>= <span class="keyword">bisect_left(keys, </span><span class="number">0</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python 代码小技巧&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://kodgv.xyz/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>比赛通用代码</title>
    <link href="http://kodgv.xyz/2019/04/08/%E6%AF%94%E8%B5%9B%E9%80%9A%E7%94%A8%E4%BB%A3%E7%A0%81/"/>
    <id>http://kodgv.xyz/2019/04/08/比赛通用代码/</id>
    <published>2019-04-08T07:58:10.000Z</published>
    <updated>2019-04-12T08:36:12.397Z</updated>
    
    <content type="html"><![CDATA[<p>比赛通用代码</p><a id="more"></a><p>[TOC]</p><h2 id="jupyter-载入代码包"><a href="#jupyter-载入代码包" class="headerlink" title="jupyter 载入代码包"></a>jupyter 载入代码包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="keyword">import</span> plotly.offline <span class="keyword">as</span> py</span><br><span class="line"><span class="keyword">import</span> plotly.graph_objs <span class="keyword">as</span> go</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line">pd.options.display.max_columns = <span class="number">100</span></span><br><span class="line">gc.enable()</span><br><span class="line"></span><br><span class="line"><span class="string">'''Displays markdown formatted output like bold, italic bold etc.'''</span></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Markdown</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bold</span><span class="params">(string)</span>:</span></span><br><span class="line">    display(Markdown(string))</span><br><span class="line"></span><br><span class="line"><span class="string">'''Ignores deprecation warning.'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ignore_warnings</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">import</span> warnings</span><br><span class="line">    warnings.filterwarnings(<span class="string">'ignore'</span>, category = DeprecationWarning)  </span><br><span class="line">bold(<span class="string">'**Merged data:**'</span>)</span><br><span class="line">display(merged.head())</span><br></pre></td></tr></table></figure><h2 id="自动压缩变量空间"><a href="#自动压缩变量空间" class="headerlink" title="自动压缩变量空间"></a>自动压缩变量空间</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_mem_usage</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="string">""" iterate through all the columns of a dataframe and modify the data type</span></span><br><span class="line"><span class="string">        to reduce memory usage.        </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">#start_mem = df.memory_usage().sum() / 1024**2</span></span><br><span class="line">    <span class="comment">#print('Memory usage of dataframe is &#123;:.2f&#125; MB'.format(start_mem))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtype</span><br><span class="line">        <span class="keyword">if</span> col_type != object:</span><br><span class="line">            c_min = df[col].min()</span><br><span class="line">            c_max = df[col].max()</span><br><span class="line">            <span class="keyword">if</span> str(col_type)[:<span class="number">3</span>] == <span class="string">'int'</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).min <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).min <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).min <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).min <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)  </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).min <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).max:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).min <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).max:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line">                </span><br><span class="line">    <span class="comment">#end_mem = df.memory_usage().sum() / 1024**2</span></span><br><span class="line">    <span class="comment">#print('Memory usage after optimization is: &#123;:.2f&#125; MB'.format(end_mem))</span></span><br><span class="line">    <span class="comment">#print('Decreased by &#123;:.1f&#125;%'.format(100 * (start_mem - end_mem) / start_mem))</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reload</span><span class="params">()</span>:</span></span><br><span class="line">    gc.collect()</span><br><span class="line">    df = pd.read_csv(<span class="string">'../input/train_V2.csv'</span>)</span><br><span class="line">    invalid_match_ids = df[df[<span class="string">'winPlacePerc'</span>].isna()][<span class="string">'matchId'</span>].values</span><br><span class="line">    df = df[-df[<span class="string">'matchId'</span>].isin(invalid_match_ids)]</span><br><span class="line">    df=reduce_mem_usage(df)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><h2 id="GPU使用"><a href="#GPU使用" class="headerlink" title="GPU使用"></a>GPU使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/<span class="number">89004</span><span class="comment">#latest-514910</span></span><br><span class="line">FYI, Kaggle provides us the use of <span class="number">7</span> Tesla P100 GPU<span class="string">'s simultaneously. Below are some tips. Python models will not run GPU by default. You must turn GPU on. First, activate GPU in kaggle kernels. Second, if using CatBoost, add the following hyperparameter, task_type = '</span>GPU<span class="string">'. If using XGBoost, add the following hyperparameter, '</span>tree_method<span class="string">': '</span>gpu_hist<span class="string">' or tree_method'</span>: <span class="string">'gpu_exact'</span>. If using LGBM, follow the instructions <span class="keyword">in</span> this kernel to recompile GPU LGBM, then add the following <span class="number">3</span> hyperparameters <span class="string">'device'</span>: <span class="string">'gpu'</span>, <span class="string">'gpu_platform_id'</span>: <span class="number">0</span>, <span class="keyword">and</span> <span class="string">'gpu_device_id'</span>: <span class="number">0.</span> Note: many posted kernels regarding CatBoost <span class="keyword">and</span> XGBoost don<span class="string">'t use GPU and can actually be 4x faster with GPU activated!</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">You are allowed to execute 7 GPU kernels simultaneously for 9 hour sessions. In one evening, you can train 100 models!! My final solution is a blend of dozens of LGBM, CatBoost and XGBoost. In one evening, you can train your models on a combined one billion generated new rows of augmented data using the power of Kaggle'</span>s GPU<span class="string">'s (where 7 have a total cost of $49000!!)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In this comp, I found that CatBoost achieved an LB 0.001 greater than LGBM. That'</span>s it. Enjoy the power!!</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛通用代码&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://kodgv.xyz/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>pandas</title>
    <link href="http://kodgv.xyz/2019/04/08/pandas/"/>
    <id>http://kodgv.xyz/2019/04/08/pandas/</id>
    <published>2019-04-08T07:43:28.000Z</published>
    <updated>2019-04-09T09:38:59.172Z</updated>
    
    <content type="html"><![CDATA[<p>pandas 操作小技巧</p><a id="more"></a><h2 id="通用操作"><a href="#通用操作" class="headerlink" title="通用操作"></a>通用操作</h2><h3 id="pandas操作出现进度条："><a href="#pandas操作出现进度条：" class="headerlink" title="pandas操作出现进度条："></a>pandas操作出现进度条：</h3><ul><li>用作迭代器</li><li>用于Pandas的操作<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line">tqdm.pandas()</span><br><span class="line">sentences = train[<span class="string">"question_text"</span>].progress_apply(<span class="keyword">lambda</span> x: x.split()).values</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> tqdm(sentences, disable = (<span class="keyword">not</span> verbose)):</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://img-blog.csdnimg.cn/20190117214323585.png" alt="在这里插入图片描述"></p><h2 id="增加"><a href="#增加" class="headerlink" title="增加"></a>增加</h2><h3 id="merge-concat-join"><a href="#merge-concat-join" class="headerlink" title="merge/concat/join"></a>merge/concat/join</h3><p><a href="https://www.e-learn.cn/content/qita/814185" target="_blank" rel="noopener">https://www.e-learn.cn/content/qita/814185</a></p><h2 id="改变"><a href="#改变" class="headerlink" title="改变"></a>改变</h2><h3 id="直接replace"><a href="#直接replace" class="headerlink" title="直接replace"></a>直接replace</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''Put Dona, Jonkheer, Countess, Sir, Lady, Don in bucket Aristocrat.'''</span></span><br><span class="line">merged.Title.replace(to_replace = [<span class="string">'Dona'</span>, <span class="string">'Jonkheer'</span>, <span class="string">'Countess'</span>, <span class="string">'Sir'</span>, <span class="string">'Lady'</span>, <span class="string">'Don'</span>], value = <span class="string">'Aristocrat'</span>, inplace = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="map映射"><a href="#map映射" class="headerlink" title="map映射"></a>map映射</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#map映射 它就会自动把类别型转为连续型</span></span><br><span class="line"><span class="comment">#如果要Inverse就直接key-value逆转</span></span><br><span class="line">size_mapping=&#123;<span class="string">'XL'</span>:<span class="number">3</span>,<span class="string">'L'</span>:<span class="number">2</span>,<span class="string">'M'</span>:<span class="number">1</span>&#125;</span><br><span class="line">df[<span class="string">'size'</span>]=df[<span class="string">'size'</span>].map(size_mapping)</span><br></pre></td></tr></table></figure><h3 id="单列分箱"><a href="#单列分箱" class="headerlink" title="单列分箱"></a>单列分箱</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''Create bin categories for Age.'''</span></span><br><span class="line">label_names = [<span class="string">'infant'</span>,<span class="string">'child'</span>,<span class="string">'teenager'</span>,<span class="string">'young_adult'</span>,<span class="string">'adult'</span>,<span class="string">'aged'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''Create range for each bin categories of Age.'''</span></span><br><span class="line">cut_points = [<span class="number">0</span>,<span class="number">5</span>,<span class="number">12</span>,<span class="number">18</span>,<span class="number">35</span>,<span class="number">60</span>,<span class="number">81</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">'''Create and view categorized Age with original Age.'''</span></span><br><span class="line">merged[<span class="string">'Age_binned'</span>] = pd.cut(merged.Age, cut_points, labels = label_names)</span><br><span class="line">display(merged[[<span class="string">'Age'</span>, <span class="string">'Age_binned'</span>]].head(<span class="number">2</span>)</span><br><span class="line">​</span><br></pre></td></tr></table></figure><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## 删除</span><br><span class="line"></span><br><span class="line">### 去重</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">train.drop_duplicates(subset = [<span class="string">'1_total_fee'</span>,<span class="string">'2_total_fee'</span>,<span class="string">'3_total_fee'</span>,</span><br><span class="line"> <span class="string">'month_traffic'</span>,<span class="string">'pay_times'</span>,<span class="string">'last_month_traffic'</span>,<span class="string">'service2_caller_time'</span>,<span class="string">'age'</span>],inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;pandas 操作小技巧&lt;/p&gt;
    
    </summary>
    
      <category term="数据竞赛" scheme="http://kodgv.xyz/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"/>
    
    
      <category term="机器学习" scheme="http://kodgv.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://kodgv.xyz/tags/python/"/>
    
      <category term="pandas" scheme="http://kodgv.xyz/tags/pandas/"/>
    
  </entry>
  
</feed>
