<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[测试集和训练集分布]]></title>
    <url>%2F2019%2F04%2F13%2F%E6%B5%8B%E8%AF%95%E9%9B%86%E5%92%8C%E8%AE%AD%E7%BB%83%E9%9B%86%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[Train &amp; Test分布主要是为了看数据的分布情况。 下面的方法是最近kaggle上GM搞的一种极其简单的探索训练集&amp;测试集分布的方案,将一个分布的问题变成一个二分类问题,该方法简单有效，如果AUC低于0.6,则我们可以认为训练集和测试集是分布平衡的,反之我们则可以认为训练集和测试集是分布不一致的,这个时候就需要注意AB榜是否会出现翻车的情况等 [TOC] 来源：https://www.kaggle.com/justfor/adversarial-validation-plus-ks-test Adversarial Validation下面的方法是最近kaggle上GM搞的一种极其简单的探索训练集&amp;测试集分布的方案,将一个分布的问题变成一个二分类问题,该方法简单有效，如果AUC低于0.6,则我们可以认为训练集和测试集是分布平衡的,反之我们则可以认为训练集和测试集是分布不一致的,这个时候就需要注意AB榜是否会出现翻车的情况等 核心点在于将y1 = np.array([0] x train.shape[0])y2 = np.array([1] x test.shape[0]) 1234567891011121314151617181920212223242526#Create label array and complete datasety1 = np.array([0]*train.shape[0])y2 = np.array([1]*test.shape[0])y = np.concatenate((y1, y2))X_data = pd.concat([train, test])X_data.reset_index(drop=True, inplace=True)#Initialize splits&amp;LGBMskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)lgb_model = lgb.LGBMClassifier(max_depth=-1, n_estimators=500, learning_rate=0.01, objective='binary', n_jobs=-1) counter = 1#Train 5-fold adversarial validation classifierfor train_index, test_index in skf.split(X_data, y): print('\nFold &#123;&#125;'.format(counter)) X_fit, X_val = X_data.loc[train_index], X_data.loc[test_index] y_fit, y_val = y[train_index], y[test_index] lgb_model.fit(X_fit, y_fit, eval_metric='auc', eval_set=[(X_val, y_val)], verbose=100, early_stopping_rounds=10) counter+=1 Kolmogorov-Smirnov Test单独看每个变量是否能过通过KS检验，不仅有图，而且有一种量化的手段 123456789101112131415161718192021222324252627#Load more packagesfrom scipy.stats import ks_2sampimport matplotlib.pyplot as pltimport seaborn as snssns.set_style('whitegrid')import warningswarnings.simplefilter(action='ignore', category=FutureWarning)warnings.filterwarnings('ignore')#Perform KS-Test for each feature from train/test. Draw its distribution. Count features based on statistics.#Plots are hidden. If you'd like to look at them - press "Output" button.hypothesisnotrejected = []hypothesisrejected = []for col in train.columns: statistic, pvalue = ks_2samp(train[col], test[col]) if pvalue&gt;=statistic: hypothesisnotrejected.append(col) if pvalue&lt;statistic: hypothesisrejected.append(col) plt.figure(figsize=(8,4)) plt.title("Kolmogorov-Smirnov test for train/test\n" "feature: &#123;&#125;, statistics: &#123;:.5f&#125;, pvalue: &#123;:5f&#125;".format(col, statistic, pvalue)) sns.kdeplot(train[col], color='blue', shade=True, label='Train') sns.kdeplot(test[col], color='green', shade=True, label='Test') plt.show()]]></content>
      <categories>
        <category>竞赛经验</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比赛规范经验]]></title>
    <url>%2F2019%2F04%2F12%2F%E6%AF%94%E8%B5%9B%E8%A7%84%E8%8C%83%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[比赛中的代码规范,代码不规范，亲人两行泪 先杂七杂八写，有一定量了再整理 思路要记在云笔记里，纸上会丢，而且换环境忘记带 临近比赛前一个月注意多保存看过的kernel，因为它们做出来之后就会删除。也就是说快结束的时候，那些都不是重要的magic 要保持一个纯洁统一模型的代码的单独文件，才方便接入]]></content>
      <categories>
        <category>竞赛经验</category>
      </categories>
      <tags>
        <tag>代码规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征构造]]></title>
    <url>%2F2019%2F04%2F11%2F%E7%89%B9%E5%BE%81%E6%9E%84%E9%80%A0%2F</url>
    <content type="text"><![CDATA[[TOC] 函数统一返回新的df，同时删除原有df 特征频数1234567# 构造特征时测试集要和训练集一样def encode_FE(df,col,test)： cv = df[col].value_counts() nm = col+'_FE' df[nm] = df[col].map(cv) test[nm] = test[col].map(cv) return df,test data augment123456789101112131415161718192021222324252627def augment(x,y,t=2): xs,xn = [],[] for i in range(t): mask = y&gt;0 x1 = x[mask].copy() ids = np.arange(x1.shape[0]) for c in range(x1.shape[1]): np.random.shuffle(ids) x1[:,c] = x1[ids][:,c] xs.append(x1) for i in range(t//2): mask = y==0 x1 = x[mask].copy() ids = np.arange(x1.shape[0]) for c in range(x1.shape[1]): np.random.shuffle(ids) x1[:,c] = x1[ids][:,c] xn.append(x1) xs = np.vstack(xs) xn = np.vstack(xn) ys = np.ones(xs.shape[0]) yn = np.zeros(xn.shape[0]) x = np.vstack([x,xs,xn]) y = np.concatenate([y,ys,yn]) return x,y]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Santander Customer Transaction Prediction 比赛经验]]></title>
    <url>%2F2019%2F04%2F11%2FSantander%20Customer%20Transaction%20Prediction%20%E6%AF%94%E8%B5%9B%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[Santander Customer Transaction Prediction 比赛，01分类问题，200维连续型匿名变量。 讲述了magic操作，count的新特征构造，特征相关性。。。 [TOC] 来源：https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920 magic kernel树模型的缺点无法学习到的东西 LGBM用竖线划分直方图，因为LGBM看不到水平差异。一个直方图会将多个值放置在一个Bin中并且产生一个较为平滑的图。如果你把多个值放在一个bin中，你会得到一个锯齿的图，其中每个bin中有些值是惟一的，有些值出现了几十次，LGBM无法学习到这些事情。 如上，构造的count图可以看出在相同值的附近存在不同频数的区别。 对特征敏感的参数设置​ 仅仅做到上述构造特征还是没有用的，因为你添加新特征到LGBM的时候设置参数feature_fraction=0.05，这会导致特征被随机的采样，破坏了var_1和var_1count之间的依赖关系(目的就是让模型需要同时学习横向和纵向)。所以设置feature_fraction=1，能从0.901到0.910，但是如果要到0.920则需要剔除原始变量之间的spurious effects,因为原始特征对模型敏感，同时相关系数普遍很低，则说明它们虽然有相关但是没有因果关联。 Use Data Augmentation (as shown in Jiwei’s awesome kernel here). You must keep original and new feature in same row. Use 200 separate models as shown in this kernel below. Merge new feature and original feature into one feature. In original data, simply add 200 to each unique value. (And don’t add new columns) 使用数据增强，就是随机打乱特征的时候，保持count和var是一致的，保证count和var的相关性，又去除了var之间相关性 单独使用count和var预测，然后200个模型再融合(一共200个var），这与第三点的merge我觉得是一致的，将两列合成一列（单纯加减应该是不够的) ​ 注意计算频数的时候，原则是同分布下越多数据越好。所以train和test在不在一起取决于它们的分布是否一致，或者如何让它们分布一致。在该比赛中就对test集进行了划分，剔除了和train不一致的数据，再合在一起做频数 代码主要参考链接 first kernel来源：https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920 magic feature不单单是用count ,而是更深入的使用count，告诉模型它所不知道的事情。比如说Unique in train 和test This value appears at least another time in data with target==1 and no 0; This value appears at least another time in data with target==0 and no 1; This value appears at least two more time in data with target==0 &amp; 1; This value is unique in data; This value is unique in data + test (only including real test samples); The other 200 (one per raw feature) features are numerical, let’s call them “not unique feat”, and correspond to the raw feature replacing values that are unique in data + test with the mean of the feature. 用该列的均值去替换该列的Unique值，使其成为非unique的列，这个操作是尝试出来的，它们也用了nan，median magic insight两个重要的节点： I looked at my LGBM trees (with only 3 leafs that’s easy to do) and noticed the trees were using the uniqueness information.通过树画图，看出树当前不能学习的东西 number of different values in train and test was not the same。 虽然数值上分布是一致的，但是在值的个数上不一致 technial part:匿名变量用NN会更好 NN的concat可以更好的处理变量之间的关系 rest magic using the shuffle augmentation (duplicate and shuffle 16 times samples with target == 1, 4 for target ==0) and added pseudo label (2700 highest predicted test points as 1 and 2000 lowest as 0) pseudo label指从测试集中取最高的为1最低的为0，加进去 shuffle augmentation 指数据增强(eda代码中有)]]></content>
      <categories>
        <category>竞赛经验</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比赛模型]]></title>
    <url>%2F2019%2F04%2F10%2F%E6%AF%94%E8%B5%9B%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[比赛通用模型代码 [TOC] 载入模型前操作归一化1234from sklearn.preprocessing import MinMaxScaler,StandardScalersc = StandardScaler()train_features = sc.fit_transform(train_features)test_features = sc.transform(test_features) 切分训练集和验证集12345678910111213141516171819# train_splitfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)# K-fold# scikit-learn k-fold cross-validationfrom sklearn.model_selection import KFold# data sample# prepare cross validationkfold = KFold(n_splits=3, shuffle = True, random_state= 1)# enumerate splitsfor train, test in kfold.split(data): print('train: %s, test: %s' % (data[train], data[test])) from sklearn.model_selection import StratifiedKFold# StratifiedKFoldfolds = StratifiedKFold(n_splits=num_folds, shuffle=False, random_state=2319)for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)): X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx] X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx] 模型xgb回归123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn########################################################################### 回归from xgboost.sklearn import XGBRegressorfrom sklearn.metrics import mean_absolute_errorfrom sklearn.model_selection import KFold, StratifiedKFold,GroupKFoldimport os# 别人的自定义损失函数,在parameter里面：object里面赋值def custom_loss(y_true,y_pred): penalty=2.0 grad=-y_true/y_pred+penalty*(1-y_true)/(1-y_pred) #梯度 hess=y_true/(y_pred**2)+penalty*(1-y_true)/(1-y_pred)**2 #2阶导 return grad,hess# 自定义评价函数def mse(y_pred,dtrain): #preds是结果（概率值），dtrain是个带label的DMatrix labels=dtrain.get_label() #提取label t=mean_absolute_error(labels, y_pred) print(t) return 'mse',tparameters = &#123;'nthread':-1, # cpu 线程数 默认最大 'objective':'reg:linear',#多分类or 回归的问题 若要自定义就替换为custom_loss（不带引号） 'learning_rate': .01, #so called `eta` value 如同学习率 'max_depth': 6,# 构建树的深度，越大越容易过拟合 'min_child_weight': 4,# 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 'silent': 1,#设置成1则没有运行信息输出，最好是设置为0. 'subsample': 0.7, # 随机采样训练样本 'colsample_bytree': 0.7,# 生成树时进行的列采样 'n_estimators': 100,# 树的个数跟num_boost_round是一样的，所以可以设置无限大，靠early_stop 'gamma':0.1,# 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。 'seed':1000 #随机种子 #'alpha':0, # L1 正则项参数 #'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。 #'num_class':10, # 类别数，多分类与 multisoftmax 并用 &#125;def mergeToOne( X, X2): X3 = [] for i in range(X.shape[0]): tmp = np.array([list(X[i]), list(X2[i])]) X3.append(list(np.hstack(tmp))) X3 = np.array(X3) return X3def get_XgbRegressor(train_data,train_target,test_data,feature_names,parameters,early_stopping_rounds,num_folds,eval_metric,model_name='model',stratified=False): ''' :param train_data: 一定是numpy :param train_target: :param parameters: :param round: :param k: :param eval_metrics:自定义 or 内置字符串 :return: ''' reg=XGBRegressor() reg.set_params(**parameters) # 定义一些变量 oof_preds = np.zeros((train_data.shape[0],)) sub_preds = np.zeros((test_data.shape[0],)) feature_importance_df = pd.DataFrame() cv_result = [] # K-flod if stratified: folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1234) else: folds = KFold(n_splits= num_folds, shuffle=True, random_state=1234) X_train_newfeature=np.zeros((1,1)) for n_flod, (train_index, val_index) in enumerate(folds.split(train_data, train_target)): train_X=train_data[train_index] val_X=train_data[val_index] train_Y=train_target[train_index] val_Y=train_target[val_index] # 参数初步定之后划分20%为验证集，准备一个watchlist 给train和validation set ,设置num_round 足够大（比如100000），以至于你能发现每一个round 的验证集预测结果， # 如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了。 watchlist= [(train_X, train_Y), (val_X, val_Y)] # early_stop 看validate的eval是否下降，这时候必须传eval_set,并取eval_set的最后一个作为validate reg.fit(train_X,train_Y,early_stopping_rounds=early_stopping_rounds, eval_set=watchlist,eval_metric=eval_metric) ## 生成gbdt新特征 new_feature = reg.apply(val_X) if X_train_newfeature.shape[0]==1: X_train_newfeature=mergeToOne(val_X,new_feature) else: X_train_newfeature = mergeToOne(val_X,new_feature) X_train_newfeature=np.concatenate((X_train_newfeature,mergeToOne(new_feature, val_X)),axis=0) print (X_train_newfeature) # 获得每次的预测值补充 oof_preds[val_index]=reg.predict(val_X) # 获得预测的平均值，这里直接加完再除m sub_preds+= reg.predict(test_data) result = mean_absolute_error(val_Y, reg.predict(val_X)) print('Fold %2d macro-f1 : %.6f' % (n_flod + 1, result)) cv_result.append(round(result,5)) gc.collect() # 默认就是gain 如果要修改要再参数定义中修改importance_type # 保存特征重要度 gain = reg.feature_importances_ fold_importance_df = pd.DataFrame(&#123;'feature': feature_names, 'gain': 100 * gain / gain.sum(), 'fold': n_flod, &#125;).sort_values('gain', ascending=False) feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0) # 进行保存 sub_preds=sub_preds/folds.n_splits new_feature=reg.apply(test_data) X_test_newfeature = mergeToOne(test_data, new_feature) if not os.path.isdir('./sub'): os.makedirs('./sub') pd.DataFrame(oof_preds,columns=['class']).to_csv('./sub/val_&#123;&#125;.csv'.format(model_name), index=False) pd.DataFrame(sub_preds, columns=['class']).to_csv('./sub/test_&#123;&#125;.csv'.format(model_name), index=False) print('cv_result', cv_result) if not os.path.isdir('./gbdt_newfeature'): os.makedirs('./gbdt_newfeature') np.save("./gbdt_newfeature/train_newfeature.npy", X_train_newfeature) np.save("./gbdt_newfeature/test_newfeature.npy", X_test_newfeature) save_importances(feature_importance_df, model_name) return reg,sub_predsdef save_importances(feature_importance_df,model_name): if not os.path.isdir('./feature_importance'): os.makedirs('./feature_importance') ft = feature_importance_df[["feature", "gain"]].groupby("feature").mean().sort_values(by="gain",ascending=False) ft.to_csv('./feature_importance/importance_lightgbm_&#123;&#125;.csv'.format(model_name), index=True) xgb回归123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114########################################################################################## 分类from sklearn.model_selection import train_test_splitfrom sklearn import metricsfrom sklearn.datasets import make_hastie_10_2from xgboost.sklearn import XGBClassifierimport numpy as npimport osimport gcclf_parameters = &#123;'nthread':-1, # cpu 线程数 默认最大 'objective':'multi:softmax',#多分类or 回归的问题 若要自定义就替换为custom_loss（不带引号） 'learning_rate': .01, #so called `eta` value 如同学习率 'max_depth': 6,# 构建树的深度，越大越容易过拟合 'min_child_weight': 4,# 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 'silent': 1,#设置成1则没有运行信息输出，最好是设置为0. 'subsample': 0.7, # 随机采样训练样本 'colsample_bytree': 0.7,# 生成树时进行的列采样 'n_estimators': 500,# 树的个数跟num_boost_round是一样的，所以可以设置无限大，靠early_stop 'gamma':0.1,# 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。 'seed':1000 #随机种子 #'alpha':0, # L1 正则项参数 #'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。 &#125;n_class=3def clf_custom_loss(y_true,y_pred): penalty=2.0 grad=-y_true/y_pred+penalty*(1-y_true)/(1-y_pred) #梯度 hess=y_true/(y_pred**2)+penalty*(1-y_true)/(1-y_pred)**2 #2阶导 return grad,hess# 自定义评价函数def clf_mse(y_pred,dtrain): #preds是结果（概率值），dtrain是个带label的DMatrix labels=dtrain.get_label() #提取label ######### 分类预测的都是概率哦，所以这里要取一个max类别 y_pred = np.argmax(y_pred.reshape(n_class, -1), axis=0) score=mean_absolute_error(labels, y_pred) return 'mse',score# 分类的时候要注意！！！！！！！！# k-flod的时候要按层次拿出来，有一个shuffler我这里就没实现了，否则预测的类别会出现变小甚至报错def get_XgbClassifer(train_data,train_target,test_data,feature_names,parameters,early_stopping_rounds,num_folds,eval_metric,model_name='model',stratified=True): ''' :param train_data: 一定是numpy :param train_target: :param parameters: :param round: :param k: :param eval_metrics:自定义 or 内置字符串 :return: ''' # 如果在param中设置，会莫名报参数不存在的错误 clf=XGBClassifier(num_class=n_class) clf.set_params(**parameters) # 定义一些变量 oof_preds = np.zeros((train_data.shape[0],n_class)) sub_preds = np.zeros((test_data.shape[0],n_class)) feature_importance_df = pd.DataFrame() cv_result = [] # K-flod if stratified: folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1234) else: folds = KFold(n_splits= num_folds, shuffle=True, random_state=1234) for n_flod,(train_index, val_index) in enumerate(folds.split(train_data,train_target)): train_X=train_data[train_index] val_X=train_data[val_index] train_Y=train_target[train_index] val_Y=train_target[val_index] # 参数初步定之后划分20%为验证集，准备一个watchlist 给train和validation set ,设置num_round 足够大（比如100000），以至于你能发现每一个round 的验证集预测结果， # 如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了。 watchlist= [(train_X, train_Y)] # early_stop 看validate的eval是否下降，这时候必须传eval_set,并取eval_set的最后一个作为validate clf.fit(train_X,train_Y,early_stopping_rounds=early_stopping_rounds, eval_set=watchlist,eval_metric=eval_metric) # 获得每次的预测值补充 oof_preds[val_index]=clf.predict_proba(val_X) # 获得预测的平均值，这里直接加完再除m sub_preds+= clf.predict_proba(test_data) # 计算当前准确率 result=mean_absolute_error(val_Y,clf.predict(val_X)) print('Fold %2d macro-f1 : %.6f' % (n_flod + 1, result)) print(type(result)) cv_result.append(round(result,5)) gc.collect() # 默认就是gain 如果要修改要再参数定义中修改importance_type # 保存特征重要度 gain = clf.feature_importances_ fold_importance_df = pd.DataFrame(&#123;'feature':feature_names, 'gain':100*gain/gain.sum(), 'fold':n_flod, &#125;).sort_values('gain',ascending=False) feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0) # 进行保存 sub_preds=sub_preds/folds.n_splits if not os.path.isdir('./cv'): os.makedirs('./cv') pd.DataFrame(oof_preds,columns=['class_'+ str(i) for i in range(n_class)]).to_csv('./cv/val_prob_&#123;&#125;.csv'.format(model_name), index= False, float_format = '%.4f') pd.DataFrame(sub_preds, columns=['class_' + str(i) for i in range(n_class)]).to_csv('./cv/test_prob_&#123;&#125;.csv'.format(model_name), index=False, float_format='%.4f') oof_preds = [np.argmax(x) for x in oof_preds] sub_preds = [np.argmax(x) for x in sub_preds] if not os.path.isdir('./sub'): os.makedirs('./sub') pd.DataFrame(oof_preds,columns=['class']).to_csv('./sub/val_&#123;&#125;.csv'.format(model_name), index=False) pd.DataFrame(sub_preds, columns=['class']).to_csv('./sub/test_&#123;&#125;.csv'.format(model_name), index=False) save_importances(feature_importance_df, model_name) return clfdef save_importances(feature_importance_df,model_name): if not os.path.isdir('./feature_importance'): os.makedirs('./feature_importance') ft = feature_importance_df[["feature", "gain"]].groupby("feature").mean().sort_values(by="gain",ascending=False) ft.to_csv('./feature_importance/importance_lightgbm_&#123;&#125;.csv'.format(model_name), index=True) lgb回归注意Lgb对分类变量会有特殊的支持 只用看参数和pythonAPI http://lightgbm.apachecn.org/ https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters 1234567891011121314151617181920212223242526272829303132333435363738394041param = &#123; 'bagging_freq': 5, 'bagging_fraction': 0.335, 'boost_from_average':'false', 'boost': 'gbdt', 'feature_fraction': 0.041, 'learning_rate': 0.0083, 'max_depth': -1, 'metric':'auc', 'min_data_in_leaf': 80, 'min_sum_hessian_in_leaf': 10.0, 'num_leaves': 13, 'num_threads': 8, 'tree_learner': 'serial', 'objective': 'binary', 'verbosity': -1&#125;num_folds = 11features = [c for c in train.columns if c not in ['ID_code', 'target']]folds = KFold(n_splits=num_folds, random_state=2319)oof = np.zeros(len(train))getVal = np.zeros(len(train))predictions = np.zeros(len(target))feature_importance_df = pd.DataFrame()print('Light GBM Model')for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)): X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx] X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx] print("Fold idx:&#123;&#125;".format(fold_ + 1)) trn_data = lgb.Dataset(X_train, label=y_train) val_data = lgb.Dataset(X_valid, label=y_valid) clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000) oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) fold_importance_df = pd.DataFrame() fold_importance_df["feature"] = features fold_importance_df["importance"] = clf.feature_importance() fold_importance_df["fold"] = fold_ + 1 feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0) predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splitsprint("CV score: &#123;:&lt;8.5f&#125;".format(roc_auc_score(target, oof))) lgb分类回归和分类一致，只是参数不一样而已 注意Lgb对分类变量会有特殊的支持 只用看参数和pythonAPI http://lightgbm.apachecn.org/ https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters Naive Bayes比较常用的手工概率blending https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899 调整参数贝叶斯优化来源:https://www.cnblogs.com/yangruiGB2312/p/9374377.html 可以说是目前最好的调参的方法 贝叶斯调参采用高斯过程，考虑之前的参数信息，不断地更新先验；网格搜索未考虑之前的参数信息 贝叶斯调参迭代次数少，速度快；网格搜索速度慢,参数多时易导致维度爆炸 贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部优最 ​ 公式推导就不在这里展开，主要阐述主要思想：给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。 ​ 假设这个寻找最优化参数的过程是一个高斯过程。高斯过程有个特点，就是当随机遍历一定的数据点并拿到结果之后，可以大致绘制出整个数据的分布曲线，当然这是也会有一定误差。 python代码 1234567891011121314151617181920from bayes_opt import BayesianOptimizationdef rf_cv(n_estimators, min_samples_split, max_features, max_depth): val = cross_val_score( RandomForestClassifier(n_estimators=int(n_estimators), min_samples_split=int(min_samples_split), max_features=min(max_features, 0.999), # float max_depth=int(max_depth), random_state=2 ), x, y, scoring='roc_auc', cv=5 ).mean() return val# 注意参数名字要对应rf_bo = BayesianOptimization( rf_cv, &#123;'n_estimators': (10, 250), 'min_samples_split': (2, 25), 'max_features': (0.1, 0.999), 'max_depth': (5, 15)&#125; ) 可以执行的操作 以查看当前最优的参数和结果(同时，我们还可以修改高斯过程的参数，高斯过程主要参数是核函数(kernel)，还有其他参数可以参考sklearn.gaussianprocess)： 123gp_param=&#123;'kernel':None&#125;rf_bo.maximize(**gp_param)rf_bo.res['max'] 上面bayes算法得到的参数并不一定最优，当然我们会遇到一种情况，就是我们已经知道有一组或是几组参数是非常好的了，我们想知道其附近有没有更好的。 1234567rf_bo.explore( &#123;'n_estimators': [10, 100, 200], 'min_samples_split': [2, 10, 20], 'max_features': [0.1, 0.5, 0.9], 'max_depth': [5, 10, 15] &#125;) 模型融合]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vector Representation]]></title>
    <url>%2F2019%2F04%2F09%2Frepresentation%2F</url>
    <content type="text"><![CDATA[​ 自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 Id537，“dog”可能表示为 Id143。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。 来源：https://www.tensorflow.org/tutorials/representation/word2vec ​ 自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 Id537，“dog”可能表示为 Id143。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。 ​ 向量空间模型 (VSM) 在连续向量空间中表示（嵌入）字词，其中语义相似的字词会映射到附近的点（“在彼此附近嵌入”）。VSM 在 NLP 方面有着悠久而丰富的历史，但所有方法均以某种方式依赖于分布假设，这种假设指明在相同上下文中显示的字词语义相同。利用该原则的不同方法可分为两类：基于计数的方法（例如潜在语义分析）以及预测方法（例如神经概率语言模型）。 ​ Word2vec 是一种计算效率特别高的预测模型，用于学习原始文本中的字词嵌入。它分为两种类型：连续词袋模型 (CBOW) 和 Skip-Gram 模型。数据量少的时候使用CBOW，数据量大的时候Skip-gram，一般都用Skip-gram。 代码tensorflow1https://www.tensorflow.org/tutorials/representation/word2vec python1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import pandas as pdfrom gensim.corpora import WikiCorpusfrom gensim.models import Word2Vecfrom gensim.models.word2vec import LineSentenceimport pandas as pdimport multiprocessingimport numpy as npL = 10path = './'save_path = path + '/w2v'if not os.path.exists(save_path): print(save_path) os.makedirs(save_path)train1 = pd.read_csv(path + '/train.csv')train = pd.read_csv(path + '/train_old.csv')test = pd.read_csv(path + '/test.csv')data = pd.concat([train, test, train1]).reset_index(drop=True).sample(frac=1, random_state=2018).fillna(0)data = data.replace('\\N', 999)sentence = []for line in list(data[['1_total_fee', '2_total_fee', '3_total_fee', '4_total_fee']].values): sentence.append([str(float(l)) for idx, l in enumerate(line)])print('training...')model = Word2Vec(sentence, size=L, window=2, min_count=1, workers=multiprocessing.cpu_count(), iter=10)print('outputing...')for fea in ['1_total_fee', '2_total_fee', '3_total_fee', '4_total_fee']: values = [] for line in list(data[fea].values): values.append(line) values = set(values) print(len(values)) w2v = [] for i in values: a = [i] a.extend(model[str(float(i))]) w2v.append(a) out_df = pd.DataFrame(w2v) name = [fea] for i in range(L): name.append(name[0] + 'W' + str(i)) out_df.columns = name out_df.to_csv(save_path + '/' + fea + '.csv', index=False) TSNE降维可视化1234567891011121314def plot_with_labels(low_dim_embs, labels, filename = 'tsne.png'): assert low_dim_embs.shape[0] &gt;= len(labels), "More labels than embeddings" plt.figure(figsize= (10, 18)) for i, label in enumerate(labels): x, y = low_dim_embs[i, :] plt.scatter(x, y) plt.annotate(label, xy = (x, y), textcoords = 'offset points', ha = 'right', va = 'bottom') plt.savefig(filename)def polt_tnse(df,df_target,plot_only=300): df_target=list(df_target.astype('str')) tsne = TSNE(perplexity = 30, n_components = 2, init = 'pca', n_iter = 5000) low_dim_embs = tsne.fit_transform(df.iloc[:plot_only][:]) labels = [df_target[i] for i in range(plot_only)] plot_with_labels(low_dim_embs, labels)]]></content>
      <categories>
        <category>竞赛经验</category>
      </categories>
      <tags>
        <tag>预处理</tag>
        <tag>分类变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征选择]]></title>
    <url>%2F2019%2F04%2F09%2F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[[TOC] 如何进行特征选择 feature importace特征重要度高如果发现特征重要度有明显很高的情况下，可以将这个特征去除再尝试，因为有可能模型过于依赖此特征而导致了过拟合的存在。比如下图中的addr_state 特征重要度低来源：https://www.kaggle.com/ogrellier/feature-selection-with-null-importances 来源：https://academic.oup.com/bioinformatics/article/26/10/1340/193348 ​ 传统做法就是根据feature importance然后就跑一个阈值，剔除低于阈值的feature。 剔除阈值的目的在于： 消除高相关的feature 提高model的variance ​ 最好最后的分数剔除分类变量，因为它主要是平衡分类变量的bias ​ 论文提出的做法是，对于树模型，它的特征重要度会对数量多的分类变量有bias，所以提出了一种基于null importance 的做法。即对target打乱s次，预测s次的特征重要度，然后再利用distribution进行分析，从而得到比较标注的特征重要度。主要的做法请看kaggle上的示例： 获取重要度，此处需要自定义一个训练模型 123456789101112131415161718192021222324252627282930313233343536# shuffle决定是否打乱y值def get_feature_importances(data, shuffle, seed=None): # Gather real features train_features = [f for f in data if f not in ['TARGET', 'SK_ID_CURR']] # Go over fold and keep track of CV score (train and valid) and feature importances # Shuffle target if required y = data['TARGET'].copy() if shuffle: # Here you could as well use a binomial distribution y = data['TARGET'].copy().sample(frac=1.0) # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True) lgb_params = &#123; 'objective': 'binary', 'boosting_type': 'rf', 'subsample': 0.623, 'colsample_bytree': 0.7, 'num_leaves': 127, 'max_depth': 8, 'seed': seed, 'bagging_freq': 1, 'n_jobs': 4 &#125; # Fit the model clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200, categorical_feature=categorical_feats) # Get feature importances imp_df = pd.DataFrame() imp_df["feature"] = list(train_features) imp_df["importance_gain"] = clf.feature_importance(importance_type='gain') imp_df["importance_split"] = clf.feature_importance(importance_type='split') imp_df['trn_score'] = roc_auc_score(y, clf.predict(data[train_features])) return imp_df 跑n轮得到Null importance 1234567891011null_imp_df = pd.DataFrame()nb_runs = 80import timestart = time.time()dsp = ''for i in range(nb_runs): # Get current run importances imp_df = get_feature_importances(data=data, shuffle=True) imp_df['run'] = i + 1 # Concat the latest importances with the old ones null_imp_df = pd.concat([null_imp_df, imp_df], axis=0) 然后画分布图 1234567891011121314151617181920def display_distributions(actual_imp_df_, null_imp_df_, feature_): plt.figure(figsize=(13, 6)) gs = gridspec.GridSpec(1, 2) # Plot Split importances ax = plt.subplot(gs[0, 0]) a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances') ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target') ax.legend() ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold') plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper()) # Plot Gain importances ax = plt.subplot(gs[0, 1]) a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances') ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target') ax.legend() ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold') plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='LIVINGAPARTMENTS_AVG') 图主要展示了什么叫做好的分布:方差小的。同时远离真实的重要度（红线） 我们想要的是在与真实目标(即实际重要性)使用时具有很高重要性的特性，而在与噪声训练时得分不高(即null importance得分)。在零假设和正态分布下，如果一个特征的红色实际重要性在蓝色分布内，那么该特征与目标不相关的可能性就很大。如果它在蓝色分布的5%范围内或者在外面，那么它就是相关的。 然后下一步要做的就是（去掉与target不相关的，去掉自己高相关的）： Drop high variance features if they are not really related to the target Remove the decaying factor on correlated features, showing their real importance (or unbiased importance) 结合一些公式，然后进行挑选 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960correlation_scores = []for _f in actual_imp_df['feature'].unique(): f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].values gain_score = 100 * (f_null_imps &lt; np.percentile(f_act_imps, 25)).sum() / f_null_imps.size f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].values split_score = 100 * (f_null_imps &lt; np.percentile(f_act_imps, 25)).sum() / f_null_imps.size correlation_scores.append((_f, split_score, gain_score))corr_scores_df = pd.DataFrame(correlation_scores, columns=['feature', 'split_score', 'gain_score'])def score_feature_selection(df=None, train_features=None, cat_feats=None, target=None): # Fit LightGBM dtrain = lgb.Dataset(df[train_features], target, free_raw_data=False, silent=True) lgb_params = &#123; 'objective': 'binary', 'boosting_type': 'gbdt', 'learning_rate': .1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'num_leaves': 31, 'max_depth': -1, 'seed': 13, 'n_jobs': 4, 'min_split_gain': .00001, 'reg_alpha': .00001, 'reg_lambda': .00001, 'metric': 'auc' &#125; # Fit the model hist = lgb.cv( params=lgb_params, train_set=dtrain, num_boost_round=2000, categorical_feature=cat_feats, nfold=5, stratified=True, shuffle=True, early_stopping_rounds=50, verbose_eval=0, seed=17 ) # Return the last mean / std values return hist['auc-mean'][-1], hist['auc-stdv'][-1]# features = [f for f in data.columns if f not in ['SK_ID_CURR', 'TARGET']]# score_feature_selection(df=data[features], train_features=features, target=data['TARGET'])for threshold in [0, 10, 20, 30 , 40, 50 ,60 , 70, 80 , 90, 95, 99]: split_feats = [_f for _f, _score, _ in correlation_scores if _score &gt;= threshold] split_cat_feats = [_f for _f, _score, _ in correlation_scores if (_score &gt;= threshold) &amp; (_f in categorical_feats)] gain_feats = [_f for _f, _, _score in correlation_scores if _score &gt;= threshold] gain_cat_feats = [_f for _f, _, _score in correlation_scores if (_score &gt;= threshold) &amp; (_f in categorical_feats)] print('Results for threshold %3d' % threshold) split_results = score_feature_selection(df=data, train_features=split_feats, cat_feats=split_cat_feats, target=data['TARGET']) print('\t SPLIT : %.6f +/- %.6f' % (split_results[0], split_results[1])) gain_results = score_feature_selection(df=data, train_features=gain_feats, cat_feats=gain_cat_feats, target=data['TARGET']) print('\t GAIN : %.6f +/- %.6f' % (gain_results[0], gain_results[1])) 特征相关性低如果特征之间的相关性很低的情况下，可以进一步检测变量之间的独立性，如果变量之间是independent意味着可以单独将这些特征进行多个模型训练再融合]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类变量：Target Encoding]]></title>
    <url>%2F2019%2F04%2F08%2Ftargetencoding%2F</url>
    <content type="text"><![CDATA[​ 当你在进行监督学习时，你经常要处理分类变量。也就是说，变量没有一个自然的数值表示。问题是大多数机器学习算法都要求输入数据是数值的。在某个时候，数据科学管道将需要将分类变量转换为数值变量。 来源：https://maxhalford.github.io/blog/target-encoding-done-the-right-way/ 前言​ 当你在进行监督学习时，你经常要处理分类变量。也就是说，变量没有一个自然的数值表示。问题是大多数机器学习算法都要求输入数据是数值的。在某个时候，数据科学管道将需要将分类变量转换为数值变量。 有很多方法可以做到这一点: Label encoding 为每个类别选择数字 One-hot encoding:为每个类别创建一个二进制列 Vector representation ：也就是word2vec，在这里您可以找到一个适合您的数据的低维子空间 Optimal binning ：在依赖于LightGBM或CatBoost等树学习器 Target encoding: 按类别平均目标值 ​ 每种方法都有其优缺点，通常取决于您的数据和需求。如果一个变量有很多类别，那么一个[One-hot encoding:方案将产生许多列，这可能导致内存问题。根据我的经验，依赖LightGBM/CatBoost是最好的分箱方法。Label encoding是没有用的，最好不要使用它。然而，如果你的分类变量恰好是有序的，那么你可以而且应该用递增的数字来表示它(例如，“cold”变成0，“mild”变成1，“hot”变成2)。word2vec和其他类似的方法很酷也很好，但是它们需要进行一些微调，而且并不总是奏效。 target encoding​ target encoding是很容易理解的一种思想，假设你有分类变量X和变量Y，然后对于X中每一个distinct的元素计算其对应Y值的平局之，然后用这个平均值替换 $ x_i $ .下面这个例子显而易见： x0 x1 y aa cc 1 aa cc 1 aa cc 1 aa cc 1 aa cc 0 bb cc 1 bb cc 0 bb cc 0 bb cc 0 bb dd 0 x0 x1 y 0.8 cc 1 0.8 cc 1 0.8 cc 1 0.8 cc 1 0.8 cc 0 0.2 cc 1 0.2 cc 0 0.2 cc 0 0.2 cc 0 0.2 dd 0 ​ Target encoding的好处在于它提取了那些可以解释Y值，比如这里得aa拥有了一个Y值的平均值0.8，这会很好的帮助下游机器学习分类算法。 ​ 目标编码的问题有一个名称:过拟合。事实上，当平均值的数值很低时，依赖平均值并不总是一个好主意。您必须记住，您正在训练的数据集是一个更大的数据集的样本。这意味着，当您将训练集中发现的任何规律应用到另一个数据集(即测试集)时，可能都不成立。比如有可能训练集内它对应的Y值都是0，但是在测试集中它对应的Y值1比较多。 ​ 得出结论，永远不要用基础的target encoding,要用也是用以下的进阶encoding。 Target encoding进阶有很多方法可以处理这个问题。交叉验证和additive smoothing可以结合使用 交叉验证一种流行的方法是使用交叉验证并计算每个切分数据集中的平均值。这就是H20和许多kaggler所做的。 None: no holdout, mean是对训练集的所有数据行的计算应用于测试数据 leave eoneout: mean是对除了当前行本身之外的所有数据行进行计算。这可以用于训练数据。当前行本身的目标不包括在平均值中，以防止过度拟合 KFold:平均值只计算out-of-fold数据(需要K-fold) 这可以用于训练数据。为了防止过拟合，目标均值是根据叠外数据计算的 additive smoothing 使用additive smoothing，因为数据集中存在数据的count较小，所以它的target值容易受到过拟合的影响。它使用了全局的平均值来smooth较少数据带来过拟合的影响 数学上它等价于: u=\frac{n\times \hat x+m\times w}{n+m}where μ is the mean we’re trying to compute (the one that’s going to replace our categorical values) n is the number of values you have ¯x is your estimated mean m is the “weight” you want to assign to the overall mean w is the overall mean 其中m就是用来调节全局的权重，根据他的经验，他发现m取300的时候适用于大多数场合。 1234567891011121314def calc_smooth_mean(df, by, on, m): # Compute the global mean mean = df[on].mean() # Compute the number of values and the mean of each group agg = df.groupby(by)[on].agg(['count', 'mean']) counts = agg['count'] means = agg['mean'] # Compute the "smoothed" means smooth = (counts * means + m * mean) / (counts + m) # Replace each value by the according smoothed mean return df[by].map(smooth)]]></content>
      <categories>
        <category>竞赛经验</category>
      </categories>
      <tags>
        <tag>预处理</tag>
        <tag>分类变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比赛EDA]]></title>
    <url>%2F2019%2F04%2F08%2F%E6%AF%94%E8%B5%9BEDA%2F</url>
    <content type="text"><![CDATA[比赛常见的EDA总结 [TOC] 检查缺失值1234567891011# https://www.kaggle.com/gpreda/santander-eda-and-predictiondef missing_data(data): total = data.isnull().sum() percent = (data.isnull().sum()/data.isnull().count()*100) tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) types = [] for col in data.columns: dtype = str(data[col].dtype) types.append(dtype) tt['Types'] = types return(np.transpose(tt)) 观察分布值观察训练集的01分布(可以拓展到任意分布对比图)12345678910111213141516171819202122# https://www.kaggle.com/gpreda/santander-eda-and-predictiondef plot_feature_distribution(df1, df2, label1, label2, features): i = 0 sns.set_style('whitegrid') plt.figure() fig, ax = plt.subplots(10,10,figsize=(18,22)) for feature in features: i += 1 plt.subplot(10,10,i) sns.distplot(df1[feature], hist=False,label=label1) sns.distplot(df2[feature], hist=False,label=label2) plt.xlabel(feature, fontsize=9) locs, labels = plt.xticks() plt.tick_params(axis='x', which='major', labelsize=6, pad=-6) plt.tick_params(axis='y', which='major', labelsize=6) plt.show(); t0 = train_df.loc[train_df['target'] == 0]t1 = train_df.loc[train_df['target'] == 1]features = train_df.columns.values[2:102]plot_feature_distribution(t0, t1, '0', '1', features) 结论：分布图怎么看，就是看如果01分布完全一模一样那么可能说明这个特征是冗余的无关特征，如果01分布不同的情况下，也做不了太多东西，只能结合特征重要度反证这些特征是有用的。 train_test的分布同理可以选择train和test的行和列的mean/std/min值 1234567891011121314151617181920212223242526272829https://www.kaggle.com/gpreda/santander-eda-and-predictionplt.figure(figsize=(16,6))plt.title("Distribution of mean values per column in the train and test set")sns.distplot(train_df[features].mean(axis=0),color="magenta",kde=True,bins=120, label='train')sns.distplot(test_df[features].mean(axis=0),color="darkblue", kde=True,bins=120, label='test')plt.legend()plt.show()plt.figure(figsize=(16,6))plt.title("Distribution of std values per row in the train and test set")sns.distplot(train_df[features].std(axis=1),color="black", kde=True,bins=120, label='train')sns.distplot(test_df[features].std(axis=1),color="red", kde=True,bins=120, label='test')plt.legend();plt.show()plt.figure(figsize=(16,6))plt.title("Distribution of std values per column in the train and test set")sns.distplot(train_df[features].std(axis=0),color="blue",kde=True,bins=120, label='train')sns.distplot(test_df[features].std(axis=0),color="green", kde=True,bins=120, label='test')plt.legend(); plt.show()t0 = train_df.loc[train_df['target'] == 0]t1 = train_df.loc[train_df['target'] == 1]plt.figure(figsize=(16,6))plt.title("Distribution of mean values per row in the train set")sns.distplot(t0[features].mean(axis=1),color="red", kde=True,bins=120, label='target = 0')sns.distplot(t1[features].mean(axis=1),color="blue", kde=True,bins=120, label='target = 1')plt.legend(); plt.show()plt.figure(figsize=(16,6))plt.title("Distribution of mean values per column in the train set")sns.distplot(t0[features].mean(axis=0),color="green", kde=True,bins=120, label='target = 0')sns.distplot(t1[features].mean(axis=0),color="darkblue", kde=True,bins=120, label='target = 1')plt.legend(); plt.show() 检查duplicate values12345678910111213141516%%timefeatures = train_df.columns.values[2:202]unique_max_train = []unique_max_test = []for feature in features: values = train_df[feature].value_counts() unique_max_train.append([feature, values.max(), values.idxmax()]) values = test_df[feature].value_counts() unique_max_test.append([feature, values.max(), values.idxmax()])# === plotdf = df.sort_values(by='n_train_unique').reset_index(drop=True)df[['n_train_unique', 'n_test_unique', 'n_overlap']].plot(kind='barh' ,figsize=(22, 100), fontsize=20, width=0.8)plt.yticks(df.index, df['feature'].values)plt.xlabel('n_unique', fontsize=20)plt.ylabel('feature', fontsize=20)plt.legend(loc='center right', fontsize=20) PCAhttps://zhuanlan.zhihu.com/p/28909807 主成分分析是统计方法里的一种降维方法，它的主要思想是将原有 个特征通过正交变换将一组可能存在相关性的特征缩减到 特征( )。举例来说，在网站用户行为数据收集过程中，会话数(Visits)，浏览页数(PV)，网站总停留时间(Time Spend Total)，访问人数(Unique Visitor)，以上这几个指标，无论是从以往的数据统计还是业务经验来看，都存在一定正相关关系，如果将这些特征喂给模型，很容易造成过拟合。 通过转化，从而剔除噪声，而且可以看到转化后的值然后把一些无关的列再剔除 PCA 使用要点 使用主成分分析，往往会丢失掉“少部分信息”（注意：这“少部分信息”仅仅指方差较小的数据，并非信息含量真的少的数据！） 因为1的特性，所以在机器学些中，不推荐使用 PCA 去优化特征达到避免过拟合的目的。 既然PCA不能避免过拟合，那为何还要使用，根据周志华老师的西瓜书中的描述： 在舍弃特征值较小的特征之后，能够使样本采集密度增 当数据受到噪声影响时，最小的特征值对对应的特征向量往往与噪声有关将他们舍弃能够在一定程度上起到去噪效果 在维度过多的情况下，变换后的坐标系代表的意义不明，不易于解释。 123456789101112131415161718# https://www.kaggle.com/roydatascience/eda-pca-lgbm-santander-transactions/notebookfrom sklearn.decomposition import KernelPCAlin_pca = KernelPCA(n_components = 2, kernel="linear", fit_inverse_transform=True)rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.0433, fit_inverse_transform=True)sig_pca = KernelPCA(n_components = 2, kernel="sigmoid", gamma=0.001, coef0=1, fit_inverse_transform=True)plt.figure(figsize=(11, 4))for subplot, pca, title in ((131, lin_pca, "Linear kernel"), (132, rbf_pca, "RBF kernel, $\gamma=0.04$"), (133, sig_pca, "Sigmoid kernel, $\gamma=10^&#123;-3&#125;, r=1$")): PCA_train_x = PCA(2).fit_transform(train_scaled) plt.subplot(subplot) plt.title(title, fontsize=14) plt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=target, cmap="nipy_spectral_r") plt.xlabel("$z_1$", fontsize=18) if subplot == 131: plt.ylabel("$z_2$", fontsize=18, rotation=0) plt.grid(True)plt.show() 热度图查看prediction123456789#https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920plt.subplot(1,2,1)sns.heatmap(x, cmap='RdBu_r', center=0.0) plt.title('VAR_'+str(j)+' Predictions without Magic',fontsize=16)plt.xticks(np.linspace(0,49,5),np.round(np.linspace(mn,mx,5),1))plt.xlabel('Var_'+str(j))plt.yticks([])plt.ylabel('')plt.show() 通过热度图可以看出模型是否明显利用了两个特征。虽然可能特征重要度也会说明，但是这样的热度图明显会更具说服力，比如说var_0，模型可以得出竖线15，横线4之类的分界线。]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy]]></title>
    <url>%2F2019%2F04%2F08%2Fnumpy%2F</url>
    <content type="text"><![CDATA[numpy 操作小技巧 增加concat/vstack/hstack 1https://blog.csdn.net/xiaodongxiexie/article/details/71774466]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python代码]]></title>
    <url>%2F2019%2F04%2F08%2Fpython%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[python 代码小技巧 [TOC] 通用测试时间12关注walltime%%time 遍历列表12for index, item in enumerate(list1): print index, item 并行遍历12for (x,y) in zip(a,b): print x,y 通过字典保存了所有中间过程，不仅命名容易，而且便于后续操作12d=&#123;d_name:get_dvalue(d_name) for d_name in d_list&#125;best_dvalue_dname=min(d.items(),key=lambda x:x[1]) 文件1234#这样会比单纯用readlines()快count = 0for index, line in enumerate(open(filepath,'r'))： count += 1 数据结构字典12from collections import defaultdict 通过这个声明dict&#123;里面的格式&#125;如 defaultdict(list) 列表生成式121. [i for i in range(k) if condition]：此时if起条件判断作用，满足条件的，将被返回成为最终生成的列表的一员。2. [i if condition else exp for exp]：此时if...else被用来赋值，满足条件的i以及else被用来生成最终的列表 自带算法库数组中查找插入的位置123返回排序插入的位置，并不是真的插入排序数组中from bisect import bisect_left, bisect_rightend = bisect_left(keys, 0)]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比赛通用代码]]></title>
    <url>%2F2019%2F04%2F08%2F%E6%AF%94%E8%B5%9B%E9%80%9A%E7%94%A8%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[比赛通用代码 [TOC] jupyter 载入代码包123456789101112131415161718192021222324252627# Load librariesimport numpy as npimport pandas as pdimport randomimport gcfrom pandas import read_csvimport plotly.offline as pyimport plotly.graph_objs as goimport seaborn as sns%matplotlib inlineimport matplotlib.pyplot as pltimport warningswarnings.filterwarnings('ignore')pd.options.display.max_columns = 100gc.enable()'''Displays markdown formatted output like bold, italic bold etc.'''from IPython.display import Markdowndef bold(string): display(Markdown(string))'''Ignores deprecation warning.'''def ignore_warnings(): import warnings warnings.filterwarnings('ignore', category = DeprecationWarning) bold('**Merged data:**')display(merged.head()) 自动压缩变量空间12345678910111213141516171819202122232425262728293031323334353637383940def reduce_mem_usage(df): """ iterate through all the columns of a dataframe and modify the data type to reduce memory usage. """ #start_mem = df.memory_usage().sum() / 1024**2 #print('Memory usage of dataframe is &#123;:.2f&#125; MB'.format(start_mem)) for col in df.columns: col_type = df[col].dtype if col_type != object: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == 'int': if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) #end_mem = df.memory_usage().sum() / 1024**2 #print('Memory usage after optimization is: &#123;:.2f&#125; MB'.format(end_mem)) #print('Decreased by &#123;:.1f&#125;%'.format(100 * (start_mem - end_mem) / start_mem)) return dfdef reload(): gc.collect() df = pd.read_csv('../input/train_V2.csv') invalid_match_ids = df[df['winPlacePerc'].isna()]['matchId'].values df = df[-df['matchId'].isin(invalid_match_ids)] df=reduce_mem_usage(df) return df GPU使用123456https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89004#latest-514910FYI, Kaggle provides us the use of 7 Tesla P100 GPU's simultaneously. Below are some tips. Python models will not run GPU by default. You must turn GPU on. First, activate GPU in kaggle kernels. Second, if using CatBoost, add the following hyperparameter, task_type = 'GPU'. If using XGBoost, add the following hyperparameter, 'tree_method': 'gpu_hist' or tree_method': 'gpu_exact'. If using LGBM, follow the instructions in this kernel to recompile GPU LGBM, then add the following 3 hyperparameters 'device': 'gpu', 'gpu_platform_id': 0, and 'gpu_device_id': 0. Note: many posted kernels regarding CatBoost and XGBoost don't use GPU and can actually be 4x faster with GPU activated!You are allowed to execute 7 GPU kernels simultaneously for 9 hour sessions. In one evening, you can train 100 models!! My final solution is a blend of dozens of LGBM, CatBoost and XGBoost. In one evening, you can train your models on a combined one billion generated new rows of augmented data using the power of Kaggle's GPU's (where 7 have a total cost of $49000!!)In this comp, I found that CatBoost achieved an LB 0.001 greater than LGBM. That's it. Enjoy the power!!]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas]]></title>
    <url>%2F2019%2F04%2F08%2Fpandas%2F</url>
    <content type="text"><![CDATA[pandas 操作小技巧 通用操作pandas操作出现进度条： 用作迭代器 用于Pandas的操作12345import pandas as pdfrom tqdm import tqdmtqdm.pandas()sentences = train["question_text"].progress_apply(lambda x: x.split()).valuesfor sentence in tqdm(sentences, disable = (not verbose)): 增加merge/concat/joinhttps://www.e-learn.cn/content/qita/814185 改变直接replace12'''Put Dona, Jonkheer, Countess, Sir, Lady, Don in bucket Aristocrat.'''merged.Title.replace(to_replace = ['Dona', 'Jonkheer', 'Countess', 'Sir', 'Lady', 'Don'], value = 'Aristocrat', inplace = True) map映射1234#map映射 它就会自动把类别型转为连续型#如果要Inverse就直接key-value逆转size_mapping=&#123;'XL':3,'L':2,'M':1&#125;df['size']=df['size'].map(size_mapping) 单列分箱12345678910'''Create bin categories for Age.'''label_names = ['infant','child','teenager','young_adult','adult','aged']'''Create range for each bin categories of Age.'''cut_points = [0,5,12,18,35,60,81]'''Create and view categorized Age with original Age.'''merged['Age_binned'] = pd.cut(merged.Age, cut_points, labels = label_names)display(merged[['Age', 'Age_binned']].head(2)​ 12345678## 删除### 去重```pythontrain.drop_duplicates(subset = ['1_total_fee','2_total_fee','3_total_fee', 'month_traffic','pay_times','last_month_traffic','service2_caller_time','age'],inplace=True)]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>pandas</tag>
        <tag>python</tag>
      </tags>
  </entry>
</search>
