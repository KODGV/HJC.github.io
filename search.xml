<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Linux]]></title>
    <url>%2F2019%2F05%2F16%2FLinux%2F</url>
    <content type="text"><![CDATA[Linux学习 查询命令帮助 man 命令man page 代号 内容说明 NAME 简短的指令、数据名称说明 SYNOPSIS 简短的指令下达语法（syntax）简介 DESCRIPTION 较为完整的说明，这部分最好仔细看看！ OPTIONS 针对 SYNOPSIS 部分中，有列举的所有可用的选项说明 COMMANDS 当这个程序（软件）在执行的时候，可以在此程序（软件）中下达的指令 FILES 这个程序或数据所使用或参考或链接到的某些文件 SEE ALSO 可以参考的，跟这个指令或数据有相关的其他说明！ EXAMPLE 一些可以参考的范例 info 命令 ​ 可读性会强很多，只是可能有些命令查不到，光标移到节点 回车可进入 U返回上一节点 N进入下一节点 P进入上 一节点 正确的关机 who 看谁在线上 netstat -a 网络连线状态 ps -aux 查看背景执行程序 sync 将内存数据写入硬盘（很重要！） Linux 快捷键编辑器快捷键 按键 进行工作 空白键 向下翻一页 [Page Down] 向下翻一页 [Page Up] 向上翻一页 [Home] 去到第一页 [End] 去到最后一页 /string 向“下”搜寻 string 这个字串，如果要搜寻 vbird 的话，就输入 /vbird ?string 向“上”搜寻 string 这个字串 n, N 利用 / 或 ? 来搜寻字串时，可以用 n 来继续下一个搜寻 （不论是 / 或 ?） ，可以利用 N 来进行“反向”搜寻。举例来说，我以 /vbird 搜寻 vbird 字串， 那么可以 n 继续往下查询，用 N 往上查询。若以 ?vbird 向上查询 vbird 字串， 那我可以用 n 继续“向上”查询，用 N 反向查询。 q 结束这次的 man page]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查重攻略]]></title>
    <url>%2F2019%2F05%2F02%2F%E6%AF%95%E4%B8%9A%E9%9C%80%E6%B1%82%2F%E6%9F%A5%E9%87%8D%E6%94%BB%E7%95%A5%2F</url>
    <content type="text"><![CDATA[降重技巧 如果文章字数足够了的话，可以将文字内容转化为图片。目前的查重系统暂时对图片是查不出来的，重复率自然不会高。 不要寄希望于标注了参考文献 主动改被动，被动改主动 前后并列的词汇调换 分析原因一二三四都可以改成1234或①②③④或者直接分出四个段落 同义近义 打开谷歌翻译，然后把论文不放心的部分翻译成英文（或日文德文俄文，随你便），再把翻译出来的再复制进去翻译成中文，搞定 查重论文目录检测举个例子正确目录用知网检测系统生成的文本复制比报告单如下图： 目录的格式是一个细节问题，微小的错误可能就导致不一样的结果，目录正确知网系统会自动识别目录分章节进行检测，可以得出各个章节的详细重复率；否则目录不完全正确那么知网检测系统不能很好的识别导致不能按照章节检测，就不能得到我们想要的章节重复率结果，甚至知网检测系统会把目录当成正文检测而使得目录全部标红而影响总文字复制比升高。]]></content>
  </entry>
  <entry>
    <title><![CDATA[elo比赛心得]]></title>
    <url>%2F2019%2F04%2F30%2FJAVA%E5%AD%A6%E4%B9%A0%2Fjava%E5%8F%82%E8%80%83%E8%B7%AF%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[java学习路线与推荐 6个月 Java 服务端入门和进阶指南https://www.zhihu.com/question/29581524 资料参考，先学第一阶段，学完再来看第二阶段的任务吧 https://www.zhihu.com/question/22340525 https://www.zhihu.com/question/307096748/answers/updated https://mp.weixin.qq.com/s/GxIRqj8s1zrnnPSOJ0203Q https://zhuanlan.zhihu.com/p/34880504 后端校招以及大牛成长转折点 自己整理第一阶段JAVA基础的学习 《Java 核心技术：卷1 基础知识》(或者《Java 编程思想》)必看 《Effective Java》 做题网站 代码规范 《Git 权威指南》 Learn Git Branching通关网站 《Maven 实战》 《重构_改善既有代码的设计》 学习代码规范。我们大致上遵循 oracle 的 Java 语言编码规范，你可以先阅读并熟悉它。Code Formatting 文件在 git@xxx/coding-standard.git，在编写代码之前，请把它导入到 IDE 中。另外，确认 IDE 已经安装 Findbugs 和 CheckStyle 插件。 开发工具 熟练使用一种 IDE。Intellij IDEA或者 Eclipse 都可以，推荐使用前者。至少熟悉常用的快捷键，会 debug(包括远程 debug)项目。 熟悉一种编辑器。比如 Vim/Emacs/Sublime Text，至少学会搜索/替换/代码补全。 开发环境 Linux 的基本使用可以通过《鸟哥的Linux私房菜：基础学习篇（第三版）》学习 bash shell 脚本可以参考《Linux Shell脚本攻略》 第二阶段刷题： 先从简单的图解算法看起，然后做题的时候不要做太多，做别人整理好的经典题比较重要。 先servelet和协议。再JAVA进阶，然后框架和数据库并行。 需要掌握三大框架SSM，需要掌握各类数据库，需要掌握各类协议 学Unix JAVA进阶 《Java并发编程的艺术》《深入理解Java虚拟机》 并发编程网：并发编程网 - ifeve.com 重点掌握java内存模型，各种锁的原理及应用，JVM GC垃圾回收原理。 第三阶段后端校招以及大牛成长转折点 源码 微服务 微架构 各种组件]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elo比赛心得]]></title>
    <url>%2F2019%2F04%2F30%2Felo%E6%AF%94%E8%B5%9B%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[比赛简介 回归问题 有异常点 商品信息， 每个用户的刷卡时间。 预测用户的忠诚度。 [TOC] 11th place solutionFEATURE ENGINEERING 123456789101112131415161718192021221. I refer the Kaggle Rank System Compute Formula（link:[https://www.kaggle.com/progression][4]) df_data['duration_sqrt_counts'] = df_data['durations']/sqrt(df_data['card_id_counts']) df_data['duration_log1p_counts'] = df_data['durations']/log1p(df_data['card_id_counts']) df_data['duration_counts'] = df_data['durations']/df_data['card_id_counts']2. Categorical features: frequence, Maxfrequence, MaxfrequenceRatio3. card_id/merchant_id/mechant_category_id/city_id (visit sequence to sequence embedding)4. purchase_amount:hist/new5. features interactions between hist/new df['purchase_amount_ratio_v3'] = df['new_purchase_amount_max']/df['hist_purchase_amount_sum'] df['purchase_amount_diff_v1'] = df['new_purchase_amount_sum']-df['hist_purchase_amount_sum'] df['purchase_amount_diff_v2'] = df['new_purchase_amount_mean']-df['hist_purchase_amount_mean'] df['purchase_amount_diff_v3'] = df['new_purchase_amount_max']-df['hist_purchase_amount_max'] df['purchase_amount_diff_v4'] = df['new_purchase_amount_min']-df['hist_purchase_amount_min'] df['pa_mlag_ratio'] = df['new_purchase_amount_sum']/(df['month_lag_mean'] - 1) df['pa_new_hist_ratio'] = df['new_purchase_amount_sum']/(df['hist_purchase_amount_sum']) df['pa_new_hist_mean_ratio'] = df['new_purchase_amount_mean']/(df['hist_purchase_amount_mean'] ) df['pa_new_hist_min_ratio'] = df['new_purchase_amount_min']/(df['hist_purchase_amount_min'] ) df['pa_new_hist_max_ratio'] = df['new_purchase_amount_max']/(df['hist_purchase_amount_max'] ) 我们有两个单独的feature sets。一个具有+1000个feature ，另一个具有+200个feature 然后，我们取+200特征集的相关矩阵，将每个特征与其关联最小的特征配对。然后我们对每一对应用了大量的聚合，结果得到了非常强大的特征。 所以我们最终得到了两个功能集，每个功能集+1000个功能。” STACKING We stacked around 32 models using bayesian regression. Our models were well varied that it yielded a score of CV:3.630X LB :3.675 STUFF THAT DID NOT WORK Of course these last two months were not all roses and rainbows. We pulled our hair trying a lot of things and we failed miserably. Here are the bloopers of our participation :D : NN. We tried designing different architectures with the main focus on having a simple NN with heavy regularization (BatchNorm and Strong Dropout) In the middle of the competition, we tried tackling the outliers detection as an anomaly detection problem using AutoEncoders trained only on the non outliers data We tried PCA for more features. And it didn’t work We tried TSNE. It didn’t work We tried FM and FFM. It did not work We tried isolation forest. Nope. Did not work. We had a Ridge-based pairwise ranker that we intended to use for outliers detection but it didn’t match with the approach we had. We tried a lot of weak models in the hope of adding diversity (simple tree-based, linear, svm, etc.). And guess what? It did not work. 如何识别异常点？ 两个不同的特征集怎么做？]]></content>
  </entry>
  <entry>
    <title><![CDATA[java核心技术基础知识]]></title>
    <url>%2F2019%2F04%2F30%2FJAVA%E5%AD%A6%E4%B9%A0%2Fjava%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[java基础知识 [TOC] java程序设计环境 JDK 是 Java Development Kit 的缩写开发人员必须要安装，Java 运行时环境（JRE), 它包含虚拟机 但不包含编译器。 Java SE 会大量出现， 相对于 Java EE ( Enterprise Edition) 和 Java ME ( Micro Edition), 它是 Java 的标准版。 java基本程序设计结构 类是构建所有 Java 应用程序和 applet 的构建块。Java 应用程序中的全部内容都必须放置在类中。 数据类型 整型 浮点型 有三种类型，正无穷大，负无穷大，NAN： Double_POSITIVE_INFINITY Double.NEGATIVEJNFINITY Double.NaN NAN与任何数字包括NAN都不相等，不能用==Double.NaN,而是要用isNaN() 字符型 byte byte 字节，数据存储容量1byte，byte作为基本数据类型表示的也是一个存储范围上的概念，有别于int、long等专门存数字的类型，这种类型的大小就是1byte,而int是4byte。存数字的话就是1byte=8位，2^8=256 即-128-127。字符的话包括字母和汉字，一个字母是1byte，一个汉字2byte。也就是可以用byte变量去存储一个英文字符，但是却存不下一个中文汉字，因为一个汉字占2byte。 总结，byte是java中的一个基本数据类型，这个数据类型的长度是1byte，此byte就是彼byte,即是基本数据类型也是存储空间的基本计量单位。 char char是Java中的保留字，与别的语言不同的是，char在Java中是16位的，因为Java用的是Unicode。不过8位的ASCII码包含在Unicode中，是从0~127的。 Java中使用Unicode的原因是，Java的Applet允许全世界范围内运行，那它就需要一种可以表述人类所有语言的字符编码。Unicode。 char本质上是一个固定占用两个字节的无符号正整数，这个正整数对应于Unicode编号，用于表示那个Unicode编号对应的字符。 由于固定占用两个字节，char只能表示Unicode编号在65536以内的字符，而不能表示超出范围的字符。 Unicode 需要注意的是，Unicode只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储。 比如，汉字”严”的unicode是十六进制数4E25，转换成二进制数足足有15位（100111000100101），也就是说这个符号的表示至少需要2个字节。表示其他更大的符号，可能需要3个字节或者4个字节，甚至更多。 这里就有两个严重的问题，第一个问题是，如何才能区别Unicode和ASCII？计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？第二个问题是，我们已经知道，英文字母只用一个字节表示就够了，如果Unicode统一规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。 它们造成的结果是：1）出现了Unicode的多种存储方式，也就是说有许多种不同的二进制格式，可以用来表示Unicode。2）Unicode在很长一段时间内无法推广，直到互联网的出现。 UTF-8 互联网的普及，强烈要求出现一种统一的编码方式。UTF-8就是在互联网上使用最广的一种Unicode的实现方式。其他实现方式还包括UTF-16（字符用两个字节或四个字节表示）和UTF-32（字符用四个字节表示），不过在互联网上基本不用。重复一遍，这里的关系是，UTF-8是Unicode的实现方式之一。 以utf8为例，utf8是一个变长编码标准，可以以1~4个字节表示一个字符，而中文占3个字节，ascII字符占1个字节。 为什么我们在java里面可以用一个char来表示一个中文呢？ 因为java是以unicode作为编码方式的。unicode是一个定长的编码标准，每个字符都是2个字节，也就是1个char类型的空间。 在编译时会把utf8的中文字符转换成对应的unicode来进行传输运算。 (总结)[https://www.zhihu.com/question/23374078] unicode是编码集，UTF8只是实现方式 一个char可以储存一个中文字符，因为它是两个byte UTF8采用8位动长的方式，可以节省内存，区别中英文 unicode不够的时候，会采取增补代码点，即用2个2位字节来表示增补字符]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比赛常用图]]></title>
    <url>%2F2019%2F04%2F29%2F%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%2F%E6%AF%94%E8%B5%9B%E5%B8%B8%E7%94%A8%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[比赛常见的EDA总结 要注意把以后整理函数的时候把图补上 plt.show()会阻碍当前程序的执行，请再最后执行如果想要不阻碍请执行plt.ion()，但是这样当程序结束时会自动关闭当前图像，所以需要再最后执行plt.ioff()以阻碍图像不被关闭 [TOC] 曲线分布图12345678910111213141516171819202122# https://www.kaggle.com/gpreda/santander-eda-and-predictiondef plot_feature_distribution(df1, df2, label1, label2, features): i = 0 sns.set_style('whitegrid') plt.figure() fig, ax = plt.subplots(10,10,figsize=(18,22)) for feature in features: i += 1 plt.subplot(10,10,i) sns.distplot(df1[feature], hist=False,label=label1) sns.distplot(df2[feature], hist=False,label=label2) plt.xlabel(feature, fontsize=9) locs, labels = plt.xticks() plt.tick_params(axis='x', which='major', labelsize=6, pad=-6) plt.tick_params(axis='y', which='major', labelsize=6) plt.show(); t0 = train_df.loc[train_df['target'] == 0]t1 = train_df.loc[train_df['target'] == 1]features = train_df.columns.values[2:102]plot_feature_distribution(t0, t1, '0', '1', features) 条形分布图123456plt.figure(figsize=(16,6))plt.title("Distribution of mean values per column in the train and test set")sns.distplot(train_df[features].mean(axis=0),color="magenta",kde=True,bins=120, label='train')sns.distplot(test_df[features].mean(axis=0),color="darkblue", kde=True,bins=120, label='test')plt.legend()plt.show() 条形图123#画条形图sns.barplot(x='Sex', y='Survived', data=train);# seaborn 的 barplot() 利用矩阵条的高度反映数值变量的集中趋势，展示的是变量的平均值sns.barplot(x='Sex', y='Survived', hue = 'Pclass', data=train);#加了图例的功能 相关性热度图12345colormap = plt.cm.RdBuplt.figure(figsize=(14,12))plt.title('Pearson Correlation of Features', y=1.05, size=15)sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True) 多变量相关性图123#多变量相关性图 要注意哦这里的变量所有的成对一一对应的g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked', u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) ) 箱图123456789101112131415161718192021222324252627'''Create a function to count total outliers. And plot variables with and without outliers.'''def outliers(variable): # Calculate 1st, 3rd quartiles and iqr. q1, q3 = variable.quantile(0.25), variable.quantile(0.75) iqr = q3 - q1 # Calculate lower fence and upper fence for outliers l_fence, u_fence = q1 - 1.5*iqr , q3 + 1.5*iqr # Any values less than l_fence and greater than u_fence are outliers. # Observations that are outliers outliers = variable[(variable&lt;l_fence) | (variable&gt;u_fence)] print('Total Outliers of', variable.name,':', outliers.count()) # Drop obsevations that are outliers filtered = variable.drop(outliers.index, axis = 0) # Create subplots fig, (ax1, ax2) = plt.subplots(2,1) # Gives space between two subplots fig.subplots_adjust(hspace = 1) # Plot variable with outliers variable.plot.box(vert = False, color = 'coral', grid = False, ax = ax1, title = 'Distribution with Outliers for %s' %variable.name) # Plot variable without outliers filtered.plot.box(vert = False, color = 'coral', grid = False, ax = ax2, title = 'Distribution without Outliers for %s' %variable.name) 刻画变量的不平衡度If skewness is less than −1 or greater than +1, the distribution can be considered as highly skewed. If skewness is between −1 and −½ or between +½ and +1, the distribution can be considered as moderately skewed. And finally if skewness is between −½ and +½, the distribution can be considered as approximately symmetric. 123456789'''#2.Density plot with skewness.'''def density_plot_and_skewness(variable): variable.plot.hist(density = True) variable.plot.kde(style = 'k--') plt.xlabel('%s'%variable.name) plt.title('Distribution of %s with Density Plot &amp; Histogram' %variable.name) print('Skewness of ', variable.name, ':') skewness = variable.skew() return display(skewness) 分类变量和分类变量的关系图123456789101112131415161718192021222324252627282930313233343536373839#############################2X2列联表展示########################################'''#1.Create a function that calculates absolute and relative frequency of Survived variable by a categorical variable. And then plots the absolute and relative frequency of Survived by a categorical variable.'''def crosstab(cat, cat_target): '''cat = categorical variable, cat_target = our target categorical variable.''' global ax, ax1 cat_grouped_by_cat_target = pd.crosstab(index = cat, columns = cat_target) cat_grouped_by_cat_target.rename(&#123;0:'Victims', 1:'Survivors'&#125;, axis = 'columns', inplace = True) # Renaming the columns pct_cat_grouped_by_cat_target = round(pd.crosstab(index = cat, columns = cat_target, normalize = 'index')*100, 2) pct_cat_grouped_by_cat_target.rename(&#123;0:'Victims(%)', 1:'Survivors(%)'&#125;, axis = 'columns', inplace = True) print('Survivals and Deaths by', cat.name,':', '\n',cat_grouped_by_cat_target ) print('\nPercentage Survivals and Deaths by', cat.name, ':','\n', pct_cat_grouped_by_cat_target) # Plot absolute frequency of Survived by a categorical variable ax = cat_grouped_by_cat_target.plot.bar(color = ['r', 'g']) plt.title('Survival vs Death Count by %s' %cat.name) abs_bar_labels() plt.show() # Plot relative frequrncy of Survived by a categorical variable ax1 = pct_cat_grouped_by_cat_target.plot.bar(color = ['r', 'g']) plt.title('Percentage Survival vs Death Count by %s' %cat.name) pct_bar_labels() plt.show()###########################################卡方检验#######################################'''#2.Create a function to calculate chi_square test between a categorical and target categorical variable.'''def chi_square(cat, cat_target): cat_grouped_by_cat_target = pd.crosstab(index = cat, columns = cat_target) test_result = stats.chi2_contingency (cat_grouped_by_cat_target) print('Chi_square test result between Survived &amp; %s' %cat.name) return display(test_result)#############################################bonferroni adjusted检验###############################'''#3.Finally create another function to calculate Bonferroni-adjusted pvalue for a categorical and target categorical variable.'''def bonferroni_adjusted(cat, cat_target): dummies = pd.get_dummies(cat) for columns in dummies: crosstab = pd.crosstab(dummies[columns], cat_target) print(stats.chi2_contingency(crosstab)) print('\nColumns:', dummies.columns) 多个变量组合对因变量的影响图123456'''Create a function that plots the impact of 3 predictor variables at a time on a target variable.'''def multivariate_analysis(cat1, cat2, cat3, cat_target): grouped = round(pd.crosstab(index = [cat1, cat2, cat3], columns = cat_target, normalize = 'index')*100, 2) grouped.rename(&#123;0:'Died%', 1:'Survived%'&#125;, axis = 1, inplace = True) ax = grouped.plot.bar(color = ['r', 'g']) plt.ylabel('Relative Frequency (%)') 热度图123456789#https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920plt.subplot(1,2,1)sns.heatmap(x, cmap='RdBu_r', center=0.0) plt.title('VAR_'+str(j)+' Predictions without Magic',fontsize=16)plt.xticks(np.linspace(0,49,5),np.round(np.linspace(mn,mx,5),1))plt.xlabel('Var_'+str(j))plt.yticks([])plt.ylabel('')plt.show() 通过热度图可以看出模型是否明显利用了两个特征。虽然可能特征重要度也会说明，但是这样的热度图明显会更具说服力，比如说var_0，模型可以得出竖线15，横线4之类的分界线。]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[迁移学习]]></title>
    <url>%2F2019%2F04%2F29%2F%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%2F%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[！！！说不一定可以切分原数据集为新旧数据集 新旧特征：新数据集为1，旧数据集为0 合并新旧数据：合并新旧来做特征处理然后有两种操作第一种是用旧训练模型，用模型预测新数据集的train和test概率，然后加在后面做特征，然后在用新数据重新做特征处理然后预测（这个比较好）。 第二种是直接上新旧合并数据训练的模型，然后直接加模型概率做新特征（这个可能会leak，但是在目前的模板上有cv应该会好一点）然后在用新数据重新做特征处理然后预测]]></content>
  </entry>
  <entry>
    <title><![CDATA[去除重复值]]></title>
    <url>%2F2019%2F04%2F29%2F%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C%2F%E5%8E%BB%E9%99%A4%E9%87%8D%E5%A4%8D%E5%80%BC%2F</url>
    <content type="text"><![CDATA[不去重的数据训练的模型 = 80%的非重复样本准确率 + 100%重复样本准确率去重的数据训练的模型 = 85%的非重复样本准确率 + 85%重复样本准确率（直接规则覆盖后100%）深度提取重复样本的真正目的是删除它们以提高模型预测非重复样本的准确率，非重复样本的准确率才是对业务有价值的 1.Giba的home credit 里的重复样本 如果八月份参加过kaggle的home credit比赛的一定知道，在最后一周才加入比赛的GM Giba通过丰富的比赛经验找到了重复样本trick帮助onodera队伍拿到了比赛的第二名，而另一位GM raddar在最后八个小时加入比赛就直接发现了重复样本问题，并solo到了45名。今天就先来分析一下他们的思路，A competition without a leak. Or is it? 其实说起来简单，在home credit比赛里，每个客户都有多达两百列特征，但是人的基本特征是不会变的，性别、年龄、生日、银行开户时间等等。打个比方，尽管北京有三千万人，但是如果两样本同一天生日，同一天领驾照，办身份证，同一天结婚，同一天生娃，那么从统计上来说，这两样本肯定就是一个人，而不必再核对身高血型样貌了。radder 就是仅用[DAYS_BIRTH,DAYS_EMPLOYED,DAYS_REGISTRATION,DAYS_ID_PUBLISH,CODE_GENDER,REGION_POPULATION_RELATIVE]六个维度的特征，就把数据集里的重复样本给找出来了，其实其它两百列特征是否一样已经不重要了。 小结：虽然我们在比赛里这个称之为trick，但背后是统计学意义的，也可以应用在工作之中。 2.活学活用 在我们最近参加的一个关于通讯用户套餐的比赛里，我们的GM piupiu也提到了这个trick。因为出题方把用户的流量消费精确到了byte，金钱消费精确到了分，通讯时间精确到了秒，所以如果用户A上月花了45.32元，用了1545MB 457kb 123byte的流量，打了78分12秒的电话，用户B上月也花了45.32元，用了1545MB 457kb 123byte的流量，打了78分12秒的电话，别管这家通讯公司用户量有多大，A和B肯定是同一用户。所以在其他队伍还在用给定的全部特征判定重复样本的时候，我们用八个特征就可以判定重复样本，把test里和train重复的样本给挑出来了。 吐槽：因而我们得到了比其他队伍更多test里的重复样本，然后定义了白名单（piupiu的职业习惯）。在我们代码开源后有一小撮萌新看不懂我们的代码不说，还觉得是piupiu把竞赛网站黑了拿到test的label（滑稽） 3.进阶 有人会说了，拿到了test里的重复的样本的label，是不是可以上一波分了～～ 答案是：并不能。因为xgb/lgb是具备非常强大的拟合能力，你不刻意找这些重复样本，xgb/lgb也能给你学出来，通过比较你会发现test里的重复样本已经全部给你预测对了。在home credit里是有重复用户的时序信息在里面可以利用，但是在我们这个比赛及大部分比赛就用不上了。 回到这个比赛，重复样本达到了全部数据的15%，test里的重复样本都预测对了，说明模型把train/test之间的重复样本的特性全学会了，有时候xgb/lgb学习能力太强也不是好事。这时候我们的GM piupiu提到了，test其实可以分为重复样本（15%）和非重复样本两部分（85%）来看待，真正对业务有意义的是非重复样本，可是所有参赛者却把重复样本过拟合的很好，我们真的目的应该是为出题方提供一个有效预测非重复样本的模型，因此提议我们应该把train里的重复样本去掉。打个比方，valid数据包含重复的和不重复两部分，不去重的数据训练的模型需要1000轮才能达到最优，去重的模型之后500轮最优，那说明有500轮是在过拟合重复样本，但这对于不重复样本来说是一个严重的过拟合，伤害了模型的泛化能力。而这个通讯比赛的重复样本达到了15%,对模型伤害很大。如果再根据包含重复样本的数据模型调参和特征工程，伤害就更大了。 简单的打个比方： 不去重的数据训练的模型 = 80%的非重复样本准确率 + 100%重复样本准确率 去重的数据训练的模型 = 85%的非重复样本准确率 + 85%重复样本准确率（直接规则覆盖后100%） 如果你要模型上线，你会上线哪个模型？ 重复样本的准确率并不重要，无论是实际业务还是比赛，一个sql就搞定了。 根据piupiu的建议，于是我照着做了，果然模型变得特别稳定，也没有其他队伍所提到的抖动也消除了，提升的分数刚好是我们最后领先第二名的差距。 所以在这个比赛里，深度提取重复样本的真正目的是删除它们以提高模型预测非重复样本的准确率，非重复样本的准确率才是对业务有价值的。 有些选手因为看到test里有重复样本就舍不得删的train里的重复样本，难道不用lgb预测的结果就不是结果了吗（流汗） 小结：虽然第二节一直在说怎么更多的提取重复样本，但我们真正提升的是模型预测非重复样本的预测能力。 吐槽：在我们代码开源后，有一小撮萌新看到我们在lgb输出的结果通过规则覆盖能上好多分，就觉得我们是在用重复样本的leak提升重复样本准确率，却不想想大家既然都能100%预测重复样本的情况下，差距在哪里… 最后的吐槽：第一次代码开源就体会了当年plantgo开源携程代码的蛋疼。如何看待携程举办的大数据比赛？ 分享代码有一小撮萌新看不懂学不会清洗重复数据这种常规操作也就算了，还根据自己对代码自己的理解莫名其妙揣测，写本文章主要是帮助我们队友piupiu辟谣，顺便也给大家分享了点有价值的干货。]]></content>
  </entry>
  <entry>
    <title><![CDATA[异常值检测]]></title>
    <url>%2F2019%2F04%2F29%2F%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C%2F%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[https://www.zhihu.com/question/38066650]]></content>
  </entry>
  <entry>
    <title><![CDATA[缺失值处理]]></title>
    <url>%2F2019%2F04%2F29%2F%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C%2F%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[数据缺失值处理 [TOC] 数据丢失的原因。 随机缺失(MAR):随机缺失意味着数据点缺失的倾向性与缺失的数据无关，而是与一些观察到的数据相关 完全随机缺失(MCAR):某个值缺失的事实与它的假设值以及其他变量的值无关。 非随机缺失(MNAR):两个可能的原因是,缺失值取决于假设的值(例如,工资高的人通常不愿透露他们的收入调查)或缺失值依赖于其他变量的值(例如假设女性一般不愿透露他们的年龄!此处年龄变量缺失值受性别变量影响) 在前两种情况下，根据缺失值的出现情况删除缺失值的数据是安全的，而在第三种情况下，删除缺失值的观察值会在模型中产生偏差。所以在移除观测结果之前，我们必须非常小心。注意，归罪法不一定能给出更好的结果。 删除成列删除（listwise deletion） 列表删除(完全案例分析)删除包含一个或多个缺失值的所有数据。特别是如果缺少的数据仅限于少量的观察，您可以选择从分析中删除这些情况。然而，在大多数情况下，使用列表删除通常是不利的。这是因为MCAR的假设(完全随机缺失)通常很少得到支持。因此，列表删除方法产生有偏差的参数和估计。 123newdata &lt;- na.omit(mydata)# In pythonmydata.dropna(inplace=True) 成对删除（pairwise deletion） 一般的备选方案，在进行多变量的联立时，只删除掉需要执行的变量的缺失数据。例如在ABC三个变量间，需要计算A和C的协方差，那么只有同时具备A/C的数据会被使用。 文献指出，当变量间的相关性普遍较低时，成对删除会产生更有效的估计值。然而当变量间的相关性较高时，建议还是使用成列删除。 理论上成对删除不建议作为成列删除的备选方案。 虚拟变量调整（哑变量，dummy variables） 新建两个变量，其中一个变量D为“是否缺失”，缺失值设为0，存在值设为1。 另一个变量X’，将缺失值设为c（可以是任何常数），存在值设为本身。 随后，对X’，D和其他变量（因变量和其他预设模型中的自变量）进行回归。这种调整的好处是它利用了所有可用的缺失数据的信息（是否缺失）。为了便利，一个好的c的设置方式是现有非缺失数据X的均数。 这样做的好处是，D的系数可以被解释成“在控制了其他变量的情况下，X具缺失数据的个体其Y的预测值减去具X平均数的个体于Y的预测值” Time-Series Specific MethodsLast Observation Carried Forward (LOCF) &amp; Next Observation Carried Backward (NOCB) Linear Interpolation Seasonal Adjustment + Linear Interpolation Mean, Median and Mode计算总体均值、中值或模态是一种非常基本的推算方法，但它没有利用时间序列特征，也没有使用变量之间的关系。它非常快，但有明显的缺点。缺点之一是平均输入减少了数据集中的方差。 Linear Regression首先，使用相关矩阵标识值缺失的变量的几个预测器。在回归方程中，选取最优预测因子作为自变量。缺少数据的变量作为因变量。采用预测变量数据完整的案例生成回归方程;然后，该方程用于预测不完全情况下的缺失值。在迭代过程中，插入缺失变量的值，然后用所有的情况来预测因变量。重复这些步骤，直到预测值之间的差值很小，即它们收敛。 它“理论上”为缺失的值提供了很好的估计。然而，这种模式有几个缺点，往往超过了优点。首先，由于替换后的值是由其他变量预测的，它们往往“太好”地匹配在一起，因此标准误差被缩小了。我们还必须假设回归方程中使用的变量之间存在线性关系，而回归方程中可能没有线性关系。 Multiple Imputation Imputation: Impute the missing entries of the incomplete data sets mtimes (m=3 in the figure). Note that imputed values are drawn from a distribution. Simulating random draws doesn’t include uncertainty in model parameters. Better approach is to use Markov Chain Monte Carlo (MCMC) simulation. This step results in m complete data sets. Analysis: Analyze each of the m completed data sets. Pooling: Integrate the m analysis results into a final result This is by far the most preferred method for imputation for the following reasons: Easy to use No biases (if imputation model is correct) 实现的代码包:fancyimpute Imputation of Categorical Variables把缺失的值作为一个类别进行填补 KNN (K Nearest Neighbors) XGBoost and Random Forest 同样也有 data imputation 该方法基于距离测度选取k个邻域，并以邻域的平均值作为估计的归一化方法。该方法需要选择最近邻的数目和距离度量。KNN既可以预测离散属性(k个近邻中最频繁的值)，也可以预测连续属性(k个近邻中均值) 距离度量根据数据的类型而变化: 连续数据:连续数据常用的距离度量是欧式、Manhattan和cos 分类数据:本例中一般使用汉明距离。它接受所有的分类属性，如果两个点之间的值不相同，则对每个属性进行计数。然后，汉明距离等于值不同的属性的数量。 KNN算法最吸引人的特点之一是易于理解和实现。KNN的非参数特性使其在某些数据可能非常“不寻常”的情况下具有优势。 KNN算法的一个明显缺点是，在分析大型数据集时非常耗时，因为它在整个数据集中搜索类似的实例。此外，由于最近邻和最近邻之间的距离相差不大，高维数据会严重降低KNN的精度]]></content>
  </entry>
  <entry>
    <title><![CDATA[胶囊网络]]></title>
    <url>%2F2019%2F04%2F27%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[胶囊网络 来源:https://spaces.ac.cn/archives/4819 直接看上面的这个文章，描述的非常详细]]></content>
      <categories>
        <category>nn学习</category>
      </categories>
      <tags>
        <tag>网络结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FocalLoss针对不平衡数据]]></title>
    <url>%2F2019%2F04%2F22%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2FFocalLoss%E9%92%88%E5%AF%B9%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[[TOC] 参考来源: https://zhuanlan.zhihu.com/p/32423092 https://www.zhihu.com/question/63581984 https://zhuanlan.zhihu.com/p/28527749 讲解​ 本质上讲，Focal Loss 就是一个解决分类问题中类别不平衡、分类难度差异的一个 loss，总之这个工作一片好评就是了。大家还可以看知乎的讨论：如何评价 Kaiming 的 Focal Loss for Dense Object Detection？ 核心思想 这样的做法就是：正样本的预测值大于 0.5 的，或者负样本的预测值小于 0.5 的，我都不更新了，把注意力集中在预测不准的那些样本，当然这个阈值可以调整。这样做能部分地达到目的，但是所需要的迭代次数会大大增加。 原因是这样的：以正样本为例，我只告诉模型正样本的预测值大于 0.5 就不更新了，却没有告诉它要“保持”大于 0.5，所以下一阶段，它的预测值就很有可能变回小于 0.5 了。当然，如果是这样的话，下一回合它又被更新了，这样反复迭代，理论上也能达到目的，但是迭代次数会大大增加。 所以，要想改进的话，重点就是“不只是要告诉模型正样本的预测值大于0.5就不更新了，而是要告诉模型当其大于0.5后就只需要保持就好了”。好比老师看到一个学生及格了就不管了，这显然是不行的。如果学生已经及格，那么应该要想办法要他保持目前这个状态甚至变得更好，而不是不管。 所以除了单纯的区分外，必须使该loss可导，这样才可以告诉模型。 目的是通过减少易分类样本的权重，从而使得模型在训练时更专注于难分类的样本。 Kaiming 大神的 Focal Loss 形式是： 如果落实到 ŷ =σ(x) 这个预测，那么就有： 特别地，如果 K 和 γ 都取 1，那么 L∗∗=Lfl。 事实上 K 和 γ 的作用都是一样的，都是调节权重曲线的陡度，只是调节的方式不一样。注意L∗∗或 Lfl 实际上都已经包含了对不均衡样本的解决方法，或者说，类别不均衡本质上就是分类难度差异的体现。 ​ 首先y’的范围是0到1，所以不管γ是多少，这个调制系数都是大于等于0的。易分类的样本再多，你的权重很小，那么对于total loss的共享也就不会太大。那么怎么控制样本权重呢？举个例子，假设一个二分类，样本x1属于类别1的y’=0.9，样本x2属于类别1的y’=0.6，显然前者更可能是类别1，假设γ=1，那么对于y’=0.9，调制系数则为0.1；对于y’=0.6，调制系数则为0.4，这个调制系数就是这个样本对loss的贡献程度，也就是权重，所以难分的样本（pt=0.6）的权重更大。 ​ 比如负样本远比正样本多的话，模型肯定会倾向于数目多的负类（可以想象全部样本都判为负类），这时候，负类的 *ŷ γ* 或 σ(Kx) 都很小，而正类的 (1−ŷ )γ 或 *σ(−Kx)* 就很大，这时候模型就会开始集中精力关注正样本。 当然，Kaiming 大神还发现对 Lfl 做个权重调整，结果会有微小提升。 通过一系列调参，得到 α=0.25, γ=2（在他的模型上）的效果最好。注意在他的任务中，正样本是属于少数样本，也就是说，本来正样本难以“匹敌”负样本，但经过 (1−ŷ )γ 和 ŷγ 的“操控”后，也许形势还逆转了，还要对正样本降权。 不过我认为这样调整只是经验结果，理论上很难有一个指导方案来决定 α 的值，如果没有大算力调参，倒不如直接让 α=0.5（均等）。 多分类Focal Loss 在多分类中的形式也很容易得到，其实就是： ŷt 是目标的预测值，一般就是经过 softmax 后的结果。 代码多分类 https://github.com/marvis/pytorch-yolo2/blob/master/FocalLoss.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.autograd import Variableclass FocalLoss(nn.Module): r""" This criterion is a implemenation of Focal Loss, which is proposed in Focal Loss for Dense Object Detection. Loss(x, class) = - \alpha (1-softmax(x)[class])^gamma \log(softmax(x)[class]) The losses are averaged across observations for each minibatch. Args: alpha(1D Tensor, Variable) : the scalar factor for this criterion gamma(float, double) : gamma &gt; 0; reduces the relative loss for well-classiﬁed examples (p &gt; .5), putting more focus on hard, misclassiﬁed examples size_average(bool): size_average(bool): By default, the losses are averaged over observations for each minibatch. However, if the field size_average is set to False, the losses are instead summed for each minibatch. """ def __init__(self, class_num, alpha=None, gamma=2, size_average=True): super(FocalLoss, self).__init__() if alpha is None: self.alpha = Variable(torch.ones(class_num, 1)) else: if isinstance(alpha, Variable): self.alpha = alpha else: self.alpha = Variable(alpha) self.gamma = gamma self.class_num = class_num self.size_average = size_average def forward(self, inputs, targets): N = inputs.size(0) print(N) C = inputs.size(1) P = F.softmax(inputs) # 这是为了获取onehot class_mask = inputs.data.new(N, C).fill_(0) class_mask = Variable(class_mask) ids = targets.view(-1, 1) class_mask.scatter_(1, ids.data, 1.) #print(class_mask) if inputs.is_cuda and not self.alpha.is_cuda: self.alpha = self.alpha.cuda() alpha = self.alpha[ids.data.view(-1)] probs = (P*class_mask).sum(1).view(-1,1) log_p = probs.log() #print('probs size= &#123;&#125;'.format(probs.size())) #print(probs) batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p #print('-----bacth_loss------') #print(batch_loss) if self.size_average: loss = batch_loss.mean() else: loss = batch_loss.sum() return loss if __name__ == "__main__": alpha = torch.rand(21, 1) print(alpha) FL = FocalLoss(class_num=5, gamma=0 ) CE = nn.CrossEntropyLoss() N = 4 C = 5 inputs = torch.rand(N, C) targets = torch.LongTensor(N).random_(C) inputs_fl = Variable(inputs.clone(), requires_grad=True) targets_fl = Variable(targets.clone()) inputs_ce = Variable(inputs.clone(), requires_grad=True) targets_ce = Variable(targets.clone()) print('----inputs----') print(inputs) print('---target-----') print(targets) fl_loss = FL(inputs_fl, targets_fl) ce_loss = CE(inputs_ce, targets_ce) print('ce = &#123;&#125;, fl =&#123;&#125;'.format(ce_loss.data[0], fl_loss.data[0])) fl_loss.backward() ce_loss.backward() #print(inputs_fl.grad.data) print(inputs_ce.grad.data) 单分类 https://www.kaggle.com/aakashnain/diving-deep-into-focal-loss https://www.kaggle.com/sfzero/focal-loss-feature-0-99994/comments 12345678910111213141516171819202122232425import torchfrom torch import nnimport torch.nn.functional as Fclass FocalLoss(nn.Module): def __init__(self, alpha=1, gamma=2, logits=True, reduction='elementwise_mean'): super(FocalLoss, self).__init__() self.alpha = alpha self.gamma = gamma self.logits = logits self.reduction = reduction def forward(self, inputs, targets): if self.logits: BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none') else: BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none') pt = torch.exp(-BCE_loss) F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss if self.reduction is None: return F_loss else: return torch.mean(F_loss) 简单版代码 1234567# https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/83363#486607def staticWeightLoss(true,pred): loss = K.binary_crossentropy(true, pred) positiveLoss = positiveWeights * loss negativeLoss = negativeWeights * loss return K.switch(K.greater(true, 0.5), positiveLoss, negativeLoss) !要注意softmax是要有两列以上，sigmod才是一列 引申这就是为什么之前别人做数据增强的时候，把预测很高的数据当作1把预测很低的数据当作0放进去加强训练。]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比赛心得集合]]></title>
    <url>%2F2019%2F04%2F21%2F%E6%AF%94%E8%B5%9B%E5%BF%83%E5%BE%97%E9%9B%86%E5%90%88%2F</url>
    <content type="text"><![CDATA[好的代码借鉴，必要的时候可以直接抄 [TOC] 比赛心得机器翻译注意力机制及其PyTorch实现 https://plmsmile.github.io/2017/10/12/Attention-based-NMT/ 各个NLP模型实现 http://www.zhongruitech.com/921029206.html 苏剑林大神博客 https://spaces.ac.cn/category/Resources QuroaNLP分类比赛心得 Quroa 识别不良句子 https://www.getit01.com/p20190314357550039/ 腾讯广告大赛比赛心得 https://zhuanlan.zhihu.com/p/38341881 摩拜杯目的地预测比赛心得 https://zhuanlan.zhihu.com/p/32151090 比赛如何不过拟合:https://www.kaggle.com/c/dont-overfit-ii]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘问答汇总]]></title>
    <url>%2F2019%2F04%2F21%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E9%97%AE%E7%AD%94%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[数据挖掘竞赛汇总 [TOC] 如何知道树模型怎么可以提高？通过画树的图，分析树当前无法分割的知识是什么，给它补充进数据里面。 如何提高分数？ 测试集和训练集尽量相似]]></content>
  </entry>
  <entry>
    <title><![CDATA[NLP代码汇总]]></title>
    <url>%2F2019%2F04%2F20%2FNLP%E4%BB%A3%E7%A0%81%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[NLP汇总 [TOC] 动态padding，节省时间比起一开始固定化padding，动态padding，可以针对当前batch的长度来Padding，明显会增快速度 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class MyDataset(data.Dataset): def __init__(self, text, lens, y=None): self.text = text self.y = y self.lens = lens def __len__(self): return len(self.lens) def __getitem__(self, index): if self.y is None: return self.text[index], self.lens[index] else: return self.text[index], self.lens[index], self.y[index] def collate_fn(batch): """ batch = [dataset[i] for i in N] """ size = len(batch[0]) if size == 3: texts, lens, y = zip(*batch) else: texts, lens = zip(*batch) lens = np.array(lens) sort_idx = np.argsort(-1 * lens) reverse_idx = np.argsort(sort_idx) max_len = min(int(np.percentile(lens, PERCENTILE)), MAX_LEN) lens = np.clip(lens, 0, max_len)[sort_idx] texts = torch.tensor(sequence.pad_sequences(texts, maxlen=max_len)[sort_idx], dtype=torch.long).cuda() if size == 3: return texts, lens, reverse_idx, torch.tensor(y, dtype=torch.float32).cuda() else: return texts, lens, reverse_idxdef build_data_loader(texts, lens, y=None, batch_size=BATCH_SIZE): dset = MyDataset(texts, lens, y) dloader = data.DataLoader(dset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn) return dloaderseqs = [[1,2,3,3,4,5,6,7], [1,2,3], [2,4,1,2,3], [1,2,4,1]]lens = [len(i) for i in seqs]data_loader = build_data_loader(seqs, lens)for batch in data_loader: seq_batch, lens_batch, reverse_idx_batch = batch breakprint(f'original seqs:')print(seqs)print(f'batch seqs, already sort by lens, and padding dynamic in batch:')print(seq_batch)print(f'reverse batch seqs:')print(seq_batch[reverse_idx_batch])h_embedding_pack = pack_padded_sequence(seq_batch,lens_batch,batch_first=True)print(h_embedding_pack) mask loss 避免无用结果的求导影响12345678910111213141516171819202122232425262728293031import torch.nn as nnimport torch.nn.functional as funcclass CustomLoss(nn.Module): def __init__(self): super(CustomLoss,self).__init__() return def forward(self, Y_hat, Y): # TRICK 3 ******************************** # before we calculate the negative log likelihood, we need to mask out the activations # this means we don't want to take into account padded items in the output vector # simplest way to think about this is to flatten ALL sequences into a REALLY long sequence # and calculate the loss on that. # flatten all the labels Y = Y.view(-1) # flatten all predictions Y_hat = Y_hat.view(-1, self.nb_tags) # create a mask by filtering out all tokens that ARE NOT the padding token tag_pad_token = self.tags['&lt;PAD&gt;'] mask = (Y &gt; tag_pad_token).float() # count how many tokens we have nb_tokens = int(torch.sum(mask).data[0]) # pick the values for the label and zero out the rest with the mask Y_hat = Y_hat[range(Y_hat.shape[0]), Y] * mask # compute cross entropy loss which ignores all &lt;PAD&gt; tokens ce_loss = -torch.sum(Y_hat) / nb_tokens return ce_loss 问题解答专区为什么LSTM不同batch的句子长度可以不一致LSTM只要保证每个batch的句子长度一致即可，所以可以使用动态padding,但是为什么不同batch的长度可以不一样？ 因为LSTM需要更新的参数是共享，就是每个状态的权重是共享的，所以就无所谓不同长度了。只要保证每个batch内能够更新权重即可。 深度学习中 number of training epochs 中的 epoch到底指什么？对于初学者来讲，有几个概念容易混淆： （1）iteration：表示1次迭代（也叫training step），每次迭代更新1次网络结构的参数； （2）batch-size：1次迭代所使用的样本量； （3）epoch：1个epoch表示过了1遍训练集中的所有样本。 一次epoch=所有训练数据forward+backward后更新参数的过程。一次iteration=[batch size]个训练数据forward+backward后更新参数过程。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python多进程]]></title>
    <url>%2F2019%2F04%2F18%2F%E4%BB%A3%E7%A0%81%E5%9F%BA%E7%A1%80%2Fpython%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[python multiprocessing模块多进程详解 [TOC] multiprocessing模块APIPool类用于需要执行的目标很多，而手动限制进程数量又太繁琐时，如果目标少且不用控制进程数量则可以用Process类。 构造方法 Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]]) processes ：使用的工作进程的数量，如果processes是None那么使用 os.cpu_count()返回的数量。 initializer： 如果initializer是None，那么每一个工作进程在开始的时候会调用initializer(*initargs)。 maxtasksperchild：工作进程退出之前可以完成的任务数，完成后用一个新的工作进程来替代原进程，来让闲置的资源被释放。maxtasksperchild默认是None，意味着只要Pool存在工作进程就会一直存活。 context: 用在制定工作进程启动时的上下文，一般使用 multiprocessing.Pool() 或者一个context对象的Pool()方法来创建一个池，两种方法都适当的设置了context。 实例方法 apply_async(func[, args[, kwds[, callback]]]) 它是非阻塞。 apply(func[, args[, kwds]])是阻塞的。 close() 关闭pool，使其不在接受新的任务。 terminate() 关闭pool，结束工作进程，不在处理未完成的任务。 join() 主进程阻塞，等待子进程的退出， join方法要在close或terminate之后使用。 Pool使用方法Pool+map函数 说明：此写法缺点在于只能通过map向函数传递一个参数。 123456789from multiprocessing import Pooldef test(i): print iif __name__=="__main__": lists=[1,2,3] pool=Pool(processes=2) #定义最大的进程数 pool.map(test,lists) #p必须是一个可迭代变量。 pool.close() pool.join() 异步进程池（非阻塞） 123456789101112131415161718from multiprocessing import Pooldef test(i): print iif __name__=="__main__": pool = Pool(processes=10) for i in xrange(500): ''' For循环中执行步骤： （1）循环遍历，将500个子进程添加到进程池（相对父进程会阻塞） （2）每次执行10个子进程，等一个子进程执行完后，立马启动新的子进程。（相对父进程不阻塞） apply_async为异步进程池写法。 异步指的是启动子进程的过程，与父进程本身的执行（print）是异步的，而For循环中往进程池添加子进程的过程，与父进程本身的执行却是同步的。 ''' pool.apply_async(test, args=(i,)) #维持执行的进程总数为10，当一个进程执行完后启动一个新进程. print “test” pool.close() pool.join() 执行顺序：For循环内执行了2个步骤，第一步：将500个对象放入进程池（阻塞）。第二步：同时执行10个子进程（非阻塞），有结束的就立即添加，维持10个子进程运行。（apply_async方法的会在执行完for循环的添加步骤后，直接执行后面的print语句，而apply方法会等所有进程池中的子进程运行完以后再执行后面的print语句） 注意：调用join之前，先调用close或者terminate方法，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束。 多进程示例代码纯建立Process1234567891011121314151617181920212223242526#encoding:utf-8from multiprocessing import Processimport os, time, random#线程启动后实际执行的代码块def r1(process_name): for i in range(5): print process_name, os.getpid() #打印出当前进程的id time.sleep(random.random())def r2(process_name): for i in range(5): print process_name, os.getpid() #打印出当前进程的id time.sleep(random.random()) if __name__ == "__main__": print "main process run..." p1 = Process(target=r1, args=('process_name1', )) #target:指定进程执行的函数，args:该函数的参数，需要使用tuple,只需要不断往后加参数即可 p2 = Process(target=r2, args=('process_name2', )) p1.start() #通过调用start方法启动进程，跟线程差不多。 p2.start() #但run方法在哪呢？待会说。。。 p1.join() #join方法也很有意思，寻思了一下午，终于理解了。待会演示。 p2.join() print "main process runned all lines..." POOL池管理123456789101112131415161718192021import mathfrom multiprocessing import Pooldef run(data, index, size): # data 传入数据，index 数据分片索引，size进程数 size = math.ceil(len(data) / size) start = size * index end = (index + 1) * size if (index + 1) * size &lt; len(data) else len(data) temp_data = data[start:end] # do something return data # 可以返回数据，在后面收集起来processor = 40res = []p = Pool(processor)for i in range(processor): res.append(p.apply_async(run, args=(data, i, processor,))) print(str(i) + ' processor started !')p.close()p.join()for i in res: print(i.get()) # 使用get获得多进程处理的结果 进程注意事项进程之间内存独立多进程与多线程最大的不同在于，多进程的每一个进程都有一份变量的拷贝，进程之间的操作互不影响，Process进程创建时，子进程会将主进程的Process对象完全复制一份，这样在主进程和子进程各有一个 Process对象，但是p.start()启动的是子进程，主进程中的Process对象作为一个静态对象存在，不执行。 就是这个地方，要注意的是，对于多进程来说，它是不会复制内存的，所以不用担心main中的变量被复制，但是它会复制main函数以外的变量 12345678910111213141516171819import multiprocessingimport timezero = 0def change_zero(): global zero for i in range(3): zero = zero + 1 print(multiprocessing.current_process().name, zero)if __name__ == '__main__': p1 = multiprocessing.Process(target = change_zero) p2 = multiprocessing.Process(target = change_zero) p1.start() p2.start() p1.join() p2.join() print(zero) 运行结果如下 1234567Process-1 1Process-1 2Process-1 3Process-2 1Process-2 2Process-2 30 共享变量Queue12345678910111213141516171819202122232425262728下面我们要讨论第一种情况，如果真的要在两个进程之间共享变量需要怎么办队列这里介绍进程之间的第一种交流方式——队列。multiprocessing模块中提供了multiprocessing.Queue，它和Queue.Queue的区别在于，它里面封装了进程之间的数据交流，不同进程可以操作同一个multiprocessing.Queue。from multiprocessing import Process, Queuedef addone(q): q.put(1)def addtwo(q): q.put(2)if __name__ == '__main__': q = Queue() p1 = Process(target=addone, args = (q, )) p2 = Process(target=addtwo, args = (q, )) p1.start() p2.start() p1.join() p2.join() print(q.get()) print(q.get())运行结果如下12这个队列是线程、进程安全的，即对队列的每一次修改中间不会被中断从而造成结果错误。 进程锁12345678进程锁既然变量在进程之间可以共享了，那么同时操作一个变量导致的不安全也随之出现。同多线程一样，进程也是通过锁来解决，而且使用方法都和多线程里相同。lock = multiprocessing.Lock()lock.acquire()lock.release()with lock:这些用法和功能都和多线程是一样的另外，multiprocessing.Semaphore Condition Event RLock也和多线程相同]]></content>
      <tags>
        <tag>多进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN]]></title>
    <url>%2F2019%2F04%2F17%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2FCNN%2F</url>
    <content type="text"><![CDATA[CNN学习 [TOC] CNN结构基础首先，提出这样一个问题：识别一幅图片是包含有字母”X”还是字母”O”？ 为了帮助指导你理解卷积神经网络，我们讲采用一个非常简化的例子：确定一幅图像是包含有”X”还是”O”？这个例子足够说明CNN背后的原理，同时它足够简单，能够避免陷入不必要的细节。 在CNN中有这样一个问题，就是每次给你一张图，你需要判断它是否含有”X”或者”O”。并且假设必须两者选其一，不是”X”就是”O”。理想的情况就像下面这个样子：标准的”X”和”O”，字母位于图像的正中央，并且比例合适，无变形 对于计算机来说，只要图像稍稍有一点变化，不是标准的，那么要解决这个问题还是不是那么容易的： 计算机要解决上面这个问题，一个比较天真的做法就是先保存一张”X”和”O”的标准图像（就像前面给出的例子），然后将其他的新给出的图像来和这两张标准图像进行对比，看看到底和哪一张图更匹配，就判断为哪个字母。 但是这么做的话，其实是非常不可靠的，因为计算机还是比较死板的。在计算机的“视觉”中，一幅图看起来就像是一个二维的像素数组（可以想象成一个棋盘），每一个位置对应一个数字。在我们这个例子当中，像素值”1”代表白色，像素值”-1”代表黑色。 当比较两幅图的时候，如果有任何一个像素值不匹配，那么这两幅图就不匹配，至少对于计算机来说是这样的。 对于这个例子，计算机认为上述两幅图中的白色像素除了中间的3*3的小方格里面是相同的，其他四个角上都不同： 因此，从表面上看，计算机判别右边那幅图不是”X”，两幅图不同，得出结论： 但是这么做，显得太不合理了。理想的情况下，我们希望，对于那些仅仅只是做了一些像平移，缩放，旋转，微变形等简单变换的图像，计算机仍然能够识别出图中的”X”和”O”。就像下面这些情况，我们希望计算机依然能够很快并且很准的识别出来： 这也就是CNN出现所要解决的问题。 Features 对于CNN来说，它是一块一块地来进行比对。它拿来比对的这个“小块”我们称之为Features（特征）。在两幅图中大致相同的位置找到一些粗糙的特征进行匹配，CNN能够更好的看到两幅图的相似性，相比起传统的整幅图逐一比对的方法。 每一个feature就像是一个小图（就是一个比较小的有值的二维数组）。不同的Feature匹配图像中不同的特征。在字母”X”的例子中，那些由对角线和交叉线组成的features基本上能够识别出大多数”X”所具有的重要特征。 这些features很有可能就是匹配任何含有字母”X”的图中字母X的四个角和它的中心。那么具体到底是怎么匹配的呢？如下： 看到这里是不是有了一点头目呢。但其实这只是第一步，你知道了这些Features是怎么在原图上面进行匹配的。但是你还不知道在这里面究竟进行的是怎样的数学计算，比如这个下面3*3的小块到底干了什么？接下来就跟进介绍里面的数学操作，也就是我们常说的“卷积”操作。 卷积(Convolution)Convolution 当给你一张新的图时，CNN并不能准确地知道这些features到底要匹配原图的哪些部分，所以它会在原图中每一个可能的位置进行尝试。这样在原始整幅图上每一个位置进行匹配计算，我们相当于把这个feature变成了一个过滤器。这个我们用来匹配的过程就被称为卷积操作，这也就是卷积神经网络名字的由来。 这个卷积操作背后的数学知识其实非常的简单。要计算一个feature和其在原图上对应的某一小块的结果，只需要简单地将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可。 如果两个像素点都是白色（也就是值均为1），那么11 = 1，如果均为黑色，那么(-1)(-1) = 1。不管哪种情况，每一对能够匹配上的像素，其相乘结果为1。类似地，任何不匹配的像素相乘结果为-1。如果一个feature（比如nn）内部所有的像素都和原图中对应一小块（nn）匹配上了，那么它们对应像素值相乘再累加就等于n2，然后除以像素点总个数n2，结果就是1。同理，如果每一个像素都不匹配，那么结果就是-1。 具体过程如下： 对于中间部分，也是一样的操作： 最后整张图算完，大概就像下面这个样子：然后换用其他feature进行同样的操作，最后得到的结果就是这样了：为了完成我们的卷积，我们不断地重复着上述过程，将feature和图中每一块进行卷积操作。最后通过每一个feature的卷积操作，我们会得到一个新的二维数组。 这也可以理解为对原始图像进行过滤的结果，我们称之为feature map，它是每一个feature从原始图像中提取出来的“特征”。其中的值，越接近为1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。这样我们的原始图，经过不同feature的卷积操作就变成了一系列的feature map。我们可以很方便，直观地将这整个操作视为一个单独的处理过程。在CNN中，我们称之为卷积层(convolution layer)，这样你可能很快就会想到后面肯定还有其他的layer。没错，后面会提到。 我们可以将卷积层看成下面这个样子：因此可想而知，CNN其实做的操作也没什么复杂的。但是尽管我们能够以这一点篇幅就描述了CNN的工作，其内部的加法，乘法和除法操作的次数其实会增加地很快。从数学的角度来说，它们会随着图像的大小，每一个filter的大小和filter的数目呈线性增长。由于有这么多因素的影响，很容易使得这个问题的计算量变得相当的庞大，这也难怪很多微处理器制造商现在都在生产制造专业的芯片来跟上CNN计算的需求。 池化(Pooling)Pooling CNN中使用的另一个有效的工具被称为“池化(Pooling)”。池化可以将一幅大的图像缩小，同时又保留其中的重要信息。池化背后的数学顶多也就是小学二年级水平。它就是将输入图像进行缩小，减少像素信息，只保留重要信息。通常情况下，池化都是22大小，比如对于max-pooling来说，就是取输入图像中22大小的块中的最大值，作为结果的像素值，相当于将原始图像缩小了4倍(注：同理，对于average-pooling来说，就是取2*2大小块的平均值作为结果的像素值)。 对于本文的这个例子，池化操作具体如下： 不足的外面补”0”： 经过最大池化操作（比如2*2大小）之后，一幅图就缩小为原来的四分之一了：然后对所有的feature map执行同样的操作，得到如下结果：因为最大池化（max-pooling）保留了每一个小块内的最大值，所以它相当于保留了这一块最佳的匹配结果（因为值越接近1表示匹配越好）。这也就意味着它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。这也就能够看出，CNN能够发现图像中是否具有某种特征，而不用在意到底在哪里具有这种特征。这也就能够帮助解决之前提到的计算机逐一像素匹配的死板做法。 当对所有的feature map执行池化操作之后，相当于一系列输入的大图变成了一系列小图。同样地，我们可以将这整个操作看作是一个操作，这也就是CNN中的池化层(pooling layer)，如下：通过加入池化层，可以很大程度上减少计算量，降低机器负载。 Normalization激活函数Relu (Rectified Linear Units)这是一个很小但是很重要的操作，叫做Relu(Rectified Linear Units)，或者修正线性单元。它的数学公式也很简单:f(x) = max(0, x) 对于输入的负值，输出全为0，对于正值，原样输出。关于其功能，更多详见这里。 下面我们看一下本文的离例子中relu激活函数具体操作： 最后，对整幅图操作之后，结果如下：同样地，在CNN中，我们这一系列操作视为一个操作，那么就得到Relu Layer，如下：Deep Learning最后，我们将上面所提到的卷积，池化，激活放在一起，就是下面这个样子：然后，我们加大网络的深度，增加更多的层，就得到深度神经网络了：然后在不同的层，我们进行可视化，就可以看到本文开头提到的先验知识里面的结果了：全连接层(Fully connected layers) 根据结果判定为”X”： 在这个过程中，我们定义这一系列操作为”全连接层“(Fully connected layers)：全连接层也能够有很多个，如下：【综合上述所有结构】 CNN三大核心思想卷积神经网络CNN的出现是为了解决MLP多层感知器全连接和梯度发散的问题。其引入三个核心思想：1.局部感知(local field)，2.权值共享(Shared Weights)，3.下采样(subsampling)。极大地提升了计算速度，减少了连接数量。 2.1 局部感知 形象地说，就是模仿你的眼睛，想想看，你在看东西的时候，目光是聚焦在一个相对很小的局部的吧？严格一些说，普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上，而在卷积神经网络中，每个隐层节点只连接到图像某个足够小局部的像素点上，从而大大减少需要训练的权值参数。 对于一个 1000∗1000 的输入图像而言，如果下一个隐藏层的神经元数目为 106 个，采用全连接则有 1000∗1000∗106=1012 个权值参数，如此数目巨大的参数几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中 10∗10的局部图像相连接，那么此时的权值参数数量为 10∗10∗106=108，将直接减少4个数量级。 如下图所示，左边是每个像素的全连接，右边是每行隔两个像素作为局部连接，因此在数量上，少了很多权值参数数量（每一条连接每一条线需要有一个权值参数，具体忘记了的可以回顾单个[神经元模型]。因此 局部感知就是：通过卷积操作，把 全连接变成局部连接 ，因为多层网络能够抽取高阶统计特性， 即使网络为局部连接，由于格外的突触连接和额外的神经交互作用，也可以使网络在不十分严格的意义下获得一个全局关系。 下面我们来详细看一下到底局部感知如何使 全连接变成局部连接，按照人工神经网络的方法，把输入图像的像素一字排开之后，每一个像素值就是一个神经元输入，需要对隐层或者输出层做全连接，如上图左侧所示。卷积神经网络引入卷积概念后，卷积核通过原图像，然后卷积核对原图像上符合卷积核大小的像素进行加权求和，每一次只是对符合卷积核的图像像素做卷积，这就是局部感知的概念，使 全连接变成局部连接。 2.2 权值共享 尽管局部感知使计算量减少了几个数量级，但权重参数数量依然很多。能不能再进一步减少呢？方法就是权值共享。 权值共享：不同的图像或者同一张图像共用一个卷积核，减少重复的卷积核。同一张图像当中可能会出现相同的特征，共享卷积核能够进一步减少权值参数。 如下图所示，为了找到鸟嘴，一个激活函数A需要检测图像左侧有没有鸟嘴，另外一个激活函数B需要检测另外一张图像中间有没有类似的鸟嘴。其实，鸟嘴都可能具有同样的特征，只需要一个激活函数C就可以了，这个时候，就可以共享同样的权值参数（也就是卷积核）。 如果使用了权值共享（共同使用一个卷积核），那么将可以大大减少卷积核的数量，加快运算速度。天下武功，唯快不破。 举个栗子，在局部连接中隐藏层的每一个神经元连接的是一个 10∗10 的局部图像，因此有 10∗10 个权值参数，将这 10∗10 个权值参数共享给剩下的神经元，也就是说隐藏层中 106 个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10∗10 个权值参数（也就是卷积核(也称滤波器)的大小。 尽管只有这么少的参数，依旧有出色的性能。但是，这样仅提取了图像的一种特征，如果要多提取出一些特征，需要增加多个卷积核，不同的卷积核能够得到图像的不同映射下的特征，称之为 Feature Map。如果有100个卷积核，最终的权值参数也仅为 100∗100=104 个而已。另外，偏置参数b也是共享的，同一种滤波器共享一个。 2.3 池化 在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。 pooling的好处有什么？\1. 这些统计特征能够有更低的维度，减少计算量。\2. 不容易过拟合，当参数过多的时候很容易造成过度拟合。\3. 缩小图像的规模，提升计算速度。 如下图所示，原图是一张500∗500 的图像，经过subsampling之后哦，变成了一张 250∗250 的图像。这样操作的好处非常明显，虽然经过权值共享和局部连接后的图像权值参数已经大大减少，但是对于计算量来说，还是非常巨大，需要消费很大的计算时间，于是为了进一步减少计算量，于是加入了subsampling这个概念，不仅仅使图像像素减少了， 同时也减少计算时间。 举个栗子：以最大池化（Max Pooling）为例，1000×1000的图像经过10×10的卷积核卷积后，得到的是991×991的特征图，然后使用2×2的池化规模，即每4个点组成的小方块中，取最大的一个作为输出，最终得到的是496×496大小的特征图。 下采样，即池化，目的是减小特征图，池化规模一般为2×2。常用的池化方法有： Pooling算法 最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。均值池化（Mean Pooling）。取4个点的均值。可训练池化。训练函数 f ，接受4个点为输入，出入1个点。 由于特征图的变长不一定是2的倍数，所以在边缘处理上也有两种方案： 保留边缘。将特征图的变长用0填充为2的倍数，然后再池化。忽略边缘。将多出来的边缘直接省去。]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>网络结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention]]></title>
    <url>%2F2019%2F04%2F15%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2FAttention%2F</url>
    <content type="text"><![CDATA[[TOC]Attention本质就是一系列注意力分配系数，也就是一系列权重参数罢了。 来源：https://www.cnblogs.com/guoyaohua/p/9429924.html 1. 什么是Attention机制？ 最近两年，注意力模型（Attention Model）被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，是深度学习技术中最值得关注与深入了解的核心技术之一。 当我们人在看一样东西的时候，我们当前时刻关注的一定是我们当前正在看的这样东西的某一地方，换句话说，当我们目光移到别处时，注意力随着目光的移动也在转移，这意味着，当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。————-（思考：对于图片，会有些特别显眼的场景会率先吸引住注意力，那是因为脑袋中对这类东西很敏感。对于文本，我们大都是带目的性的去读，顺序查找，顺序读，但是在理解的过程中，我们是根据我们自带的目的去理解，去关注的。 注意力模型应该与具体的目的(或者任务)相结合。） 从Attention的作用角度出发，我们就可以从两个角度来分类Attention种类：Spatial Attention 空间注意力和Temporal Attention 时间注意力。更具实际的应用，也可以将Attention分为Soft Attention和Hard Attention。Soft Attention是所有的数据都会注意，都会计算出相应的注意力权值，不会设置筛选条件。Hard Attention会在生成注意力权重后筛选掉一部分不符合条件的注意力，让它的注意力权值为0，即可以理解为不再注意这些不符合条件的部分。 2. 先了解编码-解码框架：Encoder-Decoder框架 目前绝大多数文献中出现的AM模型是附着在Encoder-Decoder框架下的，当然，其实AM模型可以看作一种通用的思想，本身并不依赖于Encoder-Decoder模型，这点需要注意。Encoder-Decoder框架可以看作是一种文本处理领域的研究模式，应用场景异常广泛，本身就值得细谈。 图1 抽象的Encoder-Decoder框架 Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对。 ————（思考：对很通用，X是一个问句，Y是答案；X是一个句子，Y是抽取的关系三元组；X是汉语句子，Y是汉语句子的英文翻译。等等），我们的目标是给定输入句子X，期待通过Encoder-Decoder框架来生成目标句子Y。X和Y可以是同一种语言，也可以是两种不同的语言。而X和Y分别由各自的单词序列构成： Encoder顾名思义就是对输入句子X进行编码，将输入句子通过非线性变换转化为中间语义表示C： 对于解码器Decoder来说，其任务是根据句子X的中间语义表示C和之前已经生成的历史信息y1,y2….yi-1来生成i时刻要生成的单词yi ： 每个yi都依次这么产生，那么看起来就是整个系统根据输入句子X生成了目标句子Y。 ———（思考：其实这里的Encoder-Decoder是一个序列到序列的模型seq2seq，这个模型是对顺序有依赖的。） Encoder-Decoder是个非常通用的计算框架，至于Encoder和Decoder具体使用什么模型都是由研究者自己定的，常见的比如 CNN / RNN / BiRNN / GRU / LSTM / Deep LSTM 等，这里的变化组合非常多。 （思考：人的学习过程包括输入、输出、外界评价。Encoder模型类似于人的输入学习过程，Decoder模型类似于人的输出学习过程，对输出的内容进行评价就类似于损失函数。英语老师给我上了几堂英语课，我在不断的输入Encoder；突然有一个随堂测试，我得做题输出Decoder；最后英语老师改卷子，给我一个分数，不对的地方我得反思调整我对输入数据的加工方式。） （再思考：关于英语翻译。课本上的单词和课文是原始数据输入，相当于X；我在大脑里加工这些数据，相当于Encoder模型，我的脑子里有很多加工后的数据，相当于C；现在要让我翻译一个英语句子，这个任务相当于Y，我不能翻课本，所以我只能借助我脑袋里加工的数据C去翻译这个句子，即我得动脑子，相当于Decoder。 学习的过程是什么都要学，要分类整理，要增加线索，并不知道未来的某天能用到什么，所以Encoder-Decoder是一个泛泛学习的框架） 3. Attention Model 以上介绍的Encoder-Decoder模型是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Y中每个单词的生成过程如下： 其中f是decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，是y1,y2也好，还是y3也好，他们使用的句子X的语义编码C都是一样的，没有任何区别。而语义编码C是由句子X的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实句子X中任意单词对生成某个目标单词yi来说影响力都是相同的，没有任何区别（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，估计这也是为何Google提出Sequence to Sequence模型时发现把输入句子逆序输入做翻译效果会更好的小Trick的原因）。这就是为何说这个模型没有体现出注意力的缘由。 引入AM模型，以翻译一个英语句子举例：输入X：Tom chase Jerry。 理想输出：汤姆追逐杰瑞。 应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值： （Tom,0.3）（Chase,0.2）（Jerry,0.5） 每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词Yi的时候，原先都是相同的中间语义表示C会替换成根据当前生成单词而不断变化的Ci。理解AM模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。 图2 引入AM模型的Encoder-Decoder框架 即生成目标句子单词的过程成了下面的形式： 而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下： 其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，也就是常常在论文里看到的下列公式： 假设Ci中那个i就是上面的“汤姆”，那么Tx就是3，代表输入句子的长度，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)，对应的注意力模型权值分别是0.6,0.2,0.2，所以g函数就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似下图： 图3 Ci的形成过程 这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，你怎么知道AM模型所需要的输入句子单词注意力分配概率分布值呢？就是说“汤姆”对应的概率分布： 划重点(注意力权重获取的过程)（Tom,0.3）（Chase,0.2）（Jerry,0.5）是如何得到的呢？ 为了便于说明，我们假设对图1的非AM模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图1的图转换为下图： 图4 RNN作为具体模型的Encoder-Decoder框架 注意力分配概率分布值的通用计算过程： 图5 AM注意力分配概率计算 对于采用RNN的Decoder来说，如果要生成 yi 单词，在时刻 i ，我们是可以知道在生成 Yi 之前的隐层节点i时刻的输出值 Hi 的，而我们的目的是要计算生成 Yi 时的输入句子单词“Tom”、“Chase”、“Jerry”对 Yi 来说的注意力分配概率分布，那么可以用i时刻的隐层节点状态 Hi 去一一和输入句子中每个单词对应的RNN隐层节点状态 hj 进行对比，即通过函数 F(hj,Hi) 来获得目标单词 Yi 和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值（这就得到了注意力权重）。图5显示的是当输出单词为“汤姆”时刻对应的输入句子单词的对齐概率。绝大多数AM模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。 上述内容就是论文里面常常提到的Soft Attention Model（任何数据都会给一个权值，没有筛选条件）的基本思想，你能在文献里面看到的大多数AM模型基本就是这个模型，区别很可能只是把这个模型用来解决不同的应用问题。那么怎么理解AM模型的物理含义呢？一般文献里会把AM模型看作是单词对齐模型，这是非常有道理的。目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。在其他应用里面把AM模型理解成输入句子和目标句子单词之间的对齐概率也是很顺畅的想法。 图6 Google 神经网络机器翻译系统结构图 图6所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。 当然，从概念上理解的话，把AM模型理解成影响力模型也是合理的，就是说生成目标单词的时候，输入句子每个单词对于生成这个单词有多大的影响程度。这种想法也是比较好理解AM模型物理意义的一种思维方式。 图7是论文“A Neural Attention Model for Sentence Summarization”中，Rush用AM模型来做生成式摘要给出的一个AM的一个非常直观的例子。 图7 句子生成式摘要例子 这个例子中，Encoder-Decoder框架的输入句子X是：“russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism”。对应图中纵坐标的句子。系统生成的摘要句子Y是：“russia calls for joint front against terrorism”，对应图中横坐标的句子。可以看出模型已经把句子主体部分正确地抽出来了。矩阵中每一列代表生成的目标单词对应输入句子每个单词的AM分配概率，颜色越深代表分配到的概率越大。这个例子对于直观理解AM是很有帮助作用。 《A Neural Attention Model for Sentence Summarization》论文提供的实验数据集链接(开放可用)：DUC 2004，感兴趣的朋友可以下载看看。 图8 摘要生成 开放数据集 4. Attention机制的本质思想 如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。 图9 Attention机制的本质思想 我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式： 其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。 当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。 从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。 至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。 图10 三阶段计算Attention过程 在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个 Keyi ，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式： 第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算： 第二阶段的计算结果 ai 即为 Valuei 对应的权重系数，然后进行加权求和即可得到Attention数值： 通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。 5. Self Attention模型 通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型(抛弃了传统的RNN)。 在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素和Source中的所有元素之间。而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。 elf Attention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention不同，它分别在source端和target端进行，仅与source input或者target input自身相关的Self Attention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self Attention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。因此，self Attention Attention比传统的Attention mechanism效果要好，主要原因之一是，传统的Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系 图11 可视化Self Attention实例 图12 可视化Self Attention实例 从两张图（图11、图12）可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。 很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。 但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。 ​ 但是attention无法记录词序，所以在self-attention中增加了position embedding 五种attention模型hard attention&amp;soft attention ■ 论文 | Show, Attend and Tell: Neural Image Caption Generation with Visual Attention ■ 链接 | https://www.paperweekly.site/papers/812 ■ 源码 | https://github.com/kelvinxu/arctic-captions 文章讨论的场景是图像描述生成（Image Caption Generation），对于这种场景，先放一张图，感受一下 attention 的框架。 文章提出了两种 attention 模式，即 hard attention 和 soft attention，来感受一下这两种 attention。 可以看到，hard attention 会专注于很小的区域，而 soft attention 的注意力相对发散。模型的 encoder 利用 CNN (VGG net)，提取出图像的 L 个 D 维的向量，每个向量表示图像的一部分信息。 decoder 是一个 LSTM，每个 timestep t 的输入包含三个部分，即 context vector Zt 、前一个 timestep 的 hidden state、前一个 timestep 的 output。 Zt 由 {ai} 和权重 {αti} 通过加权得到。这里的权重 αti 通过attention模型 fatt 来计算得到，而本文中的 fatt 是一个多层感知机（multilayer perceptron）。 从而可以计算。接下来文章重点讨论 hard（也叫 stochastic attention）和 soft（也叫 deterministic）两种 attention 模式。 1. Stochastic “Hard” Attention 记 St 为 decoder 第 t 个时刻的 attention 所关注的位置编号， Sti 表示第 t 时刻 attention 是否关注位置 i ， Sti 服从多元伯努利分布（multinoulli distribution）， 对于任意的 t ，Sti,i=1,2,…,L 中有且只有取 1，其余全部为 0，所以 [St1,St2,…,stL] 是 one-hot 形式。这种 attention 每次只 focus 一个位置的做法，就是“hard”称谓的来源。 Zt 也就被视为一个变量，计算如下： 问题是 αti 怎么算呢？把 αti 视为隐变量，研究模型的目标函数，进而研究目标函数对参数的梯度。直观理解，模型要根据 a=(a1,…,aL) 来生成序列 y=(y1,…,yC) ，所以目标可以是最大化 log p(y|a) ，但这里没有显式的包含 s ，所以作者利用著名的 Jensen 不等式（Jensen’s inequality）对目标函数做了转化，得到了目标函数的一个 lower bound，如下： 这里的 s ={ s1,…,sC }，是时间轴上的重点 focus 的序列，理论上这种序列共有个。 然后就用 log p(y|a) 代替原始的目标函数，对模型的参数 W 算 gradient。 然后利用蒙特卡洛方法对 s 进行抽样，我们做 N 次这样的抽样实验，记每次取到的序列是，易知的概率为，所以上面的求 gradient 的结果即为： 接下来的一些细节涉及reinforcement learning，感兴趣的同学可以去看这篇 paper。 2. Deterministic “Soft” Attention 说完“硬”的 attention，再来说说“软”的 attention。 相对来说 soft attention 很好理解，在 hard attention 里面，每个时刻 t 模型的序列 [ St1,…,StL ] 只有一个取 1，其余全部为 0，也就是说每次只 focus 一个位置，而 soft attention 每次会照顾到全部的位置，只是不同位置的权重不同罢了。这时 Zt 即为 ai 的加权求和： 这样 soft attention 是光滑的且可微的（即目标函数，也就是 LSTM 的目标函数对权重αti 是可微的，原因很简单，因为目标函数对 Zt 可微，而 Zt 对 αti 可微，根据 chain rule 可得目标函数对 αti 可微）。 文章还对这种 soft attention 做了微调： 其中，用来调节 context vector 在 LSTM 中的比重（相对于的比重）。 btw，模型的 loss function 加入了 αti 的正则项。 global attention &amp; local attention ■ 论文 | Effective Approaches to Attention-based Neural Machine Translation ■ 链接 | https://www.paperweekly.site/papers/806 ■ 源码 | https://github.com/lmthang/nmt.matlab 文章提出了两种 attention 的改进版本，即 global attention 和 local attention。先感受一下 global attention 和 local attention 长什么样子。 ▲ Global Attention ▲ Local Attention 文章指出，local attention 可以视为 hard attention 和 soft attention 的混合体（优势上的混合），因为它的计算复杂度要低于 global attention、soft attention，而且与 hard attention 不同的是，local attention 几乎处处可微，易与训练。 文章以机器翻译为场景， x1,…,xn 为 source sentence， y1,…,ym 为 target sentence， c1,…,cm 为 encoder 产生的 context vector，objective function 为： Ct 来源于 encoder 中多个 source position 所产生的 hidden states，global attention 和 local attention 的主要区别在于 attention 所 forcus 的 source positions 数目的不同：如果 attention forcus 全部的 position，则是 global attention，反之，若只 focus 一部分 position，则为 local attention。 由此可见，这里的 global attention、local attention 和 soft attention 并无本质上的区别，两篇 paper 模型的差别只是在 LSTM 结构上有微小的差别。 在 decoder 的时刻 t ，在利用 global attention 或 local attention 得到 context vector Ct之后，结合 ht ，对二者做 concatenate 操作，得到 attention hidden state。 最后利用 softmax 产出该时刻的输出： 下面重点介绍 global attention、local attention。 1. global attention global attention 在计算 context vector ct 的时候会考虑 encoder 所产生的全部hidden state。记 decoder 时刻 t 的 target hidden为 ht，encoder 的全部 hidden state 为，对于其中任意，其权重 αts 为： 而其中的，文章给出了四种种计算方法（文章称为 alignment function）： 四种方法都比较直观、简单。在得到这些权重后， ct 的计算是很自然的，即为的 weighted summation。 2. local attention global attention 可能的缺点在于每次都要扫描全部的 source hidden state，计算开销较大，对于长句翻译不利，为了提升效率，提出 local attention，每次只 focus 一小部分的 source position。 这里，context vector ct 的计算只 focus 窗口 [pt-D,pt+D] 内的 2D+1 个source hidden states（若发生越界，则忽略界外的 source hidden states）。 其中 pt 是一个 source position index，可以理解为 attention 的“焦点”，作为模型的参数， D 根据经验来选择（文章选用 10）。 关于 pt 的计算，文章给出了两种计算方案： Monotonic alignment (local-m) Predictive alignment (local-p) 其中 Wp 和 vp 是模型的参数， S 是 source sentence 的长度，易知 pt∈[0,S] 。 权重αt(s) 的计算如下： 可以看出，距离中心 pt 越远的位置，其位置上的 source hidden state 对应的权重就会被压缩地越厉害。 self-attention &amp; multiple-head attention ■ 论文 | Attention Is All You Need ■ 链接 | https://www.paperweekly.site/papers/224 ■ 源码 | https://github.com/Kyubyong/transformer ■ 论文 | Weighted Transformer Network for Machine Translation ■ 链接 | https://www.paperweekly.site/papers/2013 ■ 源码 | https://github.com/JayParks/transformer 作者首先指出，结合了 RNN（及其变体）和注意力机制的模型在序列建模领域取得了不错的成绩，但由于 RNN 的循环特性导致其不利于并行计算，所以模型的训练时间往往较长，在 GPU 上一个大一点的 seq2seq 模型通常要跑上几天，所以作者对 RNN 深恶痛绝，遂决定舍弃 RNN，只用注意力模型来进行序列的建模。 作者提出一种新型的网络结构，并起了个名字 Transformer，里面所包含的注意力机制称之为 self-attention。作者骄傲地宣称他这套 Transformer 是能够计算 input 和 output 的 representation 而不借助 RNN 的唯一的 model，所以作者说有 attention 就够了。 模型同样包含 encoder 和 decoder 两个 stage，encoder 和 decoder 都是抛弃 RNN，而是用堆叠起来的 self-attention，和 fully-connected layer 来完成，模型的架构如下： 从图中可以看出，模型共包含三个 attention 成分，分别是 encoder 的 self-attention，decoder 的 self-attention，以及连接 encoder 和 decoder 的 attention。 这三个 attention block 都是 multi-head attention 的形式，输入都是 query Q 、key K 、value V 三个元素，只是 Q 、 K 、 V 的取值不同罢了。接下来重点讨论最核心的模块 multi-head attention（多头注意力）。 multi-head attention 由多个 scaled dot-product attention 这样的基础单元经过 stack 而成。 那重点就变成 scaled dot-product attention 是什么鬼了。按字面意思理解，scaled dot-product attention 即缩放了的点乘注意力，我们来对它进行研究。 在这之前，我们先回顾一下上文提到的传统的 attention 方法（例如 global attention，score 采用 dot 形式）。 记 decoder 时刻 t 的 target hidden state 为 ht，encoder 得到的全部 source hidden state为，则 decoder 的 context vector ct 的计算过程如下： 作者先抛出三个名词 query Q、key K、value V，然后计算这三个元素的 attention。 我的写法与论文有细微差别，但为了接下来说明的简便，我姑且简化成这样。这个 Attention 的计算跟上面的 (*) 式有几分相似。 那么 Q、K、V 到底是什么？论文里讲的比较晦涩，说说我的理解。encoder 里的 attention 叫 self-attention，顾名思义，就是自己和自己做 attention。 抛开这篇论文的做法，让我们激活自己的创造力，在传统的 seq2seq 中的 encoder 阶段，我们得到 n 个时刻的 hidden states 之后，可以用每一时刻的 hidden state hi，去分别和任意的 hidden state hj,j=1,2,…,n 计算 attention，这就有点 self-attention 的意思。 回到当前的模型，由于抛弃了 RNN，encoder 过程就没了 hidden states，那拿什么做 self-attention 来自嗨呢？ 可以想到，假如作为 input 的 sequence 共有 n 个 word，那么我可以先对每一个 word 做 embedding 吧？就得到 n 个 embedding，然后我就可以用 embedding 代替 hidden state 来做 self-attention 了。所以 Q 这个矩阵里面装的就是全部的 word embedding，K、V 也是一样。 所以为什么管 Q 叫query？就是你每次拿一个 word embedding，去“查询”其和任意的 word embedding 的 match 程度（也就是 attention 的大小），你一共要做 n 轮这样的操作。 我们记 word embedding 的 dimension 为 dmodel ，所以 Q 的 shape 就是 n*dmodel， K、V 也是一样，第 i 个 word 的 embedding 为 vi，所以该 word 的 attention 应为： 那同时做全部 word 的 attention，则是： scaled dot-product attention 基本就是这样了。基于 RNN 的传统 encoder 在每个时刻会有输入和输出，而现在 encoder 由于抛弃了 RNN 序列模型，所以可以一下子把序列的全部内容输进去，来一次 self-attention 的自嗨。 理解了 scaled dot-product attention 之后，multi-head attention 就好理解了，因为就是 scaled dot-product attention 的 stacking。 先把 Q、K、V 做 linear transformation，然后对新生成的 Q’、K’、V’ 算 attention，重复这样的操作 h 次，然后把 h 次的结果做 concat，最后再做一次 linear transformation，就是 multi-head attention 这个小 block 的输出了。 以上介绍了 encoder 的 self-attention。decoder 中的 encoder-decoder attention 道理类似，可以理解为用 decoder 中的每个 vi 对 encoder 中的 vj 做一种交叉 attention。 decoder 中的 self-attention 也一样的道理，只是要注意一点，decoder 中你在用 vi 对 vj 做 attention 时，有一些 pair 是不合法的。原因在于，虽然 encoder 阶段你可以把序列的全部 word 一次全输入进去，但是 decoder 阶段却并不总是可以，想象一下你在做 inference，decoder 的产出还是按从左至右的顺序，所以你的 vi 是没机会和 vj ( j&gt;i ) 做 attention 的。 那怎么将这一点体现在 attention 的计算中呢？文中说只需要令 score(vi,vj)=-∞ 即可。为何？因为这样的话： 所以在计算 vi 的 self-attention 的时候，就能够把 vj 屏蔽掉。所以这个问题也就解决了。]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>网络结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM]]></title>
    <url>%2F2019%2F04%2F15%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2FLSTM%2F</url>
    <content type="text"><![CDATA[LSTM详解，把cell转变当作传送带的思想 来源：http://colah.github.io/posts/2015-08-Understanding-LSTMs/ [TOC] ‘递归神经网络​ 人类不会每一秒钟都从头开始思考。当你阅读这篇文章的时候，你理解每一个单词都是基于你对之前单词的理解。你不会把所有的东西都扔掉，重新开始思考。你的想法是有持久性的。​ 传统的神经网络做不到这一点，这似乎是一个主要的缺点。例如，假设您想要对电影中每一秒发生的事件进行分类。目前还不清楚传统的神经网络如何利用其对电影中先前事件的推理来为后来的事件提供信息。​ 递归神经网络解决了这个问题。它们是包含循环的网络，允许信息持续存在。 在上面的图中，一个神经网络块a，观察某个输入xt并输出一个值ht。循环允许信息从网络的一个步骤传递到下一个步骤。 ​ 这些循环使得递归神经网络看起来有点神秘。然而，如果你多想一下，就会发现它们和普通的神经网络并没有太大的不同。递归神经网络可以看作是同一网络的多个副本，每个副本都向后继网络传递一条消息。考虑一下如果我们展开循环会发生什么: 这种链式性质揭示了递归神经网络与序列和列表密切相关。它们是神经网络用来处理这些数据的自然结构。 ​ 它们确实被使用了!在过去的几年里，把RNNs应用到各种各样的问题上取得了令人难以置信的成功:语音识别、语言建模、翻译、图像字幕等等。我将把关于RNNs可以实现的惊人壮举的讨论留给Andrej Karpathy的优秀博客文章《循环神经网络的不合理有效性》。但它们真的很神奇。 ​ 这些成功的关键是使用“LSTMs”，这是一种非常特殊的递归神经网络，它在许多任务中都比标准版本运行得好得多。几乎所有基于递归神经网络的激动人心的结果都是用它们实现的。本文将探索这些LSTMs。 长期依赖的问题​ RNNs的一个吸引人的地方就是它们能够将以前的信息与现在的任务联系起来，例如使用以前的视频帧可能有助于理解现在的帧。如果RNN能做到这一点，它们将非常有用。但他们能吗?这可能需要视情况而定。 ​ 有时候，我们只需要查看最近的信息就可以执行当前的任务。例如，考虑一个语言模型，它试图根据前面的单词预测下一个单词。如果我们试图预测“云在天空中”中的最后一个单词，我们不需要任何进一步的上下文——很明显下一个单词将是天空。在这种情况下，相关信息和需要信息的地方之间的差距很小，RNNs可以学习使用过去的信息。 ​ 但也有一些情况，我们需要更多的上下文。试着预测文本中的最后一个单词“我在法国长大……我说一口流利的法语。”“最近的信息显示，下一个单词很可能是一种语言的名字，但如果我们想缩小范围，我们需要更早的法语语境。”相关信息与需要它的点之间的差距完全有可能变得非常大。不幸的是，随着这种差距的扩大，RNNs无法学会连接信息。 ​ 理论上，RNNs绝对有能力处理这种“长期依赖”。“一个人可以仔细地为他们选择参数来解决这种形式的玩具问题。遗憾的是，在实践中，RNNs似乎不能学习它们。 [Hochreiter (1991) German和Bengio, et al. (1994)对这个问题进行了深入研究，他们发现了一些非常基本的原因，解释了为什么这个问题可能很难。 LSTM网络​ Long Short Term Memory networks通常被称为“LSTMs”，是一种特殊的RNN，能够学习长期依赖关系。它们由Hochreiter &amp; Schmidhuber (1997)引入，并在随后的工作中被许多人提炼和推广。他们在各种各样的问题上都做得非常好，现在被广泛使用。 ​ LSTMs的设计是为了避免长期依赖问题。长时间记住信息实际上是他们的默认行为，而不是他们努力学习的东西! ​ 所有的递归神经网络都具有一串重复的神经网络模块的形式。在标准的RNNs中，这个重复模块有一个非常简单的结构，比如一个tanh层。 LSTMs也有类似链的结构，但是重复模块有不同的结构。不是只有一个神经网络层，而是有四个，以一种非常特殊的方式相互作用。 ​ 在上面的图中，每一条线都带有一个完整的向量，从一个节点的输出到其他节点的输入。粉色圆圈表示点化操作，比如向量加法，而黄色框表示神经网络学习层。箭头合并表示连接，而箭头分叉表示被正在被复制的内容以及复制到不同位置。 LSTM输入和输出https://www.zhihu.com/question/41949741 LSTM核心思想LSTMs的核心是 单元状态，即贯穿图顶部的水平线。 单元状态有点像传送带。它沿着整个链一直向下，只有一些很小的线性相互作用。信息很容易不加改变地沿着它流动。 LSTM有能力删除或添加信息到单元状态，并由称为门的结构小心地控制。 门是一种选择性地让信息通过的方法。它们由sigmoid神经网络层和点乘运算组成。. sigmoid层输出0到1之间的数字，描述每个组件应该通过的百分比。值0表示“不让任何东西通过”，而值1表示“让所有东西通过!”LSTM有三个这样的门，用来保护和控制单元状态。 LSTM深入剖析忘记阶段LSTM的第一步是决定要从单元格状态丢弃什么信息。这个决定是由一个叫做“忘记门”的sigmoid层做出的。“它查看ht-1和 xt，并为处于单元格状态 Ct-1的每个值输出一个介于0到1之间的数字。1表示“完全保留这个”，而0表示“完全删除这个”。 让我们回到语言模型的例子，该模型试图根据前面的所有单词预测下一个单词。在这样的问题中，单元格状态可能包括当前主语的词性，以便使用正确的代词。当我们看到一个新的主语时，我们想要忘记旧主语的词性。 选择记忆阶段下一步是决定要在单元格状态中存储哪些新信息。它有两部分。首先，一个名为“input gate layer”的sigmoid层决定要更新哪些值。接下来，tanh层创建一个新的候选值向量C ~t ，可以将其添加到状态中。在下一个步骤中，我们将把这两者结合起来对状态的更新。 在我们的语言模型示例中，我们希望将新主体的词性添加到单元格状态，以替换我们正在遗忘的旧主体。 现在是时候将旧的单元状态Ct - 1更新为新的单元状态Ct了。前面的步骤已经决定了要做什么，我们只需要实际去执行。 我们将旧状态乘以ft，忘记我们之前决定忘记的事情。然后我们将它添加到C ~t 中。这是新的候选值，按我们决定每个状态值的更新进行缩放。 在语言模型中，这是我们实际删除关于旧信息并添加新信息的地方，正如我们在前面的步骤中描述的那样。 输出阶段最后，我们需要决定输出什么。这个输出将基于我们的单元格状态，但是是经过过滤的版本。首先，我们运行一个sigmoid层，它决定要输出单元格状态的哪些部分。然后，我们将单元格状态放入tanh(将值缩放到- 1和1之间)，并将其乘以sigmoid层的输出，这样我们只输出我们决定输出的部分。 对于语言模型的例子，由于它只是看到了一个主语，所以它可能希望输出与动词相关的信息，以防接下来会发生什么。例如，它可以输出主语是单数还是复数，这样我们就知道如果主语是单数或复数，那么动词应该变成什么形式。 LSTM变形详情看来源网站。变形有很多，包括GRU，数十万种，Which of these variants is best? Do the differences matter? Greff, et al. (2015) do a nice comparison of popular variants, finding that they’re all about the same. 代码以及小例子]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>网络结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络迭代更新]]></title>
    <url>%2F2019%2F04%2F14%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%8E%87%2F</url>
    <content type="text"><![CDATA[神经网络学习率以及参数的更新 要注意区别，梯度下降计算的是参数，cyc计算的是学习率，自适应优化器学习率本身就会衰减，所以要注意两者是否冲突。 因为kernel跑不超过5epcho，所以其实学习率不会太影响，因为它更新也只能更新5次 [TOC] 来源:http://ruder.io/optimizing-gradient-descent/index.html#momentum 概述​ 调参就是指调学习率 ​ 学习速率是指导我们该如何通过损失函数的梯度调整网络权重的超参数。学习率越低，损失函数的变化速度就越慢。虽然使用低学习率可以确保我们不会错过任何局部极小值，但也意味着我们将花费更长的时间来进行收敛，特别是在被困在高原区域的情况下。 ​ 一般而言，用户可以利用过去的经验（或其他类型的学习资料）直观地设定学习率的最佳值。 因此，想得到最佳学习速率是很难做到的。下图演示了配置学习速率时可能遇到的不同情况。 对于太慢的学习速率来说，损失函数可能减小，但是按照非常浅薄的速率减小的。当进入了最优学习率区域，你将会观察到在损失函数上一次非常大的下降。进一步增加学习速率会造成损失函数值「跳来跳去」甚至在最低点附近发散。记住，最好的学习速率搭配着损失函数上最陡的下降，所以我们主要关注分析图的坡度。 Gradient descentmake a trade-off between the accuracy of the parameter update and the time it takes to perform an update. Batch gradient descent \theta = \theta - \eta \cdot \nabla_\theta J( \theta)123for i in range(nb_epochs): params_grad = evaluate_gradient(loss_function, data, params) params = params - learning_rate * params_grad 优点：保证批量梯度下降收敛于凸误差曲面的全局最小值和非凸曲面的局部最小值。 缺点：下降可能非常慢，而且要求每次计算整个epcho的数据，对内存要求比较高，而且它不允许在线更新。 Stochastic gradient descent \theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})注意每次都需要对数据进行shuffle打乱 12345for i in range(nb_epochs): np.random.shuffle(data) for example in data: params_grad = evaluate_gradient(loss_function, example, params) params = params - learning_rate * params_grad 优点：减少计算的冗余，由于每次只更新一次，速度快，对在线更新友好。 缺点：会拥有比较高的方差，会震荡比较明显，如果学习率不足够小的话可能会收敛于局部点。 SGD的学习率一般要配合退火，不断变小 Mini-batch gradient descent \theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})12345for i in range(nb_epochs): np.random.shuffle(data) for batch in get_batches(data, batch_size=50): params_grad = evaluate_gradient(loss_function, batch, params) params = params - learning_rate * params_grad 优点：减少方差 缺点：batch需要调节，在内存和速度上trade-off 总结简而言之，虽然有mini-batch，但是学习率仍然需要精心设计学习速率过小会导致收敛异常缓慢，而学习速率过大则会阻碍收敛，导致损失函数在最小值附近波动，甚至出现发散。 Adaptive OptimizersMomentum \begin{align} \begin{split} v_t &= \gamma v_{t-1} + \eta \nabla_\theta J( \theta) \\ \theta &= \theta - v_t \end{split} \end{align}要理解vt-1是上一次的更新，如果符号一致就会加速，如果符号不一致说明震荡，就会减速，这就是命名动量的意义，就像一个ball向下滚，它向下加速，向上却会受到空气阻力 优点：动量项对于梯度指向相同方向的维度增加，对于梯度改变方向的维度减少更新。因此，我们获得更快的收敛速度和减少振荡。 Nesterov accelerated gradient然而，一个从山上滚下来的球，盲目地跟着斜坡走，是非常令人不满意的。我们想要一个更聪明的球，一个知道它要去哪里的球，这样它就知道在山再次倾斜之前减速。 \begin{align} \begin{split} v_t &= \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} ) \\ \theta &= \theta - v_t \end{split} \end{align}跟上面Momentum公式的唯一区别在于，梯度不是根据当前参数位置，而是根据先走了本来计划要走的一步后，达到的参数位置计算出来的。 对于这个改动，很多文章给出的解释是，能够让算法提前看到前方的地形梯度，如果前面的梯度比当前位置的梯度大，那我就可以把步子迈得比原来大一些，如果前面的梯度比现在的梯度小，那我就可以把步子迈得小一些。这个大一些、小一些，都是相对于原来不看前方梯度、只看当前位置梯度的情况来说的。 有很复杂的推导过程:https://zhuanlan.zhihu.com/p/22810533 在原始形式中，Nesterov Accelerated Gradient（NAG）算法相对于Momentum的改进在于，以“向前看”看到的梯度而不是当前位置梯度去更新。经过变换之后的等效形式中，NAG算法相对于Momentum多了一个本次梯度相对上次梯度的变化量，这个变化量本质上是对目标函数二阶导的近似。由于利用了二阶导的信息，NAG算法才会比Momentum具有更快的收敛速度。 Adagrad针对不同的参数设置不同的学习率，根据频繁更新的学习率小，不频繁更新的学习率大。 For brevity, we use gtgt to denote the gradient at time step tt. gt,igt,i is then the partial derivative of the objective function w.r.t. to the parameter θiθi at time step tt: g_{t, i} = \nabla_\theta J( \theta_{t, i})The SGD update for every parameter θiθi at each time step tt then becomes: \theta_{t+1, i} = \theta_{t, i} - \eta \cdot g_{t, i}In its update rule, Adagrad modifies the general learning rate ηη at each time step tt for every parameter θiθi based on the past gradients that have been computed for θiθi: \theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}Gt∈Rd×d是一个对角矩阵，每个对角线位置i,i的值累加到t次迭代的对应参数 θi 梯度平方和。ϵ是平滑项，防止除零操作，一般取值1e−8。为什么分母要进行平方根的原因是去掉平方根操作算法的表现会大打折扣。 缺点：G_t会了累积越来越大，分母累积的和会越来越大。这样学习到后来的阶段，网络的更新能力会越来越弱，能学到的更多知识的能力也越来越弱 优点：因为有G_t的存在，所以n不用调，一般设0.01默认就可以了。 Adadelta修复上面的缺点，梯度和是递归的定义成历史梯度平方的衰减平均值。动态平均值E[g^2]_t仅仅取决于当前的梯度值与上一时刻的平均值.这样就不需要算累积的量 E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t \Delta \theta_t = - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t} \\ \Delta \theta_t = - \dfrac{\eta}{RMS[g]_{t}} g_t RMS[\Delta \theta]_{t} = \sqrt{E[\Delta \theta^2]_t + \epsilon}在此处Adadelta其实还是依赖于全局学习率的，但是作者做了一定处理，经过近似牛顿迭代法之后： \begin{align} \begin{split} \Delta \theta_t &= - \dfrac{RMS[\Delta \theta]_{t-1}}{RMS[g]_{t}} g_{t} \\ \theta_{t+1} &= \theta_t + \Delta \theta_t \end{split} \end{align}RMSprop相比于AdaGrad的历史梯度： RMSProp增加了一个衰减系数来控制历史信息的获取多少： 简单来讲，设置全局学习率之后，每次通过，全局学习率逐参数的除以经过衰减系数控制的历史梯度平方和的平方根，使得每个参数的学习率不同 那么它起到的作用是什么呢？ 起到的效果是在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。 Adam其实就是Momentum+RMSProp的结合，然后再修正其偏差 \begin{align} \begin{split} m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \end{split} \end{align}修正偏差： \begin{align} \begin{split} \hat{m}_t &= \dfrac{m_t}{1 - \beta^t_1} \\ \hat{v}_t &= \dfrac{v_t}{1 - \beta^t_2} \end{split} \end{align}计算梯度： \theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_tThe authors propose default values of 0.9 for β1, 0.999 for β2, and 10−8for ϵ. 1.Adams可能不收敛 文中各大优化算法的学习率：其中，SGD没有用到二阶动量，因此学习率是恒定的（实际使用过程中会采用学习率衰减策略，因此学习率递减）。AdaGrad的二阶动量不断累积，单调递增，因此学习率是单调递减的。因此，这两类算法会使得学习率不断递减，最终收敛到0，模型也得以收敛。 但AdaDelta和Adam则不然。二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得 可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。 2.Adams可能错失全局最优解 ​ 吐槽Adam最狠的 The Marginal Value of Adaptive Gradient Methods in Machine Learning 。文中说到，同样的一个优化问题，不同的优化算法可能会找到不同的答案，但自适应学习率的算法往往找到非常差的答案。他们通过一个特定的数据例子说明，自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。 总结那么，现在应该使用哪个优化器呢?如果您的输入数据是稀疏的，那么您可能使用自适应学习率方法之一获得最佳结果。另一个好处是，您不需要调优学习率，但是可以使用缺省值获得最佳结果。 总之，RMSprop是Adagrad的一个扩展，它处理的是学习速度的急剧下降。它与Adadelta相同，只是Adadelta在numinator update规则中使用参数更新的RMS。最后，Adam为RMSprop添加了偏差修正和动量。到目前为止，RMSprop、Adadelta和Adam都是非常相似的算法，它们在相似的环境中都表现得很好。Kingma等人[14:1]表明，当梯度变得更稀疏时，它的偏倚校正帮助Adam在优化的最后略微优于RMSprop。到目前为止，Adam可能是最好的选择。 有趣的是，许多最近的论文使用SGD和一个简单的学习速率退火时间表。正如所示，SGD通常能够找到最小值，但是它可能比一些优化器花费的时间要长得多，更依赖于健壮的初始化和退火调度，并且可能会陷入鞍点而不是局部极小值。因此，如果你关心快速收敛和训练一个深度或复杂的神经网络，你应该选择一种自适应学习速率方法。 Additional strategiesShuffling and Curriculum Learning​ 通常，我们希望避免以有意义的顺序为模型提供训练示例，因为这可能会影响优化算法。因此，在每个epoch之后重新整理训练数据通常是一个好主意。 ​ 另一方面，在某些情况下，我们的目标是逐步解决更难的问题，按有意义的顺序提供训练示例实际上可能会提高性能和更好的收敛性。建立这种有意义顺序的方法称为课程学习 Batch normalization​ 为了便于学习，我们通常用零均值和单位方差初始化参数的初始值，从而对参数的初始值进行标准化。随着训练的进展，我们在不同程度上更新参数，我们失去了这种标准化，这减慢了训练的速度，并随着网络变得更深而放大了变化。 ​ 批处理规范化[27]为每个小批处理重新建立这些规范化，并通过操作反向传播更改。通过将标准化作为模型体系结构的一部分，我们可以使用更高的学习率，而不太关注初始化参数。批处理规范化还可以作为一个正则化器，减少(有时甚至消除)退出的需要。 Early stopping​ 你应该观察验证集上的误差，并且停止如果它没有足够的提高了。 newest algorithnsCyclical Learning Rates两个特点： 它为我们提供了一种在训练过程中有效地控制学习率的方法，方法是在上下界之间以三角形的方式改变学习率 它为我们提供了一个非常不错的估计，即哪种学习率的范围适合您的特定网络。 There are a number of parameters to play around with here: step size: during how many epochs will the LR go up from the lower bound, up to the upper bound. max_lr: the highest LR in the schedule. base_lr: the lowest LR in the schedule, in practice: the author of the paper suggests to take this a factor R smaller than the max_lr. Our used factor was 6. 它的核心思想:这个学习率策略的本质来自于一个观察，增加学习率会有短暂的负面影响，但是长远来看有好处。这个观察启发了我们的想法，让学习率在一个范围内变化，而不是用常值或指数递减啥的。所以只需要设置上下界和周期变化就可以了。大量的实验尝试了各种形式，triangular window (linear), a Welch window (parabolic) and a Hann window (sinusoidal)，他们的结果差不多。就采用triangular窗吧 代码实现 Step 1: find the upper LR define an initial learning rate, the lower boundary of the range you want to test (let’s say 1e-7) define an upper boundary of the range (let’s say 0.1) define an exponential scheme to run through this step by step: 12lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (lr_find_epochs * len( dataloaders["train"])))scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda) Next, do a run (I used two epochs) through your network. At each step (each batch size): capture the LR, capture the loss and optimize the gradients: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# Create the modelmodel = CNN().to(device)optimizer = torch.optim.SGD(model.parameters(), start_lr)criterion = nn.CrossEntropyLoss()# Make lists to capture the logslr_find_loss = []lr_find_lr = []iter = 0smoothing = 0.05for i in range(lr_find_epochs): print("epoch &#123;&#125;".format(i)) for inputs, labels in dataloaders["train"]: # Send to device inputs = inputs.to(device) labels = labels.to(device) # Training mode and zero gradients model.train() optimizer.zero_grad() # Get outputs to calc loss outputs = model(inputs) loss = criterion(outputs, labels) # Backward pass loss.backward() optimizer.step() # Update LR scheduler.step() lr_step = optimizer.state_dict()["param_groups"][0]["lr"] lr_find_lr.append(lr_step) # smooth the loss if iter==0: lr_find_loss.append(loss) else: loss = smoothing * loss + (1 - smoothing) * lr_find_loss[-1] lr_find_loss.append(loss) iter += 1 观察图，确定一个范围。根据fast.ai的课程描述，一个好的上界不是在最低点，而是左移10倍左右，一个好的下界是上界，除以一个因子6 Step 2: CLR scheduler123456789101112131415def cyclical_lr(stepsize, min_lr=3e-2, max_lr=3e-3): # Scaler: we can adapt this if we do not want the triangular CLR scaler = lambda x: 1. # Lambda function to calculate the LR lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize) # Additional function to see where on the cycle we are def relative(it, stepsize): cycle = math.floor(1 + it / (2 * stepsize)) x = abs(it / stepsize - 2 * cycle + 1) return max(0, (1 - x)) * scaler(cycle) return lr_lambda Step 3: wrap it1234567model = CNN().to(device)criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=1.)step_size = 4*len(train_loader)clr = cyclical_lr(step_size, min_lr=end_lr/factor, max_lr=end_lr)scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr]) Step 4: train1234567891011121314151617for i, (images, labels) in enumerate(train_loader): images, labels = images.to(device), labels.to(device) #Clear the gradients optimizer.zero_grad() #Forward propagation outputs = model(images) #Calculating loss with softmax to obtain cross entropy loss loss = criterion(outputs, labels) #Backward propation loss.backward() scheduler.step() # &gt; Where the magic happens #Updating gradients optimizer.step() AdamWhttps://github.com/mpyrozhok/adamwr https://zhuanlan.zhihu.com/p/52084949 所以在 2018 年，你应该做什么来代替 3e-4 Adam 工作流程呢？有很多东西需要考虑，如批量大小、动量等。但是，更好的工作流程将是： 使用 LR Range Test 找到最佳学习率，并完整地检查当前模型和数据。 始终使用学习率调度器，该调度器会改变上一步中找到的学习率，可以是 CLR 或 Restart。 如果需要 Adam，请使用具有适当权值衰减的 AdamW，而不是当前流行框架中使用的默认权值衰减。 如果想实现超收敛，可以进一步尝试一周期策略。]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>学习率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN]]></title>
    <url>%2F2019%2F04%2F14%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2FRNN%2F</url>
    <content type="text"><![CDATA[RNN快速学习 [TOC] 一、从单层网络谈起在学习RNN之前，首先要了解一下最基本的单层网络，它的结构如图： 输入是x，经过变换Wx+b和激活函数f得到输出y。相信大家对这个已经非常熟悉了。 二、经典的RNN结构（N vs N）在实际应用中，我们还会遇到很多序列形的数据： 如： 自然语言处理问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推。 语音处理。此时，x1、x2、x3……是每帧的声音信号。 时间序列问题。例如每天的股票价格等等 序列形的数据就不太好用原始的神经网络处理了。为了建模序列问题，RNN引入了隐状态h（hidden state）的概念，h可以对序列形的数据提取特征，接着再转换为输出。先从h1的计算开始看： 图示中记号的含义是： 圆圈或方块表示的是向量。 一个箭头就表示对该向量做一次变换。如上图中h0和x1分别有一个箭头连接，就表示对h0和x1各做了一次变换。 在很多论文中也会出现类似的记号，初学的时候很容易搞乱，但只要把握住以上两点，就可以比较轻松地理解图示背后的含义。 h2的计算和h1类似。要注意的是，在计算时，每一步使用的参数U、W、b都是一样的，也就是说每个步骤的参数都是共享的，这是RNN的重要特点，一定要牢记。 依次计算剩下来的（使用相同的参数U、W、b）： 我们这里为了方便起见，只画出序列长度为4的情况，实际上，这个计算过程可以无限地持续下去。 我们目前的RNN还没有输出，得到输出值的方法就是直接通过h进行计算： 正如之前所说，一个箭头就表示对对应的向量做一次类似于f(Wx+b)的变换，这里的这个箭头就表示对h1进行一次变换，得到输出y1。 剩下的输出类似进行（使用和y1同样的参数V和c）： OK！大功告成！这就是最经典的RNN结构，我们像搭积木一样把它搭好了。它的输入是x1, x2, …..xn，输出为y1, y2, …yn，也就是说，输入和输出序列必须要是等长的。 由于这个限制的存在，经典RNN的适用范围比较小，但也有一些问题适合用经典的RNN结构建模，如： 计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。 输入为字符，输出为下一个字符的概率。这就是著名的Char RNN（详细介绍请参考：The Unreasonable Effectiveness of Recurrent Neural Networks，Char RNN可以用来生成文章，诗歌，甚至是代码，非常有意思）。 三、N VS 1有的时候，我们要处理的问题输入是一个序列，输出是一个单独的值而不是序列，应该怎样建模呢？实际上，我们只在最后一个h上进行输出变换就可以了： 这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。 四、1 VS N输入不是序列而输出为序列的情况怎么处理？我们可以只在序列开始进行输入计算： 还有一种结构是把输入信息X作为每个阶段的输入： 下图省略了一些X的圆圈，是一个等价表示： 这种1 VS N的结构可以处理的问题有： 从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子 从类别生成语音或音乐等 五、N vs M下面我们来介绍RNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。 原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。 为此，Encoder-Decoder结构先将输入数据编码成一个上下文向量c： 得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。 拿到c之后，就用另一个RNN网络对其进行解码，这部分RNN网络被称为Decoder。具体做法就是将c当做之前的初始状态h0输入到Decoder中： 还有一种做法是将c当做每一步的输入： 由于这种Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如： 机器翻译。Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的 文本摘要。输入是一段文本序列，输出是这段文本序列的摘要序列。 阅读理解。将输入的文章和问题分别编码，再对其进行解码得到问题的答案。 语音识别。输入是语音信号序列，输出是文字序列。 六、Attention机制在Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征c再解码，因此， c中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个c可能存不下那么多信息，就会造成翻译精度的下降。 Attention机制通过在每个时间输入不同的c来解决这个问题，下图是带有Attention机制的Decoder： 每一个c会自动去选取与当前所要输出的y最合适的上下文信息。具体来说，我们用 衡量Encoder中第j阶段的hj和解码时第i阶段的相关性，最终Decoder中第i阶段的输入的上下文信息 就来自于所有 对 的加权和。 以机器翻译为例（将中文翻译成英文）： 输入的序列是“我爱中国”，因此，Encoder中的h1、h2、h3、h4就可以分别看做是“我”、“爱”、“中”、“国”所代表的信息。在翻译成英语时，第一个上下文c1应该和“我”这个字最相关，因此对应的 就比较大，而相应的 、 、 就比较小。c2应该和“爱”最相关，因此对应的 就比较大。最后的c3和h3、h4最相关，因此 、 的值就比较大。 至此，关于Attention模型，我们就只剩最后一个问题了，那就是：这些权重 是怎么来的？ 事实上， 同样是从模型中学出的，它实际和Decoder的第i-1阶段的隐状态、Encoder第j个阶段的隐状态有关。 同样还是拿上面的机器翻译举例， 的计算（此时箭头就表示对h’和 同时做变换）： 的计算： 的计算： 以上就是带有Attention的Encoder-Decoder模型计算的全过程。 七、总结本文主要讲了N vs N，N vs 1、1 vs N、N vs M四种经典的RNN模型，以及如何使用Attention结构。希望能对大家有所帮助。 可能有小伙伴发现没有LSTM的内容，其实是因为LSTM从外部看和RNN完全一样，因此上面的所有结构对LSTM都是通用的，想了解LSTM内部结构的可以参考这篇文章：Understanding LSTM Networks，写得非常好，推荐阅读。 所以RNN不是一定是seqtoseq的模型，其一般图为：]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>网络结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch学习]]></title>
    <url>%2F2019%2F04%2F14%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2Fpytorch%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[pytorch学习日记 [TOC] APItorch.utils.data.Dataset并行化读取数据len：代表样本数量。len(obj)等价于obj.len()。getitem：返回一条数据或一个样本。obj[index]等价于obj.getitem。 建议将节奏的图片等高负载的操作放到这里，因为多进程时会并行调用这个函数，这样做可以加速。dataset中应尽量只包含只读对象，避免修改任何可变对象。因为如果使用多进程，可变对象要加锁，但后面讲到的dataloader的设计使其难以加锁。如下面例子中的self.num可能在多进程下出问题 12345678910111213class CustomDataset(data.Dataset):#需要继承data.Dataset def __init__(self): # TODO # 1. Initialize file path or list of file. # 初始化变量 pass def __getitem__(self, index): # TODO # 需要返回batch中的data，即返回index下标的数值 pass def __len__(self): # the total size of your dataset. return 0 示例 12345678910111213141516171819202122232425262728293031323334353637383940414243class MNIST(data.Dataset): def __init__(self, root, train=True, transform=None, target_transform=None, download=False): self.root = root self.transform = transform self.target_transform = target_transform self.train = train # training set or test set if download: self.download() if not self._check_exists(): raise RuntimeError('Dataset not found.' + ' You can use download=True to download it') if self.train: self.train_data, self.train_labels = torch.load( os.path.join(root, self.processed_folder, self.training_file)) else: self.test_data, self.test_labels = torch.load(os.path.join(root, self.processed_folder, self.test_file)) def __getitem__(self, index): if self.train: img, target = self.train_data[index], self.train_labels[index] else: img, target = self.test_data[index], self.test_labels[index] # doing this so that it is consistent with all other datasets # to return a PIL Image img = Image.fromarray(img.numpy(), mode='L') if self.transform is not None: img = self.transform(img) if self.target_transform is not None: target = self.target_transform(target) return img, target def __len__(self): if self.train: return 60000 else: return 10000 torch.utils.data.Dataloadtorch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None)[SOURCE] Data loader. Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.dataset：加载的数据集(Dataset对象)batch_size：batch sizeshuffle:：是否将数据打乱sampler： 样本抽样，后续会详细介绍num_workers：使用多进程加载的进程数，0代表不使用多进程collate_fn： 如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃 重点在于collate_fn 是不是看着有点头大，没有关系，我们先搞清楚他的输入是什么。这里可以看到他的输入被命名为batch，但是我们还是不知道到底是什么，可以猜测应该是一个batch size的数据。我们继续往后找，可以找到这个地方。 Paste_Image.png 我们可以从这里看到collate_fn在这里进行了调用，那么他的输入我们就找到了，从这里看这就是一个list，list中的每个元素就是self.data[i]，如果你在往上看，可以看到这个self.data就是我们需要预先定义的Dataset，那么这里self.data[i]就等价于我们在Dataset里面定义的__getitem__这个函数。 所以我们知道了collate_fn这个函数的输入就是一个list，list的长度是一个batch size，list中的每个元素都是__getitem__得到的结果。 注意如果dataset里面返回的不是一个值而是元组的话：[tuple]，这时候就需要用texts, lens, y = zip(*batch)来解开 squeeze&amp;unsqueeze 首先初始化一个a 可以看出a的维度为（2，3） 在第二维增加一个维度，使其维度变为（2，1，3）可以看出a的维度已经变为（2，1，3）了，同样如果需要在倒数第二个维度上增加一个维度，那么使用b.unsqueeze(-2)二、squeeze()函数介绍 首先得到一个维度为（1，2，3）的tensor（张量）由图中可以看出c的维度为（1，2，3）2.下面使用squeeze()函数将第一维去掉可见，维度已经变为（2，3）3.另外可以看出维度并没有变化，仍然为（1，2，3），这是因为只有维度为1时才会去掉。 pack_padded_sequence&amp;pad_packed_sequence踩坑： 要注意pad回来的会自动补0序列的，注意这些0序列，会对后续的操作产生影响 要注意0是补在后面，不是补在前面，这一点坑了我两天 动态padding会快，但是注意pack和pad会额外需要耗时 注意预测的时候要reverse回来，不然没法做stacking RNN处理变长序列参考的的代码：https://zhuanlan.zhihu.com/p/40391002 pytorch 处理变长序列过程中各个tensor变化情况：https://blog.csdn.net/qq_27505047/article/details/78764888 kaggle比赛代码：https://www.kaggle.com/johnfarrell/plasticc-2018-emb-gru torch.nn.utils.rnn.pack_padded_sequence() 这里的pack，理解成压紧比较好。 将一个 填充过的变长序列 压紧。（填充时候，会有冗余，所以压紧一下） 其中pack的过程为：（注意pack的形式，不是按行压，而是按列压） （下面方框内为PackedSequence对象，由data和batch_sizes组成） 输入的形状可以是(T×B× )。T是最长序列长度，B是batch size，`代表任意维度(可以是0)。如果batch_first=True的话，那么相应的input size就是(B×T×*)`。 读PyTorch源码学习RNN（1） pack之后得到的每个batch就是一个timestep,也就是说timestep的时候取都是1的状态(a1和b1)，它实际上就是告诉Model为每个序列构建不同数量的状态。 pack和pad要统一是否使用batch_first=True，否则转换维度会出问题 torch.gather() &amp; torch.sactter_()https://blog.csdn.net/Teeyohuang/article/details/82186666 问题解答专区如何自定义forward函数只需要继承nn.Module，然后重写forward函数，注意调用时机为该模块名字 module(参数)，这里得参数跟forward得参数一致 如何自定义LOSS函数https://www.zhihu.com/question/66988664 如何打印网络和参数结构https://github.com/nmhkahn/torchsummaryX 如何释放显存12gc.collect()torch.cuda.empty_cache() 正确的更新参数方式12345epcho:scheduler.step() batch: total_loss.backward() optimizer_G.step() loss.backward()在前，然后跟一个step。 那么为什么optimizer.step()需要放在每一个batch训练中，而不是epoch训练中，这是因为现在的mini-batch训练模式是假定每一个训练集就只有mini-batch这样大，因此实际上可以将每一次mini-batch看做是一次训练，一次训练更新一次参数空间，因而optimizer.step()放在这里。 scheduler.step（）按照Pytorch的定义是用来更新优化器的学习率的，一般是按照epoch为单位进行更换，即多少个epoch后更换一次学习率，因而scheduler.step()放在epoch这个大循环下。 如何打印loss变化情况 用数组存储所有loss和acc和lr然后就可以打印了]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试集和训练集分布]]></title>
    <url>%2F2019%2F04%2F13%2F%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C%2F%E6%B5%8B%E8%AF%95%E9%9B%86%E5%92%8C%E8%AE%AD%E7%BB%83%E9%9B%86%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[Train &amp; Test分布主要是为了看数据的分布情况。 下面的方法是最近kaggle上GM搞的一种极其简单的探索训练集&amp;测试集分布的方案,将一个分布的问题变成一个二分类问题,该方法简单有效，如果AUC低于0.6,则我们可以认为训练集和测试集是分布平衡的,反之我们则可以认为训练集和测试集是分布不一致的,这个时候就需要注意AB榜是否会出现翻车的情况等 [TOC] 来源：https://www.kaggle.com/justfor/adversarial-validation-plus-ks-test Adversarial Validation下面的方法是最近kaggle上GM搞的一种极其简单的探索训练集&amp;测试集分布的方案,将一个分布的问题变成一个二分类问题,该方法简单有效，如果AUC低于0.6,则我们可以认为训练集和测试集是分布平衡的,反之我们则可以认为训练集和测试集是分布不一致的,这个时候就需要注意AB榜是否会出现翻车的情况等 核心点在于将y1 = np.array([0] x train.shape[0])y2 = np.array([1] x test.shape[0]) 1234567891011121314151617181920212223242526#Create label array and complete datasety1 = np.array([0]*train.shape[0])y2 = np.array([1]*test.shape[0])y = np.concatenate((y1, y2))X_data = pd.concat([train, test])X_data.reset_index(drop=True, inplace=True)#Initialize splits&amp;LGBMskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)lgb_model = lgb.LGBMClassifier(max_depth=-1, n_estimators=500, learning_rate=0.01, objective='binary', n_jobs=-1) counter = 1#Train 5-fold adversarial validation classifierfor train_index, test_index in skf.split(X_data, y): print('\nFold &#123;&#125;'.format(counter)) X_fit, X_val = X_data.loc[train_index], X_data.loc[test_index] y_fit, y_val = y[train_index], y[test_index] lgb_model.fit(X_fit, y_fit, eval_metric='auc', eval_set=[(X_val, y_val)], verbose=100, early_stopping_rounds=10) counter+=1 Kolmogorov-Smirnov Test单独看每个变量是否能过通过KS检验，不仅有图，而且有一种量化的手段 123456789101112131415161718192021222324252627#Load more packagesfrom scipy.stats import ks_2sampimport matplotlib.pyplot as pltimport seaborn as snssns.set_style('whitegrid')import warningswarnings.simplefilter(action='ignore', category=FutureWarning)warnings.filterwarnings('ignore')#Perform KS-Test for each feature from train/test. Draw its distribution. Count features based on statistics.#Plots are hidden. If you'd like to look at them - press "Output" button.hypothesisnotrejected = []hypothesisrejected = []for col in train.columns: statistic, pvalue = ks_2samp(train[col], test[col]) if pvalue&gt;=statistic: hypothesisnotrejected.append(col) if pvalue&lt;statistic: hypothesisrejected.append(col) plt.figure(figsize=(8,4)) plt.title("Kolmogorov-Smirnov test for train/test\n" "feature: &#123;&#125;, statistics: &#123;:.5f&#125;, pvalue: &#123;:5f&#125;".format(col, statistic, pvalue)) sns.kdeplot(train[col], color='blue', shade=True, label='Train') sns.kdeplot(test[col], color='green', shade=True, label='Test') plt.show()]]></content>
      <categories>
        <category>竞赛经验</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比赛规范经验]]></title>
    <url>%2F2019%2F04%2F12%2F%E6%AF%94%E8%B5%9B%E8%A7%84%E8%8C%83%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[比赛中的代码规范,代码不规范，亲人两行泪 先杂七杂八写，有一定量了再整理 思路要记在云笔记里，纸上会丢，而且换环境忘记带 临近比赛前一个月注意多保存看过的kernel，因为它们做出来之后就会删除。也就是说快结束的时候，那些都不是重要的magic 要保持一个纯洁统一模型的代码的单独文件，才方便接入 积累的代码，要注意按照代码规范记录 注意每类特征的解耦存放，这样对后面的迁移都回非常有用 模型的文件名就应该带上分数，版本不是最重要的，分数的复现才是它的唯一价值 用好excel多个记录]]></content>
      <categories>
        <category>竞赛经验</category>
      </categories>
      <tags>
        <tag>代码规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征构造]]></title>
    <url>%2F2019%2F04%2F11%2F%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%2F%E7%89%B9%E5%BE%81%E6%9E%84%E9%80%A0%2F</url>
    <content type="text"><![CDATA[[TOC] 思路 一阶特征 在绝大多数推荐类别的比赛场景中，如果仅仅是为了取得不错的成绩，有一个比较简单的特征挖掘思路：越细粒度的特征越重要，时间越近的特征越重要，未来的特征简直无敌。我们在进行特征组合的时候，其实也是在将特征的粒度变细。比如aid只有173维，age有3维，特征组合之后变成了1733，这个时候粒度就变细了。粒度越细，越容易定位到某一个单独的用户。粒度细了才能代表一个用户.*特征的处理，其实是一个就能不能够还原特征 排序特征：对于同一维的特征的纵向排序，得到新维特征注意排序需要归一化，除以总长度，虽然我没看懂为什么需要归一化(注意mean排序，max排序，对分组后的统计特征进行排序) 时差特征：计算各种交互中距离上次的时长和距离下次的时长 时序特征：前24小时的均值，前1，2，3，7，30，60，360天的均值 倒数特征：统计计算这是第几次交互和倒数第几次交互，因为越新越重要 离散特征：等值均分，等量均分,根据排序特征可以等量划分，不要自己随便划分可能会好一点 计数特征：横向统计离散特征中1-10出现的次数，得到一个新的10维计数特征 类别特征编码：逻辑回归和线性回归只能处理数值型的特征？普遍用one-hot 分组统计特征：各类统计值，不过前提是要找到一个groupby的跳板才可以用 分特征序列统计特征：根据特征向量计算其各类统计值 递减统计特征：做4个X的mean，3个X的mean，2个X的mean这样递减 转化率：用k折，就是k-1份构造字典，然后去看第k份的转化率。也就是构造出两个字典第一个是total记录每个值出现的次数，第二个是pos记录每个值对应y为1出现的次数构造测试集的时候再使用训练全集来看 https://zhuanlan.zhihu.com/p/38341881 整数/小数特征：将一个特征的整数和小数单独拆开。（注意小数这里要乘于倍数让他变回整数） 位数特征：可能需要观察一下，数字本身在某一位上是否存在特征 差值特征：让有前后关系进行相减 异常特征：根据value_count看出数据的分布，设置0/1变量，如/法或者%取模法 映射特征：具体查看通用代码包内 相对特征：减掉均值，来表示水平差距 group编码特征：对组合变量进行labelencoder（这里使用的是ngroup） 新旧特征：旧数据集标为1，新数据集为0 rate特征：只要看到有sum可能就要用rate isduplicated：因为需要做去重的操作，然后drop还是要照常drop,因为drop只是去重，保留了第一条，所以is_duplicate还是有用的，表示它是否属于重复的数据，true/False 新的特征 “days_to_side” 表示当天距离月初月末的最短距离 xgb训练生成新叶子节点特征（通用代码包），一般都是xgb+LR虽然我不知道能不能用xgb+xgb（不能） 定义月初月中月末定义 1df["day"]=df["day"].apply(lambda x:0 if x&lt;=7 else 2 if x&gt;=24 else 1) 二阶特征 交叉特征：二阶特征不是单纯的笛卡尔积，而是考虑两个对象之间的关系，这两个对象要有一定意义上的业务关系，差值，比例等等；三阶组合特征，考虑时间 组合特征：主要是在于特征表达，特征的思考不是如何考虑因变量，而是如何还原该用户的特征，所以有时候可能不用过于去考虑最后的评分公式·AXB的特征的构造，是要离散型和离散型才可以，然后成独热（如果他们是有相关的，AXB就已经是足够的）当然，这种原始特征，一般在比赛中，我们都会添加。同样对一些人人都想到的特征，比如什么用户特征和广告特征的组合特征之类的，其实是不需要看什么分布的，直接扔进去gbdt看一波重要性分析，然后分析一下皮尔森系数，淘汰一下不重要的特征和相关程度高的特征，其实基本就完事了。 参考代码： ijcj比赛的特征 转化率的特征 代码函数统一返回新的df，同时删除原有df 特征频数1234567# 构造特征时测试集要和训练集一样def encode_FE(df,col,test)： cv = df[col].value_counts() nm = col+'_FE' df[nm] = df[col].map(cv) test[nm] = test[col].map(cv) return df,test data augment123456789101112131415161718192021222324252627def augment(x,y,t=2): xs,xn = [],[] for i in range(t): mask = y&gt;0 x1 = x[mask].copy() ids = np.arange(x1.shape[0]) for c in range(x1.shape[1]): np.random.shuffle(ids) x1[:,c] = x1[ids][:,c] xs.append(x1) for i in range(t//2): mask = y==0 x1 = x[mask].copy() ids = np.arange(x1.shape[0]) for c in range(x1.shape[1]): np.random.shuffle(ids) x1[:,c] = x1[ids][:,c] xn.append(x1) xs = np.vstack(xs) xn = np.vstack(xn) ys = np.ones(xs.shape[0]) yn = np.zeros(xn.shape[0]) x = np.vstack([x,xs,xn]) y = np.concatenate([y,ys,yn]) return x,y]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Santander Customer Transaction Prediction 比赛经验]]></title>
    <url>%2F2019%2F04%2F11%2F%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C%2FSantander%20Customer%20Transaction%20Prediction%20%E6%AF%94%E8%B5%9B%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[Santander Customer Transaction Prediction 比赛，01分类问题，200维连续型匿名变量。 讲述了magic操作，count的新特征构造，特征相关性。。。 [TOC] 来源：https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920 magic kernel树模型的缺点无法学习到的东西 LGBM用竖线划分直方图，因为LGBM看不到水平差异。一个直方图会将多个值放置在一个Bin中并且产生一个较为平滑的图。如果你把多个值放在一个bin中，你会得到一个锯齿的图，其中每个bin中有些值是惟一的，有些值出现了几十次，LGBM无法学习到这些事情。 如上，构造的count图可以看出在相同值的附近存在不同频数的区别。 对特征敏感的参数设置​ 仅仅做到上述构造特征还是没有用的，因为你添加新特征到LGBM的时候设置参数feature_fraction=0.05，这会导致特征被随机的采样，破坏了var_1和var_1count之间的依赖关系(目的就是让模型需要同时学习横向和纵向)。所以设置feature_fraction=1，能从0.901到0.910，但是如果要到0.920则需要剔除原始变量之间的spurious effects,因为原始特征对模型敏感，同时相关系数普遍很低，则说明它们虽然有相关但是没有因果关联。 Use Data Augmentation (as shown in Jiwei’s awesome kernel here). You must keep original and new feature in same row. Use 200 separate models as shown in this kernel below. Merge new feature and original feature into one feature. In original data, simply add 200 to each unique value. (And don’t add new columns) 使用数据增强，就是随机打乱特征的时候，保持count和var是一致的，保证count和var的相关性，又去除了var之间相关性 单独使用count和var预测，然后200个模型再融合(一共200个var），这与第三点的merge我觉得是一致的，将两列合成一列（单纯加减应该是不够的) ​ 注意计算频数的时候，原则是同分布下越多数据越好。所以train和test在不在一起取决于它们的分布是否一致，或者如何让它们分布一致。在该比赛中就对test集进行了划分，剔除了和train不一致的数据，再合在一起做频数 代码主要参考链接 first kernel来源：https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920 magic feature不单单是用count ,而是更深入的使用count，告诉模型它所不知道的事情。比如说Unique in train 和test This value appears at least another time in data with target==1 and no 0; This value appears at least another time in data with target==0 and no 1; This value appears at least two more time in data with target==0 &amp; 1; This value is unique in data; This value is unique in data + test (only including real test samples); The other 200 (one per raw feature) features are numerical, let’s call them “not unique feat”, and correspond to the raw feature replacing values that are unique in data + test with the mean of the feature. 用该列的均值去替换该列的Unique值，使其成为非unique的列，这个操作是尝试出来的，它们也用了nan，median magic insight两个重要的节点： I looked at my LGBM trees (with only 3 leafs that’s easy to do) and noticed the trees were using the uniqueness information.通过树画图，看出树当前不能学习的东西 number of different values in train and test was not the same。 虽然数值上分布是一致的，但是在值的个数上不一致 technial part:匿名变量用NN会更好 NN的concat可以更好的处理变量之间的关系 augment magic using the shuffle augmentation (duplicate and shuffle 16 times samples with target == 1, 4 for target ==0) and added pseudo label (2700 highest predicted test points as 1 and 2000 lowest as 0) pseudo label指从测试集中取最高的为1最低的为0，加进去 shuffle augmentation 指数据增强(eda代码中有)]]></content>
      <categories>
        <category>竞赛经验</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比赛模型]]></title>
    <url>%2F2019%2F04%2F10%2F%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%2F%E6%AF%94%E8%B5%9B%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[比赛通用模型代码 [TOC] 载入模型前操作归一化1234from sklearn.preprocessing import MinMaxScaler,StandardScalersc = StandardScaler()train_features = sc.fit_transform(train_features)test_features = sc.transform(test_features) 切分训练集和验证集12345678910111213141516171819# train_splitfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)# K-fold# scikit-learn k-fold cross-validationfrom sklearn.model_selection import KFold# data sample# prepare cross validationkfold = KFold(n_splits=3, shuffle = True, random_state= 1)# enumerate splitsfor train, test in kfold.split(data): print('train: %s, test: %s' % (data[train], data[test])) from sklearn.model_selection import StratifiedKFold# StratifiedKFoldfolds = StratifiedKFold(n_splits=num_folds, shuffle=False, random_state=2319)for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)): X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx] X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx] 模型http://www.cnblogs.com/jasonfreak/p/5720137.html 很重要的调参 模型汇总LightGBM Catboost Xgboost H2oRF H2oGBM. xgb回归123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn########################################################################### 回归from xgboost.sklearn import XGBRegressorfrom sklearn.metrics import mean_absolute_errorfrom sklearn.model_selection import KFold, StratifiedKFold,GroupKFoldimport os# 别人的自定义损失函数,在parameter里面：object里面赋值def custom_loss(y_true,y_pred): penalty=2.0 grad=-y_true/y_pred+penalty*(1-y_true)/(1-y_pred) #梯度 hess=y_true/(y_pred**2)+penalty*(1-y_true)/(1-y_pred)**2 #2阶导 return grad,hess# 自定义评价函数def mse(y_pred,dtrain): #preds是结果（概率值），dtrain是个带label的DMatrix labels=dtrain.get_label() #提取label t=mean_absolute_error(labels, y_pred) print(t) return 'mse',tparameters = &#123;'nthread':-1, # cpu 线程数 默认最大 'objective':'reg:linear',#多分类or 回归的问题 若要自定义就替换为custom_loss（不带引号） 'learning_rate': .01, #so called `eta` value 如同学习率 'max_depth': 6,# 构建树的深度，越大越容易过拟合 'min_child_weight': 4,# 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 'silent': 1,#设置成1则没有运行信息输出，最好是设置为0. 'subsample': 0.7, # 随机采样训练样本 'colsample_bytree': 0.7,# 生成树时进行的列采样 'n_estimators': 100,# 树的个数跟num_boost_round是一样的，所以可以设置无限大，靠early_stop 'gamma':0.1,# 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。 'seed':1000 #随机种子 #'alpha':0, # L1 正则项参数 #'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。 #'num_class':10, # 类别数，多分类与 multisoftmax 并用 &#125;def mergeToOne( X, X2): X3 = [] for i in range(X.shape[0]): tmp = np.array([list(X[i]), list(X2[i])]) X3.append(list(np.hstack(tmp))) X3 = np.array(X3) return X3def get_XgbRegressor(train_data,train_target,test_data,feature_names,parameters,early_stopping_rounds,num_folds,eval_metric,model_name='model',stratified=False): ''' :param train_data: 一定是numpy :param train_target: :param parameters: :param round: :param k: :param eval_metrics:自定义 or 内置字符串 :return: ''' reg=XGBRegressor() reg.set_params(**parameters) # 定义一些变量 oof_preds = np.zeros((train_data.shape[0],)) sub_preds = np.zeros((test_data.shape[0],)) feature_importance_df = pd.DataFrame() cv_result = [] # K-flod if stratified: folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1234) else: folds = KFold(n_splits= num_folds, shuffle=True, random_state=1234) X_train_newfeature=np.zeros((1,1)) for n_flod, (train_index, val_index) in enumerate(folds.split(train_data, train_target)): train_X=train_data[train_index] val_X=train_data[val_index] train_Y=train_target[train_index] val_Y=train_target[val_index] # 参数初步定之后划分20%为验证集，准备一个watchlist 给train和validation set ,设置num_round 足够大（比如100000），以至于你能发现每一个round 的验证集预测结果， # 如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了。 watchlist= [(train_X, train_Y), (val_X, val_Y)] # early_stop 看validate的eval是否下降，这时候必须传eval_set,并取eval_set的最后一个作为validate reg.fit(train_X,train_Y,early_stopping_rounds=early_stopping_rounds, eval_set=watchlist,eval_metric=eval_metric) ## 生成gbdt新特征 new_feature = reg.apply(val_X) if X_train_newfeature.shape[0]==1: X_train_newfeature=mergeToOne(val_X,new_feature) else: X_train_newfeature = mergeToOne(val_X,new_feature) X_train_newfeature=np.concatenate((X_train_newfeature,mergeToOne(new_feature, val_X)),axis=0) print (X_train_newfeature) # 获得每次的预测值补充 oof_preds[val_index]=reg.predict(val_X) # 获得预测的平均值，这里直接加完再除m sub_preds+= reg.predict(test_data) result = mean_absolute_error(val_Y, reg.predict(val_X)) print('Fold %2d macro-f1 : %.6f' % (n_flod + 1, result)) cv_result.append(round(result,5)) gc.collect() # 默认就是gain 如果要修改要再参数定义中修改importance_type # 保存特征重要度 gain = reg.feature_importances_ fold_importance_df = pd.DataFrame(&#123;'feature': feature_names, 'gain': 100 * gain / gain.sum(), 'fold': n_flod, &#125;).sort_values('gain', ascending=False) feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0) # 进行保存 sub_preds=sub_preds/folds.n_splits new_feature=reg.apply(test_data) X_test_newfeature = mergeToOne(test_data, new_feature) if not os.path.isdir('./sub'): os.makedirs('./sub') pd.DataFrame(oof_preds,columns=['class']).to_csv('./sub/val_&#123;&#125;.csv'.format(model_name), index=False) pd.DataFrame(sub_preds, columns=['class']).to_csv('./sub/test_&#123;&#125;.csv'.format(model_name), index=False) print('cv_result', cv_result) if not os.path.isdir('./gbdt_newfeature'): os.makedirs('./gbdt_newfeature') np.save("./gbdt_newfeature/train_newfeature.npy", X_train_newfeature) np.save("./gbdt_newfeature/test_newfeature.npy", X_test_newfeature) save_importances(feature_importance_df, model_name) return reg,sub_predsdef save_importances(feature_importance_df,model_name): if not os.path.isdir('./feature_importance'): os.makedirs('./feature_importance') ft = feature_importance_df[["feature", "gain"]].groupby("feature").mean().sort_values(by="gain",ascending=False) ft.to_csv('./feature_importance/importance_lightgbm_&#123;&#125;.csv'.format(model_name), index=True) xgb分类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114########################################################################################## 分类from sklearn.model_selection import train_test_splitfrom sklearn import metricsfrom sklearn.datasets import make_hastie_10_2from xgboost.sklearn import XGBClassifierimport numpy as npimport osimport gcclf_parameters = &#123;'nthread':-1, # cpu 线程数 默认最大 'objective':'multi:softmax',#多分类or 回归的问题 若要自定义就替换为custom_loss（不带引号） 'learning_rate': .01, #so called `eta` value 如同学习率 'max_depth': 6,# 构建树的深度，越大越容易过拟合 'min_child_weight': 4,# 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 'silent': 1,#设置成1则没有运行信息输出，最好是设置为0. 'subsample': 0.7, # 随机采样训练样本 'colsample_bytree': 0.7,# 生成树时进行的列采样 'n_estimators': 500,# 树的个数跟num_boost_round是一样的，所以可以设置无限大，靠early_stop 'gamma':0.1,# 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。 'seed':1000 #随机种子 #'alpha':0, # L1 正则项参数 #'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。 &#125;n_class=3def clf_custom_loss(y_true,y_pred): penalty=2.0 grad=-y_true/y_pred+penalty*(1-y_true)/(1-y_pred) #梯度 hess=y_true/(y_pred**2)+penalty*(1-y_true)/(1-y_pred)**2 #2阶导 return grad,hess# 自定义评价函数def clf_mse(y_pred,dtrain): #preds是结果（概率值），dtrain是个带label的DMatrix labels=dtrain.get_label() #提取label ######### 分类预测的都是概率哦，所以这里要取一个max类别 y_pred = np.argmax(y_pred.reshape(n_class, -1), axis=0) score=mean_absolute_error(labels, y_pred) return 'mse',score# 分类的时候要注意！！！！！！！！# k-flod的时候要按层次拿出来，有一个shuffler我这里就没实现了，否则预测的类别会出现变小甚至报错def get_XgbClassifer(train_data,train_target,test_data,feature_names,parameters,early_stopping_rounds,num_folds,eval_metric,model_name='model',stratified=True): ''' :param train_data: 一定是numpy :param train_target: :param parameters: :param round: :param k: :param eval_metrics:自定义 or 内置字符串 :return: ''' # 如果在param中设置，会莫名报参数不存在的错误 clf=XGBClassifier(num_class=n_class) clf.set_params(**parameters) # 定义一些变量 oof_preds = np.zeros((train_data.shape[0],n_class)) sub_preds = np.zeros((test_data.shape[0],n_class)) feature_importance_df = pd.DataFrame() cv_result = [] # K-flod if stratified: folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1234) else: folds = KFold(n_splits= num_folds, shuffle=True, random_state=1234) for n_flod,(train_index, val_index) in enumerate(folds.split(train_data,train_target)): train_X=train_data[train_index] val_X=train_data[val_index] train_Y=train_target[train_index] val_Y=train_target[val_index] # 参数初步定之后划分20%为验证集，准备一个watchlist 给train和validation set ,设置num_round 足够大（比如100000），以至于你能发现每一个round 的验证集预测结果， # 如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了。 watchlist= [(train_X, train_Y)] # early_stop 看validate的eval是否下降，这时候必须传eval_set,并取eval_set的最后一个作为validate clf.fit(train_X,train_Y,early_stopping_rounds=early_stopping_rounds, eval_set=watchlist,eval_metric=eval_metric) # 获得每次的预测值补充 oof_preds[val_index]=clf.predict_proba(val_X) # 获得预测的平均值，这里直接加完再除m sub_preds+= clf.predict_proba(test_data) # 计算当前准确率 result=mean_absolute_error(val_Y,clf.predict(val_X)) print('Fold %2d macro-f1 : %.6f' % (n_flod + 1, result)) print(type(result)) cv_result.append(round(result,5)) gc.collect() # 默认就是gain 如果要修改要再参数定义中修改importance_type # 保存特征重要度 gain = clf.feature_importances_ fold_importance_df = pd.DataFrame(&#123;'feature':feature_names, 'gain':100*gain/gain.sum(), 'fold':n_flod, &#125;).sort_values('gain',ascending=False) feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0) # 进行保存 sub_preds=sub_preds/folds.n_splits if not os.path.isdir('./cv'): os.makedirs('./cv') pd.DataFrame(oof_preds,columns=['class_'+ str(i) for i in range(n_class)]).to_csv('./cv/val_prob_&#123;&#125;.csv'.format(model_name), index= False, float_format = '%.4f') pd.DataFrame(sub_preds, columns=['class_' + str(i) for i in range(n_class)]).to_csv('./cv/test_prob_&#123;&#125;.csv'.format(model_name), index=False, float_format='%.4f') oof_preds = [np.argmax(x) for x in oof_preds] sub_preds = [np.argmax(x) for x in sub_preds] if not os.path.isdir('./sub'): os.makedirs('./sub') pd.DataFrame(oof_preds,columns=['class']).to_csv('./sub/val_&#123;&#125;.csv'.format(model_name), index=False) pd.DataFrame(sub_preds, columns=['class']).to_csv('./sub/test_&#123;&#125;.csv'.format(model_name), index=False) save_importances(feature_importance_df, model_name) return clfdef save_importances(feature_importance_df,model_name): if not os.path.isdir('./feature_importance'): os.makedirs('./feature_importance') ft = feature_importance_df[["feature", "gain"]].groupby("feature").mean().sort_values(by="gain",ascending=False) ft.to_csv('./feature_importance/importance_lightgbm_&#123;&#125;.csv'.format(model_name), index=True) lgb回归注意Lgb对分类变量会有特殊的支持 只用看参数和pythonAPI http://lightgbm.apachecn.org/ https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters 1234567891011121314151617181920212223242526272829303132333435363738394041param = &#123; 'bagging_freq': 5, 'bagging_fraction': 0.335, 'boost_from_average':'false', 'boost': 'gbdt', 'feature_fraction': 0.041, 'learning_rate': 0.0083, 'max_depth': -1, 'metric':'auc', 'min_data_in_leaf': 80, 'min_sum_hessian_in_leaf': 10.0, 'num_leaves': 13, 'num_threads': 8, 'tree_learner': 'serial', 'objective': 'binary', 'verbosity': -1&#125;num_folds = 11features = [c for c in train.columns if c not in ['ID_code', 'target']]folds = KFold(n_splits=num_folds, random_state=2319)oof = np.zeros(len(train))getVal = np.zeros(len(train))predictions = np.zeros(len(target))feature_importance_df = pd.DataFrame()print('Light GBM Model')for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)): X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx] X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx] print("Fold idx:&#123;&#125;".format(fold_ + 1)) trn_data = lgb.Dataset(X_train, label=y_train) val_data = lgb.Dataset(X_valid, label=y_valid) clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000) oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) fold_importance_df = pd.DataFrame() fold_importance_df["feature"] = features fold_importance_df["importance"] = clf.feature_importance() fold_importance_df["fold"] = fold_ + 1 feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0) predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splitsprint("CV score: &#123;:&lt;8.5f&#125;".format(roc_auc_score(target, oof))) lgb分类回归和分类一致，只是参数不一样而已 注意Lgb对分类变量会有特殊的支持 只用看参数和pythonAPI http://lightgbm.apachecn.org/ https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters Naive Bayes比较常用的手工概率blending https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899 GBDT系数说明参考http://www.cnblogs.com/jasonfreak/p/5720137.htmlGradientBoostingClassifier支持二进制和多类分类 123456789101112131415161718192021222324252627282930313233343536373839from sklearn.datasets import make_hastie_10_2from sklearn.ensemble import GradientBoostingClassifierX, y = make_hastie_10_2(random_state=0)X_train, X_test = X[:2000], X[2000:]y_train, y_test = y[:2000], y[2000:]clf = GradientBoostingClassifier(loss='deviance', ##损失函数默认deviance deviance具有概率输出的分类的偏差n_estimators=100, ##默认100 回归树个数 弱学习器个数learning_rate=0.1, ##默认0.1学习速率/步长0.0-1.0的超参数 每个树学习前一个树的残差的步长max_depth=3, ## 默认值为3每个回归树的深度 控制树的大小 也可用叶节点的数量max leaf nodes控制subsample=1, ##树生成时对样本采样 选择子样本&lt;1.0导致方差的减少和偏差的增加min_samples_split=2, ##生成子节点所需的最小样本数 如果是浮点数代表是百分比min_samples_leaf=1, ##叶节点所需的最小样本数 如果是浮点数代表是百分比max_features=None, ##在寻找最佳分割点要考虑的特征数量auto全选/sqrt开方/log2对数/None全选/int自定义几个/float百分比max_leaf_nodes=None, ##叶节点的数量 None不限数量min_impurity_split=1e-7, ##停止分裂叶子节点的阈值verbose=0, ##打印输出 大于1打印每棵树的进度和性能warm_start=False, ##True在前面基础上增量训练(重设参数减少训练次数) False默认擦除重新训练random_state=0 ##随机种子-方便重现).fit(X_train, y_train) ##多类别回归建议使用随机森林print clf.score(X_test, y_test) ##tp / (tp + fp)正实例占所有正实例的比例test_y= clf.predict(X_test)test_y= clf.predict_proba(X_test)[:,1] ##预测概率print clf.feature_importances_ ##输出特征重要性print clf.train_score_ ##每次迭代后分数##test_y= clf.predict(X_test)##from sklearn.metrics import precision_score##precision_score(test_y, y_test,average='micro') ##tp / (tp + fp)##from sklearn import metrics##fpr, tpr, thresholds = metrics.roc_curve(y_test, test_y)##print("auc : %.4g" % metrics.auc(fpr, tpr)y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,1] ##预测概率from sklearn import metricsfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pro)print("auc : %.4g" % metrics.auc(fpr, tpr),x%10000/100,x%100) #auc表示一print "AUC Score (Train): %f" % metrics.roc_auc_score(y_test, y_pro) #auc表示二 两种方式等价print"Accuracy : %.4g" % metrics.accuracy_score(y_test, y_pre) ##等价于clf.score(X_test, y_test) sklearn.ensemble.GradientBoostingRegressor 123456789101112131415161718192021222324import numpy as npfrom sklearn.metrics import mean_squared_errorfrom sklearn.datasets import make_friedman1from sklearn.ensemble import GradientBoostingRegressorX, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)X_train, X_test = X[:200], X[200:]y_train, y_test = y[:200], y[200:]est = GradientBoostingRegressor(loss='ls', ##默认ls损失函数'ls'是指最小二乘回归lad'（最小绝对偏差）'huber'是两者的组合n_estimators=100, ##默认100 回归树个数 弱学习器个数learning_rate=0.1, ##默认0.1学习速率/步长0.0-1.0的超参数 每个树学习前一个树的残差的步长max_depth=3, ## 默认值为3每个回归树的深度 控制树的大小 也可用叶节点的数量max leaf nodes控制subsample=1, ##用于拟合个别基础学习器的样本分数 选择子样本&lt;1.0导致方差的减少和偏差的增加min_samples_split=2, ##生成子节点所需的最小样本数 如果是浮点数代表是百分比min_samples_leaf=1, ##叶节点所需的最小样本数 如果是浮点数代表是百分比max_features=None, ##在寻找最佳分割点要考虑的特征数量auto全选/sqrt开方/log2对数/None全选/int自定义几个/float百分比max_leaf_nodes=None, ##叶节点的数量 None不限数量min_impurity_split=1e-7, ##停止分裂叶子节点的阈值verbose=0, ##打印输出 大于1打印每棵树的进度和性能warm_start=False, ##True在前面基础上增量训练 False默认擦除重新训练 增加树random_state=0 ##随机种子-方便重现).fit(X_train, y_train)mean_squared_error(y_test, est.predict(X_test)) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import numpy as npfrom sklearn import ensemblefrom sklearn import datasetsfrom sklearn.utils import shufflefrom sklearn.metrics import mean_squared_errorfrom sklearn.metrics import r2_scoreboston = datasets.load_boston()X, y = shuffle(boston.data, boston.target, random_state=13) #抽取X = X.astype(np.float32)offset = int(X.shape[0] * 0.9) #设置取0.9做样本X_train, y_train = X[:offset], y[:offset]X_test, y_test = X[offset:], y[offset:]##参数可以放入一个字典当中params = &#123;'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2, 'learning_rate': 0.01, 'loss': 'ls'&#125;clf = ensemble.GradientBoostingRegressor(**params)clf.fit(X_train, y_train)mse = mean_squared_error(y_test, clf.predict(X_test))r2 = r2_score(y_test, clf.predict(X_test))print("MSE: %.4f" % mse) ##输出均方误差print("r^2 on test data : %f" % r2) ##R^2 拟合优度=(预测值-均值)^2之和/(真实值-均值)^2之和##绘图查看import matplotlib.pyplot as plttest_score = np.zeros((params['n_estimators'],), dtype=np.float64)##计算每次迭代分数变化for i, y_pred in enumerate(clf.staged_predict(X_test)): test_score[i] = clf.loss_(y_test, y_pred)plt.figure(figsize=(12, 6))plt.subplot(1, 2, 1)plt.title('Deviance')plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-', label='Training Set Deviance')plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-', label='Test Set Deviance')plt.legend(loc='upper right')plt.xlabel('Boosting Iterations')plt.ylabel('Deviance')##输出特征重要性feature_importance = clf.feature_importances_# make importances relative to max importancefeature_importance = 100.0 * (feature_importance / feature_importance.max())sorted_idx = np.argsort(feature_importance) ##返回的是数组值从小到大的索引值pos = np.arange(sorted_idx.shape[0]) + .5plt.subplot(1, 2, 2)plt.barh(pos, feature_importance[sorted_idx], align='center')plt.yticks(pos, boston.feature_names[sorted_idx])plt.xlabel('Relative Importance')plt.title('Variable Importance')plt.show() 调整参数贝叶斯优化来源:https://www.cnblogs.com/yangruiGB2312/p/9374377.html 可以说是目前最好的调参的方法 贝叶斯调参采用高斯过程，考虑之前的参数信息，不断地更新先验；网格搜索未考虑之前的参数信息 贝叶斯调参迭代次数少，速度快；网格搜索速度慢,参数多时易导致维度爆炸 贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部优最 ​ 公式推导就不在这里展开，主要阐述主要思想：给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。 ​ 假设这个寻找最优化参数的过程是一个高斯过程。高斯过程有个特点，就是当随机遍历一定的数据点并拿到结果之后，可以大致绘制出整个数据的分布曲线，当然这是也会有一定误差。 python代码 1234567891011121314151617181920from bayes_opt import BayesianOptimizationdef rf_cv(n_estimators, min_samples_split, max_features, max_depth): val = cross_val_score( RandomForestClassifier(n_estimators=int(n_estimators), min_samples_split=int(min_samples_split), max_features=min(max_features, 0.999), # float max_depth=int(max_depth), random_state=2 ), x, y, scoring='roc_auc', cv=5 ).mean() return val# 注意参数名字要对应rf_bo = BayesianOptimization( rf_cv, &#123;'n_estimators': (10, 250), 'min_samples_split': (2, 25), 'max_features': (0.1, 0.999), 'max_depth': (5, 15)&#125; ) 可以执行的操作 以查看当前最优的参数和结果(同时，我们还可以修改高斯过程的参数，高斯过程主要参数是核函数(kernel)，还有其他参数可以参考sklearn.gaussianprocess)： 123gp_param=&#123;'kernel':None&#125;rf_bo.maximize(**gp_param)rf_bo.res['max'] 上面bayes算法得到的参数并不一定最优，当然我们会遇到一种情况，就是我们已经知道有一组或是几组参数是非常好的了，我们想知道其附近有没有更好的。 1234567rf_bo.explore( &#123;'n_estimators': [10, 100, 200], 'min_samples_split': [2, 10, 20], 'max_features': [0.1, 0.5, 0.9], 'max_depth': [5, 10, 15] &#125;) 网格搜索调整超参数可以先固定一个参数 最优化后继续调整第一步：确定学习速率和tree_based 给个常见初始值 根据是否类别不平衡调节max_depth,min_child_weight,gamma,subsample,scale_pos_weightmax_depth=3 起始值在4-6之间都是不错的选择。min_child_weight比较小的值解决极不平衡的分类问题eg:1subsample, colsample_bytree = 0.8: 这个是最常见的初始值了scale_pos_weight = 1: 这个值是因为类别十分不平衡。第二步： max_depth 和 min_weight 对最终结果有很大的影响 ‘max_depth’:range(3,10,2), ‘min_child_weight’:range(1,6,2)先大范围地粗调参数，然后再小范围地微调。第三步：gamma参数调优‘gamma’:[i/10.0 for i in range(0,5)]第四步：调整subsample 和 colsample_bytree 参数 ‘subsample’:[i/100.0 for i in range(75,90,5)], ‘colsample_bytree’:[i/100.0 for i in range(75,90,5)]第五步：正则化参数调优‘reg_alpha’:[1e-5, 1e-2, 0.1, 1, 100]‘reg_lambda’第六步：降低学习速率learning_rate =0.01, 12345678910111213141516from sklearn.model_selection import GridSearchCVtuned_parameters= [&#123;'n_estimators':[100,200,500], 'max_depth':[3,5,7], ##range(3,10,2) 'learning_rate':[0.5, 1.0], 'subsample':[0.75,0.8,0.85,0.9] &#125;]tuned_parameters= [&#123;'n_estimators':[100,200,500,1000] &#125;]clf = GridSearchCV(XGBClassifier(silent=0,nthread=4,learning_rate= 0.5,min_child_weight=1, max_depth=3,gamma=0,subsample=1,colsample_bytree=1,reg_lambda=1,seed=1000), param_grid=tuned_parameters,scoring='roc_auc',n_jobs=4,iid=False,cv=5) clf.fit(X_train, y_train)##clf.grid_scores_, clf.best_params_, clf.best_score_print(clf.best_params_)y_true, y_pred = y_test, clf.predict(X_test)print"Accuracy : %.4g" % metrics.accuracy_score(y_true, y_pred) y_proba=clf.predict_proba(X_test)[:,1]print "AUC Score (Train): %f" % metrics.roc_auc_score(y_true, y_proba) 123456789101112131415161718192021from sklearn.model_selection import GridSearchCVparameters= [&#123;'learning_rate':[0.01,0.1,0.3],'n_estimators':[1000,1200,1500,2000,2500]&#125;]clf = GridSearchCV(XGBClassifier( max_depth=3, min_child_weight=1, gamma=0.5, subsample=0.6, colsample_bytree=0.6, objective= 'binary:logistic', #逻辑回归损失函数 scale_pos_weight=1, reg_alpha=0, reg_lambda=1, seed=27 ), param_grid=parameters,scoring='roc_auc') clf.fit(X_train, y_train)print(clf.best_params_) y_pre= clf.predict(X_test)y_pro= clf.predict_proba(X_test)[:,1] print "AUC Score : %f" % metrics.roc_auc_score(y_test, y_pro) print"Accuracy : %.4g" % metrics.accuracy_score(y_test, y_pre) 模型融合voting1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#why choose one model, when you can pick them all with voting classifier#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another modelvote_est = [ #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html ('ada', ensemble.AdaBoostClassifier()), ('bc', ensemble.BaggingClassifier()), ('etc',ensemble.ExtraTreesClassifier()), ('gbc', ensemble.GradientBoostingClassifier()), ('rfc', ensemble.RandomForestClassifier()), #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc ('gpc', gaussian_process.GaussianProcessClassifier()), #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression ('lr', linear_model.LogisticRegressionCV()), #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html ('bnb', naive_bayes.BernoulliNB()), ('gnb', naive_bayes.GaussianNB()), #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html ('knn', neighbors.KNeighborsClassifier()), #SVM: http://scikit-learn.org/stable/modules/svm.html ('svc', svm.SVC(probability=True)), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html ('xgb', XGBClassifier())]#Hard Vote or majority rulesvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')vote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv = cv_split)vote_hard.fit(data1[data1_x_bin], data1[Target])print("Hard Voting Training w/bin score mean: &#123;:.2f&#125;". format(vote_hard_cv['train_score'].mean()*100)) print("Hard Voting Test w/bin score mean: &#123;:.2f&#125;". format(vote_hard_cv['test_score'].mean()*100))print("Hard Voting Test w/bin score 3*std: +/- &#123;:.2f&#125;". format(vote_hard_cv['test_score'].std()*100*3))print('-'*10)#Soft Vote or weighted probabilitiesvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')vote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv = cv_split)vote_soft.fit(data1[data1_x_bin], data1[Target])print("Soft Voting Training w/bin score mean: &#123;:.2f&#125;". format(vote_soft_cv['train_score'].mean()*100)) print("Soft Voting Test w/bin score mean: &#123;:.2f&#125;". format(vote_soft_cv['test_score'].mean()*100))print("Soft Voting Test w/bin score 3*std: +/- &#123;:.2f&#125;". format(vote_soft_cv['test_score'].std()*100*3))print('-'*10) 模型评价1​ ‘’’Return prediction to use it in another function.’’’def x_val_predict(model): from sklearn.model_selection import cross_val_predict predicted = cross_val_predict(model, X_train, y_train, cv = 10) return predicted # Now we can use it in another function by assigning the function to its return value. ‘’’#1.Confusion matrix.’’’def confusion_matrix(model): predicted = x_val_predict(model) confusion_matrix = pd.crosstab(y_train, predicted, rownames = [‘Actual’], colnames = [‘Predicted/Classified’], margins = True) # We use pandas crosstab return display(confusion_matrix) ‘’’#2.Precision score.’’’def precision_score(model): from sklearn.metrics import precision_score predicted = x_val_predict(model) precision_score = precision_score(y_train, predicted) return display(precision_score) ‘’’#3.Recall score.’’’def recall_score(model): from sklearn.metrics import recall_score predicted = x_val_predict(model) recall_score = recall_score(y_train, predicted) return display(recall_score) ‘’’#4.Specificity score.’’’def specificity_score(model): from sklearn.metrics import confusion_matrix predicted = x_val_predict(model) tn, fp, fn, tp = confusion_matrix(y_train, predicted).ravel() specificity_score = tn / (tn + fp) return display(specificity_score) ‘’’#5.F1 score.’’’def f1_score(model): from sklearn.metrics import f1_score predicted = x_val_predict(model) f1_score = f1_score(y_train, predicted) return display(f1_score) ‘’’#6.Classification report.’’’def classification_report(model): from sklearn.metrics import classification_report predicted = x_val_predict(model) classification_report = classification_report(y_train, predicted) return print(classification_report) ‘’’#7.Plot precision-recall vs threshold curve.’’’def precision_recall_vs_threshold(model): from sklearn.metrics import precision_recall_curve probablity = model.predict_proba(X_train)[:, 1] precision, recall, threshold = precision_recall_curve(y_train, probablity) plt.figure(figsize = (18, 4)) plt.plot(threshold, precision[:-1], ‘b-‘, label = ‘precision’, lw = 3.7) plt.plot(threshold, recall[:-1], ‘g’, label = ‘recall’, lw = 3.7) plt.xlabel(‘Threshold’) plt.legend(loc = ‘best’) plt.ylim([0, 1]) ‘’’#8.Plot recall vs precision curve.’’’def plot_precision_vs_recall(model): from sklearn.metrics import precision_recall_curve probablity = model.predict_proba(X_train)[:, 1] precision, recall, threshold = precision_recall_curve(y_train, probablity) plt.figure(figsize = (18, 5)) plt.plot(recall, precision, ‘r-‘, lw = 3.7) plt.ylabel(‘Recall’) plt.xlabel(‘Precision’) plt.axis([0, 1.5, 0, 1.5]) ‘’’#9.Plot ROC curve with AUC score.’’’def plot_roc_and_auc_score(model): from sklearn.metrics import roc_curve, roc_auc_score probablity = model.predict_proba(X_train)[:, 1] false_positive_rate, true_positive_rate, threshold = roc_curve(y_train, probablity) auc_score = roc_auc_score(y_train, probablity) plt.figure(figsize = (18, 5)) plt.plot(false_positive_rate, true_positive_rate, label = “ROC CURVE, AREA = “+ str(auc_score)) plt.plot([0, 1], [0, 1], ‘black’, lw = 3.7) plt.xlabel(‘False Positive Rate (1-Specificity)’) plt.ylabel(‘True Positive Rate (Sensitivity)’) plt.axis([0, 1, 0, 1]) plt.legend(loc = 4) ​]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vector Representation]]></title>
    <url>%2F2019%2F04%2F09%2F%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C%2FVectorrepresentation%2F</url>
    <content type="text"><![CDATA[​ 自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 Id537，“dog”可能表示为 Id143。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。 来源：https://www.tensorflow.org/tutorials/representation/word2vec ​ 自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 Id537，“dog”可能表示为 Id143。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。 ​ 向量空间模型 (VSM) 在连续向量空间中表示（嵌入）字词，其中语义相似的字词会映射到附近的点（“在彼此附近嵌入”）。VSM 在 NLP 方面有着悠久而丰富的历史，但所有方法均以某种方式依赖于分布假设，这种假设指明在相同上下文中显示的字词语义相同。利用该原则的不同方法可分为两类：基于计数的方法（例如潜在语义分析）以及预测方法（例如神经概率语言模型）。 ​ Word2vec 是一种计算效率特别高的预测模型，用于学习原始文本中的字词嵌入。它分为两种类型：连续词袋模型 (CBOW) 和 Skip-Gram 模型。数据量少的时候使用CBOW，数据量大的时候Skip-gram，一般都用Skip-gram。 代码tensorflow1https://www.tensorflow.org/tutorials/representation/word2vec python1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import pandas as pdfrom gensim.corpora import WikiCorpusfrom gensim.models import Word2Vecfrom gensim.models.word2vec import LineSentenceimport pandas as pdimport multiprocessingimport numpy as npL = 10path = './'save_path = path + '/w2v'if not os.path.exists(save_path): print(save_path) os.makedirs(save_path)train1 = pd.read_csv(path + '/train.csv')train = pd.read_csv(path + '/train_old.csv')test = pd.read_csv(path + '/test.csv')data = pd.concat([train, test, train1]).reset_index(drop=True).sample(frac=1, random_state=2018).fillna(0)data = data.replace('\\N', 999)sentence = []for line in list(data[['1_total_fee', '2_total_fee', '3_total_fee', '4_total_fee']].values): sentence.append([str(float(l)) for idx, l in enumerate(line)])print('training...')model = Word2Vec(sentence, size=L, window=2, min_count=1, workers=multiprocessing.cpu_count(), iter=10)print('outputing...')for fea in ['1_total_fee', '2_total_fee', '3_total_fee', '4_total_fee']: values = [] for line in list(data[fea].values): values.append(line) values = set(values) print(len(values)) w2v = [] for i in values: a = [i] a.extend(model[str(float(i))]) w2v.append(a) out_df = pd.DataFrame(w2v) name = [fea] for i in range(L): name.append(name[0] + 'W' + str(i)) out_df.columns = name out_df.to_csv(save_path + '/' + fea + '.csv', index=False) TSNE降维可视化1234567891011121314def plot_with_labels(low_dim_embs, labels, filename = 'tsne.png'): assert low_dim_embs.shape[0] &gt;= len(labels), "More labels than embeddings" plt.figure(figsize= (10, 18)) for i, label in enumerate(labels): x, y = low_dim_embs[i, :] plt.scatter(x, y) plt.annotate(label, xy = (x, y), textcoords = 'offset points', ha = 'right', va = 'bottom') plt.savefig(filename)def polt_tnse(df,df_target,plot_only=300): df_target=list(df_target.astype('str')) tsne = TSNE(perplexity = 30, n_components = 2, init = 'pca', n_iter = 5000) low_dim_embs = tsne.fit_transform(df.iloc[:plot_only][:]) labels = [df_target[i] for i in range(plot_only)] plot_with_labels(low_dim_embs, labels)]]></content>
      <categories>
        <category>竞赛经验</category>
      </categories>
      <tags>
        <tag>预处理</tag>
        <tag>分类变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征选择]]></title>
    <url>%2F2019%2F04%2F09%2F%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%2F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[[TOC] 如何进行特征选择 Features with a high percentage of missing values according to athreshold Collinear (highly correlated) features Features with zero importance in a tree-based model Features with low importance Features with a single unique value feature importace特征重要度高如果发现特征重要度有明显很高的情况下，可以将这个特征去除再尝试，因为有可能模型过于依赖此特征而导致了过拟合的存在。比如下图中的addr_state 特征重要度低来源：https://www.kaggle.com/ogrellier/feature-selection-with-null-importances 来源：https://academic.oup.com/bioinformatics/article/26/10/1340/193348 ​ 传统做法就是根据feature importance然后就跑一个阈值，剔除低于阈值的feature。 剔除阈值的目的在于： 消除高相关的feature 提高model的variance ​ 最好最后的分数剔除分类变量，因为它主要是平衡分类变量的bias ​ 论文提出的做法是，对于树模型，它的特征重要度会对数量多的分类变量有bias，所以提出了一种基于null importance 的做法。即对target打乱s次，预测s次的特征重要度，然后再利用distribution进行分析，从而得到比较标注的特征重要度。主要的做法请看kaggle上的示例： 获取重要度，此处需要自定义一个训练模型 123456789101112131415161718192021222324252627282930313233343536# shuffle决定是否打乱y值def get_feature_importances(data, shuffle, seed=None): # Gather real features train_features = [f for f in data if f not in ['TARGET', 'SK_ID_CURR']] # Go over fold and keep track of CV score (train and valid) and feature importances # Shuffle target if required y = data['TARGET'].copy() if shuffle: # Here you could as well use a binomial distribution y = data['TARGET'].copy().sample(frac=1.0) # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True) lgb_params = &#123; 'objective': 'binary', 'boosting_type': 'rf', 'subsample': 0.623, 'colsample_bytree': 0.7, 'num_leaves': 127, 'max_depth': 8, 'seed': seed, 'bagging_freq': 1, 'n_jobs': 4 &#125; # Fit the model clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200, categorical_feature=categorical_feats) # Get feature importances imp_df = pd.DataFrame() imp_df["feature"] = list(train_features) imp_df["importance_gain"] = clf.feature_importance(importance_type='gain') imp_df["importance_split"] = clf.feature_importance(importance_type='split') imp_df['trn_score'] = roc_auc_score(y, clf.predict(data[train_features])) return imp_df 跑n轮得到Null importance 1234567891011null_imp_df = pd.DataFrame()nb_runs = 80import timestart = time.time()dsp = ''for i in range(nb_runs): # Get current run importances imp_df = get_feature_importances(data=data, shuffle=True) imp_df['run'] = i + 1 # Concat the latest importances with the old ones null_imp_df = pd.concat([null_imp_df, imp_df], axis=0) 然后画分布图 1234567891011121314151617181920def display_distributions(actual_imp_df_, null_imp_df_, feature_): plt.figure(figsize=(13, 6)) gs = gridspec.GridSpec(1, 2) # Plot Split importances ax = plt.subplot(gs[0, 0]) a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances') ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target') ax.legend() ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold') plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper()) # Plot Gain importances ax = plt.subplot(gs[0, 1]) a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances') ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target') ax.legend() ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold') plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='LIVINGAPARTMENTS_AVG') 图主要展示了什么叫做好的分布:方差小的。同时远离真实的重要度（红线） 我们想要的是在与真实目标(即实际重要性)使用时具有很高重要性的特性，而在与噪声训练时得分不高(即null importance得分)。在零假设和正态分布下，如果一个特征的红色实际重要性在蓝色分布内，那么该特征与目标不相关的可能性就很大。如果它在蓝色分布的5%范围内或者在外面，那么它就是相关的。 然后下一步要做的就是（去掉与target不相关的，去掉自己高相关的）： Drop high variance features if they are not really related to the target Remove the decaying factor on correlated features, showing their real importance (or unbiased importance) 结合一些公式，然后进行挑选 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960correlation_scores = []for _f in actual_imp_df['feature'].unique(): f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].values gain_score = 100 * (f_null_imps &lt; np.percentile(f_act_imps, 25)).sum() / f_null_imps.size f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].values split_score = 100 * (f_null_imps &lt; np.percentile(f_act_imps, 25)).sum() / f_null_imps.size correlation_scores.append((_f, split_score, gain_score))corr_scores_df = pd.DataFrame(correlation_scores, columns=['feature', 'split_score', 'gain_score'])def score_feature_selection(df=None, train_features=None, cat_feats=None, target=None): # Fit LightGBM dtrain = lgb.Dataset(df[train_features], target, free_raw_data=False, silent=True) lgb_params = &#123; 'objective': 'binary', 'boosting_type': 'gbdt', 'learning_rate': .1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'num_leaves': 31, 'max_depth': -1, 'seed': 13, 'n_jobs': 4, 'min_split_gain': .00001, 'reg_alpha': .00001, 'reg_lambda': .00001, 'metric': 'auc' &#125; # Fit the model hist = lgb.cv( params=lgb_params, train_set=dtrain, num_boost_round=2000, categorical_feature=cat_feats, nfold=5, stratified=True, shuffle=True, early_stopping_rounds=50, verbose_eval=0, seed=17 ) # Return the last mean / std values return hist['auc-mean'][-1], hist['auc-stdv'][-1]# features = [f for f in data.columns if f not in ['SK_ID_CURR', 'TARGET']]# score_feature_selection(df=data[features], train_features=features, target=data['TARGET'])for threshold in [0, 10, 20, 30 , 40, 50 ,60 , 70, 80 , 90, 95, 99]: split_feats = [_f for _f, _score, _ in correlation_scores if _score &gt;= threshold] split_cat_feats = [_f for _f, _score, _ in correlation_scores if (_score &gt;= threshold) &amp; (_f in categorical_feats)] gain_feats = [_f for _f, _, _score in correlation_scores if _score &gt;= threshold] gain_cat_feats = [_f for _f, _, _score in correlation_scores if (_score &gt;= threshold) &amp; (_f in categorical_feats)] print('Results for threshold %3d' % threshold) split_results = score_feature_selection(df=data, train_features=split_feats, cat_feats=split_cat_feats, target=data['TARGET']) print('\t SPLIT : %.6f +/- %.6f' % (split_results[0], split_results[1])) gain_results = score_feature_selection(df=data, train_features=gain_feats, cat_feats=gain_cat_feats, target=data['TARGET']) print('\t GAIN : %.6f +/- %.6f' % (gain_results[0], gain_results[1])) 特征相关性低如果特征之间的相关性很低的情况下，可以进一步检测变量之间的独立性，如果变量之间是independent意味着可以单独将这些特征进行多个模型训练再融合 特征贡献度低树模型中零贡献度或者低贡献度的特征可以去除]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类变量：Target Encoding]]></title>
    <url>%2F2019%2F04%2F08%2F%E7%AB%9E%E8%B5%9B%E7%BB%8F%E9%AA%8C%2Ftargetencoding%2F</url>
    <content type="text"><![CDATA[​ 当你在进行监督学习时，你经常要处理分类变量。也就是说，变量没有一个自然的数值表示。问题是大多数机器学习算法都要求输入数据是数值的。在某个时候，数据科学管道将需要将分类变量转换为数值变量。 来源：https://maxhalford.github.io/blog/target-encoding-done-the-right-way/ 前言​ 当你在进行监督学习时，你经常要处理分类变量。也就是说，变量没有一个自然的数值表示。问题是大多数机器学习算法都要求输入数据是数值的。在某个时候，数据科学管道将需要将分类变量转换为数值变量。 有很多方法可以做到这一点: Label encoding 为每个类别选择数字 One-hot encoding:为每个类别创建一个二进制列 Vector representation ：也就是word2vec，在这里您可以找到一个适合您的数据的低维子空间 Optimal binning ：在依赖于LightGBM或CatBoost等树学习器 Target encoding: 按类别平均目标值 ​ 每种方法都有其优缺点，通常取决于您的数据和需求。如果一个变量有很多类别，那么一个[One-hot encoding:方案将产生许多列，这可能导致内存问题。根据我的经验，依赖LightGBM/CatBoost是最好的分箱方法。Label encoding是没有用的，最好不要使用它。然而，如果你的分类变量恰好是有序的，那么你可以而且应该用递增的数字来表示它(例如，“cold”变成0，“mild”变成1，“hot”变成2)。word2vec和其他类似的方法很酷也很好，但是它们需要进行一些微调，而且并不总是奏效。 target encoding​ target encoding是很容易理解的一种思想，假设你有分类变量X和变量Y，然后对于X中每一个distinct的元素计算其对应Y值的平局之，然后用这个平均值替换 $ x_i $ .下面这个例子显而易见： x0 x1 y aa cc 1 aa cc 1 aa cc 1 aa cc 1 aa cc 0 bb cc 1 bb cc 0 bb cc 0 bb cc 0 bb dd 0 x0 x1 y 0.8 cc 1 0.8 cc 1 0.8 cc 1 0.8 cc 1 0.8 cc 0 0.2 cc 1 0.2 cc 0 0.2 cc 0 0.2 cc 0 0.2 dd 0 ​ Target encoding的好处在于它提取了那些可以解释Y值，比如这里得aa拥有了一个Y值的平均值0.8，这会很好的帮助下游机器学习分类算法。 ​ 目标编码的问题有一个名称:过拟合。事实上，当平均值的数值很低时，依赖平均值并不总是一个好主意。您必须记住，您正在训练的数据集是一个更大的数据集的样本。这意味着，当您将训练集中发现的任何规律应用到另一个数据集(即测试集)时，可能都不成立。比如有可能训练集内它对应的Y值都是0，但是在测试集中它对应的Y值1比较多。 ​ 得出结论，永远不要用基础的target encoding,要用也是用以下的进阶encoding。 Target encoding进阶有很多方法可以处理这个问题。交叉验证和additive smoothing可以结合使用 交叉验证一种流行的方法是使用交叉验证并计算每个切分数据集中的平均值。这就是H20和许多kaggler所做的。 None: no holdout, mean是对训练集的所有数据行的计算应用于测试数据 leave eoneout: mean是对除了当前行本身之外的所有数据行进行计算。这可以用于训练数据。当前行本身的目标不包括在平均值中，以防止过度拟合 KFold:平均值只计算out-of-fold数据(需要K-fold) 这可以用于训练数据。为了防止过拟合，目标均值是根据叠外数据计算的 additive smoothing 使用additive smoothing，因为数据集中存在数据的count较小，所以它的target值容易受到过拟合的影响。它使用了全局的平均值来smooth较少数据带来过拟合的影响 数学上它等价于: u=\frac{n\times \hat x+m\times w}{n+m}where μ is the mean we’re trying to compute (the one that’s going to replace our categorical values) n is the number of values you have ¯x is your estimated mean m is the “weight” you want to assign to the overall mean w is the overall mean 其中m就是用来调节全局的权重，根据他的经验，他发现m取300的时候适用于大多数场合。 1234567891011121314def calc_smooth_mean(df, by, on, m): # Compute the global mean mean = df[on].mean() # Compute the number of values and the mean of each group agg = df.groupby(by)[on].agg(['count', 'mean']) counts = agg['count'] means = agg['mean'] # Compute the "smoothed" means smooth = (counts * means + m * mean) / (counts + m) # Replace each value by the according smoothed mean return df[by].map(smooth)]]></content>
      <categories>
        <category>竞赛经验</category>
      </categories>
      <tags>
        <tag>预处理</tag>
        <tag>分类变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比赛EDA]]></title>
    <url>%2F2019%2F04%2F08%2F%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%2F%E6%AF%94%E8%B5%9BEDA%2F</url>
    <content type="text"><![CDATA[比赛常见的EDA总结 [TOC] 数值型与数值型 分布依赖图 类别型和数值型 箱图(看的是中间那条众线) 类别型和类别型 条形图 检查缺失值1234567891011# https://www.kaggle.com/gpreda/santander-eda-and-predictiondef missing_data(data): total = data.isnull().sum() percent = (data.isnull().sum()/data.isnull().count()*100) tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) types = [] for col in data.columns: dtype = str(data[col].dtype) types.append(dtype) tt['Types'] = types return(np.transpose(tt)) 观察分布值观察训练集的01分布(可以拓展到任意分布对比图)12345678910111213141516171819202122# https://www.kaggle.com/gpreda/santander-eda-and-predictiondef plot_feature_distribution(df1, df2, label1, label2, features): i = 0 sns.set_style('whitegrid') plt.figure() fig, ax = plt.subplots(10,10,figsize=(18,22)) for feature in features: i += 1 plt.subplot(10,10,i) sns.distplot(df1[feature], hist=False,label=label1) sns.distplot(df2[feature], hist=False,label=label2) plt.xlabel(feature, fontsize=9) locs, labels = plt.xticks() plt.tick_params(axis='x', which='major', labelsize=6, pad=-6) plt.tick_params(axis='y', which='major', labelsize=6) plt.show(); t0 = train_df.loc[train_df['target'] == 0]t1 = train_df.loc[train_df['target'] == 1]features = train_df.columns.values[2:102]plot_feature_distribution(t0, t1, '0', '1', features) 结论：分布图怎么看，就是看如果01分布完全一模一样那么可能说明这个特征是冗余的无关特征，如果01分布不同的情况下，也做不了太多东西，只能结合特征重要度反证这些特征是有用的。 train_test的分布同理可以选择train和test的行和列的mean/std/min值 1234567891011121314151617181920212223242526272829https://www.kaggle.com/gpreda/santander-eda-and-predictionplt.figure(figsize=(16,6))plt.title("Distribution of mean values per column in the train and test set")sns.distplot(train_df[features].mean(axis=0),color="magenta",kde=True,bins=120, label='train')sns.distplot(test_df[features].mean(axis=0),color="darkblue", kde=True,bins=120, label='test')plt.legend()plt.show()plt.figure(figsize=(16,6))plt.title("Distribution of std values per row in the train and test set")sns.distplot(train_df[features].std(axis=1),color="black", kde=True,bins=120, label='train')sns.distplot(test_df[features].std(axis=1),color="red", kde=True,bins=120, label='test')plt.legend();plt.show()plt.figure(figsize=(16,6))plt.title("Distribution of std values per column in the train and test set")sns.distplot(train_df[features].std(axis=0),color="blue",kde=True,bins=120, label='train')sns.distplot(test_df[features].std(axis=0),color="green", kde=True,bins=120, label='test')plt.legend(); plt.show()t0 = train_df.loc[train_df['target'] == 0]t1 = train_df.loc[train_df['target'] == 1]plt.figure(figsize=(16,6))plt.title("Distribution of mean values per row in the train set")sns.distplot(t0[features].mean(axis=1),color="red", kde=True,bins=120, label='target = 0')sns.distplot(t1[features].mean(axis=1),color="blue", kde=True,bins=120, label='target = 1')plt.legend(); plt.show()plt.figure(figsize=(16,6))plt.title("Distribution of mean values per column in the train set")sns.distplot(t0[features].mean(axis=0),color="green", kde=True,bins=120, label='target = 0')sns.distplot(t1[features].mean(axis=0),color="darkblue", kde=True,bins=120, label='target = 1')plt.legend(); plt.show() 检查duplicate values12345678910111213141516%%timefeatures = train_df.columns.values[2:202]unique_max_train = []unique_max_test = []for feature in features: values = train_df[feature].value_counts() unique_max_train.append([feature, values.max(), values.idxmax()]) values = test_df[feature].value_counts() unique_max_test.append([feature, values.max(), values.idxmax()])# === plotdf = df.sort_values(by='n_train_unique').reset_index(drop=True)df[['n_train_unique', 'n_test_unique', 'n_overlap']].plot(kind='barh' ,figsize=(22, 100), fontsize=20, width=0.8)plt.yticks(df.index, df['feature'].values)plt.xlabel('n_unique', fontsize=20)plt.ylabel('feature', fontsize=20)plt.legend(loc='center right', fontsize=20) PCAhttps://zhuanlan.zhihu.com/p/28909807 主成分分析是统计方法里的一种降维方法，它的主要思想是将原有 个特征通过正交变换将一组可能存在相关性的特征缩减到 特征( )。举例来说，在网站用户行为数据收集过程中，会话数(Visits)，浏览页数(PV)，网站总停留时间(Time Spend Total)，访问人数(Unique Visitor)，以上这几个指标，无论是从以往的数据统计还是业务经验来看，都存在一定正相关关系，如果将这些特征喂给模型，很容易造成过拟合。 通过转化，从而剔除噪声，而且可以看到转化后的值然后把一些无关的列再剔除 PCA 使用要点 使用主成分分析，往往会丢失掉“少部分信息”（注意：这“少部分信息”仅仅指方差较小的数据，并非信息含量真的少的数据！） 因为1的特性，所以在机器学些中，不推荐使用 PCA 去优化特征达到避免过拟合的目的。 既然PCA不能避免过拟合，那为何还要使用，根据周志华老师的西瓜书中的描述： 在舍弃特征值较小的特征之后，能够使样本采集密度增 当数据受到噪声影响时，最小的特征值对对应的特征向量往往与噪声有关将他们舍弃能够在一定程度上起到去噪效果 在维度过多的情况下，变换后的坐标系代表的意义不明，不易于解释。 123456789101112131415161718# https://www.kaggle.com/roydatascience/eda-pca-lgbm-santander-transactions/notebookfrom sklearn.decomposition import KernelPCAlin_pca = KernelPCA(n_components = 2, kernel="linear", fit_inverse_transform=True)rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.0433, fit_inverse_transform=True)sig_pca = KernelPCA(n_components = 2, kernel="sigmoid", gamma=0.001, coef0=1, fit_inverse_transform=True)plt.figure(figsize=(11, 4))for subplot, pca, title in ((131, lin_pca, "Linear kernel"), (132, rbf_pca, "RBF kernel, $\gamma=0.04$"), (133, sig_pca, "Sigmoid kernel, $\gamma=10^&#123;-3&#125;, r=1$")): PCA_train_x = PCA(2).fit_transform(train_scaled) plt.subplot(subplot) plt.title(title, fontsize=14) plt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=target, cmap="nipy_spectral_r") plt.xlabel("$z_1$", fontsize=18) if subplot == 131: plt.ylabel("$z_2$", fontsize=18, rotation=0) plt.grid(True)plt.show() 编码1234567891011121314# 独热编码 one_hot编码，不常见的不编，只编常见的，然后不常见的选择性是否当成一类def one_hot_encoder(train,column,n=100,nan_as_category=False): tmp = train[column].value_counts().to_frame() values = list(tmp[tmp[column]&gt;n].index) train.loc[train[column].isin(values),column+'N'] = train.loc[train[column].isin(values),column] train = pd.get_dummies(train, columns=[column+'N'], dummy_na=nan_as_category) return train# 编码编码def encode_count(df,column_name): lbl = preprocessing.LabelEncoder() lbl.fit(list(df[column_name].values)) df[column_name] = lbl.transform(list(df[column_name].values)) return df]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy]]></title>
    <url>%2F2019%2F04%2F08%2F%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%2Fnumpy%2F</url>
    <content type="text"><![CDATA[numpy 操作小技巧 增加concat/vstack/hstack 1https://blog.csdn.net/xiaodongxiexie/article/details/71774466]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy]]></title>
    <url>%2F2019%2F04%2F08%2F%E4%BB%A3%E7%A0%81%E5%9F%BA%E7%A1%80%2Fnumpy%2F</url>
    <content type="text"><![CDATA[numpy 操作小技巧 增加concat/vstack/hstack 1https://blog.csdn.net/xiaodongxiexie/article/details/71774466]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python代码]]></title>
    <url>%2F2019%2F04%2F08%2F%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%2Fpython%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[python 代码小技巧 [TOC] 通用测试时间12关注walltime%%time 遍历列表12for index, item in enumerate(list1): print index, item 并行遍历12for (x,y) in zip(a,b): print x,y 通过字典保存了所有中间过程，不仅命名容易，而且便于后续操作12d=&#123;d_name:get_dvalue(d_name) for d_name in d_list&#125;best_dvalue_dname=min(d.items(),key=lambda x:x[1]) 文件1234#这样会比单纯用readlines()快count = 0for index, line in enumerate(open(filepath,'r'))： count += 1 数据结构字典12from collections import defaultdict 通过这个声明dict&#123;里面的格式&#125;如 defaultdict(list) 列表生成式121. [i for i in range(k) if condition]：此时if起条件判断作用，满足条件的，将被返回成为最终生成的列表的一员。2. [i if condition else exp for exp]：此时if...else被用来赋值，满足条件的i以及else被用来生成最终的列表 提高处理速度多进程提高数据预处理df和numpy都可以，注意windows下保证两个代码分开，否则函数无法识别主进程和子进程 jupyter上代码： 123456789101112131415# 这样子会很好的传参数def np_parallelize_run(func,df,nlp): df_split = np.array_split(df, num_partitions) params=[] for sub_df in df_split: param = &#123;&#125; param['text']=sub_df param['nlp']=nlp params.append(param) pool = Pool(num_cores) df = np.concatenate(pool.map(func, params)) #df = sp.vstack(pool.map(func, df_split), format='csr') faster and mem efficient for pool.close() pool.join() return df py文件上的代码 12345def spacy_split(param): text,nlp = param['text'],param['nlp'] docs = nlp.pipe(text) print('finish') return list(docs) 自带算法库数组中查找插入的位置123返回排序插入的位置，并不是真的插入排序数组中from bisect import bisect_left, bisect_rightend = bisect_left(keys, 0)]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python代码]]></title>
    <url>%2F2019%2F04%2F08%2F%E4%BB%A3%E7%A0%81%E5%9F%BA%E7%A1%80%2Fpython%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[python 代码小技巧 [TOC] 通用测试时间12关注walltime%%time 遍历列表12for index, item in enumerate(list1): print index, item 并行遍历12for (x,y) in zip(a,b): print x,y 通过字典保存了所有中间过程，不仅命名容易，而且便于后续操作12d=&#123;d_name:get_dvalue(d_name) for d_name in d_list&#125;best_dvalue_dname=min(d.items(),key=lambda x:x[1]) 文件1234#这样会比单纯用readlines()快count = 0for index, line in enumerate(open(filepath,'r'))： count += 1 数据结构字典12from collections import defaultdict 通过这个声明dict&#123;里面的格式&#125;如 defaultdict(list) 列表生成式121. [i for i in range(k) if condition]：此时if起条件判断作用，满足条件的，将被返回成为最终生成的列表的一员。2. [i if condition else exp for exp]：此时if...else被用来赋值，满足条件的i以及else被用来生成最终的列表 提高处理速度多进程提高数据预处理df和numpy都可以，注意windows下保证两个代码分开，否则函数无法识别主进程和子进程 jupyter上代码： 123456789101112131415# 这样子会很好的传参数def np_parallelize_run(func,df,nlp): df_split = np.array_split(df, num_partitions) params=[] for sub_df in df_split: param = &#123;&#125; param['text']=sub_df param['nlp']=nlp params.append(param) pool = Pool(num_cores) df = np.concatenate(pool.map(func, params)) #df = sp.vstack(pool.map(func, df_split), format='csr') faster and mem efficient for pool.close() pool.join() return df py文件上的代码 12345def spacy_split(param): text,nlp = param['text'],param['nlp'] docs = nlp.pipe(text) print('finish') return list(docs) 自带算法库数组中查找插入的位置123返回排序插入的位置，并不是真的插入排序数组中from bisect import bisect_left, bisect_rightend = bisect_left(keys, 0)]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比赛通用代码]]></title>
    <url>%2F2019%2F04%2F08%2F%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%2F%E6%AF%94%E8%B5%9B%E9%80%9A%E7%94%A8%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[比赛通用代码 [TOC] jupyter 载入代码包123456789101112131415161718192021222324252627# Load librariesimport numpy as npimport timeimport pandas as pdimport randomimport gcfrom pandas import read_csvimport plotly.offline as pyimport plotly.graph_objs as goimport seaborn as sns%matplotlib inlineimport matplotlib.pyplot as pltimport warningswarnings.filterwarnings('ignore')pd.options.display.max_columns = 100gc.enable()'''Displays markdown formatted output like bold, italic bold etc.'''from IPython.display import Markdowndef bold(string): display(Markdown(string))'''Ignores deprecation warning.'''def ignore_warnings(): import warnings warnings.filterwarnings('ignore', category = DeprecationWarning) bold('**Merged data:**') 自动压缩变量空间12345678910111213141516171819202122232425262728293031323334353637383940def reduce_mem_usage(df): """ iterate through all the columns of a dataframe and modify the data type to reduce memory usage. """ #start_mem = df.memory_usage().sum() / 1024**2 #print('Memory usage of dataframe is &#123;:.2f&#125; MB'.format(start_mem)) for col in df.columns: col_type = df[col].dtype if col_type != object: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == 'int': if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) #end_mem = df.memory_usage().sum() / 1024**2 #print('Memory usage after optimization is: &#123;:.2f&#125; MB'.format(end_mem)) #print('Decreased by &#123;:.1f&#125;%'.format(100 * (start_mem - end_mem) / start_mem)) return dfdef reload(): gc.collect() df = pd.read_csv('../input/train_V2.csv') invalid_match_ids = df[df['winPlacePerc'].isna()]['matchId'].values df = df[-df['matchId'].isin(invalid_match_ids)] df=reduce_mem_usage(df) return df GPU使用123456https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89004#latest-514910FYI, Kaggle provides us the use of 7 Tesla P100 GPU's simultaneously. Below are some tips. Python models will not run GPU by default. You must turn GPU on. First, activate GPU in kaggle kernels. Second, if using CatBoost, add the following hyperparameter, task_type = 'GPU'. If using XGBoost, add the following hyperparameter, 'tree_method': 'gpu_hist' or tree_method': 'gpu_exact'. If using LGBM, follow the instructions in this kernel to recompile GPU LGBM, then add the following 3 hyperparameters 'device': 'gpu', 'gpu_platform_id': 0, and 'gpu_device_id': 0. Note: many posted kernels regarding CatBoost and XGBoost don't use GPU and can actually be 4x faster with GPU activated!You are allowed to execute 7 GPU kernels simultaneously for 9 hour sessions. In one evening, you can train 100 models!! My final solution is a blend of dozens of LGBM, CatBoost and XGBoost. In one evening, you can train your models on a combined one billion generated new rows of augmented data using the power of Kaggle's GPU's (where 7 have a total cost of $49000!!)In this comp, I found that CatBoost achieved an LB 0.001 greater than LGBM. That's it. Enjoy the power!!]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas]]></title>
    <url>%2F2019%2F04%2F08%2F%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%2Fpandas%2F</url>
    <content type="text"><![CDATA[pandas 操作小技巧 通用操作pandas操作出现进度条： 用作迭代器 用于Pandas的操作12345import pandas as pdfrom tqdm import tqdmtqdm.pandas()sentences = train["question_text"].progress_apply(lambda x: x.split()).valuesfor sentence in tqdm(sentences, disable = (not verbose)): 增加merge/concat/joinhttps://www.e-learn.cn/content/qita/814185 改变直接replace12'''Put Dona, Jonkheer, Countess, Sir, Lady, Don in bucket Aristocrat.'''merged.Title.replace(to_replace = ['Dona', 'Jonkheer', 'Countess', 'Sir', 'Lady', 'Don'], value = 'Aristocrat', inplace = True) map映射1234#map映射 它就会自动把类别型转为连续型#如果要Inverse就直接key-value逆转size_mapping=&#123;'XL':3,'L':2,'M':1&#125;df['size']=df['size'].map(size_mapping) 单列分箱12345678910'''Create bin categories for Age.'''label_names = ['infant','child','teenager','young_adult','adult','aged']'''Create range for each bin categories of Age.'''cut_points = [0,5,12,18,35,60,81]'''Create and view categorized Age with original Age.'''merged['Age_binned'] = pd.cut(merged.Age, cut_points, labels = label_names)display(merged[['Age', 'Age_binned']].head(2)​ 12345678## 删除### 去重```pythontrain.drop_duplicates(subset = ['1_total_fee','2_total_fee','3_total_fee', 'month_traffic','pay_times','last_month_traffic','service2_caller_time','age'],inplace=True)]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas]]></title>
    <url>%2F2019%2F04%2F08%2F%E4%BB%A3%E7%A0%81%E5%9F%BA%E7%A1%80%2Fpandas%2F</url>
    <content type="text"><![CDATA[pandas 操作小技巧 通用操作pandas操作出现进度条： 用作迭代器 用于Pandas的操作12345import pandas as pdfrom tqdm import tqdmtqdm.pandas()sentences = train["question_text"].progress_apply(lambda x: x.split()).valuesfor sentence in tqdm(sentences, disable = (not verbose)): 增加merge/concat/joinhttps://www.e-learn.cn/content/qita/814185 改变直接replace12'''Put Dona, Jonkheer, Countess, Sir, Lady, Don in bucket Aristocrat.'''merged.Title.replace(to_replace = ['Dona', 'Jonkheer', 'Countess', 'Sir', 'Lady', 'Don'], value = 'Aristocrat', inplace = True) map映射1234#map映射 它就会自动把类别型转为连续型#如果要Inverse就直接key-value逆转size_mapping=&#123;'XL':3,'L':2,'M':1&#125;df['size']=df['size'].map(size_mapping) 单列分箱12345678910'''Create bin categories for Age.'''label_names = ['infant','child','teenager','young_adult','adult','aged']'''Create range for each bin categories of Age.'''cut_points = [0,5,12,18,35,60,81]'''Create and view categorized Age with original Age.'''merged['Age_binned'] = pd.cut(merged.Age, cut_points, labels = label_names)display(merged[['Age', 'Age_binned']].head(2)​ 12345678## 删除### 去重```pythontrain.drop_duplicates(subset = ['1_total_fee','2_total_fee','3_total_fee', 'month_traffic','pay_times','last_month_traffic','service2_caller_time','age'],inplace=True)]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
</search>
