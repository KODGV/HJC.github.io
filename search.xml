<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[比赛模型]]></title>
    <url>%2F2019%2F04%2F10%2F%E6%AF%94%E8%B5%9B%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[[TOC] 载入模型前操作归一化1234from sklearn.preprocessing import MinMaxScaler,StandardScalersc = StandardScaler()train_features = sc.fit_transform(train_features)test_features = sc.transform(test_features) 切分训练集和验证集12345678910111213141516171819# train_splitfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)# K-fold# scikit-learn k-fold cross-validationfrom sklearn.model_selection import KFold# data sample# prepare cross validationkfold = KFold(n_splits=3, shuffle = True, random_state= 1)# enumerate splitsfor train, test in kfold.split(data): print('train: %s, test: %s' % (data[train], data[test])) from sklearn.model_selection import StratifiedKFold# StratifiedKFoldfolds = StratifiedKFold(n_splits=num_folds, shuffle=False, random_state=2319)for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)): X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx] X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx] 模型 调整参数贝叶斯优化来源:https://www.cnblogs.com/yangruiGB2312/p/9374377.html 可以说是目前最好的调参的方法 贝叶斯调参采用高斯过程，考虑之前的参数信息，不断地更新先验；网格搜索未考虑之前的参数信息 贝叶斯调参迭代次数少，速度快；网格搜索速度慢,参数多时易导致维度爆炸 贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部优最 ​ 公式推导就不在这里展开，主要阐述主要思想：给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。 ​ 假设这个寻找最优化参数的过程是一个高斯过程。高斯过程有个特点，就是当随机遍历一定的数据点并拿到结果之后，可以大致绘制出整个数据的分布曲线，当然这是也会有一定误差。 python代码 1234567891011121314151617181920from bayes_opt import BayesianOptimizationdef rf_cv(n_estimators, min_samples_split, max_features, max_depth): val = cross_val_score( RandomForestClassifier(n_estimators=int(n_estimators), min_samples_split=int(min_samples_split), max_features=min(max_features, 0.999), # float max_depth=int(max_depth), random_state=2 ), x, y, scoring='roc_auc', cv=5 ).mean() return val# 注意名字要对应rf_bo = BayesianOptimization( rf_cv, &#123;'n_estimators': (10, 250), 'min_samples_split': (2, 25), 'max_features': (0.1, 0.999), 'max_depth': (5, 15)&#125; )]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vector Representation]]></title>
    <url>%2F2019%2F04%2F09%2Frepresentation%2F</url>
    <content type="text"><![CDATA[​ 自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 Id537，“dog”可能表示为 Id143。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。 来源：https://www.tensorflow.org/tutorials/representation/word2vec ​ 自然语言处理系统一直以来都将字词视为离散的原子符号，因此“cat”可能表示为 Id537，“dog”可能表示为 Id143。这些编码是任意的，并未向系统提供有关各个符号之间可能存在的关系的有用信息。这意味着模型在处理有关“狗”的数据时，几乎不可能利用到它所学的关于“猫”的知识（例如它们都属于动物、宠物，有四条腿等）。将字词表示为唯一的离散 ID 还会导致数据稀疏性，并且通常意味着我们可能需要更多数据才能成功训练统计模型。使用向量表示法可以扫除其中一些障碍。 ​ 向量空间模型 (VSM) 在连续向量空间中表示（嵌入）字词，其中语义相似的字词会映射到附近的点（“在彼此附近嵌入”）。VSM 在 NLP 方面有着悠久而丰富的历史，但所有方法均以某种方式依赖于分布假设，这种假设指明在相同上下文中显示的字词语义相同。利用该原则的不同方法可分为两类：基于计数的方法（例如潜在语义分析）以及预测方法（例如神经概率语言模型）。 ​ Word2vec 是一种计算效率特别高的预测模型，用于学习原始文本中的字词嵌入。它分为两种类型：连续词袋模型 (CBOW) 和 Skip-Gram 模型。数据量少的时候使用CBOW，数据量大的时候Skip-gram，一般都用Skip-gram。 代码tensorflow1https://www.tensorflow.org/tutorials/representation/word2vec python1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import pandas as pdfrom gensim.corpora import WikiCorpusfrom gensim.models import Word2Vecfrom gensim.models.word2vec import LineSentenceimport pandas as pdimport multiprocessingimport numpy as npL = 10path = './'save_path = path + '/w2v'if not os.path.exists(save_path): print(save_path) os.makedirs(save_path)train1 = pd.read_csv(path + '/train.csv')train = pd.read_csv(path + '/train_old.csv')test = pd.read_csv(path + '/test.csv')data = pd.concat([train, test, train1]).reset_index(drop=True).sample(frac=1, random_state=2018).fillna(0)data = data.replace('\\N', 999)sentence = []for line in list(data[['1_total_fee', '2_total_fee', '3_total_fee', '4_total_fee']].values): sentence.append([str(float(l)) for idx, l in enumerate(line)])print('training...')model = Word2Vec(sentence, size=L, window=2, min_count=1, workers=multiprocessing.cpu_count(), iter=10)print('outputing...')for fea in ['1_total_fee', '2_total_fee', '3_total_fee', '4_total_fee']: values = [] for line in list(data[fea].values): values.append(line) values = set(values) print(len(values)) w2v = [] for i in values: a = [i] a.extend(model[str(float(i))]) w2v.append(a) out_df = pd.DataFrame(w2v) name = [fea] for i in range(L): name.append(name[0] + 'W' + str(i)) out_df.columns = name out_df.to_csv(save_path + '/' + fea + '.csv', index=False) TSNE降维可视化1234567891011121314def plot_with_labels(low_dim_embs, labels, filename = 'tsne.png'): assert low_dim_embs.shape[0] &gt;= len(labels), "More labels than embeddings" plt.figure(figsize= (10, 18)) for i, label in enumerate(labels): x, y = low_dim_embs[i, :] plt.scatter(x, y) plt.annotate(label, xy = (x, y), textcoords = 'offset points', ha = 'right', va = 'bottom') plt.savefig(filename)def polt_tnse(df,df_target,plot_only=300): df_target=list(df_target.astype('str')) tsne = TSNE(perplexity = 30, n_components = 2, init = 'pca', n_iter = 5000) low_dim_embs = tsne.fit_transform(df.iloc[:plot_only][:]) labels = [df_target[i] for i in range(plot_only)] plot_with_labels(low_dim_embs, labels)]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>预处理</tag>
        <tag>分类变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征选择]]></title>
    <url>%2F2019%2F04%2F09%2F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[[TOC] 如何进行特征选择 feature importace特征重要度高如果发现特征重要度有明显很高的情况下，可以将这个特征去除再尝试，因为有可能模型过于依赖此特征而导致了过拟合的存在。比如下图中的addr_state 特征重要度低来源：https://www.kaggle.com/ogrellier/feature-selection-with-null-importances 来源：https://academic.oup.com/bioinformatics/article/26/10/1340/193348 ​ 传统做法就是根据feature importance然后就跑一个阈值，剔除低于阈值的feature。 剔除阈值的目的在于： 消除高相关的feature 提高model的variance ​ 最好最后的分数剔除分类变量，因为它主要是平衡分类变量的bias ​ 论文提出的做法是，对于树模型，它的特征重要度会对数量多的分类变量有bias，所以提出了一种基于null importance 的做法。即对target打乱s次，预测s次的特征重要度，然后再利用distribution进行分析，从而得到比较标注的特征重要度。主要的做法请看kaggle上的示例： 获取重要度，此处需要自定义一个训练模型 123456789101112131415161718192021222324252627282930313233343536# shuffle决定是否打乱y值def get_feature_importances(data, shuffle, seed=None): # Gather real features train_features = [f for f in data if f not in ['TARGET', 'SK_ID_CURR']] # Go over fold and keep track of CV score (train and valid) and feature importances # Shuffle target if required y = data['TARGET'].copy() if shuffle: # Here you could as well use a binomial distribution y = data['TARGET'].copy().sample(frac=1.0) # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True) lgb_params = &#123; 'objective': 'binary', 'boosting_type': 'rf', 'subsample': 0.623, 'colsample_bytree': 0.7, 'num_leaves': 127, 'max_depth': 8, 'seed': seed, 'bagging_freq': 1, 'n_jobs': 4 &#125; # Fit the model clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200, categorical_feature=categorical_feats) # Get feature importances imp_df = pd.DataFrame() imp_df["feature"] = list(train_features) imp_df["importance_gain"] = clf.feature_importance(importance_type='gain') imp_df["importance_split"] = clf.feature_importance(importance_type='split') imp_df['trn_score'] = roc_auc_score(y, clf.predict(data[train_features])) return imp_df 跑n轮得到Null importance 1234567891011null_imp_df = pd.DataFrame()nb_runs = 80import timestart = time.time()dsp = ''for i in range(nb_runs): # Get current run importances imp_df = get_feature_importances(data=data, shuffle=True) imp_df['run'] = i + 1 # Concat the latest importances with the old ones null_imp_df = pd.concat([null_imp_df, imp_df], axis=0) 然后画分布图 1234567891011121314151617181920def display_distributions(actual_imp_df_, null_imp_df_, feature_): plt.figure(figsize=(13, 6)) gs = gridspec.GridSpec(1, 2) # Plot Split importances ax = plt.subplot(gs[0, 0]) a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances') ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target') ax.legend() ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold') plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper()) # Plot Gain importances ax = plt.subplot(gs[0, 1]) a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances') ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target') ax.legend() ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold') plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='LIVINGAPARTMENTS_AVG') 图主要展示了什么叫做好的分布:方差小的。同时远离真实的重要度（红线） 我们想要的是在与真实目标(即实际重要性)使用时具有很高重要性的特性，而在与噪声训练时得分不高(即null importance得分)。在零假设和正态分布下，如果一个特征的红色实际重要性在蓝色分布内，那么该特征与目标不相关的可能性就很大。如果它在蓝色分布的5%范围内或者在外面，那么它就是相关的。 然后下一步要做的就是（去掉与target不相关的，去掉自己高相关的）： Drop high variance features if they are not really related to the target Remove the decaying factor on correlated features, showing their real importance (or unbiased importance) 结合一些公式，然后进行挑选 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960correlation_scores = []for _f in actual_imp_df['feature'].unique(): f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].values gain_score = 100 * (f_null_imps &lt; np.percentile(f_act_imps, 25)).sum() / f_null_imps.size f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].values split_score = 100 * (f_null_imps &lt; np.percentile(f_act_imps, 25)).sum() / f_null_imps.size correlation_scores.append((_f, split_score, gain_score))corr_scores_df = pd.DataFrame(correlation_scores, columns=['feature', 'split_score', 'gain_score'])def score_feature_selection(df=None, train_features=None, cat_feats=None, target=None): # Fit LightGBM dtrain = lgb.Dataset(df[train_features], target, free_raw_data=False, silent=True) lgb_params = &#123; 'objective': 'binary', 'boosting_type': 'gbdt', 'learning_rate': .1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'num_leaves': 31, 'max_depth': -1, 'seed': 13, 'n_jobs': 4, 'min_split_gain': .00001, 'reg_alpha': .00001, 'reg_lambda': .00001, 'metric': 'auc' &#125; # Fit the model hist = lgb.cv( params=lgb_params, train_set=dtrain, num_boost_round=2000, categorical_feature=cat_feats, nfold=5, stratified=True, shuffle=True, early_stopping_rounds=50, verbose_eval=0, seed=17 ) # Return the last mean / std values return hist['auc-mean'][-1], hist['auc-stdv'][-1]# features = [f for f in data.columns if f not in ['SK_ID_CURR', 'TARGET']]# score_feature_selection(df=data[features], train_features=features, target=data['TARGET'])for threshold in [0, 10, 20, 30 , 40, 50 ,60 , 70, 80 , 90, 95, 99]: split_feats = [_f for _f, _score, _ in correlation_scores if _score &gt;= threshold] split_cat_feats = [_f for _f, _score, _ in correlation_scores if (_score &gt;= threshold) &amp; (_f in categorical_feats)] gain_feats = [_f for _f, _, _score in correlation_scores if _score &gt;= threshold] gain_cat_feats = [_f for _f, _, _score in correlation_scores if (_score &gt;= threshold) &amp; (_f in categorical_feats)] print('Results for threshold %3d' % threshold) split_results = score_feature_selection(df=data, train_features=split_feats, cat_feats=split_cat_feats, target=data['TARGET']) print('\t SPLIT : %.6f +/- %.6f' % (split_results[0], split_results[1])) gain_results = score_feature_selection(df=data, train_features=gain_feats, cat_feats=gain_cat_feats, target=data['TARGET']) print('\t GAIN : %.6f +/- %.6f' % (gain_results[0], gain_results[1]))]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类变量：Target Encoding]]></title>
    <url>%2F2019%2F04%2F08%2Ftargetencoding%2F</url>
    <content type="text"><![CDATA[​ 当你在进行监督学习时，你经常要处理分类变量。也就是说，变量没有一个自然的数值表示。问题是大多数机器学习算法都要求输入数据是数值的。在某个时候，数据科学管道将需要将分类变量转换为数值变量。 来源：https://maxhalford.github.io/blog/target-encoding-done-the-right-way/ 前言​ 当你在进行监督学习时，你经常要处理分类变量。也就是说，变量没有一个自然的数值表示。问题是大多数机器学习算法都要求输入数据是数值的。在某个时候，数据科学管道将需要将分类变量转换为数值变量。 有很多方法可以做到这一点: Label encoding 为每个类别选择数字 One-hot encoding:为每个类别创建一个二进制列 Vector representation ：也就是word2vec，在这里您可以找到一个适合您的数据的低维子空间 Optimal binning ：在依赖于LightGBM或CatBoost等树学习器 Target encoding: 按类别平均目标值 ​ 每种方法都有其优缺点，通常取决于您的数据和需求。如果一个变量有很多类别，那么一个[One-hot encoding:方案将产生许多列，这可能导致内存问题。根据我的经验，依赖LightGBM/CatBoost是最好的分箱方法。Label encoding是没有用的，最好不要使用它。然而，如果你的分类变量恰好是有序的，那么你可以而且应该用递增的数字来表示它(例如，“cold”变成0，“mild”变成1，“hot”变成2)。word2vec和其他类似的方法很酷也很好，但是它们需要进行一些微调，而且并不总是奏效。 target encoding​ target encoding是很容易理解的一种思想，假设你有分类变量X和变量Y，然后对于X中每一个distinct的元素计算其对应Y值的平局之，然后用这个平均值替换 $ x_i $ .下面这个例子显而易见： x0 x1 y aa cc 1 aa cc 1 aa cc 1 aa cc 1 aa cc 0 bb cc 1 bb cc 0 bb cc 0 bb cc 0 bb dd 0 x0 x1 y 0.8 cc 1 0.8 cc 1 0.8 cc 1 0.8 cc 1 0.8 cc 0 0.2 cc 1 0.2 cc 0 0.2 cc 0 0.2 cc 0 0.2 dd 0 ​ Target encoding的好处在于它提取了那些可以解释Y值，比如这里得aa拥有了一个Y值的平均值0.8，这会很好的帮助下游机器学习分类算法。 ​ 目标编码的问题有一个名称:过拟合。事实上，当平均值的数值很低时，依赖平均值并不总是一个好主意。您必须记住，您正在训练的数据集是一个更大的数据集的样本。这意味着，当您将训练集中发现的任何规律应用到另一个数据集(即测试集)时，可能都不成立。比如有可能训练集内它对应的Y值都是0，但是在测试集中它对应的Y值1比较多。 ​ 得出结论，永远不要用基础的target encoding,要用也是用以下的进阶encoding。 Target encoding进阶有很多方法可以处理这个问题。交叉验证和additive smoothing可以结合使用 交叉验证一种流行的方法是使用交叉验证并计算每个切分数据集中的平均值。这就是H20和许多kaggler所做的。 None: no holdout, mean是对训练集的所有数据行的计算应用于测试数据 leave eoneout: mean是对除了当前行本身之外的所有数据行进行计算。这可以用于训练数据。当前行本身的目标不包括在平均值中，以防止过度拟合 KFold:平均值只计算out-of-fold数据(需要K-fold) 这可以用于训练数据。为了防止过拟合，目标均值是根据叠外数据计算的 additive smoothing 使用additive smoothing，因为数据集中存在数据的count较小，所以它的target值容易受到过拟合的影响。它使用了全局的平均值来smooth较少数据带来过拟合的影响 数学上它等价于: u=\frac{n\times \hat x+m\times w}{n+m}where μ is the mean we’re trying to compute (the one that’s going to replace our categorical values) n is the number of values you have ¯x is your estimated mean m is the “weight” you want to assign to the overall mean w is the overall mean 其中m就是用来调节全局的权重，根据他的经验，他发现m取300的时候适用于大多数场合。 1234567891011121314def calc_smooth_mean(df, by, on, m): # Compute the global mean mean = df[on].mean() # Compute the number of values and the mean of each group agg = df.groupby(by)[on].agg(['count', 'mean']) counts = agg['count'] means = agg['mean'] # Compute the "smoothed" means smooth = (counts * means + m * mean) / (counts + m) # Replace each value by the according smoothed mean return df[by].map(smooth)]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>预处理</tag>
        <tag>分类变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比赛EDA]]></title>
    <url>%2F2019%2F04%2F08%2F%E6%AF%94%E8%B5%9BEDA%2F</url>
    <content type="text"><![CDATA[比赛常见的EDA总结 [TOC] 检查缺失值1234567891011# https://www.kaggle.com/gpreda/santander-eda-and-predictiondef missing_data(data): total = data.isnull().sum() percent = (data.isnull().sum()/data.isnull().count()*100) tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) types = [] for col in data.columns: dtype = str(data[col].dtype) types.append(dtype) tt['Types'] = types return(np.transpose(tt)) 观察分布值观察训练集的01分布(可以拓展到任意分布对比图)12345678910111213141516171819202122# https://www.kaggle.com/gpreda/santander-eda-and-predictiondef plot_feature_distribution(df1, df2, label1, label2, features): i = 0 sns.set_style('whitegrid') plt.figure() fig, ax = plt.subplots(10,10,figsize=(18,22)) for feature in features: i += 1 plt.subplot(10,10,i) sns.distplot(df1[feature], hist=False,label=label1) sns.distplot(df2[feature], hist=False,label=label2) plt.xlabel(feature, fontsize=9) locs, labels = plt.xticks() plt.tick_params(axis='x', which='major', labelsize=6, pad=-6) plt.tick_params(axis='y', which='major', labelsize=6) plt.show(); t0 = train_df.loc[train_df['target'] == 0]t1 = train_df.loc[train_df['target'] == 1]features = train_df.columns.values[2:102]plot_feature_distribution(t0, t1, '0', '1', features) 结论：We can observe that there is a considerable number of features with significant different distribution for the two target values.For example, var_0, var_1, var_2, var_5, var_9, var_13, var_106, var_109, var_139 and many others. Also some features, like var_2, var_13, var_26, var_55, var_175, var_184, var_196 shows a distribution that resambles to a bivariate distribution. train_test的分布同理可以选择train和test的行和列的mean/std/min值 1234567891011121314151617181920212223242526272829https://www.kaggle.com/gpreda/santander-eda-and-predictionplt.figure(figsize=(16,6))plt.title("Distribution of mean values per column in the train and test set")sns.distplot(train_df[features].mean(axis=0),color="magenta",kde=True,bins=120, label='train')sns.distplot(test_df[features].mean(axis=0),color="darkblue", kde=True,bins=120, label='test')plt.legend()plt.show()plt.figure(figsize=(16,6))plt.title("Distribution of std values per row in the train and test set")sns.distplot(train_df[features].std(axis=1),color="black", kde=True,bins=120, label='train')sns.distplot(test_df[features].std(axis=1),color="red", kde=True,bins=120, label='test')plt.legend();plt.show()plt.figure(figsize=(16,6))plt.title("Distribution of std values per column in the train and test set")sns.distplot(train_df[features].std(axis=0),color="blue",kde=True,bins=120, label='train')sns.distplot(test_df[features].std(axis=0),color="green", kde=True,bins=120, label='test')plt.legend(); plt.show()t0 = train_df.loc[train_df['target'] == 0]t1 = train_df.loc[train_df['target'] == 1]plt.figure(figsize=(16,6))plt.title("Distribution of mean values per row in the train set")sns.distplot(t0[features].mean(axis=1),color="red", kde=True,bins=120, label='target = 0')sns.distplot(t1[features].mean(axis=1),color="blue", kde=True,bins=120, label='target = 1')plt.legend(); plt.show()plt.figure(figsize=(16,6))plt.title("Distribution of mean values per column in the train set")sns.distplot(t0[features].mean(axis=0),color="green", kde=True,bins=120, label='target = 0')sns.distplot(t1[features].mean(axis=0),color="darkblue", kde=True,bins=120, label='target = 1')plt.legend(); plt.show() 检查duplicate values123456789%%timefeatures = train_df.columns.values[2:202]unique_max_train = []unique_max_test = []for feature in features: values = train_df[feature].value_counts() unique_max_train.append([feature, values.max(), values.idxmax()]) values = test_df[feature].value_counts() unique_max_test.append([feature, values.max(), values.idxmax()]) PCAhttps://zhuanlan.zhihu.com/p/28909807 主成分分析是统计方法里的一种降维方法，它的主要思想是将原有 个特征通过正交变换将一组可能存在相关性的特征缩减到 特征( )。举例来说，在网站用户行为数据收集过程中，会话数(Visits)，浏览页数(PV)，网站总停留时间(Time Spend Total)，访问人数(Unique Visitor)，以上这几个指标，无论是从以往的数据统计还是业务经验来看，都存在一定正相关关系，如果将这些特征喂给模型，很容易造成过拟合。 通过转化，从而剔除噪声，而且可以看到转化后的值然后把一些无关的列再剔除 PCA 使用要点 使用主成分分析，往往会丢失掉“少部分信息”（注意：这“少部分信息”仅仅指方差较小的数据，并非信息含量真的少的数据！） 因为1的特性，所以在机器学些中，不推荐使用 PCA 去优化特征达到避免过拟合的目的。 既然PCA不能避免过拟合，那为何还要使用，根据周志华老师的西瓜书中的描述： 在舍弃特征值较小的特征之后，能够使样本采集密度增 当数据受到噪声影响时，最小的特征值对对应的特征向量往往与噪声有关将他们舍弃能够在一定程度上起到去噪效果 在维度过多的情况下，变换后的坐标系代表的意义不明，不易于解释。 12345678910111213141516171819202122# https://www.kaggle.com/roydatascience/eda-pca-lgbm-santander-transactions/notebookfrom sklearn.decomposition import KernelPCAlin_pca = KernelPCA(n_components = 2, kernel="linear", fit_inverse_transform=True)rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.0433, fit_inverse_transform=True)sig_pca = KernelPCA(n_components = 2, kernel="sigmoid", gamma=0.001, coef0=1, fit_inverse_transform=True)plt.figure(figsize=(11, 4))for subplot, pca, title in ((131, lin_pca, "Linear kernel"), (132, rbf_pca, "RBF kernel, $\gamma=0.04$"), (133, sig_pca, "Sigmoid kernel, $\gamma=10^&#123;-3&#125;, r=1$")): PCA_train_x = PCA(2).fit_transform(train_scaled) plt.subplot(subplot) plt.title(title, fontsize=14) plt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=target, cmap="nipy_spectral_r") plt.xlabel("$z_1$", fontsize=18) if subplot == 131: plt.ylabel("$z_2$", fontsize=18, rotation=0) plt.grid(True)plt.show()]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy]]></title>
    <url>%2F2019%2F04%2F08%2Fnumpy%2F</url>
    <content type="text"><![CDATA[numpy 操作小技巧 增加concat/vstack/hstack 1https://blog.csdn.net/xiaodongxiexie/article/details/71774466]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python代码]]></title>
    <url>%2F2019%2F04%2F08%2Fpython%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[python 代码小技巧 [TOC] 通用测试时间12关注walltime%%time 遍历列表12for index, item in enumerate(list1): print index, item 并行遍历12for (x,y) in zip(a,b): print x,y 通过字典保存了所有中间过程，不仅命名容易，而且便于后续操作12d=&#123;d_name:get_dvalue(d_name) for d_name in d_list&#125;best_dvalue_dname=min(d.items(),key=lambda x:x[1]) 文件1234#这样会比单纯用readlines()快count = 0for index, line in enumerate(open(filepath,'r'))： count += 1 数据结构字典12from collections import defaultdict 通过这个声明dict&#123;里面的格式&#125;如 defaultdict(list) 列表生成式121. [i for i in range(k) if condition]：此时if起条件判断作用，满足条件的，将被返回成为最终生成的列表的一员。2. [i if condition else exp for exp]：此时if...else被用来赋值，满足条件的i以及else被用来生成最终的列表 自带算法库数组中查找插入的位置123返回排序插入的位置，并不是真的插入排序数组中from bisect import bisect_left, bisect_rightend = bisect_left(keys, 0)]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比赛通用代码]]></title>
    <url>%2F2019%2F04%2F08%2F%E6%AF%94%E8%B5%9B%E9%80%9A%E7%94%A8%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[比赛通用代码 [TOC] jupyter 载入代码包123456789101112131415161718192021222324252627# Load librariesimport numpy as npimport pandas as pdimport randomimport gcfrom pandas import read_csvimport plotly.offline as pyimport plotly.graph_objs as goimport seaborn as sns%matplotlib inlineimport matplotlib.pyplot as pltimport warningswarnings.filterwarnings('ignore')pd.options.display.max_columns = 100gc.enable()'''Displays markdown formatted output like bold, italic bold etc.'''from IPython.display import Markdowndef bold(string): display(Markdown(string))'''Ignores deprecation warning.'''def ignore_warnings(): import warnings warnings.filterwarnings('ignore', category = DeprecationWarning) bold('**Merged data:**')display(merged.head()) 自动压缩变量空间12345678910111213141516171819202122232425262728293031323334353637383940def reduce_mem_usage(df): """ iterate through all the columns of a dataframe and modify the data type to reduce memory usage. """ #start_mem = df.memory_usage().sum() / 1024**2 #print('Memory usage of dataframe is &#123;:.2f&#125; MB'.format(start_mem)) for col in df.columns: col_type = df[col].dtype if col_type != object: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == 'int': if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) #end_mem = df.memory_usage().sum() / 1024**2 #print('Memory usage after optimization is: &#123;:.2f&#125; MB'.format(end_mem)) #print('Decreased by &#123;:.1f&#125;%'.format(100 * (start_mem - end_mem) / start_mem)) return dfdef reload(): gc.collect() df = pd.read_csv('../input/train_V2.csv') invalid_match_ids = df[df['winPlacePerc'].isna()]['matchId'].values df = df[-df['matchId'].isin(invalid_match_ids)] df=reduce_mem_usage(df) return df]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas]]></title>
    <url>%2F2019%2F04%2F08%2Fpandas%2F</url>
    <content type="text"><![CDATA[pandas 操作小技巧 通用操作pandas操作出现进度条： 用作迭代器 用于Pandas的操作12345import pandas as pdfrom tqdm import tqdmtqdm.pandas()sentences = train["question_text"].progress_apply(lambda x: x.split()).valuesfor sentence in tqdm(sentences, disable = (not verbose)): 增加merge/concat/joinhttps://www.e-learn.cn/content/qita/814185 改变直接replace12'''Put Dona, Jonkheer, Countess, Sir, Lady, Don in bucket Aristocrat.'''merged.Title.replace(to_replace = ['Dona', 'Jonkheer', 'Countess', 'Sir', 'Lady', 'Don'], value = 'Aristocrat', inplace = True) map映射1234#map映射 它就会自动把类别型转为连续型#如果要Inverse就直接key-value逆转size_mapping=&#123;'XL':3,'L':2,'M':1&#125;df['size']=df['size'].map(size_mapping) 单列分箱12345678910'''Create bin categories for Age.'''label_names = ['infant','child','teenager','young_adult','adult','aged']'''Create range for each bin categories of Age.'''cut_points = [0,5,12,18,35,60,81]'''Create and view categorized Age with original Age.'''merged['Age_binned'] = pd.cut(merged.Age, cut_points, labels = label_names)display(merged[['Age', 'Age_binned']].head(2)​ 12345678## 删除### 去重```pythontrain.drop_duplicates(subset = ['1_total_fee','2_total_fee','3_total_fee', 'month_traffic','pay_times','last_month_traffic','service2_caller_time','age'],inplace=True)]]></content>
      <categories>
        <category>数据竞赛</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
</search>
